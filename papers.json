{
  "updated": "2025-11-29",
  "total": 406,
  "keywords": [
    "model stealing attack",
    "model extraction attack",
    "neural network extraction attack",
    "stealing machine learning model",
    "model stealing defense",
    "model extraction",
    "LoRA extraction attack",
    "LLM model stealing",
    "knockoff nets",
    "stealing functionality black-box",
    "DNN model stealing",
    "DNN weights leakage",
    "neural network weight extraction",
    "side-channel model extraction",
    "recover parameters neural network",
    "electromagnetic analysis neural network",
    "extract parameters neural network",
    "side-channel attack deep learning",
    "power analysis neural network weights",
    "GPU leak DNN weights",
    "stealing language model",
    "steal production model"
  ],
  "papers": [
    {
      "paper_id": "7044d075fba8c188f716a25618d522c808a67a96",
      "title": "A Survey of Model Extraction Attacks and Defenses in Distributed Computing Environments",
      "abstract": "Model Extraction Attacks (MEAs) threaten modern machine learning systems by enabling adversaries to steal models, exposing intellectual property and training data. With the increasing deployment of machine learning models in distributed computing environments, including cloud, edge, and federated learning settings, each paradigm introduces distinct vulnerabilities and challenges. Without a unified perspective on MEAs across these distributed environments, organizations risk fragmented defenses, inadequate risk assessments, and substantial economic and privacy losses. This survey is motivated by the urgent need to understand how the unique characteristics of cloud, edge, and federated deployments shape attack vectors and defense requirements. We systematically examine the evolution of attack methodologies and defense mechanisms across these environments, demonstrating how environmental factors influence security strategies in critical sectors such as autonomous vehicles, healthcare, and financial services. By synthesizing recent advances in MEAs research and discussing the limitations of current evaluation practices, this survey provides essential insights for developing robust and adaptive defense strategies. Our comprehensive approach highlights the importance of integrating protective measures across the entire distributed computing landscape to ensure the secure deployment of machine learning models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/7044d075fba8c188f716a25618d522c808a67a96",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f94fe2776e4258650baffb9b0100518076aacdad",
      "title": "Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment",
      "abstract": "Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on image classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-Steal), the first stealing attack against medical MLLMs. ADA-Steal relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yaling Shen",
        "Zhixiong Zhuang",
        "Kun Yuan",
        "Maria-Irina Nicolae",
        "N. Navab",
        "N. Padoy",
        "Mario Fritz"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/f94fe2776e4258650baffb9b0100518076aacdad",
      "pdf_url": "",
      "publication_date": "2025-02-04",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "253a59d979d560456c2984742466b796a983da0e",
      "title": "ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have gained traction in Graph-based Machine Learning as a Service (GMLaaS) platforms, yet they remain vulnerable to graph-based model extraction attacks (MEAs), where adversaries reconstruct surrogate models by querying the victim model. Existing defense mechanisms, such as watermarking and fingerprinting, suffer from poor real-time performance, susceptibility to evasion, or reliance on post-attack verification, making them inadequate for handling the dynamic characteristics of graph-based MEA variants. To address these limitations, we propose ATOM, a novel real-time MEA detection framework tailored for GNNs. ATOM integrates sequential modeling and reinforcement learning to dynamically detect evolving attack patterns, while leveraging k-core embedding to capture the structural properties, enhancing detection precision. Furthermore, we provide theoretical analysis to characterize query behaviors and optimize detection strategies. Extensive experiments on multiple real-world datasets demonstrate that ATOM outperforms existing approaches in detection performance, maintaining stable across different time steps, thereby offering a more effective defense mechanism for GMLaaS environments. Our source code is available at https://github.com/LabRAI/ATOM.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Zhan Cheng",
        "Bolin Shen",
        "Tianming Sha",
        "Yuan Gao",
        "Shibo Li",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/253a59d979d560456c2984742466b796a983da0e",
      "pdf_url": "",
      "publication_date": "2025-03-20",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3df3a63e65eb6ab71049334466369f33dab37236",
      "title": "Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach",
      "abstract": "Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Yurong Wu",
        "Fangwen Mu",
        "Qiuhong Zhang",
        "Jinjing Zhao",
        "Xinrun Xu",
        "Lingrui Mei",
        "Yang Wu",
        "Lin Shi",
        "Junjie Wang",
        "Zhiming Ding",
        "Yiwei Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3df3a63e65eb6ab71049334466369f33dab37236",
      "pdf_url": "",
      "publication_date": "2025-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "title": "Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy",
      "abstract": "Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: \\textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9\\% and MI leakage increased by 17.4\\% on average. We release the code of PhiMM through a link.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zhenyuan Guo",
        "Yi Shi",
        "Wenlong Meng",
        "Chen Gong",
        "Chengkun Wei",
        "Wenzhi Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "pdf_url": "",
      "publication_date": "2025-02-17",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4c167947e7fe8fd78118504627930f1935b11924",
      "title": "MER-Inspector: Assessing Model Extraction Risks from An Attack-Agnostic Perspective",
      "abstract": "Information leakage issues in machine learning-based Web applications have attracted increasing attention. While the risk of data privacy leakage has been rigorously analyzed, the theory of model function leakage, known as Model Extraction Attacks (MEAs), has not been well studied. In this paper, we are the first to understand MEAs theoretically from an attack-agnostic perspective and to propose analytical metrics for evaluating model extraction risks. By using the Neural Tangent Kernel (NTK) theory, we formulate the linearized MEA as a regularized kernel classification problem and then derive the fidelity gap and generalization error bounds of the attack performance. Based on these theoretical analyses, we propose a new theoretical metric called Model Recovery Complexity (MRC), which measures the distance of weight changes between the victim and surrogate models to quantify risk. Additionally, we find that victim model accuracy, which shows a strong positive correlation with model extraction risk, can serve as an empirical metric. By integrating these two metrics, we propose a framework, namely Model Extraction Risk Inspector (MER-Inspector), to compare the extraction risks of models under different model architectures by utilizing relative metric values. We conduct extensive experiments on 16 model architectures and 5 datasets. The experimental results demonstrate that the proposed metrics have a high correlation with model extraction risks, and MER-Inspector can accurately compare the extraction risks of any two models with up to 89.58%.",
      "year": 2025,
      "venue": "The Web Conference",
      "authors": [
        "Xinwei Zhang",
        "Haibo Hu",
        "Qingqing Ye",
        "Li Bai",
        "Huadi Zheng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4c167947e7fe8fd78118504627930f1935b11924",
      "pdf_url": "",
      "publication_date": "2025-04-22",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a1dbeef30b37363111eb815ff7fd2a0b8e7da83c",
      "title": "Adversarial Autoencoder based Model Extraction Attacks for Collaborative DNN Inference at Edge",
      "abstract": "Deep neural networks (DNNs) are influencing a wide range of applications from safety-critical to security-sensitive use cases. In many such use cases, the DNN inference process relies on distributed systems involving IoT devices and edge/cloud severs as participants where a pre-trained DNN model is partitioned/split onto multiple parts and the participants collaboratively execute them. However, often such collaboration requires dynamic DNN partitioning information to be exchanged among the participants over unsecured network or via relays/hops which can lead to novel privacy vulnerabilities. In this paper, we propose a DNN model extraction attack that exploits such vulnerabilities to not only extract the original input data, but also reconstruct the entire victim DNN model. Specifically, the proposed attack model utilizes extracted/leaked data and adversarial autoencoders to generate and train a shadow model that closely mimics the behavior of the original victim model. The proposed attack is query-free and does not require the attacker to have any prior information about the victim model and input data. Using an IoT-edge hardware testbed running collaborative DNN inference, we demonstrate the effectiveness of the proposed attack model in extracting the victim model with high levels of certainty across many realistic scenarios.",
      "year": 2025,
      "venue": "IEEE/IFIP Network Operations and Management Symposium",
      "authors": [
        "Manal Zneit",
        "Xiaojie Zhang",
        "Motahare Mounesan",
        "S. Debroy"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a1dbeef30b37363111eb815ff7fd2a0b8e7da83c",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0d3384cb78be25ed28a3544f175cab59236093dd",
      "title": "HoneypotNet: Backdoor Attacks Against Model Extraction",
      "abstract": "Model extraction attacks are one type of inference-time attacks that approximate the functionality and performance of a black-box victim model by launching a certain number of queries to the model and then leveraging the model's predictions to train a substitute model. These attacks pose severe security threats to production models and MLaaS platforms and could cause significant monetary losses to the model owners. A body of work has proposed to defend machine learning models against model extraction attacks, including both active defense methods that modify the model's outputs or increase the query overhead to avoid extraction and passive defense methods that detect malicious queries or leverage watermarks to perform post-verification. In this work, we introduce a new defense paradigm called attack as defense which modifies the model's output to be poisonous such that any malicious users that attempt to use the output to train a substitute model will be poisoned. To this end, we propose a novel lightweight backdoor attack method dubbed HoneypotNet that replaces the classification layer of the victim model with a honeypot layer and then fine-tunes the honeypot layer with a shadow model (to simulate model extraction) via bi-level optimization to modify its output to be poisonous while remaining the original performance. We empirically demonstrate on four commonly used benchmark datasets that HoneypotNet can inject backdoors into substitute models with a high success rate. The injected backdoor not only facilitates ownership verification but also disrupts the functionality of substitute models, serving as a significant deterrent to model extraction attacks.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yixu Wang",
        "Tianle Gu",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0d3384cb78be25ed28a3544f175cab59236093dd",
      "pdf_url": "",
      "publication_date": "2025-01-02",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "26be7a4a13776ac194912a70e97783bf2e587c24",
      "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models",
      "abstract": "Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt-targeted attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments.",
      "year": 2025,
      "venue": "Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/26be7a4a13776ac194912a70e97783bf2e587c24",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "title": "Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker can combine the data knowledge of multiple attackers to create a more effective attack model, which can be referred to cross-dataset attacks. Moreover, if knowledge can be extracted with the help of Large Language Models (LLMs), the attack capability will be more significant. In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and LLMs. The LLM is applied to process datasets with different data structures in cross-dataset attacks. Each attacker fine-tunes the LLM on their specific dataset to generate a tailored attack model. We then introduce a novel model merging method to integrate the parameters of these attacker-specific models effectively. The result is a merged attack model with superior generalization capabilities, enabling effective attacks not only on the attackers\u2019 datasets but also on previously unseen (out-of-domain) datasets. We conducted extensive experiments in four datasets to demonstrate the effectiveness of our method. Additional experiments with three different GNN and LLM architectures further illustrate the generality of our approach. In summary, we present a new link stealing attack method that facilitates collaboration among multiple attackers to develop a powerful, universal attack model that reflects realistic real-world scenarios.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Wenhan Chang",
        "Wei Ren",
        "Wanlei Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0c72a0508a3f3944660e786e83a7f4d56c50ecf1",
      "title": "Towards Effective Prompt Stealing Attack against Text-to-Image Diffusion Models",
      "abstract": "Text-to-Image (T2I) models, represented by DALL$\\cdot$E and Midjourney, have gained huge popularity for creating realistic images. The quality of these images relies on the carefully engineered prompts, which have become valuable intellectual property. While skilled prompters showcase their AI-generated art on markets to attract buyers, this business incidentally exposes them to \\textit{prompt stealing attacks}. Existing state-of-the-art attack techniques reconstruct the prompts from a fixed set of modifiers (i.e., style descriptions) with model-specific training, which exhibit restricted adaptability and effectiveness to diverse showcases (i.e., target images) and diffusion models. To alleviate these limitations, we propose Prometheus, a training-free, proxy-in-the-loop, search-based prompt-stealing attack, which reverse-engineers the valuable prompts of the showcases by interacting with a local proxy model. It consists of three innovative designs. First, we introduce dynamic modifiers, as a supplement to static modifiers used in prior works. These dynamic modifiers provide more details specific to the showcases, and we exploit NLP analysis to generate them on the fly. Second, we design a contextual matching algorithm to sort both dynamic and static modifiers. This offline process helps reduce the search space of the subsequent step. Third, we interact with a local proxy model to invert the prompts with a greedy search algorithm. Based on the feedback guidance, we refine the prompt to achieve higher fidelity. The evaluation results show that Prometheus successfully extracts prompts from popular platforms like PromptBase and AIFrog against diverse victim models, including Midjourney, Leonardo.ai, and DALL$\\cdot$E, with an ASR improvement of 25.0\\%. We also validate that Prometheus is resistant to extensive potential defenses, further highlighting its severity in practice.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shiqian Zhao",
        "Chong Wang",
        "Yiming Li",
        "Yihao Huang",
        "Wenjie Qu",
        "Siew-Kei Lam",
        "Yi Xie",
        "Kangjie Chen",
        "Jie Zhang",
        "Tianwei Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/0c72a0508a3f3944660e786e83a7f4d56c50ecf1",
      "pdf_url": "",
      "publication_date": "2025-08-09",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "369d7792462ab184ed6dc53cab70b9b101d9d034",
      "title": "Sim4Rec: Data-Free Model Extraction Attack on Sequential Recommendation",
      "abstract": "Model extraction attack shows promising performance in revealing sequential recommendation (SeqRec) robustness, e.g., as an upstream task of transfer-based attack to provide optimization feedback for downstream attacks. However, existing work either heavily relies on impractical prior knowledge or has impressive attack performance. In this paper, we focus on data-free model extraction attack on SeqRec, which aims to efficiently train a surrogate model that closely imitates the target model in a practical setting. Conducting such an attack is challenging. First, imitating sequential training data for accurate model extraction is hard without prior knowledge. Second, limited queries for the target model require the attack to be efficient. To address these challenges, we propose a novel adversarial framework Sim4Rec which includes two modules, i.e., controllable sequence generation and reinforced adversarial distillation. The former allows a sequential generator to produce synthetic data similar to training data through pre-training with controllable generated samples. The latter efficiently extracts the target model via reinforced adversarial knowledge distillation. Extensive experiments demonstrate the advancement of Sim4Rec.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yihao Wang",
        "Jiajie Su",
        "Chaochao Chen",
        "Meng Han",
        "Chi Zhang",
        "Jun Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/369d7792462ab184ed6dc53cab70b9b101d9d034",
      "pdf_url": "",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1b5caff453174871e8c9b374b72659a7c63fa830",
      "title": "CopyQNN: Quantum Neural Network Extraction Attack under Varying Quantum Noise",
      "abstract": "Quantum Neural Networks (QNNs) have shown significant value across domains, with well-trained QNNs representing critical intellectual property often deployed via cloud-based QNN-as-a-Service (QNNaaS) platforms. Recent work has examined QNN model extraction attacks using classical and emerging quantum strategies. These attacks involve adversaries querying QNNaaS platforms to obtain labeled data for training local substitute QNNs that replicate the functionality of cloud-based models. However, existing approaches have largely over-looked the impact of varying quantum noise inherent in noisy intermediate-scale quantum (NISQ) computers, limiting their effectiveness in real-world settings. To address this limitation, we propose the CopyQNN framework, which employs a three-step data cleaning method to eliminate noisy data based on its noise sensitivity. This is followed by the integration of contrastive and transfer learning within the quantum domain, enabling efficient training of substitute QNNs using a limited but cleaned set of queried data. Experimental results on NISQ computers demonstrate that a practical implementation of CopyQNN significantly outperforms state-of-the-art QNN extraction attacks, achieving an average performance improvement of 8.73% across all tasks while reducing the number of required queries by 90\u00d7, with only a modest increase in hardware overhead.",
      "year": 2025,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhenxiao Fu",
        "Leyi Zhao",
        "Xuhong Zhang",
        "Yilun Xu",
        "Gang Huang",
        "Fan Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1b5caff453174871e8c9b374b72659a7c63fa830",
      "pdf_url": "",
      "publication_date": "2025-04-01",
      "keywords_matched": [
        "model extraction attack",
        "neural network extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e569cd7a6612a86e89c2e09c36f75fdcce6dd453",
      "title": "Delving into Cryptanalytic Extraction of PReLU Neural Networks",
      "abstract": "The machine learning problem of model extraction was first introduced in 1991 and gained prominence as a cryptanalytic challenge starting with Crypto 2020. For over three decades, research in this field has primarily focused on ReLU-based neural networks. In this work, we take the first step towards the cryptanalytic extraction of PReLU neural networks, which employ more complex nonlinear activation functions than their ReLU counterparts. We propose a raw output-based parameter recovery attack for PReLU networks and extend it to more restrictive scenarios where only the top-m probability scores are accessible. Our attacks are rigorously evaluated through end-to-end experiments on diverse PReLU neural networks, including models trained on the MNIST dataset. To the best of our knowledge, this is the first practical demonstration of PReLU neural network extraction across three distinct attack scenarios.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yi Chen",
        "Xiaoyang Dong",
        "Ruijie Ma",
        "Yan Shen",
        "Anyu Wang",
        "Hongbo Yu",
        "Xiaoyun Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/e569cd7a6612a86e89c2e09c36f75fdcce6dd453",
      "pdf_url": "",
      "publication_date": "2025-09-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "192907ef8a1835d42ff7ae7af95048bea4c9f2f0",
      "title": "Implementation of a cell neural network under electromagnetic radiation with complex dynamics",
      "abstract": null,
      "year": 2025,
      "venue": "Nonlinear dynamics",
      "authors": [
        "Tao Ma",
        "Jun Mou",
        "Wanzhong Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/192907ef8a1835d42ff7ae7af95048bea4c9f2f0",
      "pdf_url": "",
      "publication_date": "2025-04-05",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "af515bad09321d88acc64d11a05e3f35fa8f328e",
      "title": "Dimensionless Physics\u2010Informed Neural Network for Electromagnetic Field Modelling of Permanent Magnet Eddy Current Coupler",
      "abstract": "To design the permanent magnetic eddy current couplers (PMECCs), modelling the magnetic field is essential. Traditional equivalent magnetic circuit methods and analytical methods often rely heavily on expert experience, whereas finite element methods (FEM) demand significant computational resources and time. Recently, the physics\u2010informed neural network (PINN) has emerged as a novel approach for modelling electromagnetic fields. To fully harness the potential of PINN, eliminate reliance on data sets, and enhance the generalisation ability of multi\u2010scale physical systems, we simplify the physical model of PMECCs and analyse its inherent boundary conditions based on the fundamental properties of electromagnetic fields. A dimensionless and unsupervised PINN, employing dimensional analysis to reduce the dimensions of the physical variables in the system was proposed. The dimensionless PINN (DPINN) is trained through unsupervised learning to solve the magnetic field equations and predict PMECC performance. Furthermore, dimensional analysis and transfer learning method are applied to enable the network to address a broader class of problems, resulting in a 92% reduction in training cost. The solution results, compared with those from the finite element method and analytical solution, exhibit similar error magnitudes (10\u22124\u00a0Wb/m), confirming the method's high accuracy.",
      "year": 2025,
      "venue": "IET electric power applications",
      "authors": [
        "Jiaxing Wang",
        "Dazhi Wang",
        "Sihan Wang",
        "Wenhui Li",
        "Yanqi Jiang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/af515bad09321d88acc64d11a05e3f35fa8f328e",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e9534b0f74ff371aa086ecc30d95a633ecad7ddc",
      "title": "Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses",
      "abstract": "The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ganghua Wang",
        "Yuhong Yang",
        "Jie Ding"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e9534b0f74ff371aa086ecc30d95a633ecad7ddc",
      "pdf_url": "",
      "publication_date": "2025-02-21",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
      "title": "Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging",
      "abstract": "Model merging has emerged as a promising approach for updating large language models (LLMs) by integrating multiple domain-specific models into a cross-domain merged model. Despite its utility and plug-and-play nature, unmonitored mergers can introduce significant security vulnerabilities, such as backdoor attacks and model merging abuse. In this paper, we identify a novel and more realistic attack surface where a malicious merger can extract targeted personally identifiable information (PII) from an aligned model with model merging. Specifically, we propose \\texttt{Merger-as-a-Stealer}, a two-stage framework to achieve this attack: First, the attacker fine-tunes a malicious model to force it to respond to any PII-related queries. The attacker then uploads this malicious model to the model merging conductor and obtains the merged model. Second, the attacker inputs direct PII-related queries to the merged model to extract targeted PII. Extensive experiments demonstrate that \\texttt{Merger-as-a-Stealer} successfully executes attacks against various LLMs and model merging methods across diverse settings, highlighting the effectiveness of the proposed framework. Given that this attack enables character-level extraction for targeted PII without requiring any additional knowledge from the attacker, we stress the necessity for improved model alignment and more robust defense mechanisms to mitigate such threats.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Lin Lu",
        "Zhigang Zuo",
        "Ziji Sheng",
        "Pan Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "90360529f01cf996d62369fd0c47af3b1823c7f4",
      "title": "Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning",
      "abstract": "Federated Learning (FL) is a collaborative learning framework designed to protect client data, yet it remains highly vulnerable to Intellectual Property (IP) threats. Model extraction (ME) attack poses a significant risk to Machine-Learning-as-a-Service (MLaaS) platforms, enabling attackers to replicate confidential models by querying Black-Box (without internal insight) APIs. Despite FL\u2019s privacy-preserving goals, its distributed nature makes it particularly susceptible to such attacks. This paper examines the vulnerability of the FL-based victim model to two types of model extraction attacks. For various federated clients built under NVFlare platform, we implemented ME attack across two deep-learning architectures and three image datasets. We evaluate the proposed ME attack performance using various metrics, including accuracy, fidelity, and KL divergence. The experiments show that for various FL clients, the accuracy and fidelity of the extraction model are closely related to the size of the attack query set. Additionally, we explore a transfer learning-based approach where pre-trained models serve as the starting point for the extraction process. The results indicate that the accuracy and fidelity of the fine-tuned pre-trained extraction models are notably higher, particularly with smaller query sets, highlighting potential advantages for attackers.",
      "year": 2025,
      "venue": "International Conference on Wireless Communications and Mobile Computing",
      "authors": [
        "Sayyed Farid Ahamed",
        "Sandip Roy",
        "Soumya Banerjee",
        "Marc Vucovich",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "E. Bowen",
        "Sachin Shetty"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/90360529f01cf996d62369fd0c47af3b1823c7f4",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "268ce1e9492455e825a3894b0f2713d14e376d36",
      "title": "ADAGE: Active Defenses Against GNN Extraction",
      "abstract": "Graph Neural Networks (GNNs) achieve high performance in various real-world applications, such as drug discovery, traffic states prediction, and recommendation systems. The fact that building powerful GNNs requires a large amount of training data, powerful computing resources, and human expertise turns the models into lucrative targets for model stealing attacks. Prior work has revealed that the threat vector of stealing attacks against GNNs is large and diverse, as an attacker can leverage various heterogeneous signals ranging from node labels to high-dimensional node embeddings to create a local copy of the target GNN at a fraction of the original training costs. This diversity in the threat vector renders the design of effective and general defenses challenging and existing defenses usually focus on one particular stealing setup. Additionally, they solely provide means to identify stolen model copies rather than preventing the attack. To close this gap, we propose the first and general Active Defense Against GNN Extraction (ADAGE). ADAGE builds on the observation that stealing a model's full functionality requires highly diverse queries to leak its behavior across the input space. Our defense monitors this query diversity and progressively perturbs outputs as the accumulated leakage grows. In contrast to prior work, ADAGE can prevent stealing across all common attack setups. Our extensive experimental evaluation using six benchmark datasets, four GNN models, and three types of adaptive attackers shows that ADAGE penalizes attackers to the degree of rendering stealing impossible, whilst preserving predictive performance on downstream tasks. ADAGE, thereby, contributes towards securely sharing valuable GNNs in the future.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jing Xu",
        "Franziska Boenisch",
        "Adam Dziedzic"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/268ce1e9492455e825a3894b0f2713d14e376d36",
      "pdf_url": "",
      "publication_date": "2025-02-27",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dd898d4226db4c4dca01e46de96fe3acbc6087ec",
      "title": "A Method for Extracting Black Box Models Based on Interpretable Attention",
      "abstract": "Deep neural networks have achieved remarkable success in face recognition. However, their vulnerability has attracted considerable attention. Researchers can analyse the weaknesses of face recognition models by extracting their functionality, aiming to enhance the security performance of these models. The findings of the study reveal that current model extraction methods are afflicted with notable drawbacks, namely low similarity in capturing model functionality and insufficient availability of samples. These limitations significantly impede the analysis of model security performance. We propose an interpretable attention\u2010based method for black\u2010box model extraction, enhancing the similarity between substitute and victim model functionality. Our main contributions are summarized as follows: (i) This study addresses the issue of limited sample training caused by the restricted number of black\u2010box hard label queries. (ii) By applying input perturbations, we obtain feedback from deep black\u2010box models, enabling us to identify facial local regions and the distribution of feature weights that positively influence predictions. (iii) By normalizing the feature weight distribution matrix and associating it with the attention weight matrix, the construction of an attention mask for the dataset is achieved, enabling differential attention to features in different regions. (iv) Leveraging a pre\u2010trained base model, we extract relevant knowledge and features, facilitating cross\u2010domain knowledge transfer. Experiments on Emore, PubFig and CASIA\u2010WebFace show that our method outperforms traditional methods by 10%\u201320% in model consistency for the same query budget. Also, our method achieves the highest model stealing consistency on the three datasets: 94.51%, 93.27% and 91.74%, respectively.",
      "year": 2025,
      "venue": "Expert Syst. J. Knowl. Eng.",
      "authors": [
        "Lijun Gao",
        "Huibin Tian",
        "Kai Liu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/dd898d4226db4c4dca01e46de96fe3acbc6087ec",
      "pdf_url": "",
      "publication_date": "2025-06-03",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "71ff23a33bc4db47a76808717acc78d9b04a7e1e",
      "title": "Entangled Threats: A Unified Kill Chain Model for Quantum Machine Learning Security",
      "abstract": "Quantum Machine Learning (QML) systems inherit vulnerabilities from classical machine learning while introducing new attack surfaces rooted in the physical and algorithmic layers of quantum computing. Despite a growing body of research on individual attack vectors - ranging from adversarial poisoning and evasion to circuit-level backdoors, side-channel leakage, and model extraction - these threats are often analyzed in isolation, with unrealistic assumptions about attacker capabilities and system environments. This fragmentation hampers the development of effective, holistic defense strategies. In this work, we argue that QML security requires more structured modeling of the attack surface, capturing not only individual techniques but also their relationships, prerequisites, and potential impact across the QML pipeline. We propose adapting kill chain models, widely used in classical IT and cybersecurity, to the quantum machine learning context. Such models allow for structured reasoning about attacker objectives, capabilities, and possible multi-stage attack paths - spanning reconnaissance, initial access, manipulation, persistence, and exfiltration. Based on extensive literature analysis, we present a detailed taxonomy of QML attack vectors mapped to corresponding stages in a quantum-aware kill chain framework that is inspired by the MITRE ATLAS for classical machine learning. We highlight interdependencies between physical-level threats (like side-channel leakage and crosstalk faults), data and algorithm manipulation (such as poisoning or circuit backdoors), and privacy attacks (including model extraction and training data inference). This work provides a foundation for more realistic threat modeling and proactive security-in-depth design in the emerging field of quantum machine learning.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Pascal Debus",
        "Maximilian Wendlinger",
        "Kilian Tscharke",
        "Daniel Herr",
        "Cedric Br\u00fcgmann",
        "Daniel de Mello",
        "J. Ulmanis",
        "Alexander Erhard",
        "Arthur Schmidt",
        "Fabian Petsch"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/71ff23a33bc4db47a76808717acc78d9b04a7e1e",
      "pdf_url": "",
      "publication_date": "2025-07-11",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ffbf082c9d726c1fb4e03a9e6045bec193ab3050",
      "title": "Design and Analysis of Memristive Electromagnetic Radiation in a Hopfield Neural Network",
      "abstract": "This study introduces a memristive Hopfield neural network (M-HNN) model to investigate electromagnetic radiation impacts on neural dynamics in complex electromagnetic environments. The proposed framework integrates a magnetic flux-controlled memristor into a three-neuron Hopfield architecture, revealing significant alterations in network dynamics through comprehensive nonlinear analysis. Numerical investigations demonstrate that memristor-induced electromagnetic effects induce distinctive phenomena, including coexisting attractors, transient chaotic states, symmetric bifurcation diagrams and attractor structures, and constant chaos. The proposed system can generate more than 12 different attractors and extends the chaotic region. Compared with the chaotic range of the baseline Hopfield neural network (HNN), the expansion amplitude reaches 933%. Dynamic characteristics are systematically examined using phase trajectory analysis, bifurcation mapping, and Lyapunov exponent quantification. Experimental validation via a DSP-based hardware implementation confirms the model\u2019s operational feasibility and consistency with numerical predictions, establishing a reliable platform for electromagnetic\u2013neural interaction studies.",
      "year": 2025,
      "venue": "Symmetry",
      "authors": [
        "Zhimin Gu",
        "Bin Hu",
        "Hongxin Zhang",
        "Xiaodan Wang",
        "Yaning Qi",
        "Min Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ffbf082c9d726c1fb4e03a9e6045bec193ab3050",
      "pdf_url": "",
      "publication_date": "2025-08-19",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c328191268d06f97c892bcd16613c472b423e88e",
      "title": "Location of Underground Multilayer Media Based on BP Neural Network and Near-Field Electromagnetic Signal",
      "abstract": "Underground near-field electromagnetic positioning system is based on near-field electromagnetic ranging (NFER) technology. Obtaining the thickness of underground multilayer media during the positioning process introduces an ill-posed problem in the traditional electromagnetic model due to matrix inversion. Therefore, we propose a positioning method based on backpropagation (BP) neural network, which avoids the matrix inversion and contains the ranging model and positioning model. The positioning model is based on the ranging model and simultaneously predicts the ranging value and the thickness of each layer. The positioning is realized by utilizing the thickness value and range value combined with 2D-direction of arrival (DOA). This positioning method simplifies the positioning process compared with the trilateration algorithm. The results of the validation sets show that the positioning accuracy can reach 0.6 m at a depth of 40 m. Garson algorithm performs sensitivity analysis on the BP neural network, and it can be concluded that the signal angle contributes the most to the prediction of the results. Ultimately, the results reveal that the BP neural network-based positioning method performs well in nonhomogeneous media (NH) environments across various depth spaces and signal-to-noise ratios (SNRs).",
      "year": 2025,
      "venue": "IEEE Sensors Journal",
      "authors": [
        "Haonan Hou",
        "Xiaotong Zhang",
        "Xiaofen Wang",
        "Yadong Wan",
        "Haodong Shi",
        "Wen Liu",
        "Peng Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/c328191268d06f97c892bcd16613c472b423e88e",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "be0d667c4b910dad74a39e135697d7d7957512e1",
      "title": "Side-Channel Analysis of Integrate-and-Fire Neurons Within Spiking Neural Networks",
      "abstract": "Spiking neural networks gain increasing attention in constraint edge devices due to event-based low-power operation and little resource usage. Such edge devices often allow physical access, opening the door for Side-Channel Analysis. In this work, we introduce a novel robust attack strategy on the neuron level to retrieve the trained parameters of an implemented spiking neural network. Utilizing horizontal correlation power analysis, we demonstrate how to recover the weights and thresholds of a feed-forward spiking neural network implementation. We verify our methodology with real-world measurements of localized electromagnetic emanations of an FPGA design. Additionally, we propose countermeasures against the introduced novel attack approach. We evaluate shuffling and masking as countermeasures to protect the implementation against our proposed attack and demonstrate their effectiveness and limitations.",
      "year": 2025,
      "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
      "authors": [
        "Matthias Probst",
        "Manuel Brosch",
        "G. Sigl"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/be0d667c4b910dad74a39e135697d7d7957512e1",
      "pdf_url": "",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "title": "Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack",
      "abstract": "Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes \\textit{activation inversion attack} (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Chenxi Dai",
        "Lin Lu",
        "Pan Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f9b4f91ceca3254f882789b330b7c9044bf408a8",
      "title": "SoK: Are Watermarks in LLMs Ready for Deployment?",
      "abstract": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs. To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kieu Dang",
        "Phung Lai",
        "Nhathai Phan",
        "Yelong Shen",
        "Ruoming Jin",
        "Abdallah Khreishah",
        "My T. Thai"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f9b4f91ceca3254f882789b330b7c9044bf408a8",
      "pdf_url": "",
      "publication_date": "2025-06-05",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bbd216e8ca69d2583e1d203a4fdf8e9bf4e07f3d",
      "title": "Inside the Mind of an Attacker: Review Sistematika Tujuan Pencurian Machine Learning Model",
      "abstract": "Abstrak - Pencurian model (model stealing) menjadi salah satu ancaman serius dalam penerapan machine learning modern, terutama pada layanan berbasis API dan cloud. Artikel ini mengulas secara sistematik berbagai tujuan di balik serangan pencurian model untuk memahami motif penyerang dan implikasinya bagi pengembang sistem. Metode penulisan berupa kajian literatur terkini yang mengklasifikasikan tujuan pencurian ke dalam delapan kategori utama: (1) pencurian properti internal seperti arsitektur, bobot, dan hyperparameter; (2) peniruan perilaku model untuk menghasilkan efektivitas setara dan konsistensi prediksi pada data normal maupun adversarial; (3) transfer pengetahuan untuk distillation dan deployment ringan; (4) serangan privasi berupa membership inference dan model inversion; (5) monetisasi dengan menjual model bajakan atau menyediakan layanan API ilegal; (6) pencurian kemampuan pertahanan adversarial untuk meningkatkan efektivitas serangan; (7) spionase industri untuk reverse engineering model pesaing; serta (8) penghindaran regulasi dengan mencuri model yang sudah tersertifikasi. Review ini menegaskan bahwa ancaman pencurian model tidak hanya merugikan secara teknis, tetapi juga membuka peluang eksploitasi ekonomi ilegal, kebocoran data sensitif, dan persaingan usaha tidak sehat. Pemahaman yang detail atas ragam tujuan ini diharapkan mendorong perancang sistem untuk mengembangkan strategi pertahanan yang lebih cermat dan menyeluruh.Kata kunci: Machine Learning; Model Stealing; API; Adversarial; Transfer Pengetahuan;\u00a0Abstract - Model stealing has become one of the most serious threats in modern machine learning applications, especially in API- and cloud-based services. This article systematically reviews the various objectives behind model stealing attacks to understand the attackers\u2019 motivations and their implications for system developers. The writing method is a current literature review that classifies model stealing objectives into eight main categories: (1) theft of internal properties such as architecture, weights, and hyperparameters; (2) imitation of model behavior to achieve comparable effectiveness and prediction consistency on both normal and adversarial data; (3) knowledge transfer for distillation and lightweight deployment; (4) privacy attacks through membership inference and model inversion; (5) monetization by selling stolen models or offering illegal API services; (6) stealing adversarial robustness to improve attack effectiveness; (7) industrial espionage for reverse engineering competitor models; and (8) regulatory evasion by stealing pre-certified models. This review emphasizes that model stealing threats are not merely technical issues but also open opportunities for illegal economic exploitation, leakage of sensitive data, and unfair business competition. A detailed understanding of these diverse objectives is expected to encourage system designers to develop more careful and comprehensive defense strategies.Keywords: Machine Learning; Model Stealing; API; Adversarial; Knowledge Transfer;",
      "year": 2025,
      "venue": "Jurnal Nasional Komputasi dan Teknologi Informasi (JNKTI)",
      "authors": [
        "Mulkan Fadhli"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bbd216e8ca69d2583e1d203a4fdf8e9bf4e07f3d",
      "pdf_url": "",
      "publication_date": "2025-07-27",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2e50f4dc488356303b0f76a1db35d5e672ef3b28",
      "title": "Attackers Can Do Better: Over- and Understated Factors of Model Stealing Attacks",
      "abstract": "Machine learning (ML) models were shown to be vulnerable to model stealing attacks, which lead to intellectual property infringement. Among other attack methods, substitute model training is an all-encompassing attack applicable to any machine learning model whose behaviour can be approximated from input-output queries. Whereas previous works mainly focused on improving the performance of substitute models by, e.g. developing a new substitute training method, there have been only limited ablation studies that try to understand the impact the strength of an attacker has on the substitute model's performance. As a result, different authors came to diverse, sometimes contradicting, conclusions. In this work, we exhaustively examine the ambivalent influence of different factors resulting from varying the attacker's capabilities and knowledge on a substitute training attack. Our findings suggest that some of the factors that have been considered important in the past are, in fact, not that influential; instead, we discover new correlations between attack conditions and success rate. In particular, we demonstrate that better-performing target models enable higher-fidelity attacks and explain the intuition behind this phenomenon. Further, we propose to shift the focus from the complexity of target models toward the complexity of their learning tasks. Therefore, for the substitute model, rather than aiming for a higher architecture complexity, we suggest focusing on getting data of higher complexity and an appropriate architecture. Finally, we demonstrate that even in the most limited data-free scenario, there is no need to overcompensate weak knowledge with unrealistic capabilities in the form of millions of queries. Our results often exceed or match the performance of previous attacks that assume a stronger attacker, suggesting that these stronger attacks are likely endangering a model owner's intellectual property to a significantly higher degree than shown until now.",
      "year": 2025,
      "venue": "2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "Andreas Rauber"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2e50f4dc488356303b0f76a1db35d5e672ef3b28",
      "pdf_url": "",
      "publication_date": "2025-03-08",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b066eaa019bf6e94540dc54e735defb19d910bf8",
      "title": "Stealix: Model Stealing via Prompt Evolution",
      "abstract": "Model stealing poses a significant security risk in machine learning by enabling attackers to replicate a black-box model without access to its training data, thus jeopardizing intellectual property and exposing sensitive information. Recent methods that use pre-trained diffusion models for data synthesis improve efficiency and performance but rely heavily on manually crafted prompts, limiting automation and scalability, especially for attackers with little expertise. To assess the risks posed by open-source pre-trained models, we propose a more realistic threat model that eliminates the need for prompt design skills or knowledge of class names. In this context, we introduce Stealix, the first approach to perform model stealing without predefined prompts. Stealix uses two open-source pre-trained models to infer the victim model's data distribution, and iteratively refines prompts through a genetic algorithm, progressively improving the precision and diversity of synthetic images. Our experimental results demonstrate that Stealix significantly outperforms other methods, even those with access to class names or fine-grained prompts, while operating under the same query budget. These findings highlight the scalability of our approach and suggest that the risks posed by pre-trained generative models in model stealing may be greater than previously recognized.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhixiong Zhuang",
        "Hui-Po Wang",
        "Maria-Irina Nicolae",
        "Mario Fritz"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b066eaa019bf6e94540dc54e735defb19d910bf8",
      "pdf_url": "",
      "publication_date": "2025-06-06",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5ad7f669afa84855e00c743422026c8c5e91e411",
      "title": "I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks",
      "abstract": "Model stealing attacks endanger the confidentiality of machine learning models offered as a service. Although these models are kept secret, a malicious party can query a model to label data samples and train their own substitute model, violating intellectual property. While novel attacks in the field are continually being published, their design and evaluations are not standardised, making it challenging to compare prior works and assess progress in the field. This paper is the first to address this gap by providing recommendations for designing and evaluating model stealing attacks. To this end, we study the largest group of attacks that rely on training a substitute model -- those attacking image classification models. We propose the first comprehensive threat model and develop a framework for attack comparison. Further, we analyse attack setups from related works to understand which tasks and models have been studied the most. Based on our findings, we present best practices for attack development before, during, and beyond experiments and derive an extensive list of open research questions regarding the evaluation of model stealing attacks. Our findings and recommendations also transfer to other problem domains, hence establishing the first generic evaluation methodology for model stealing attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "Kathrin Grosse",
        "Andreas Rauber"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5ad7f669afa84855e00c743422026c8c5e91e411",
      "pdf_url": "",
      "publication_date": "2025-08-29",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1d3483a45263d2f67dedba087ff267194c7cfa1d",
      "title": "Model-Guardian: Protecting against Data-Free Model Stealing Using Gradient Representations and Deceptive Predictions",
      "abstract": "Model stealing attack is increasingly threatening the confidentiality of machine learning models deployed in the cloud. Recent studies reveal that adversaries can exploit data synthesis techniques to steal machine learning models even in scenarios devoid of real data, leading to data-free model stealing attacks. Existing defenses against such attacks suffer from limitations, including poor effectiveness, insufficient generalization ability, and low comprehensiveness. In response, this paper introduces a novel defense framework named Model-Guardian. Comprising two components, Data-Free Model Stealing Detector (DFMS-Detector) and Deceptive Predictions (DPreds), Model-Guardian is designed to address the shortcomings of current defenses with the help of the artifact properties of synthetic samples and gradient representations of samples. Extensive experiments on seven prevalent data-free model stealing attacks showcase the effectiveness and superior generalization ability of Model-Guardian, outperforming eleven defense methods and establishing a new state-of-the-art performance. Notably, this work pioneers the utilization of various GANs and diffusion models for generating highly realistic query samples in attacks, with Model-Guardian demonstrating accurate detection capabilities.",
      "year": 2025,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Yu Xuan",
        "Zhendong Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1d3483a45263d2f67dedba087ff267194c7cfa1d",
      "pdf_url": "",
      "publication_date": "2025-03-23",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "509a77d3e7a4a48ff7b191fe1802db4d597de8c8",
      "title": "Defenses against model stealing attacks in MLaaS: literature review and challenges",
      "abstract": null,
      "year": 2025,
      "venue": "Cluster Computing",
      "authors": [
        "Aouatef Mahani",
        "O. Kazar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/509a77d3e7a4a48ff7b191fe1802db4d597de8c8",
      "pdf_url": "",
      "publication_date": "2025-08-19",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16cfca0d74d329b2b3f2d927047468f09300e33e",
      "title": "CERBEROS: Compression-Based Efficient and Robust Optimized Security for Model Stealing Defense",
      "abstract": "Model stealing attacks pose an increasing threat to the confidentiality and intellectual property of artificial intelligence (AI) models. Existing defenses\u2013such as query monitoring, output perturbation, multi-model output variation, and post hoc verification\u2013fall short in on-device applications where models must run under strict memory and computation budgets. These approaches typically incur high memory or latency overhead due to their reliance on auxiliary models or additional inference-time processing. To address these limitations, we propose CERBEROS, a defense framework designed to achieve security against model stealing with deployability in resource-constrained environments. At its core, CERBEROS introduces a novel neural architecture with multiple classification heads trained jointly for output diversification, while sharing a single feature extraction backbone to minimize unnecessary memory usage. At inference, CERBEROS reveals the prediction of a randomly selected head, thereby misleading adversaries while preserving test accuracy for legitimate users, without requiring separate models or costly output modification. In addition, we integrate structured pruning into training to compress the backbone while retaining the classification heads. This ensures that functional diversity across heads remains achievable even under tight resource constraints. Our experiments show that CERBEROS effectively mitigates model replication attacks while consistently maintaining task performance across widely used convolutional neural networks and benchmark datasets. Furthermore, it achieves significant reductions in memory consumption and inference latency compared to prior defenses, offering a practical and efficient solution for securing on-device AI models.",
      "year": 2025,
      "venue": "IEEE Access",
      "authors": [
        "Sohyun Keum",
        "Jeonghyun Lee",
        "Sangkyun Lee"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/16cfca0d74d329b2b3f2d927047468f09300e33e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16743a0a8daad27538a0bc734ed42abca3a14289",
      "title": "Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy",
      "abstract": "Malicious users attempt to replicate commercial models functionally at low cost by training a clone model with query responses. It is challenging to timely prevent such model-stealing attacks to achieve strong protection and maintain utility. In this paper, we propose a novel non-parametric detector called Account-aware Distribution Discrepancy (ADD) to recognize queries from malicious users by leveraging account-wise local dependency. We formulate each class as a Multivariate Normal distribution (MVN) in the feature space and measure the malicious score as the sum of weighted class-wise distribution discrepancy. The ADD detector is combined with random-based prediction poisoning to yield a plug-and-play defense module named D-ADD for image classification models. Results of extensive experimental studies show that D-ADD achieves strong defense against different types of attacks with little interference in serving benign users for both soft and hard-label settings.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jian-Ping Mei",
        "Weibin Zhang",
        "Jie Chen",
        "Xuyun Zhang",
        "Tiantian Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/16743a0a8daad27538a0bc734ed42abca3a14289",
      "pdf_url": "",
      "publication_date": "2025-03-16",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f8f0626c98083f77d2f5c776cc3a4b91fa273b73",
      "title": "Self-enhancing defense for protecting against model stealing attacks on deep learning systems",
      "abstract": null,
      "year": 2025,
      "venue": "Expert systems with applications",
      "authors": [
        "Chenlong Zhang",
        "Senlin Luo",
        "Jiawei Li",
        "Limin Pan",
        "Chuan Lu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8f0626c98083f77d2f5c776cc3a4b91fa273b73",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e445ba60a5f0dd22cb9188df23c228c5ec7a1f45",
      "title": "Model Rake: A Defense Against Stealing Attacks in Split Learning",
      "abstract": "Split learning is a prominent framework for vertical federated learning, where multiple clients collaborate with a central server for model training by exchanging intermediate embeddings. Recently, it is shown that an adversarial server can exploit the intermediate embeddings to train surrogate models to replace the bottom models on the clients (i.e., model stealing). The surrogate models can also be used to reconstruct private training data of the clients (i.e., data stealing).\n\nTo defend against these stealing attacks, we propose Model Rake (i.e., Rake), which runs two bottom models on each client and differentiates their output spaces to make the two models distinct. Rake hinders the stealing attacks because it is difficult for a surrogate model to approximate two distinct bottom models. We prove that, under some assumptions, the surrogate model converges to the average of the two bottom models and thus will be inaccurate. Extensive experiments show that Rake is much more effective than existing methods in defending against both model and data stealing attacks, and the accuracy of normal model training is not affected.",
      "year": 2025,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Qinbo Zhang",
        "Xiao Yan",
        "Yanfeng Zhao",
        "Fangcheng Fu",
        "Quanqing Xu",
        "Yukai Ding",
        "Xiaokai Zhou",
        "Chuang Hu",
        "Jiawei Jiang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e445ba60a5f0dd22cb9188df23c228c5ec7a1f45",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f2aaca56a4bbc4e9358f0af26e67f8cae4ebf476",
      "title": "Digital Scapegoat: An Incentive Deception Model for Resisting Unknown APT Stealing Attacks on Critical Data Resource",
      "abstract": "It is a challenging problem to resist unknown advanced persistent threats (APTs) on stealing data resources in an information system of critical infrastructures, because APT attackers have very specific objectives and compromise the system stealthily and slowly. We observe that it is a necessary condition for APT attackers to achieve their campaigns via controlling unknown Trojans to access and exfiltrate critical files. We present a theoretical model called Digital Scapegoat (abbreviated as DS-IDep) that constructs an Incentive Deception defense schema to hijack the attacker\u2019s access to critical files and redirect it to avatar files without awareness. We propose a FlipIDep Game model (<inline-formula> <tex-math notation=\"LaTeX\">$G_{F}$ </tex-math></inline-formula>) and a Markov Game model (<inline-formula> <tex-math notation=\"LaTeX\">$G_{M}$ </tex-math></inline-formula>) to characterize completely the payoffs, equilibria, and best strategies from the perspective of the attacker and the defender respectively. We also design an exponential risk propagation model to evaluate the ability of DS-IDep to eliminate stealing impact when the risk is propagated between states. Theoretically, we can achieve the objective of stealing impact elimination (<inline-formula> <tex-math notation=\"LaTeX\">$L_{K} \\lt 0.001$ </tex-math></inline-formula>) when the ratio of incentive deception exceeds 0.7 (<inline-formula> <tex-math notation=\"LaTeX\">$\\eta \\gt 0.7$ </tex-math></inline-formula>) and the probability of an attack operation bypassing the defense surface is less than 0.1 (<inline-formula> <tex-math notation=\"LaTeX\">$r^{*}\\times \\mu \\lt 0.1$ </tex-math></inline-formula>) under Stackelberg strategies. We develop a kernel-level incentive deception defense surface according to the theoretical parameters of the DS-IDep. The experimental results show that DS-IDep can resist APT stealing attacks from unknown Trojans. We also evaluate the DS-IDep in five well-known software applications. It demonstrates that DS-IDep can address unknown attacks from compromised software with less than 10% performance overhead.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xiaochun Yun",
        "Guangjun Wu",
        "Shuhao Li",
        "Qi Song",
        "Zixian Tang",
        "Zhenyu Cheng"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f2aaca56a4bbc4e9358f0af26e67f8cae4ebf476",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d7acc4f16a3e3c33a9544bbe80a72ab218b23869",
      "title": "On Stealing Graph Neural Network Models",
      "abstract": "Current graph neural network (GNN) model-stealing methods rely heavily on queries to the victim model, assuming no hard query limits. However, in reality, the number of allowed queries can be severely limited. In this paper, we demonstrate how an adversary can extract a GNN with very limited interactions with the model. Our approach first enables the adversary to obtain the model backbone without making direct queries to the victim model and then to strategically utilize a fixed query limit to extract the most informative data. The experiments on eight real-world datasets demonstrate the effectiveness of the attack, even under a very restricted query limit and under defense against model extraction in place. Our findings underscore the need for robust defenses against GNN model extraction threats.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Marcin Podhajski",
        "Jan Dubi'nski",
        "Franziska Boenisch",
        "Adam Dziedzic",
        "Agnieszka Prkegowska",
        "Tomasz P. Michalak"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d7acc4f16a3e3c33a9544bbe80a72ab218b23869",
      "pdf_url": "",
      "publication_date": "2025-11-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5102a7c89adeba6818f90b1964106397a2fb7a37",
      "title": "Dynamic Gradient Compression and Attack Defense Strategy for Privacy Enhancement of Heterogeneous Data in Federated Learning",
      "abstract": "An innovatively constructed dynamic perception mechanism based on temporal and spatial dual dimensions is proposed. In particular, it dynamically adjusts the gradient compression ratio depending on the convergence rate of the model (e.g., using high compression ratio for fast convergence at early stage and fine-tuning later), which is realized by integrating loss variation rate and training cycle into compression ratio equation. In spatial dimension, this method adapts to heterogeneous data distributions by introducing a sample weight factor into the non-IID measurement index so that the heterogeneity of data can be quantified. A dynamic privacy budget allocation strategy based on data sensitivity matrix ensures adaptive noise injection and hierarchical encryption. In contrast to traditional methods, the anomaly detection module introduces high order statistical moments (skewness, kurtosis), combined with machine learning based attack classification methods, to detect gradient poisoning and model stealing attacks in real time.",
      "year": 2025,
      "venue": "2025 10th International Symposium on Advances in Electrical, Electronics and Computer Engineering (ISAEECE)",
      "authors": [
        "Conghui Wei",
        "Yaqian Lu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5102a7c89adeba6818f90b1964106397a2fb7a37",
      "pdf_url": "",
      "publication_date": "2025-06-20",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f8caf3ee31a4d1d4d72f87997c104aaeb56e0a33",
      "title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features",
      "abstract": "Large vision models achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property for its owner. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to these personalized models. However, in this paper, we reveal that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized models by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by the output differences between the shadow and victim models. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Linghui Zhu",
        "Yiming Li",
        "Haiqin Weng",
        "Yan Liu",
        "Tianwei Zhang",
        "Shu-Tao Xia",
        "Zhi Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8caf3ee31a4d1d4d72f87997c104aaeb56e0a33",
      "pdf_url": "",
      "publication_date": "2025-06-24",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4ced0ca56a1f623ebe1860f2551ee5b8011c9b87",
      "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data",
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries. Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods. We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yixu Wang",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4ced0ca56a1f623ebe1860f2551ee5b8011c9b87",
      "pdf_url": "",
      "publication_date": "2025-09-28",
      "keywords_matched": [
        "model extraction attack",
        "model extraction",
        "LoRA extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "title": "\u03b4-STEAL: LLM Stealing Attack with Local Differential Privacy",
      "abstract": "Large language models (LLMs) demonstrate remarkable capabilities across various tasks. However, their deployment introduces significant risks related to intellectual property. In this context, we focus on model stealing attacks, where adversaries replicate the behaviors of these models to steal services. These attacks are highly relevant to proprietary LLMs and pose serious threats to revenue and financial stability. To mitigate these risks, the watermarking solution embeds imperceptible patterns in LLM outputs, enabling model traceability and intellectual property verification. In this paper, we study the vulnerability of LLM service providers by introducing $\\delta$-STEAL, a novel model stealing attack that bypasses the service provider's watermark detectors while preserving the adversary's model utility. $\\delta$-STEAL injects noise into the token embeddings of the adversary's model during fine-tuning in a way that satisfies local differential privacy (LDP) guarantees. The adversary queries the service provider's model to collect outputs and form input-output training pairs. By applying LDP-preserving noise to these pairs, $\\delta$-STEAL obfuscates watermark signals, making it difficult for the service provider to determine whether its outputs were used, thereby preventing claims of model theft. Our experiments show that $\\delta$-STEAL with lightweight modifications achieves attack success rates of up to $96.95\\%$ without significantly compromising the adversary's model utility. The noise scale in LDP controls the trade-off between attack effectiveness and model utility. This poses a significant risk, as even robust watermarks can be bypassed, allowing adversaries to deceive watermark detectors and undermine current intellectual property protection methods.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kieu Dang",
        "Phung Lai",
        "Nhathai Phan",
        "Yelong Shen",
        "Ruoming Jin",
        "Abdallah Khreishah"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "pdf_url": "",
      "publication_date": "2025-10-24",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "056036a3ce3a21daf80167dee622b6b515f9490e",
      "title": "Black-box model functionality stealing for Vietnamese sentiment analysis",
      "abstract": "Black-box deep learning models often keep critical components such as model architecture, hyperparameters, and training data confidential, allowing users to observe only the inputs and outputs without understanding their internal workings. Consequently, there is growing interested in developing \"knockoff\" models that replicate the behavior of these black-box models without direct access to internal details. We have conducted extensive studies on function extraction attacks targeting English text sentiment analysis models. By employing random or adaptive sampling methods, we have successfully reconstructed knockoff models that achieve functionality equivalent to the original models with high similarity. In this study, we extend our investigation to sentiment analysis datasets in Vietnamese. Experimental results demonstrate that for black-box models in Vietnamese text sentiment analysis, our method remains effective, successfully constructing models with equivalent functionality.",
      "year": 2025,
      "venue": "Journal of Military Science and Technology",
      "authors": [
        "Cong Pham",
        "Viet-Binh Do",
        "Trung-Nguyen Hoang",
        "Cao-Truong Tran"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/056036a3ce3a21daf80167dee622b6b515f9490e",
      "pdf_url": "",
      "publication_date": "2025-06-25",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bb3cf7ad6f2528490b40a8266ca1fa4dc5b929f4",
      "title": "Assessing Risk of Stealing Proprietary Models for Medical Imaging Tasks",
      "abstract": "The success of deep learning in medical imaging applications has led several companies to deploy proprietary models in diagnostic workflows, offering monetized services. Even though model weights are hidden to protect the intellectual property of the service provider, these models are exposed to model stealing (MS) attacks, where adversaries can clone the model's functionality by querying it with a proxy dataset and training a thief model on the acquired predictions. While extensively studied on general vision tasks, the susceptibility of medical imaging models to MS attacks remains inadequately explored. This paper investigates the vulnerability of black-box medical imaging models to MS attacks under realistic conditions where the adversary lacks access to the victim model's training data and operates with limited query budgets. We demonstrate that adversaries can effectively execute MS attacks by using publicly available datasets. To further enhance MS capabilities with limited query budgets, we propose a two-step model stealing approach termed QueryWise. This method capitalizes on unlabeled data obtained from a proxy distribution to train the thief model without incurring additional queries. Evaluation on two medical imaging models for Gallbladder Cancer and COVID-19 classification substantiates the effectiveness of the proposed attack. The source code is available at https://github.com/rajankita/QueryWise.",
      "year": 2025,
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "authors": [
        "Ankita Raj",
        "Harsh Swaika",
        "Deepankar Varma",
        "Chetan Arora"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bb3cf7ad6f2528490b40a8266ca1fa4dc5b929f4",
      "pdf_url": "",
      "publication_date": "2025-06-24",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c100542ca883890adf1fa185e07e13e9f4e5ec46",
      "title": "DeepAW: A Customized DNN Watermarking Scheme Against Unreliable Participants",
      "abstract": "Training DNNs requires large amounts of labeled data, costly computational resources, and tremendous human effort, resulting in such models being a valuable commodity. In collaborative learning scenarios, unreliable participants are widespread due to data collected from a diverse set of end-users that differ in quality and quantity. It is important to note that failure to take into account the contributions of all participants in the collaborative model training process when sharing the model with them could potentially result in a deterioration in collaborative efforts. In this paper, we propose a customized DNN watermarking scheme to safeguard the model ownership, namely DeepAW, achieving robustness to model stealing attacks and collaborative fairness in the presence of unreliable participants. Specifically, DeepAW leverages the tightly binding between the embedded watermarking and the model performance to defend against the model stealing attacks, resulting in the sharp decline of the model performance encountering any attempt at watermarking modification. DeepAW achieves collaborative fairness by detecting unreliable participants and customizing the model performance according to the participants' contributions. Furthermore, we set up three model stealing attacks and four types of unreliable participants. The experimental results demonstrate the effectiveness, robustness, and collaborative fairness of DeepAW.",
      "year": 2025,
      "venue": "IEEE Transactions on Network Science and Engineering",
      "authors": [
        "Shen Lin",
        "Xiaoyu Zhang",
        "Xu Ma",
        "Xiaofeng Chen",
        "Willy Susilo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c100542ca883890adf1fa185e07e13e9f4e5ec46",
      "pdf_url": "",
      "publication_date": "2025-07-01",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "23bb050bbdf0d710f9f6f665e227b25c1d6ee034",
      "title": "Catch the Star: Weight Recovery Attack Using Side-Channel Star Map Against DNN Accelerator",
      "abstract": "The rapid development of Artificial Intelligence (AI) technology must be connected to the arithmetic support of high-performance hardware. However, when the deep neural network (DNN) accelerator performs inference tasks at the edge end, the sensitive data of DNN will generate leakage through side-channel information. The adversary can recover the model structure and weight parameters of DNN by using the side-channel information, which seriously affects the protection of necessary intellectual property (IP) of DNN, so the hardware security of the DNN accelerator is critical. In the current research of Side-channel attack (SCA) for matrix multiplication units, such as systolic arrays, the linear multiplication operation leads to a more extensive weights search space for the SCA, and extracting all the weight parameters requires higher attack conditions. This article proposes a new power SCA method, which includes a Collision-Correlation Power Analysis (Collision-CPA) and Correlation-based Weight Search Algorithm (C-WSA) to address the problem. The Collision-CPA reduces the attack conditions for the SCA by building multiple Hamming Distance (HD)-based power leakage models for the systolic array. Meanwhile, the C-WSA dramatically reduces the weights search space. In addition, the concept of a Side-channel star map (SCSM) is proposed for the first time in this article, and the adversary can quickly and accurately locate the correct weight information in the SCSM. Through experiments, we recover all the weight parameters of a $3\\times 3$ systolic array based on 100000 power traces, in which the weight search space is reduced by up to 97.7%. For the DNN accelerator at the edge, especially the systolic array structure, our proposed novel SCA aligns more with practical attack scenarios, with lower attack conditions, and higher attack efficiency.",
      "year": 2025,
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": [
        "Le Wu",
        "Liji Wu",
        "Xiang Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/23bb050bbdf0d710f9f6f665e227b25c1d6ee034",
      "pdf_url": "",
      "publication_date": "2025-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e685da2d597d7b478a514bf26d78f5d9131e6b98",
      "title": "MACPruning: Dynamic Operation Pruning to Mitigate Side-Channel DNN Model Extraction",
      "abstract": "As deep learning gains popularity, edge IoT devices have seen proliferating deployment of pre-trained Deep Neural Network (DNN) models. These DNNs represent valuable intellectual property and face significant confidentiality threats from side-channel analysis (SCA), particularly non-invasive Differential Electromagnetic (EM) Analysis (DEMA), which retrieves individual model parameters from EM traces collected during model inference. Traditional SCA mitigation methods, such as masking and shuffling, can still be applied to DNN inference, but will incur significant performance degradation due to the large volume of operations and parameters. Based on the insight that DNN models have high redundancy and are robust to input variation, we introduce MACPruning, a novel lightweight defense against DEMA-based parameter extraction attacks, exploiting specific characteristics of DNN execution. The design principle of MACPruning is to randomly deactivate input pixels and prune the operations (typically multiply-accumulate-MAC) on those pixels. The technique removes certain leakages and overall redistributes weight-dependent EM leakages temporally, and thus effectively mitigates DEMA. To maintain DNN performance, we propose an importance-aware pixel map that preserves critical input pixels, keeping randomness in the defense while minimizing its impact on DNN performance due to operation pruning. We conduct a comprehensive security analysis of MACPruning on various datasets for DNNs on edge devices. Our evaluations demonstrate that MACPruning effectively reduces EM leakages with minimal impact on the model accuracy and negligible computational overhead.",
      "year": 2025,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Ruyi Ding",
        "Gongye Cheng",
        "Davis Ranney",
        "A. A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e685da2d597d7b478a514bf26d78f5d9131e6b98",
      "pdf_url": "",
      "publication_date": "2025-02-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ec45414afec5007925db471273111da8bf389234",
      "title": "Arithmetic Masking Countermeasure to Mitigate Side-Channel-Based Model Extraction Attack on DNN Accelerator",
      "abstract": null,
      "year": 2025,
      "venue": "ACNS Workshops",
      "authors": [
        "Hirokatsu Yamasaki",
        "Kota Yoshida",
        "Yuta Fukuda",
        "Takeshi Fujino"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ec45414afec5007925db471273111da8bf389234",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a2c361d27c70f4b61da39732c338c54145a4cb82",
      "title": "Exploiting Power Side-Channel Vulnerabilities in XGBoost Accelerator",
      "abstract": "XGBoost (eXtreme Gradient Boosting), a widelyused decision tree algorithm, plays a crucial role in applications such as ransomware and fraud detection. While its performance is well-established, its security against model extraction on hardware platforms like Field Programmable Gate Arrays (FPGAs) has not been fully explored. In this paper, we demonstrate a significant vulnerability where sensitive model data can be leaked from an XGBoost implementation through side-channel attacks (SCAs). By analyzing variations in power consumption, we show how an attacker can infer node features within the XGBoost model, leading to the extraction of critical data. We conduct an experiment using the XGBoost accelerator FAXID on the Sakura-X platform, demonstrating a method to deduce model decisions by monitoring power consumptions. The results show that on average 367k tests are sufficient to leak sensitive values. Our findings underscore the need for improved hardware and algorithmic protections to safeguard machine learning models from these types of attacks.",
      "year": 2025,
      "venue": "Design Automation Conference",
      "authors": [
        "Yimeng Xiao",
        "Archit Gajjar",
        "Aydin Aysu",
        "Paul Franzon"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a2c361d27c70f4b61da39732c338c54145a4cb82",
      "pdf_url": "",
      "publication_date": "2025-06-22",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "title": "Stealing Part of a Production Language Model",
      "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \\$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nicholas Carlini",
        "Daniel Paleka",
        "K. Dvijotham",
        "Thomas Steinke",
        "Jonathan Hayase",
        "A. F. Cooper",
        "Katherine Lee",
        "Matthew Jagielski",
        "Milad Nasr",
        "Arthur Conmy",
        "Eric Wallace",
        "D. Rolnick",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 124,
      "url": "https://www.semanticscholar.org/paper/b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "pdf_url": "",
      "publication_date": "2024-03-11",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c7af46b35061e856aa3332ac2eec6a7ccee0cb35",
      "title": "Watermark Stealing in Large Language Models",
      "abstract": "LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as hypothesized in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nikola Jovanovi'c",
        "Robin Staab",
        "Martin T. Vechev"
      ],
      "citation_count": 70,
      "url": "https://www.semanticscholar.org/paper/c7af46b35061e856aa3332ac2eec6a7ccee0cb35",
      "pdf_url": "",
      "publication_date": "2024-02-29",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9487d29364645c2f086387ff817ad5fd14b33c41",
      "title": "A Comprehensive Defense Framework Against Model Extraction Attacks",
      "abstract": "As a promising service, Machine Learning as a Service (MLaaS) provides personalized inference functions for clients through paid APIs. Nevertheless, it is vulnerable to model extraction attacks, in which an attacker can extract a functionally-equivalent model by repeatedly querying the APIs with crafted samples. While numerous works have been proposed to defend against model extraction attacks, existing efforts are accompanied by limitations and low comprehensiveness. In this article, we propose AMAO, a comprehensive defense framework against model extraction attacks. Specifically, AMAO consists of four interlinked successive phases: adversarial training is first exploited to weaken the effectiveness of model extraction attacks. Then, malicious query detection is used to detect malicious queries and mark malicious users. After that, we develop a label-flipping poisoning attack to instruct the adaptive query responses to malicious users. Besides, the image pHash algorithm is employed to ensure the indistinguishability of the query responses. Finally, the perturbed results are served as a backdoor to verify the ownership of any suspicious model. Extensive experiments demonstrate that AMAO outperforms existing defenses in defending against model extraction attacks and is also robust against the adaptive adversary who is aware of the defense.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Wenbo Jiang",
        "Hongwei Li",
        "Guowen Xu",
        "Tianwei Zhang",
        "Rongxing Lu"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/9487d29364645c2f086387ff817ad5fd14b33c41",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "title": "Prompt Stealing Attacks Against Large Language Models",
      "abstract": "The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zeyang Sha",
        "Yang Zhang"
      ],
      "citation_count": 43,
      "url": "https://www.semanticscholar.org/paper/3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "pdf_url": "",
      "publication_date": "2024-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "title": "Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses",
      "abstract": "Deep Neural Networks (DNNs) have revolutionized various domains with their exceptional performance across numerous applications. However, Model Inversion (MI) attacks, which disclose private information about the training dataset by abusing access to the trained models, have emerged as a formidable privacy threat. Given a trained network, these attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training samples, posing significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive and systematic overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this realm and presents a holistic survey. Firstly, our work briefly reviews early MI studies on traditional machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on Deep Neural Networks (DNNs) across multiple modalities and learning tasks. By meticulously analyzing their distinctive features, we summarize and classify these methods into different categories and provide a novel taxonomy. Finally, this paper discusses promising research directions and presents potential solutions to open issues. To facilitate further study on MI attacks and defenses, we have implemented an open-source model inversion toolbox on GitHub (https://github.com/ffhibnese/Model-Inversion-Attack-ToolBox).",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hao Fang",
        "Yixiang Qiu",
        "Hongyao Yu",
        "Wenbo Yu",
        "Jiawei Kong",
        "Baoli Chong",
        "Bin Chen",
        "Xuan Wang",
        "Shutao Xia"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "pdf_url": "",
      "publication_date": "2024-02-06",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
      "abstract": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call\"neural phishing\". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Ashwinee Panda",
        "Christopher A. Choquette-Choo",
        "Zhengming Zhang",
        "Yaoqing Yang",
        "Prateek Mittal"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "title": "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models",
      "abstract": "Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Shanglun Feng",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "pdf_url": "",
      "publication_date": "2024-03-30",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3c3479215bcc775f8e37d7d5055a16b698904c9a",
      "title": "FedDSE: Distribution-aware Sub-model Extraction for Federated Learning over Resource-constrained Devices",
      "abstract": "Sub-model extraction based federated learning has emerged as a popular strategy for training models on resource-constrained devices. However, existing methods treat all clients equally and extract sub-models using predetermined rules, which disregard the statistical heterogeneity across clients and may lead to fierce competition among them. Specifically, this paper identifies that when making predictions, different clients tend to activate different neurons of the entire model related to their respective distributions. If highly activated neurons from some clients with one distribution are incorporated into the sub-model allocated to other clients with different distributions, they will be forced to fit the new distributions, which can hinder their activation over the previous clients and result in a performance reduction. Motivated by this finding, we propose a novel method called FedDSE, which can reduce the conflicts among clients by extracting sub-models based on the data distribution of each client. The core idea of FedDSE is to empower each client to adaptively extract neurons from the entire model based on their activation over the local dataset. We theoretically show that FedDSE can achieve an improved classification score and convergence over general neural networks with the ReLU activation function. Experimental results on various datasets and models show that FedDSE outperforms all state-of-the-art baselines.",
      "year": 2024,
      "venue": "The Web Conference",
      "authors": [
        "Haozhao Wang",
        "Yabo Jia",
        "Meng Zhang",
        "Qi Hu",
        "Hao Ren",
        "Peng Sun",
        "Yonggang Wen",
        "Tianwei Zhang"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/3c3479215bcc775f8e37d7d5055a16b698904c9a",
      "pdf_url": "",
      "publication_date": "2024-05-13",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "650f6db095276ca03d03f4951587d7383ca3b39d",
      "title": "ModelGuard: Information-Theoretic Defense Against Model Extraction Attacks",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Minxue Tang",
        "Anna Dai",
        "Louis DiValentin",
        "Aolin Ding",
        "Amin Hass",
        "Neil Zhenqiang Gong",
        "Yiran Chen",
        "Helen Li"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/650f6db095276ca03d03f4951587d7383ca3b39d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "550a79a7e688347af18bf5752361c47f0af1cf40",
      "title": "Large Language Model Watermark Stealing With Mixed Integer Programming",
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM's parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/550a79a7e688347af18bf5752361c47f0af1cf40",
      "pdf_url": "",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5201913bb3b941e0d42606969da5c1f927aeb48b",
      "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks",
      "abstract": "Large language models (LLMs) possess extensive knowledge and question-answering capabilities, having been widely deployed in privacy-sensitive domains like finance and medical consultation. During LLM inferences, cache-sharing methods are commonly employed to enhance efficiency by reusing cached states or responses for the same or similar inference requests. However, we identify that these cache mechanisms pose a risk of private input leakage, as the caching can result in observable variations in response times, making them a strong candidate for a timing-based attack hint. In this study, we propose a novel timing-based side-channel attack to execute input theft in LLMs inference. The cache-based attack faces the challenge of constructing candidate inputs in a large search space to hit and steal cached user queries. To address these challenges, we propose two primary components. The input constructor employs machine learning techniques and LLM-based approaches for vocabulary correlation learning while implementing optimized search mechanisms for generalized input construction. The time analyzer implements statistical time fitting with outlier elimination to identify cache hit patterns, continuously providing feedback to refine the constructor's search strategy. We conduct experiments across two cache mechanisms and the results demonstrate that our approach consistently attains high attack success rates in various applications. Our work highlights the security vulnerabilities associated with performance optimizations, underscoring the necessity of prioritizing privacy and security alongside enhancements in LLM inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Xinyao Zheng",
        "Husheng Han",
        "Shangyi Shi",
        "Qiyan Fang",
        "Zidong Du",
        "Xing Hu",
        "Qi Guo"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/5201913bb3b941e0d42606969da5c1f927aeb48b",
      "pdf_url": "",
      "publication_date": "2024-11-27",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "14b7cdf6349611cdc3d271c1672ef275e0d101ae",
      "title": "MEA-Defender: A Robust Watermark against Model Extraction Attack",
      "abstract": "Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been trained using deep learning algorithms. To protect the Intellectual Property (IP) of the original owners over such DNN models, backdoor-based watermarks have been extensively studied. However, most of such watermarks fail upon model extraction attack, which utilizes input samples to query the target model and obtains the corresponding outputs, thus training a substitute model using such input-output pairs. In this paper, we propose a novel watermark to protect IP of DNN models against model extraction, named MEA-Defender. In particular, we obtain the watermark by combining two samples from two source classes in the input domain and design a watermark loss function that makes the output domain of the watermark within that of the main task samples. Since both the input domain and the output domain of our watermark are indispensable parts of those of the main task samples, the watermark will be extracted into the stolen model along with the main task during model extraction. We conduct extensive experiments on four model extraction attacks, using five datasets and six models trained based on supervised learning and self-supervised learning algorithms. The experimental results demonstrate that MEA-Defender is highly robust against different model extraction attacks, and various watermark removal/detection approaches.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Peizhuo Lv",
        "Hualong Ma",
        "Kai Chen",
        "Jiachen Zhou",
        "Shengzhi Zhang",
        "Ruigang Liang",
        "Shenchen Zhu",
        "Pan Li",
        "Yingjun Zhang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/14b7cdf6349611cdc3d271c1672ef275e0d101ae",
      "pdf_url": "https://arxiv.org/pdf/2401.15239",
      "publication_date": "2024-01-26",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "title": "TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) have been widely applied in various scenarios. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct model stealing (MS) attacks. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead. To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices. The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE, which can freshly authorize each request based on its input. Extensive experiments show that TransLinkGuard achieves the same security as the black-box guarantees with negligible overhead.",
      "year": 2024,
      "venue": "ACM Multimedia",
      "authors": [
        "Qinfeng Li",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Yangfan Xie",
        "Xuhong Zhang",
        "Tianyu Du",
        "Sheng Cheng",
        "Xun Wang",
        "Jianwei Yin"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "pdf_url": "https://arxiv.org/pdf/2404.11121",
      "publication_date": "2024-04-17",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7bed6f6101204efdf04181aafa511ca55644b559",
      "title": "Data Stealing Attacks against Large Language Models via Backdooring",
      "abstract": "Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring. Our method mainly focuses on the scenario of model customization and is conducted in two phases, including backdoor training and backdoor activation, which allow for the extraction of private information without prior knowledge of the model\u2019s architecture or training data. During the model customization stage, attackers inject the backdoor into the pre-trained LLM by poisoning a small ratio of the training dataset. During the inference stage, attackers can extract private information from the third-party knowledge database by incorporating the pre-defined backdoor trigger. Our method leverages the customization process of LLMs, injecting a stealthy backdoor that can be triggered after deployment to retrieve private data. We demonstrate the effectiveness of our proposed attack through extensive experiments, achieving a notable attack success rate. Extensive experiments demonstrate the effectiveness of our stealing attack in popular LLM architectures, as well as stealthiness during normal inference.",
      "year": 2024,
      "venue": "Electronics",
      "authors": [
        "Jiaming He",
        "Guanyu Hou",
        "Xinyue Jia",
        "Yangyang Chen",
        "Wenqi Liao",
        "Yinhang Zhou",
        "Rang Zhou"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/7bed6f6101204efdf04181aafa511ca55644b559",
      "pdf_url": "https://doi.org/10.3390/electronics13142858",
      "publication_date": "2024-07-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0e536b831cf1053d2c523d2e16656ad27c7369f2",
      "title": "Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model",
      "abstract": "Aligning language models (LMs) with human preferences has become a key area of research, enabling these models to meet diverse user needs better. Inspired by weak-to-strong generalization, where a strong LM fine-tuned on labels generated by a weaker model can consistently outperform its weak supervisor, we extend this idea to model alignment. In this work, we observe that the alignment behavior in weaker models can be effectively transferred to stronger models and even exhibit an amplification effect. Based on this insight, we propose a method called Weak-to-Strong Preference Optimization (WSPO), which achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model. Experiments demonstrate that WSPO delivers outstanding performance, improving the win rate of Qwen2-7B-Instruct on Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04 length-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our results suggest that using the weak model to elicit a strong model with a high alignment ability is feasible.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Wenhong Zhu",
        "Zhiwei He",
        "Xiaofeng Wang",
        "Pengfei Liu",
        "Rui Wang"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/0e536b831cf1053d2c523d2e16656ad27c7369f2",
      "pdf_url": "",
      "publication_date": "2024-10-24",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6d556b1dbb4a351b12a759c6581ed903e728c09e",
      "title": "SoK: All You Need to Know About On-Device ML Model Extraction - The Gap Between Research and Practice",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Tushar Nayan",
        "Qiming Guo",
        "Mohammed A. Duniawi",
        "Marcus Botacin",
        "Selcuk Uluagac",
        "Ruimin Sun"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/6d556b1dbb4a351b12a759c6581ed903e728c09e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "36d37aaf4bb16db3e5cc37c8b4ff55d4babb71ba",
      "title": "A realistic model extraction attack against graph neural networks",
      "abstract": null,
      "year": 2024,
      "venue": "Knowledge-Based Systems",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Hanjin Tong",
        "Wanlei Zhou"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/36d37aaf4bb16db3e5cc37c8b4ff55d4babb71ba",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1e8b05c5b703f443b5470bef2b3aca1b064b1709",
      "title": "DeMistify: Identifying On-Device Machine Learning Models Stealing and Reuse Vulnerabilities in Mobile Apps",
      "abstract": "Mobile apps have become popular for providing artificial intelligence (AI) services via on-device machine learning (ML) techniques. Unlike accomplishing these AI services on remote servers traditionally, these on-device techniques process sensitive information required by AI services locally, which can mitigate the severe con-cerns of the sensitive data collection on the remote side. However, these on-device techniques have to push the core of ML expertise (e.g., models) to smartphones locally, which are still subject to similar vulnerabilities on the remote clouds and servers, especially when facing the model stealing attack. To defend against these attacks, developers have taken various protective measures. Unfor-tunately, we have found that these protections are still insufficient, and on-device ML models in mobile apps could be extracted and reused without limitation. To better demonstrate its inadequate protection and the feasibility of this attack, this paper presents DeMistify, which statically locates ML models within an app, slices relevant execution components, and finally generates scripts auto-matically to instrument mobile apps to successfully steal and reuse target ML models freely. To evaluate DeMistify and demonstrate its applicability, we apply it on 1,511 top mobile apps using on-device ML expertise for several ML services based on their install numbers from Google Play and DeMistify can successfully execute 1250 of them (82.73%). In addition, an in-depth study is conducted to understand the on-device ML ecosystem in the mobile application.",
      "year": 2024,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Pengcheng Ren",
        "Chaoshun Zuo",
        "Xiaofeng Liu",
        "Wenrui Diao",
        "Qingchuan Zhao",
        "Shanqing Guo"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/1e8b05c5b703f443b5470bef2b3aca1b064b1709",
      "pdf_url": "",
      "publication_date": "2024-02-06",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d9588dc5028b7e66b5fec940fd9e31a8e0a6070b",
      "title": "High-Frequency Matters: Attack and Defense for Image-Processing Model Watermarking",
      "abstract": "In recent years, there has been significant advancement in the field of model watermarking techniques. However, the protection of image-processing neural networks remains a challenge, with only a limited number of methods being developed. The objective of these techniques is to embed a watermark in the output images of the target generative network, so that the watermark signal can be detected in the output of a surrogate model obtained through model extraction attacks. This promising technique, however, has certain limits. Analysis of the frequency domain reveals that the watermark signal is mainly concealed in the high-frequency components of the output. Thus, we propose an overwriting attack that involves forging another watermark in the output of the generative network. The experimental results demonstrate the efficacy of this attack in sabotaging existing watermarking schemes for image-processing networks with an almost 100% success rate. To counter this attack, we propose an adversarial framework for the watermarking network. The framework incorporates a specially-designed adversarial training step, where the watermarking network is trained to defend against the overwriting network, thereby enhancing its robustness. Additionally, we observe an overfitting phenomenon in the existing watermarking method, which can render it ineffective. To address this issue, we modify the training process to eliminate the overfitting problem.",
      "year": 2024,
      "venue": "IEEE Transactions on Services Computing",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Chi Liu",
        "Shui Yu",
        "Wanlei Zhou"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/d9588dc5028b7e66b5fec940fd9e31a8e0a6070b",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0e8594a161f1670e939a16699bd5cc8fd2f8335a",
      "title": "QUEEN: Query Unlearning Against Model Extraction",
      "abstract": "Model extraction attacks currently pose a non-negligible threat to the security and privacy of deep learning models. By querying the model with a small dataset and using the query results as the ground-truth labels, an adversary can steal a piracy model with performance comparable to the original model. Two key issues that cause the threat are, on the one hand, accurate and unlimited queries can be obtained by the adversary; on the other hand, the adversary can aggregate the query results to train the model step by step. The existing defenses usually employ model watermarking or fingerprinting to protect the ownership. However, these methods cannot proactively prevent the violation from happening. To mitigate the threat, we propose QUEEN (QUEry unlEarNing) that proactively launches counterattacks on potential model extraction attacks from the very beginning. To limit the potential threat, QUEEN has sensitivity measurement and outputs perturbation that prevents the adversary from training a piracy model with high performance. In sensitivity measurement, QUEEN measures the single query sensitivity by its distance from the center of its cluster in the feature space. To reduce the learning accuracy of attacks, for the highly sensitive query batch, QUEEN applies query unlearning, which is implemented by gradient reverse to perturb the softmax output such that the piracy model will generate reverse gradients to worsen its performance unconsciously. Experiments show that QUEEN outperforms the state-of-the-art defenses against various model extraction attacks with a relatively low cost to the model accuracy. The artifact is publicly available at https://github.com/MaraPapMann/QUEEN.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Lefeng Zhang",
        "Bo Liu",
        "Derui Wang",
        "Wanlei Zhou",
        "Minhui Xue"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/0e8594a161f1670e939a16699bd5cc8fd2f8335a",
      "pdf_url": "http://arxiv.org/pdf/2407.01251",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8f0679e01cb36c995fc5fbc9d364160bd72ce8cd",
      "title": "Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum Neural Networks",
      "abstract": "Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack. In this study, we assess the efficacy of such attacks in the realm of quantum computing. Our findings revealed that model stealing attacks can produce clone models achieving up to 0.9 \u00d7 and 0.99 \u00d7 clone test accuracy when trained using Top-1 and Top-k labels, respectively (k: num_classes). To defend against these attacks, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP). Despite limited success with our defense techniques, it has led to an important discovery: QML models trained on noisy hardwares are naturally resistant to perturbation or obfuscation-based defenses or attacks.",
      "year": 2024,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Satwik Kundu",
        "Debarshi Kundu",
        "Swaroop Ghosh"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/8f0679e01cb36c995fc5fbc9d364160bd72ce8cd",
      "pdf_url": "https://arxiv.org/pdf/2402.11687",
      "publication_date": "2024-02-18",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f3892c3b89b4b6fca4465308fcc9a99388eb019b",
      "title": "QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines",
      "abstract": "Variational quantum circuits (VQCs) have become a powerful tool for implementing Quantum Neural Networks (QNNs), addressing a wide range of complex problems. Well-trained VQCs serve as valuable intellectual assets hosted on cloud-based Noisy Intermediate Scale Quantum (NISQ) computers, making them susceptible to malicious VQC stealing attacks. However, traditional model extraction techniques designed for classical machine learning models encounter challenges when applied to NISQ computers due to significant noise in current devices. In this paper, we introduce QuantumLeak, an effective and accurate QNN model extraction technique from cloud-based NISQ machines. Compared to existing classical model stealing techniques, QuantumLeak improves local VQC accuracy by 4.99%~7.35% across diverse datasets and VQC architectures.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhenxiao Fu",
        "Min Yang",
        "Cheng Chu",
        "Yilun Xu",
        "Gang Huang",
        "Fan Chen"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/f3892c3b89b4b6fca4465308fcc9a99388eb019b",
      "pdf_url": "http://arxiv.org/pdf/2403.10790",
      "publication_date": "2024-03-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3434f582cc9c2818785e2920241d43d932625539",
      "title": "ModelShield: Adaptive and Robust Watermark Against Model Extraction Attack",
      "abstract": "Large language models (LLMs) demonstrate general intelligence across a variety of machine learning tasks, thereby enhancing the commercial value of their intellectual property (IP). To protect this IP, model owners typically allow user access only in a black-box manner, however, adversaries can still utilize model extraction attacks to steal the model intelligence encoded in model generation. Watermarking technology offers a promising solution for defending against such attacks by embedding unique identifiers into the model-generated content. However, existing watermarking methods often compromise the quality of generated content due to heuristic alterations and lack robust mechanisms to counteract adversarial strategies, thus limiting their practicality in real-world scenarios. In this paper, we introduce an adaptive and robust watermarking method (named ModelShield) to protect the IP of LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs to autonomously insert watermarks into their generated content to avoid the degradation of model content. We also propose a robust watermark detection mechanism capable of effectively identifying watermark signals under the interference of varying adversarial strategies. Besides, ModelShield is a plug-and-play method that does not require additional model training, enhancing its applicability in LLM deployments. Extensive evaluations on two real-world datasets and three LLMs demonstrate that our method surpasses existing methods in terms of defense effectiveness and robustness while significantly reducing the degradation of watermarking on the model-generated content.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Kaiyi Pang",
        "Tao Qi",
        "Chuhan Wu",
        "Minhao Bai",
        "Minghu Jiang",
        "Yongfeng Huang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/3434f582cc9c2818785e2920241d43d932625539",
      "pdf_url": "",
      "publication_date": "2024-05-03",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "cefefe1b62baa5ac5067f31e32ee6f5a004b4222",
      "title": "Quantitative analysis of the electromagnetic hybrid nanofluid flow within the gap of two tubes using deep learning neural networks",
      "abstract": "Purpose(1) A mathematical model for the Hybrid nanofluids flow is used as carriers for delivering drugs. (2) The flow conditions are controlled to enable drug-loaded nanofluids to flow through the smaller gap between the two tubes. (3) Hybrid nanofluids (HNFs) made from silver (Ag) and titanium dioxide (TiO2) nanoparticles are analyzed for applications of drug delivery. (Ag) and (TiO2) (NPs) are suitable candidates for cancer treatment due to their excellent biocompatibility, high photoactivity, and low toxicity. (4) The new strategy of artificial neural networks (ANN) is used which is machine-based and more prominent in validation, and comparison with other techniques.Design/methodology/approachThe two Tubes are settled in such a manner that the gap between them is uniform. The Control Volume Finite Element Method; Rk-4 and Artificial Neural Network (ANN).Findings(1) From the obtained results it is observed that the dispersion and distribution of drug-loaded nanoparticles within the body will be improved by the convective motion caused by hybrid nanofluids. The effectiveness and uniformity of drug delivery to target tissues or organs is improved based on the uniform flow and uniform gap. (2) The targeting efficiency of nanofluids is further improved with the addition of the magnetic field. (3) The size of the cylinders, and flow rate, are considered uniform to optimize the drug delivery.Research limitations/implications(1)The flow phenomena is considered laminar, one can use the same idea through a turbulent flow case. (2) The gap is considered uniform and will be interesting if someone extends the idea as non-uniform.Practical implications(1) To deliver drugs to the targeted area, a suitable mathematical model is required. (2) The analysis of hybrid nanofluids (HNFs) derived from silver (Ag) and titanium dioxide (TiO2) nanoparticles is conducted for the purpose of drug delivery. The biocompatibility, high photoactivity, and low toxicity of (Ag) and (TiO2) (NPs) make them ideal candidates for cancer treatment. (3) Machine-based artificial neural networks (ANN) have a new strategy that is more prominent in validation compared to other techniques.Social implicationsThe drug delivery model is a useful strategy for new researchers. (1) They can extend this idea using a non-uniform gap. (2) The flow is considered uniform, the new researchers can extend the idea using a turbulent case. (3) Other hybrid nanofluids flow, in the same model for other industrial usages are possible.Originality/valueAll the obtained results are new. The experimental thermophysical results are used from the existing literature and references are provided.",
      "year": 2024,
      "venue": "Multidiscipline Modeling in Materials and Structures",
      "authors": [
        "Majid Amin",
        "Fuad A. Awwad",
        "Emad A. A. Ismail",
        "Muhammad Ishaq",
        "T. Gul",
        "T. Khan"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/cefefe1b62baa5ac5067f31e32ee6f5a004b4222",
      "pdf_url": "",
      "publication_date": "2024-06-05",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b892ac05369526432384a4cdf1d4d087f8bc45de",
      "title": "Beyond Slow Signs in High-fidelity Model Extraction",
      "abstract": "Deep neural networks, costly to train and rich in intellectual property value, are increasingly threatened by model extraction attacks that compromise their confidentiality. Previous attacks have succeeded in reverse-engineering model parameters up to a precision of float64 for models trained on random data with at most three hidden layers using cryptanalytical techniques. However, the process was identified to be very time consuming and not feasible for larger and deeper models trained on standard benchmarks. Our study evaluates the feasibility of parameter extraction methods of Carlini et al. [1] further enhanced by Canales-Mart\\'inez et al. [2] for models trained on standard benchmarks. We introduce a unified codebase that integrates previous methods and reveal that computational tools can significantly influence performance. We develop further optimisations to the end-to-end attack and improve the efficiency of extracting weight signs by up to 14.8 times compared to former methods through the identification of easier and harder to extract neurons. Contrary to prior assumptions, we identify extraction of weights, not extraction of weight signs, as the critical bottleneck. With our improvements, a 16,721 parameter model with 2 hidden layers trained on MNIST is extracted within only 98 minutes compared to at least 150 minutes previously. Finally, addressing methodological deficiencies observed in previous studies, we propose new ways of robust benchmarking for future model extraction attacks.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Hanna Foerster",
        "Robert D. Mullins",
        "Ilia Shumailov",
        "Jamie Hayes"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/b892ac05369526432384a4cdf1d4d087f8bc45de",
      "pdf_url": "",
      "publication_date": "2024-06-14",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c535f60659724b84d5a2169d434617ba49f005bf",
      "title": "CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) exhibit strong generalization capabilities across diverse tasks and are increasingly deployed on edge devices for efficiency and privacy reasons. However, deploying proprietary LLMs at the edge without adequate protection introduces critical security threats. Attackers can extract model weights and architectures, enabling unauthorized copying and misuse. Even when protective measures prevent full extraction of model weights, attackers may still perform advanced attacks, such as fine-tuning, to further exploit the model. Existing defenses against these threats typically incur significant computational and communication overhead, making them impractical for edge deployment. To safeguard the edge-deployed LLMs, we introduce CoreGuard, a computation- and communication-efficient protection method. CoreGuard employs an efficient protection protocol to reduce computational overhead and minimize communication overhead via a propagation protocol. Extensive experiments show that CoreGuard achieves upper-bound security protection with negligible overhead.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Qinfeng Li",
        "Yangfan Xie",
        "Tianyu Du",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Hao Peng",
        "Xinkui Zhao",
        "Xianwei Zhu",
        "Jianwei Yin",
        "Xuhong Zhang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/c535f60659724b84d5a2169d434617ba49f005bf",
      "pdf_url": "",
      "publication_date": "2024-10-16",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "74ed0e78bba8fda5a72d3db69593f4caeca82559",
      "title": "TPUXtract: An Exhaustive Hyperparameter Extraction Framework",
      "abstract": "Model stealing attacks on AI/ML devices undermine intellectual property rights, compromise the competitive advantage of the original model developers, and potentially expose sensitive data embedded in the model\u2019s behavior to unauthorized parties. While previous research works have demonstrated successful side-channelbased model recovery in embedded microcontrollers and FPGA-based accelerators, the exploration of attacks on commercial ML accelerators remains largely unexplored. Moreover, prior side-channel attacks fail when they encounter previously unknown models. This paper demonstrates the first successful model extraction attack on the Google Edge Tensor Processing Unit (TPU), an off-the-shelf ML accelerator. Specifically, we show a hyperparameter stealing attack that can extract all layer configurations including the layer type, number of nodes, kernel/filter sizes, number of filters, strides, padding, and activation function. Most notably, our attack is the first comprehensive attack that can extract previously unseen models. This is achieved through an online template-building approach instead of a pre-trained ML-based approach used in prior works. Our results on a black-box Google Edge TPU evaluation show that, through obtained electromagnetic traces, our proposed framework can achieve 99.91% accuracy, making it the most accurate one to date. Our findings indicate that attackers can successfully extract various types of models on a black-box commercial TPU with utmost detail and call for countermeasures.",
      "year": 2024,
      "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.",
      "authors": [
        "Ashley Kurian",
        "Anuj Dubey",
        "Ferhat Yaman",
        "Aydin Aysu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/74ed0e78bba8fda5a72d3db69593f4caeca82559",
      "pdf_url": "https://doi.org/10.46586/tches.v2025.i1.78-103",
      "publication_date": "2024-12-09",
      "keywords_matched": [
        "model stealing attack",
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9caf69e7bab1932d0b77dd20dc8f47e1b340135a",
      "title": "Defense against Model Extraction Attack by Bayesian Active Watermarking",
      "abstract": null,
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhenyi Wang",
        "Yihan Wu",
        "Heng Huang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9caf69e7bab1932d0b77dd20dc8f47e1b340135a",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "40d2bc169204b49a9cfa659b5398791ac5860caf",
      "title": "Unveiling the Secrets without Data: Can Graph Neural Networks Be Exploited through Data-Free Model Extraction Attacks?",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yu-Lin Zhuang",
        "Chuan Shi",
        "Mengmei Zhang",
        "Jinghui Chen",
        "Lingjuan Lyu",
        "Pan Zhou"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/40d2bc169204b49a9cfa659b5398791ac5860caf",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0beb8f26d688ce182cea25c361e0c3311b1afb69",
      "title": "Inversion-Guided Defense: Detecting Model Stealing Attacks by Output Inverting",
      "abstract": "Model stealing attacks involve creating copies of machine learning models that have similar functionalities to the original model without proper authorization. Such attacks raise significant concerns about the intellectual property of the machine learning models. Nonetheless, current defense mechanisms against such attacks tend to exhibit certain drawbacks, notably in terms of utility, and robustness. For example, watermarking-based defenses require victim models to be retrained for embedding watermarks, which can potentially impact the main task performance. Moreover, other defenses, especially fingerprinting-based methods, often rely on specific samples like adversarial examples to verify ownership of the target model. These approaches might prove less robust against adaptive attacks, such as model stealing with adversarial training. It remains unclear whether normal examples, as opposed to adversarial ones, can effectively reflect the characteristics of stolen models. To tackle these challenges, we propose a novel method that leverages a neural network as a decoder to inverse the suspicious model\u2019s outputs. Inspired by model inversion attacks, we argue that this decoding process will unveil hidden patterns inherent in the original outputs of the suspicious model. Drawing from these decoding outcomes, we calculate specific metrics to determine the legitimacy of the suspicious models. We validate the efficacy of our defense technique against diverse model stealing attacks, specifically within the domain of classification tasks based on deep neural networks.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Shuai Zhou",
        "Tianqing Zhu",
        "Dayong Ye",
        "Wanlei Zhou",
        "Wei Zhao"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0beb8f26d688ce182cea25c361e0c3311b1afb69",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d3b74a3fdb14c606b9daaf61c5409f7d58c3b3aa",
      "title": "Design of Time-Delay Convolutional Neural Networks(TDCNN) Model for Feature Extraction for Side-Channel Attacks",
      "abstract": ": This work explores a novel method of SCA profiling to address compatibility problems and strengthen Deep Learning (DL) models. Convolutional Neural Networks are proposed in this research as a countermeasure to misalignment-focused countermeasures. \u201dTime-Delay Convolutional Neural Networks\u201d (TDCNN) is more accurate than \u201dConvolutional Neural Network,\u201d yet it\u2019s still acceptable. It\u2019s true that TDCNNs are neural networks based on convolution learned on single spatial information, just as side-channel tracings. However, given to recent surge in popularity of CNNs, particularly from the year 2012 when CNN framework (\u201dAlexNet\u201d) achieved Image Net Large Scale Visual Recognition Competition which is a notable image detection competition, a novel TDCNN has been termed out in DL literature. Currently, it needs to employ the characteristics related to CNN design, including declaring that one input feature equals 1 for instance, to establish a TDCNN in the most widely used DL libraries.",
      "year": 2024,
      "venue": "International Journal of Computing and Digital Systems",
      "authors": [
        "Amjed Abbas Ahmed",
        "Mohammad Kamrul Hasan",
        "Shahrul Azman Mohd Noah",
        "Azana Hafizah Aman"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/d3b74a3fdb14c606b9daaf61c5409f7d58c3b3aa",
      "pdf_url": "https://journal.uob.edu.bh:443/bitstream/123456789/5437/3/IJCDS160127_1570996366.pdf",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "94621729b6ca9e811d8a052dc3457e98df457676",
      "title": "Fully Exploiting Every Real Sample: SuperPixel Sample Gradient Model Stealing",
      "abstract": "Model stealing (MS) involves querying and observing the output of a machine learning model to steal its capabilities. The quality of queried data is crucial, yet obtaining a large amount of real data for MS is often challenging. Recent works have reduced reliance on real data by using generative models. However, when high-dimensional query data is required, these methods are impractical due to the high costs of querying and the risk of model collapse. In this work, we propose using sample gradients (SG) to enhance the utility of each real sample, as SG provides crucial guidance on the decision boundaries of the victim model. However, utilizing SG in the model stealing scenario faces two challenges: 1. Pixel-level gradient estimation requires ex-tensive query volume and is susceptible to defenses. 2. The estimation of sample gradients has a significant variance. This paper proposes Superpixel Sample Gradient stealing (SPSG) for model stealing under the constraint of limited real samples. With the basic idea of imitating the victim model's low-variance patch-level gradients instead ofpixel-level gradients, SPSG achieves efficient sample gradient es-timation through two steps. First, we perform patch-wise perturbations on query images to estimate the average gradient in different regions of the image. Then, we filter the gradients through a threshold strategy to reduce variance. Exhaustive experiments demonstrate that, with the same number of real samples, SPSG achieves accuracy, agreements, and adversarial success rate significantly surpassing the current state-of-the-art MS methods. Codes are available at https://github.com/zyI123456aBISPSG_attack.",
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Yunlong Zhao",
        "Xiaoheng Deng",
        "Yijing Liu",
        "Xin-jun Pei",
        "Jiazhi Xia",
        "Wei Chen"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/94621729b6ca9e811d8a052dc3457e98df457676",
      "pdf_url": "https://arxiv.org/pdf/2406.18540",
      "publication_date": "2024-05-18",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "title": "Model Stealing for Any Low-Rank Language Model",
      "abstract": "Model stealing, where a learner tries to recover an unknown model via carefully chosen queries, is a critical problem in machine learning, as it threatens the security of proprietary models and the privacy of data they are trained on. In recent years, there has been particular interest in stealing large language models (LLMs). In this paper, we aim to build a theoretical understanding of stealing language models by studying a simple and mathematically tractable setting. We study model stealing for Hidden Markov Models (HMMs), and more generally low-rank language models. We assume that the learner works in the conditional query model, introduced by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient algorithm in the conditional query model, for learning any low-rank distribution. In other words, our algorithm succeeds at stealing any language model whose output distribution is low-rank. This improves upon the previous result which also requires the unknown distribution to have high \u201cfidelity\u201d \u2013 a property that holds only in restricted cases. There are two key insights behind our algorithm: First, we represent the conditional distributions at each timestep by constructing barycentric spanners among a collection of vectors of exponentially large dimension. Second, for sampling from our representation, we iteratively solve a sequence of convex optimization problems that involve projection in relative entropy to prevent compounding of errors over the length of the sequence. This is an interesting example where, at least theoretically, allowing a machine learning model to solve more complex problems at inference time can lead to drastic improvements in its performance.",
      "year": 2024,
      "venue": "Symposium on the Theory of Computing",
      "authors": [
        "Allen Liu",
        "Ankur Moitra"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "pdf_url": "",
      "publication_date": "2024-11-12",
      "keywords_matched": [
        "stealing language model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "389f1fad962f58799327a0112526452d2da5157d",
      "title": "TEXTKNOCKOFF: KNOCKOFF NETS FOR STEALING FUNCTIONALITY OF TEXT SENTIMENT MODELS",
      "abstract": "Most commercial machine learning models today are designed to require significant amounts of time, money, and human effort. Therefore, intrinsic information about the model (such as architecture, hyperparameters, and training data) needs to be kept confidential. These models are referred to as black boxes, and there is an increasing amount of research focused on both attacking and protecting them. Recent publications have often concentrated on the field of computer vision; in contrast, there is still relatively little research on methods for attacking black box models with textual data. This article introduces a research method for extracting the functionality of a black box model in the task of text sentiment analysis. The method has been effectively tested based on random sampling techniques to reconstruct a new model with equivalent functionality to the original model, achieving high accuracy (94.46% compared to 94.92%) and high similarity (96.82%).",
      "year": 2024,
      "venue": "Journal of Science and Technique",
      "authors": [
        "X. Pham"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/389f1fad962f58799327a0112526452d2da5157d",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "knockoff nets"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0219db84513f4c2604ed15fce52c8f810b463ba5",
      "title": "Can't Hide Behind the API: Stealing Black-Box Commercial Embedding Models",
      "abstract": "Embedding models that generate dense vector representations of text are widely used and hold significant commercial value. Companies such as OpenAI and Cohere offer proprietary embedding models via paid APIs, but despite being\"hidden\"behind APIs, these models are not protected from theft. We present, to our knowledge, the first effort to\"steal\"these models for retrieval by training thief models on text-embedding pairs obtained from the APIs. Our experiments demonstrate that it is possible to replicate the retrieval effectiveness of commercial embedding models with a cost of under $300. Notably, our methods allow for distilling from multiple teachers into a single robust student model, and for distilling into presumably smaller models with fewer dimension vectors, yet competitive retrieval effectiveness. Our findings raise important considerations for deploying commercial embedding models and suggest measures to mitigate the risk of model theft.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "M. Tamber",
        "Jasper Xian",
        "Jimmy Lin"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/0219db84513f4c2604ed15fce52c8f810b463ba5",
      "pdf_url": "",
      "publication_date": "2024-06-13",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a55cc1ebaab3de336386f292cc4028f6e5586ac0",
      "title": "Trained to Leak: Hiding Trojan Side-Channels in Neural Network Weights",
      "abstract": "Applications driven by neural networks (NNs) have been advancing various work flows in industries and everyday life. FPGA accelerators are a popular low latency solution for NN inference in the cloud, edge devices and critical systems, offering efficiency and availability. Additionally, cloud FPGAs enable maximizing resource utilization by sharing one device with multiple users in a multi-tenant scenario. However, due to the high energy costs, hardware requirements and time consumption for training an NN, using machine learning services or acquiring pre-trained models has become increasingly popular. This creates a trust issue that potentially puts the privacy of the user at risk. Specifically, malicious mechanisms may be hidden in the weights of the NN. We show that by manipulating the training process of an NN, the power consumption and resulting leakage can be manipulated to correlate strongly with the networks output, allowing the reliable recovery of the classification results through remote power side-channel analysis. In comparison to power traces from a benign model, which leak less information, our trained-in Trojan Side-Channel enhances the credibility and reliability of the stolen outputs, making them more usable and valuable for malicious intent.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Vincent Meyers",
        "Michael Hefenbrock",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a55cc1ebaab3de336386f292cc4028f6e5586ac0",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f63f7d623e1738fbc314143a1ad1812045caffff",
      "title": "Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are recognized as potent tools for processing real-world data organized in graph structures. Especially inductive GNNs, which allow for the processing of graph-structured data without relying on predefined graph structures, are becoming increasingly important in a wide range of applications. As such these networks become attractive targets for model-stealing attacks where an adversary seeks to replicate the functionality of the targeted network. Significant efforts have been devoted to developing model-stealing attacks that extract models trained on images and texts. However, little attention has been given to stealing GNNs trained on graph data. This paper identifies a new method of performing unsupervised model-stealing attacks against inductive GNNs, utilizing graph contrastive learning and spectral graph augmentations to efficiently extract information from the targeted model. The new type of attack is thoroughly evaluated on six datasets and the results show that our approach outperforms the current state-of-the-art by Shen et al. (2021). In particular, our attack surpasses the baseline across all benchmarks, attaining superior fidelity and downstream accuracy of the stolen model while necessitating fewer queries directed toward the target model.",
      "year": 2024,
      "venue": "European Conference on Artificial Intelligence",
      "authors": [
        "Marcin Podhajski",
        "Jan Dubi'nski",
        "Franziska Boenisch",
        "Adam Dziedzic",
        "A. Pregowska",
        "Tomasz P. Michalak"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f63f7d623e1738fbc314143a1ad1812045caffff",
      "pdf_url": "",
      "publication_date": "2024-05-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2b28136746a29ff698f57106c292d0d6e0181629",
      "title": "Efficient Data-Free Model Stealing with Label Diversity",
      "abstract": "Machine learning as a Service (MLaaS) allows users to query the machine learning model in an API manner, which provides an opportunity for users to enjoy the benefits brought by the high-performance model trained on valuable data. This interface boosts the proliferation of machine learning based applications, while on the other hand, it introduces the attack surface for model stealing attacks. Existing model stealing attacks have relaxed their attack assumptions to the data-free setting, while keeping the effectiveness. However, these methods are complex and consist of several components, which obscure the core on which the attack really depends. In this paper, we revisit the model stealing problem from a diversity perspective and demonstrate that keeping the generated data samples more diverse across all the classes is the critical point for improving the attack performance. Based on this conjecture, we provide a simplified attack framework. We empirically signify our conjecture by evaluating the effectiveness of our attack, and experimental results show that our approach is able to achieve comparable or even better performance compared with the state-of-the-art method. Furthermore, benefiting from the absence of redundant components, our method demonstrates its advantages in attack efficiency and query budget.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yiyong Liu",
        "Rui Wen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2b28136746a29ff698f57106c292d0d6e0181629",
      "pdf_url": "",
      "publication_date": "2024-03-29",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a916a716b23490f87844b3ff44cb2d18284b2ac6",
      "title": "Efficient Model Stealing Defense with Noise Transition Matrix",
      "abstract": null,
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Dong-Dong Wu",
        "Chilin Fu",
        "Weichang Wu",
        "Wenwen Xia",
        "Xiaolu Zhang",
        "Jun Zhou",
        "Min-Ling Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a916a716b23490f87844b3ff44cb2d18284b2ac6",
      "pdf_url": "",
      "publication_date": "2024-06-16",
      "keywords_matched": [
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4595c8a7e1571c9974b2393ed127f05fabe72164",
      "title": "Model Stealing Detection for IoT Services Based on Multidimensional Features",
      "abstract": "Model stealing (MS) attacks pose a significant security concern for machine learning models on cloud platforms, as they can reconstruct a substitute model with limited effort to evade ownership. While detection-based methods show promise in preventing MS attacks, they often face practical challenges. Specifically, setting an appropriate threshold to distinguish malicious features from benign ones is a difficult task, often leading to a tradeoff between false alarm rates and detection accuracy. To address this challenge, we design a multidimensional feature extraction-and-distinction scheme called MED. It is achieved through a two-layer optimization: 1) the inner layer of extraction to maximize the difference of extracted multidimensional features between attack and benign samples and 2) the outer layer of distinction to maximize the accuracy of distinguishing malicious features automatically. Recognizing that different MS attacks result in varied features, we design a group of feature extraction functions in the inner layer optimization, which addresses the limitations of single-feature-based detection methods. Further, we employ three differently characterized models for distinction, enabling MED to distinguish different types of malicious features. Comprehensive experiments are conducted to evaluate the effectiveness of the proposed scheme: MED can detect all types of MS attacks with no more than 100 samples, with an average detection rate greater than 0.99.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xinjing Liu",
        "Taifeng Liu",
        "Hao Yang",
        "Jiakang Dong",
        "Zuobin Ying",
        "Zhuo Ma"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4595c8a7e1571c9974b2393ed127f05fabe72164",
      "pdf_url": "",
      "publication_date": "2024-12-15",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a6854f91dbad0b39a0289872ac838545a9c18225",
      "title": "Stealing Watermarks of Large Language Models via Mixed Integer Programming",
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM\u2019s parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
      "year": 2024,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a6854f91dbad0b39a0289872ac838545a9c18225",
      "pdf_url": "",
      "publication_date": "2024-12-09",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f2109f7f2412308738e03a60f79946cb1ad2aa7a",
      "title": "Alignment-Aware Model Extraction Attacks on Large Language Models",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zi Liang",
        "Qingqing Ye",
        "Yanyun Wang",
        "Sen Zhang",
        "Yaxin Xiao",
        "Ronghua Li",
        "Jianliang Xu",
        "Haibo Hu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f2109f7f2412308738e03a60f79946cb1ad2aa7a",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "87004d053c0c2b0f91c293ef26d817d5a05e017c",
      "title": "Exploring Zero-Day Attacks on Machine Learning and Deep Learning Algorithms",
      "abstract": "In the rapidly evolving field of artificial intelligence, machine learning (ML) and deep learning (DL) algorithms have emerged as powerful tools for solving complex problems in various domains, including cyber security. However, as these algorithms become increasingly prevalent, they also face new security challenges. One of the most significant of these challenges is the threat of zero-day attacks, which exploit unknown and unpredictable vulnerabilities in the algorithms or the data they process. \nThis paper provides a comprehensive overview of zero-day attacks on ML/DL algorithms, exploring their types, causes, effects, and potential countermeasures. The paper begins by introducing the concept and definition of zero-day attacks, providing a clear understanding of this emerging threat. It then reviews the existing research on zero-day attacks on ML/DL algorithms, focusing on three main categories: data poisoning attacks, adversarial input attacks, and model stealing attacks. Each of these attack types poses unique challenges and requires specific countermeasures. \nThe paper also discusses the potential impacts and risks of these attacks on various application domains. For instance, in facial expression recognition, an adversarial input attack could lead to misclassification of emotions, with serious implications for user experience and system integrity. In object classification, a data poisoning attack could cause the algorithm to misidentify critical objects, potentially endangering human lives in applications like autonomous driving. In satellite intersection recognition, a model stealing attack could compromise national security by revealing sensitive information. \nFinally, the paper presents some possible protection methods against zero-day attacks on ML/DL algorithms. These include anomaly detection techniques to identify unusual patterns in the data or the algorithm\u2019s behaviour, model verification and validation methods to ensure the algorithm\u2019s correctness and robustness, federated learning approaches to protect the privacy of the training data, and differential privacy techniques to add noise to the data or the algorithm\u2019s outputs to prevent information leakage. \nThe paper concludes by highlighting some open issues and future directions for research in this area, emphasizing the need for ongoing efforts to secure ML/DL algorithms against zero-day attacks.",
      "year": 2024,
      "venue": "European Conference on Cyber Warfare and Security",
      "authors": [
        "Marie Kov\u00e1\u0159ov\u00e1"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/87004d053c0c2b0f91c293ef26d817d5a05e017c",
      "pdf_url": "https://papers.academic-conferences.org/index.php/eccws/article/download/2310/2134",
      "publication_date": "2024-06-21",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "be6dc29e9773c5242b0a44877df60af0feae9adb",
      "title": "Sample Correlation for Fingerprinting Deep Face Recognition",
      "abstract": "Face recognition has witnessed remarkable advancements in recent years, thanks to the development of deep learning techniques. However, an off-the-shelf face recognition model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting, as a model stealing detection method, aims to verify whether a suspect model is stolen from the victim model, gaining more and more attention nowadays. Previous methods always utilize transferable adversarial examples as the model fingerprint, but this method is known to be sensitive to adversarial defense and transfer learning techniques. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-JC that selects JPEG compressed samples as model inputs and calculates the correlation matrix among their model outputs. Extensive results validate that SAC successfully defends against various model stealing attacks in deep face recognition, encompassing face verification and face emotion recognition, exhibiting the highest performance in terms of AUC, p-value and F1 score. Furthermore, we extend our evaluation of SAC-JC to object recognition datasets including Tiny-ImageNet and CIFAR10, which also demonstrates the superior performance of SAC-JC to previous methods. The code will be available at https://github.com/guanjiyang/SAC_JC.",
      "year": 2024,
      "venue": "International Journal of Computer Vision",
      "authors": [
        "Jiyang Guan",
        "Jian Liang",
        "Yanbo Wang",
        "R. He"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/be6dc29e9773c5242b0a44877df60af0feae9adb",
      "pdf_url": "",
      "publication_date": "2024-10-25",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "94dd5cd82b6a082dff3dd697d63191325cf6830b",
      "title": "Knowledge Distillation-Based Model Extraction Attack using Private Counterfactual Explanations",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Fatima Ezzeddine",
        "Omran Ayoub",
        "Silvia Giordano"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/94dd5cd82b6a082dff3dd697d63191325cf6830b",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b3744d928fcd84b7e2296d5983ba199a300955e0",
      "title": "Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices",
      "abstract": "With growing popularity, deep learning (DL) models are becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large models. Most of those DL models are proprietary to the companies who thus strive to keep their private models safe from the model extraction attack (MEA), whose aim is to steal the model by training surrogate models. Nowadays, companies are inclined to offload the models from central servers to edge/endpoint devices. As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim model and obtain various pieces of the model information, such as the model architecture (MA) and image dimension (ID). Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and defensive sides in that they may learn which pieces of information exposed by SCA are more important than the others. Our analysis additionally reveals that by grasping the victim model information from SCA, MEA can get highly effective and successful even without any prior knowledge of the model. Finally, to evince the practicality of our analysis results, we empirically apply SCA, and subsequently, carry out MEA under realistic threat assumptions. The results show up to 5.8 times better performance than when the adversary has no model information about the victim model.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Younghan Lee",
        "Sohee Jun",
        "Yungi Cho",
        "Woorim Han",
        "Hyungon Moon",
        "Y. Paek"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/b3744d928fcd84b7e2296d5983ba199a300955e0",
      "pdf_url": "",
      "publication_date": "2024-03-05",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1abdfdd7cd2d325e09b82d4d3a5dee97000d27a0",
      "title": "Layer Sequence Extraction of Optimized DNNs Using Side-Channel Information Leaks",
      "abstract": "Deep neural network (DNN) intellectual property (IP) models must be kept undisclosed to avoid revealing trade secrets. Recent works have devised machine learning techniques that leverage on side-channel information leakage of the target platform to reverse engineer DNN architectures. However, these works fail to perform successful attacks on DNNs that have undergone performance optimizations (i.e., operator fusion) using DNN compilers, e.g., Apache tensor virtual machine (TVM). We propose a two-phase attack framework to infer the layer sequences of optimized DNNs through side-channel information leakage. In the first phase, we use a recurrent network with multihead attention components to learn the intra and interlayer fusion patterns from GPU traces of TVM-optimized DNNs, in order to accurately predict the operation distribution. The second phase uses a model to learn the run-time temporal correlations between operations and layers, which enables the prediction of layer sequence. An encoding strategy is proposed to overcome the convergence issues faced by existing learning-based methods when inferring the layer sequences of optimized DNNs. Extensive experiments show that our learning-based framework outperforms state-of-the-art DNN model extraction techniques. Our framework is also the first to effectively reverse engineer both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) using side-channel leakage.",
      "year": 2024,
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": [
        "Yidan Sun",
        "Guiyuan Jiang",
        "Xinwang Liu",
        "Peilan He",
        "Siew-Kei Lam"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1abdfdd7cd2d325e09b82d4d3a5dee97000d27a0",
      "pdf_url": "",
      "publication_date": "2024-10-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c0aedde76f74f8a7db4568d688b1baf921499acf",
      "title": "Prediction of Electromagnetic Field Exposure at 20\u2013100 GHz for Clothed Human Body Using an Adaptively Reconfigurable Architecture Neural Network With Weight Analysis (RAWA-NN) Framework",
      "abstract": "In the context of forthcoming sixth-generation (6G) wireless communication, the sub-terahertz and terahertz frequency spectrum are anticipated. At such high frequencies, electromagnetic field (EMF) exposure assessment becomes significantly challenging, requiring substantial computational resources. This article is the first to utilize machine learning (ML) to predict EMF exposure levels for the clothed human body at 20\u2013100 GHz, including temperature rises and absorbed power density (APD) at the exposed skin surface. To predict the EMF exposure, a reconfigurable architecture neural network with weight analysis (RAWA-NN) framework is proposed. This framework is based on the deep neural network (DNN) integrating the proposed weights-analyzer module and optimization module. The proposed novel framework streamlines the training process and reduces training time, while simultaneously adaptively optimizes the hyperparameters (hidden layers and hidden sizes) without the necessity for manual intervention during training and optimization. The model was trained using 70% of forearm data, with the remaining data for testing. Data from other body parts, such as the abdomen and quadriceps, was used to validate the model generalization. Compared to conventional dosimetry analysis, relative difference (RD) across various parameters remains below 2.6% across various parameters, for the same body part of the forearm, and below 9.5% for other body parts. There is an approximate four orders of magnitude improvement in assessment speed.",
      "year": 2024,
      "venue": "IEEE Transactions on Antennas and Propagation",
      "authors": [
        "Ming Yao",
        "Zhaohui Wei",
        "Kun Li",
        "Gert Fr\u00f8lund Pedersen",
        "Shuai Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c0aedde76f74f8a7db4568d688b1baf921499acf",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "aee0bc1bc011d8da14aa209d0af984a9cc6b227f",
      "title": "Streamlining DNN Obfuscation to Defend Against Model Stealing Attacks",
      "abstract": "Side-channel-based Deep Neural Network (DNN) model stealing has become a major concern with the advent of learning-based attacks. In respond to this threat, defence mechanisms have been presented to obfuscate the DNN execution, making it difficult to infer the correlation between side-channel information and DNN architecture. However, state-of-the-art (SOTA) DNN obfuscation is time-consuming, requires expert-level changes in existing DNN compilers (e.g., Tensor Virtual Machine (TVM)), and often relies on prior knowledge of the attack models. In this work, we study the impact of various obfuscation levels on the defence effectiveness, and present a streamlined DNN obfuscation process that is extremely fast and is agnostic to any attack models. Our study reveals that by just modifying the scheduling of DNN operations on the GPU, we can achieve comparable defense performance as the SOTA in an attack agnostic manner. We also propose a simple algorithm that determines an effective scheduling configuration for mitigating DNN model stealing at a fraction of a time required by SOTA obfuscation methods. Our method can be easily integrated into existing DNN compilers as a security feature, even by non-experts, to protect their DNN against side-channel attacks.",
      "year": 2024,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Yidan Sun",
        "Siew-Kei Lam",
        "Guiyuan Jiang",
        "Peilan He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/aee0bc1bc011d8da14aa209d0af984a9cc6b227f",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "model stealing attack",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2df09db143feb7aa0fc30ac548f0aaa4c628c3cd",
      "title": "Stealthy Imitation: Reward-guided Environment-free Policy Stealing",
      "abstract": "Deep reinforcement learning policies, which are integral to modern control systems, represent valuable intellectual property. The development of these policies demands considerable resources, such as domain expertise, simulation fidelity, and real-world validation. These policies are potentially vulnerable to model stealing attacks, which aim to replicate their functionality using only black-box access. In this paper, we propose Stealthy Imitation, the first attack designed to steal policies without access to the environment or knowledge of the input range. This setup has not been considered by previous model stealing methods. Lacking access to the victim's input states distribution, Stealthy Imitation fits a reward model that allows to approximate it. We show that the victim policy is harder to imitate when the distribution of the attack queries matches that of the victim. We evaluate our approach across diverse, high-dimensional control tasks and consistently outperform prior data-free approaches adapted for policy stealing. Lastly, we propose a countermeasure that significantly diminishes the effectiveness of the attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhixiong Zhuang",
        "Maria-Irina Nicolae",
        "Mario Fritz"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/2df09db143feb7aa0fc30ac548f0aaa4c628c3cd",
      "pdf_url": "",
      "publication_date": "2024-05-11",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "title": "Large Language Models for Link Stealing Attacks Against Graph Neural Networks",
      "abstract": "Graph data contains rich node features and unique edge information, which have been applied across various domains, such as citation networks or recommendation systems. Graph Neural Networks (GNNs) are specialized for handling such data and have shown impressive performance in many applications. However, GNNs may contain of sensitive information and susceptible to privacy attacks. For example, link stealing is a type of attack in which attackers infer whether two nodes are linked or not. Previous link stealing attacks primarily relied on posterior probabilities from the target GNN model, neglecting the significance of node features. Additionally, variations in node classes across different datasets lead to different dimensions of posterior probabilities. The handling of these varying data dimensions posed a challenge in using a single model to effectively conduct link stealing attacks on different datasets. To address these challenges, we introduce Large Language Models (LLMs) to perform link stealing attacks on GNNs. LLMs can effectively integrate textual features and exhibit strong generalizability, enabling attacks to handle diverse data dimensions across various datasets. We design two distinct LLM prompts to effectively combine textual features and posterior probabilities of graph nodes. Through these designed prompts, we fine-tune the LLM to adapt to the link stealing attack task. Furthermore, we fine-tune the LLM using multiple datasets and enable the LLM to learn features from different datasets simultaneously. Experimental results show that our approach significantly enhances the performance of existing link stealing attack tasks in both white-box and black-box scenarios. Our method can execute link stealing attacks across different datasets using only a single model, making link stealing attacks more applicable to real-world scenarios.",
      "year": 2024,
      "venue": "IEEE Transactions on Big Data",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Hui Sun",
        "Wanlei Zhou",
        "Philip S. Yu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "pdf_url": "http://arxiv.org/pdf/2406.16963",
      "publication_date": "2024-06-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a968c61fc5a14ce01db2a67b6ab87e30248a0a19",
      "title": "Stealing the Invisible: Unveiling Pre-Trained CNN Models Through Adversarial Examples and Timing Side-Channels",
      "abstract": "Machine learning, with its myriad applications, has become an integral component of numerous AI systems. A common practice in this domain is the use of transfer learning, where a pre-trained model\u2019s architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it is crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present ArchWhisperer, a model fingerprinting attack approach based on the novel observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with model inference times is used to further enhance our attack in terms of attack effectiveness as well as query budget. ArchWhisperer is designed for typical user-level access in remote MLaaS environments and it exploits varying misclassifications of adversarial images across different models to fingerprint several renowned Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures. We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required. We have presented our results over 27 pre-trained models of different CNN and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under 20. This is a marked improvement compared to state-of-the-art works.",
      "year": 2024,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Shubhi Shukla",
        "Manaar Alam",
        "Pabitra Mitra",
        "Debdeep Mukhopadhyay"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/a968c61fc5a14ce01db2a67b6ab87e30248a0a19",
      "pdf_url": "https://arxiv.org/pdf/2402.11953",
      "publication_date": "2024-02-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dca9242b227eebb3f052139dcb6a45c4dcbfde83",
      "title": "\"Yes, My LoRD.\" Guiding Language Model Extraction with Locality Reinforced Distillation",
      "abstract": "Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that I) The convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and II) LoRD can reduce query complexity while mitigating watermark protection through our exploration-based stealing. Extensive experiments validate the superiority of our method in extracting various state-of-the-art commercial LLMs. Our code is available at: https://github.com/liangzid/LoRD-MEA .",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zi Liang",
        "Qingqing Ye",
        "Yanyun Wang",
        "Sen Zhang",
        "Yaxin Xiao",
        "Ronghua Li",
        "Jianliang Xu",
        "Haibo Hu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/dca9242b227eebb3f052139dcb6a45c4dcbfde83",
      "pdf_url": "",
      "publication_date": "2024-09-04",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "55414dc21b091006bf868b28008c9fc30fa38dca",
      "title": "Model Extraction Attack against On-device Deep Learning with Power Side Channel",
      "abstract": "The proliferation of on-device deep learning models in resource-constrained environments has led to significant advancements in privacy-preserving machine learning. However, the deployment of these models also introduces new security challenges, one of which is the vulnerability to model extraction attacks. In this paper, we investigate a novel attack with power side channel to extract on-device deep learning model deployed, which poses a substantial threat to on-device deep learning systems. By carefully monitoring power consumption during inference, an adversary can gain insights into the model\u2019s internal behavior, potentially compromising the model\u2019s intellectual property and sensitive data. Through experiments on a real-world embedded device (Jetson Nano) and various types of deep learning models, we demonstrate that the proposed attack can extract models with high fidelity. Based on experiments, we find that the power side channel-assisted model extraction attack can achieve high attacking success rate, up to 96.7% and 87.5% under close world and open world settings. This research sheds light on the evolving landscape of security threats in the context of on-device DL and provides valuable insights into safeguarding these models from potential adversaries.",
      "year": 2024,
      "venue": "IEEE International Symposium on Quality Electronic Design",
      "authors": [
        "Jiali Liu",
        "Han Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/55414dc21b091006bf868b28008c9fc30fa38dca",
      "pdf_url": "",
      "publication_date": "2024-04-03",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dfbbf3fa36cd03aa41d4170ba672e332f8295bd0",
      "title": "Unveiling Intellectual Property Vulnerabilities of GAN-Based Distributed Machine Learning through Model Extraction Attacks",
      "abstract": "Generative Adversarial Networks (GANs), as a cornerstone of artificial intelligence (AI), are widely recognized as the intellectual property (IP) of their owners, given the sensitivity of the training data and the commercial value tied to the models. Model extraction attacks, which aim to steal well-trained proprietary models, pose a significant threat to model IP. Nevertheless, current research predominately focuses on the context of machine learning as a service (MLaaS), where the emphasis lies in understanding the attack knowledge acquired through black-box API queries. This restricted perspective exposes a critical gap in investigating model extraction attacks within realistic distributed settings for generative tasks. In this work, we present the first investigation into model extraction attacks against GANs in distributed settings. We provide a comprehensive attack taxonomy, considering three different levels of knowledge the adversary can obtain in practice. Based on it, we introduce a novel model extraction attack named MoEx, which focuses on the GAN-based distributed learning scenario, i.e., Multi-Discriminator GANs, a typical asymmetric distributed setting. MoEx uses the objective function simulation, leveraging data exchanged during the learning process, to approximate the GAN generator owned by the server. We define two attack goals for MoEx, fidelity extraction and accuracy extraction. Then we comprehensively evaluate the effectiveness of MoEx's two goals with real-world datasets. Our results demonstrate its robust capabilities in extracting generators with high fidelity and accuracy compared with existing methods.",
      "year": 2024,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Mengyao Ma",
        "Shuofeng Liu",
        "M.A.P. Chamikara",
        "Mohan Baruwal Chhetri",
        "Guangdong Bai"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/dfbbf3fa36cd03aa41d4170ba672e332f8295bd0",
      "pdf_url": "",
      "publication_date": "2024-10-21",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fbfb0ed07c0c8c84a959e0f4de6db90d5d65772f",
      "title": "Efficient and Effective Model Extraction",
      "abstract": "Model extraction aims to steal a functionally similar copy from a machine learning as a service (MLaaS) API with minimal overhead, typically for illicit profit or as a precursor to further attacks, posing a significant threat to the MLaaS ecosystem. However, recent studies have shown that model extraction is highly inefficient, particularly when the target task distribution is unavailable. In such cases, even substantially increasing the attack budget fails to produce a sufficiently similar replica, reducing the adversary\u2019s motivation to pursue extraction attacks. In this paper, we revisit the elementary design choices throughout the extraction lifecycle. We propose an embarrassingly simple yet dramatically effective algorithm, Efficient and Effective Model Extraction (E3), focusing on both query preparation and training routine. E3 achieves superior generalization compared to state-of-the-art methods while minimizing computational costs. For instance, with only 0.005\u00d7 the query budget and less than 0.2\u00d7 the runtime, E3 outperforms classical generative model based data-free model extraction by an absolute accuracy improvement of over 50% on CIFAR-10. Our findings underscore the persistent threat posed by model extraction and suggest that it could serve as a valuable benchmarking algorithm for future security evaluations.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Hongyu Zhu",
        "Wentao Hu",
        "Sichu Liang",
        "Fangqi Li",
        "Wenwen Wang",
        "Shilin Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/fbfb0ed07c0c8c84a959e0f4de6db90d5d65772f",
      "pdf_url": "",
      "publication_date": "2024-09-21",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7c1924e7f6a9c335ebb83c50996fa93e4bb62bcb",
      "title": "Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples",
      "abstract": "We introduce Adversarial Sparse Teacher (AST), a robust defense method against distillation-based model stealing attacks. Our approach trains a teacher model using adversarial examples to produce sparse logit responses and increase the entropy of the output distribution. Typically, a model generates a peak in its output corresponding to its prediction. By leveraging adversarial examples, AST modifies the teacher model\u2019s original response, embedding a few altered logits into the output, while keeping the primary response slightly higher. Concurrently, all remaining logits are elevated to further increase the output distribution\u2019s entropy. All these complex manipulations are performed using an optimization function with our proposed Exponential Predictive Divergence (EPD) loss function. EPD allows us to maintain higher entropy levels compared to traditional KL divergence, effectively confusing attackers. Experiments on the CIFAR-10 and CIFAR-100 datasets demonstrate that AST outperforms state-of-the-art methods, providing effective defense against model stealing, while preserving high accuracy. The source codes are publicly available at https://github.com/codeofanon/AdversarialSparseTeacher",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "E. Y\u0131lmaz",
        "H. Keles"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/7c1924e7f6a9c335ebb83c50996fa93e4bb62bcb",
      "pdf_url": "",
      "publication_date": "2024-03-08",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "15a81a4579949690c706f1bd9bbdda93681c35c3",
      "title": "Poisoning-Free Defense Against Black-Box Model Extraction",
      "abstract": "Recent research has shown that an adversary can use a surrogate model to steal the functionality of a target deep learning model even under the black-box condition and without data curation, while the existing defense mainly relies on API poisoning to disturb the surrogate training. Unfortunately, due to poisoning, the defense is achieved at the price of fidelity loss, sacrificing the interests of honest users. To solve this problem, we propose an Adversarial Fine-Tuning (AdvFT) framework, incorporating the generative adversarial network (GAN) structure that disturbs the feature representations of out-of-distribution (OOD) queries while preserving those of in-distribution (ID) ones, circumventing the need for OOD sample collection and API poisoning. Extensive experiments verify the effectiveness of the proposed framework. Code is available at github.com/Hatins/AdvFT.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Haitian Zhang",
        "Guang Hua",
        "Wen Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/15a81a4579949690c706f1bd9bbdda93681c35c3",
      "pdf_url": "",
      "publication_date": "2024-04-14",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4fa2c8c744b37ba9b227cb3de8cdec448c09c1d7",
      "title": "Side-Channel Analysis of OpenVINO-based Neural Network Models",
      "abstract": "Embedded devices with neural network accelerators offer great versatility for their users, reducing the need to use cloud-based services. At the same time, they introduce new security challenges in the area of hardware attacks, the most prominent being side-channel analysis (SCA). It was shown that SCA can recover model parameters with a high accuracy, posing a threat to entities that wish to keep their models confidential. In this paper, we explore the susceptibility of quantized models implemented in OpenVINO, an embedded framework for deploying neural networks on embedded and Edge devices. We show that it is possible to recover model parameters with high precision, allowing the recovered model to perform very close to the original one. Our experiments on GoogleNet v1 show only a 1% difference in the Top 1 and a 0.64% difference in the Top 5 accuracies.",
      "year": 2024,
      "venue": "ARES",
      "authors": [
        "Dirmanto Jap",
        "J. Breier",
        "Zdenko Lehock'y",
        "S. Bhasin",
        "Xiaolu Hou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4fa2c8c744b37ba9b227cb3de8cdec448c09c1d7",
      "pdf_url": "",
      "publication_date": "2024-07-23",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a54e2156b2f83a7c5c70c524862ce35e0a696d1f",
      "title": "Dynamic behaviors analysis of fraction-order neural network under memristive electromagnetic induction",
      "abstract": "The dynamic behaviors of coupled neurons with different mathematical representations have received more and more attention in recent years. The coupling between heterogeneous neurons can show richer dynamic phenomena, which is of great significance in understanding the function of the human brain. In this paper, we present a fraction-order heterogeneous network with three neurons that is built by coupling a FN neuron with two HR neurons. Complex electromagnetic surroundings have meaningful physical impacts on the electrical activities of neurons. To imitate the effects of electromagnetic induction on the three-neuron heterogeneous network, we introduce a fraction-order locally active memristor in the neural network. The characteristics of this memristor are been carefully analyzed by pinched hysteresis loops and its locally active characteristic is been proved by the power-off plot and the DC v-i plot. Then, the parameter-dependent dynamic activities are investigated numerically using several dynamical analysis methods, such as the phase diagrams, bifurcation diagrams, Lyapunov exponent spectrums, and attraction basins. Furthermore, abundant dynamic behaviors, including coexisting activities, anti-monotonicity phenomena, transient chaos and firing patterns are revealed in this network, which support further investigation of the firing patterns of the human brain. In particular, complex dynamics, including coexisting attractors, anti-monotonicity, and firing patterns, can be influenced by the order and strength of electrical synaptic coupling and electromagnetic induction. The control of the bistable state can be realized through the time feedback control method, so that the bistable state can be transformed into an ideal monostable state. The study of the fraction-order memristive neural network may expand the field of view for understanding the collective behaviors of neurons. Finally, based on the ARM platform, we give a digital implementation of the fraction-order memristive neural network, which can verify the consistency with the numerical simulation results. In the future, we will explore more interesting memristive neural networks and research different types of methods to control the firing behaviors of the networks.",
      "year": 2024,
      "venue": "Acta Physica Sinica",
      "authors": [
        "Dawei Ding",
        "Mouyuan Wang",
        "Jin Wang",
        "Zongli Yang",
        "Yan Niu",
        "Wei Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/a54e2156b2f83a7c5c70c524862ce35e0a696d1f",
      "pdf_url": "https://wulixb.iphy.ac.cn/pdf-content/10.7498/aps.73.20231792.pdf",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f1cb6613d9ed25be4f17f758ba78d0991860aa4b",
      "title": "Analysis and Impact of Electromagnetic Field Leakage in WPT on Central Nervous System: A Neural Network Approach",
      "abstract": "The potential for wireless power transfer (WPT) technologies to transform the charging capabilities of a wide range of applications, from consumer electronics to medical devices has attracted a lot of attention. But uncertainties remain about how these technology's electromagnetic field (EMF) leakage affects human health, especially the central nervous system (CNS). To find a way to assess and forecast all of these consequences, this study explores at the possible effects of EMF leakage from WPT systems on the central nervous system (CNS) and suggests a distinctive approach that employs the applications for neural networks. Using real- world data and computational models, this study will produce an accurate model of EMF absorption and propagation in biological tissues, with a specific focus on the central nervous system. By focusing insights into potential risks to health and informing the emergence of requirements which assure the safe implementation of wireless power technologies, the research will enhance our understanding of the connections between EMF exposure and the central nervous system. The ultimate objective of this study is to encourage safer and more sustainable wireless charging technologies by enhancing the accuracy and efficiency of risk assessment in circumstances of EMF exposure from WPT systems through the implementation of Machine Learning (ML) strategies",
      "year": 2024,
      "venue": "2024 IEEE 4th International Conference on Sustainable Energy and Future Electric Transportation (SEFET)",
      "authors": [
        "Garima Shukla",
        "Vanshaj Awasthi",
        "Prafull Goswami",
        "Sofia Singh"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/f1cb6613d9ed25be4f17f758ba78d0991860aa4b",
      "pdf_url": "",
      "publication_date": "2024-07-31",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "21c62f2ef68323099480c80318e48baae8b9098f",
      "title": "GuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack",
      "abstract": "Large language model (LLM) companies provide Embedding as a Service (EaaS) to assist the individual in efficiently dealing with downstream tasks such as text classification and recommendation. However, recent works reveal the risk of the model stealing attack, posing a financial threat to EaaS providers. To protect the copyright of EaaS, we propose GuardEmb, a dynamic embedding watermarking method, striking a balance between enhancing watermark detectability and preserving embedding functionality. Our approach involves selecting special tokens and perturbing embeddings containing these tokens to inject watermarks. Simultaneously, we train a verifier to detect these watermarks. In the event of an attacker attempting to replicate our EaaS for profit, their model inherits our watermarks. For watermark verification, we construct verification texts to query the suspicious EaaS, and the verifier identifies our watermarks within the responses, effectively tracing copyright infringement. Extensive experiments across diverse datasets showcase the high detectability of our watermark method, even in out-of-distribution scenarios, without compromising embedding functionality. Our code is publicly available at https://github. com/Melodramass/Dynamic-Watermark .",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Liaoyaqi Wang",
        "Minhao Cheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/21c62f2ef68323099480c80318e48baae8b9098f",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a57be99734b62dfd2a54a15eb46c7359fdb7db58",
      "title": "SwiftThief: Enhancing Query Efficiency of Model Stealing by Contrastive Learning",
      "abstract": "Model-stealing attacks are emerging as a severe threat to AI-based services because an adversary can create models that duplicate the functionality of the black-box AI models inside the services with regular query-based access. To avoid detection or query costs, the model-stealing adversary must consider minimizing the number of queries to obtain an accurate clone model. To achieve this goal, we propose SwiftThief, a novel model-stealing framework that utilizes both queried and unqueried data to reduce query complexity. In particular, SwiftThief uses contrastive learning, a recent technique for representation learning. We formulate a new objective function for model stealing consisting of self-supervised (for abundant unqueried inputs from public datasets) and soft-supervised (for queried inputs) contrastive losses, jointly optimized with an output matching loss (for queried inputs). In addition, we suggest a new sampling strategy to prioritize rarely queried classes to improve attack performance. Our experiments proved that SwiftThief could significantly enhance the efficiency of model-stealing attacks compared to the existing methods, achieving similar attack performance using only half of the query budgets of the competing approaches. Also, SwiftThief showed high competence even when a defense was activated for the victims.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Jeonghyun Lee",
        "Sungmin Han",
        "Sangkyun Lee"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a57be99734b62dfd2a54a15eb46c7359fdb7db58",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9c616cb25371fb66f3dacabafe1abc81a5fbaae5",
      "title": "I Can Retrieve More than Images: Contrastive Stealing Attack against Deep Hashing Models",
      "abstract": "Deep hashing models have revolutionized traditional hashing methods by delivering superior performance, and have been applied in real-world applications such as Pinterest and Amazon, which are known as deep hashing-based retrieval systems. Behind their revolutionary representation capability, the requirements for training a deep hashing model expose it to the risks of potential model stealing attacks - a cheap way to mimic the well-trained hashing performance while circumventing the demanding requirements. Since the attacker is able to obtain the outputs of deep hashing models by querying the retrieval systems, the conventional stealing attacks relying on matching exact outputs can not be applied in this problem. In this paper, we propose a contrastive-based and GAN-enhanced stealing framework to leverage the informative knowledge of retrieved data. Our empirical results demonstrate that our stealing framework can train a substitute hashing model with a retrieval accuracy ranging from 80% to 110% of the target hashing model while utilizing significantly fewer training resources. Furthermore, we conduct attacks on the target hashing model using adversarial examples generated by the stolen model, resulting in an attack success rate that can be 3 times higher compared to attacks conducted without the substitute model. Finally, we leverage existing defense strategies to mitigate our attack, resulting in a stealing effectiveness decrease of no more than 4%.",
      "year": 2024,
      "venue": "2024 IEEE International Conference on Web Services (ICWS)",
      "authors": [
        "X. You",
        "Mi Zhang",
        "Jianwei Xu",
        "Min Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9c616cb25371fb66f3dacabafe1abc81a5fbaae5",
      "pdf_url": "",
      "publication_date": "2024-07-07",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "45b2b736cd8c66cb2bbb7efeb0907807d8f6fbe0",
      "title": "GanTextKnockoff: stealing text sentiment analysis model functionality using synthetic data",
      "abstract": "Today, black-box machine learning models are often subject to extraction attacks that aim to retrieve their internal information. Black-box model extraction attacks are typically conducted by providing input data and, based on observing the output results, constructing a new model that functions equivalently to the original. This process is usually carried out by leveraging available data from public repositories or synthetic data generated by generative models. Most model extraction attack methods using synthetic data have been concentrated in the field of computer vision, with minimal research focused on model extraction in natural language processing. In this paper, we propose a method that utilizes synthetic textual data to construct a new model with high accuracy and similarity to the original black-box sentiment analysis model.",
      "year": 2024,
      "venue": "Journal of Military Science and Technology",
      "authors": [
        "Cong Pham",
        "Trung-Nguyen Hoang",
        "Cao-Truong Tran",
        "Viet-Binh Do"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/45b2b736cd8c66cb2bbb7efeb0907807d8f6fbe0",
      "pdf_url": "",
      "publication_date": "2024-12-30",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9166ef92f94c521f1b5f78c51bac2dac805115a7",
      "title": "Quantum Neural Network Extraction Attack via Split Co-Teaching",
      "abstract": "Quantum Neural Networks (QNNs), now offered as QNN-as-a-Service (QNNaaS), have become key targets for model extraction attacks. Existing methods use ensemble learning to train substitute QNNs, but our analysis reveals significant limitations in real-world environments, where noise and cost constraints undermine their effectiveness. In this work, we introduce a novel attack, split co-teaching, which uses label variations to split queried data by noise sensitivity and employs co-teaching schemes to enhance extraction accuracy. The experimental results show that our approach outperforms classical extraction attacks by 6.5%~9.5% and existing QNN extraction methods by 0.1%~3.7% across various tasks.",
      "year": 2024,
      "venue": "2025 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)",
      "authors": [
        "Zhenxiao Fu",
        "Fan Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9166ef92f94c521f1b5f78c51bac2dac805115a7",
      "pdf_url": "",
      "publication_date": "2024-09-03",
      "keywords_matched": [
        "model extraction attack",
        "neural network extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d011a91d041692e9915c99f4e0d3c9b2136bb3f",
      "title": "STMS: An Out-Of-Distribution Model Stealing Method Based on Causality",
      "abstract": "Machine learning, particularly deep learning, is extensively applied in various real-life scenarios. However, recent research has highlighted the severe infringement of privacy and intellectual property caused by model stealing attacks. Therefore, more researchers are dedicated to studying the principles and methods of such attacks to promote the security development of artificial intelligence. Most of the existing model stealing attacks rely on prior information of the attacked models and consider ideal conditions. In order to better understand and defend against model stealing in real-world scenarios, we propose a novel model stealing method, named STMS, based on causal inference learning. For the first time, we introduce the problem of out-of-distribution generalization into the model stealing domain. The proposed approach operates under more challenging conditions, where the training and testing data of the target model are unknown, black-box, hard-label outputs, and there is a distribution shift during the testing phase. STMS achieves comparable or better stealing accuracy and generalization performance than prior works on multiple datasets and tasks. Moreover, this universal framework can be applied to improve the effectiveness of other model stealing methods and can also be migrated to other areas of machine learning.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Zhendong Zhao",
        "Yu Xuan",
        "Bisheng Tang",
        "Xiaoying Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3d011a91d041692e9915c99f4e0d3c9b2136bb3f",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "931a9beaf77e0019bcfa2412b8210b83cb70aa1a",
      "title": "MCD: Defense Against Query-Based Black-Box Surrogate Attacks",
      "abstract": "Deep neural networks (DNNs) is susceptible to surrogate attacks, where adversaries use surrogate data and corresponding outputs from the target model to build their own stolen model. Model stealing attacks jeopardize model privacy and model owners' commercial benefits. To address this issue, this paper proposes a hybrid protection approach-Maximize the confidence differences between benign samples and adversarial samples (MCD), to protect models from theft. Firstly, the LogitNorm approach is used to overcome the overconfidence problem in adversary query classification. Then, samples are divided into four groups according to ES and RS. Different groups are poisoned by different degrees. In addition to enhancing defensive performance and accounting for model integrity, the MCD uses a trigger to confirm the cloned model's owner. Experimental results show that the MCD defends against a variety of original models and attack techniques well. Against KnockoffNets and DFME attacks, the MCD yields an average defense performance of 54.58 % on five datasets, which is a great improvement over other defenses. Compared to other poisoning techniques, the Strong Poisoning (SP) module reduces the adversary's accuracy by 48.23 % on average. Additionally, the MCD overcomes the issue of OOD overconfidence while safeguarding the model accuracy in OOD detection and reduces the misclassification rate of ID samples for multiple OOD datasets.",
      "year": 2024,
      "venue": "IEEE International Conference on Systems, Man and Cybernetics",
      "authors": [
        "Yiwen Zou",
        "Wing W. Y. Ng",
        "Xueli Zhang",
        "Brick Loo",
        "Xingfu Yan",
        "Ran Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/931a9beaf77e0019bcfa2412b8210b83cb70aa1a",
      "pdf_url": "",
      "publication_date": "2024-10-06",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8d0fa6b361fc6319eac027da608a99f03b541111",
      "title": "Detecting Backdoor Attacks in Black-Box Neural Networks through Hardware Performance Counters",
      "abstract": "Deep Neural Networks (DNNs) have made significant strides, but their susceptibility to backdoor attacks still remains a concern. Most defenses typically assume access to white-box models or poisoned data, requirements that are often not feasible in practice, especially for proprietary DNNs. Existing defenses in a black-box setting usually rely on confidence scores of DNN's predictions. However, this exposes DNNs to the risk of model stealing attacks, a significant concern for proprietary DNNs. In this paper, we introduce a novel strategy for detecting back-doors, focusing on a more realistic black-box scenario where only hard-label (i.e., without any prediction confidence) query access is available. Our strategy utilizes data flow dynamics in a computational environment during DNN inference to identify potential backdoor inputs and is agnostic of trigger types or their locations in the input. We observe that a clean image and its corresponding backdoor counterpart with a trigger induce distinct patterns across various microarchitectural activities during the inference phase. We exploit these variations captured by Hardware Performance Counters (HPCs) and use principles of the Gaussian Mixture Model to detect backdoor inputs. To the best of our knowledge, this is the first work that utilizes HPCs for detecting backdoors in DNNs. Extensive evaluation considering a range of benchmark datasets, DNN architectures, and trigger patterns shows the efficacy of the proposed method in distinguishing between clean and backdoor inputs using HPCs.",
      "year": 2024,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Manaar Alam",
        "Yue Wang",
        "Michail Maniatakos"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8d0fa6b361fc6319eac027da608a99f03b541111",
      "pdf_url": "",
      "publication_date": "2024-03-25",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3f25ac027cb369ed864d3d7ef10a00b1f5877738",
      "title": "Extracting DNN Architectures via Runtime Profiling on Mobile GPUs",
      "abstract": "Deep Neural Networks (DNNs) have become invaluable intellectual property for AI providers due to advancements fueled by a decade of research and development. However, recent studies have demonstrated the effectiveness of model extraction attacks, which threaten this value by stealing DNN models. These attacks can lead to misuse of personal data, safety risks in critical systems, and the spread of misinformation. This paper explores model extraction attacks on DNN models deployed on mobile devices, using runtime profiles as a side-channel. Since mobile devices are resource constrained, DNN deployments require optimization efforts to reduce latency. The main hurdle in extracting DNN architectures in this scenario is that optimization techniques, such as operator-level and graph-level fusion, can obfuscate the association between runtime profile operators and their corresponding DNN layers, posing challenges for adversaries to accurately predict the computation performed. To overcome this, we propose a novel method analyzing GPU call profiles to identify the original DNN architecture. Our approach achieves full accuracy in extracting DNN architectures from a predefined set, even when layer information is obscured. For unseen architectures, a layer-by-layer hyperparameter extraction method guided by sub-layer patterns is introduced, also achieving high accuracy. This research achieves two firsts: 1) targeting mobile GPUs for DNN architecture extraction and 2) successfully extracting architectures from optimized models with fused layers.",
      "year": 2024,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Dong Hyub Kim",
        "Jonah O\u2019Brien Weiss",
        "Sandip Kundu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3f25ac027cb369ed864d3d7ef10a00b1f5877738",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ff6c345c3d75b658760a19f9b368fd5266fa500c",
      "title": "Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data",
      "abstract": "With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as \\emph{multi-view data}, it becomes feasible to effectively defend functionality stealing attacks. Based on this perspective, we introduce a novel watermarking technique based on Multi-view dATa, called MAT, for efficiently embedding watermarks within DNNs. This approach involves constructing a trigger set with multi-view data and incorporating a simple feature-based regularization method for training the source model. We validate our method across various benchmarks and demonstrate its efficacy in defending against model extraction attacks, surpassing relevant baselines by a significant margin. The code is available at: \\href{https://github.com/liyuxuan-github/MAT}{https://github.com/liyuxuan-github/MAT}.",
      "year": 2024,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yuxuan Li",
        "Sarthak Kumar Maharana",
        "Yunhui Guo"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ff6c345c3d75b658760a19f9b368fd5266fa500c",
      "pdf_url": "",
      "publication_date": "2024-03-15",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "517a03d4025905d23928bbfdd4b1dfe595873ab3",
      "title": "TrustZoneTunnel: A Cross-World Pattern History Table-Based Microarchitectural Side-Channel Attack",
      "abstract": "ARM's TrustZone is a hardware-based trusted execution environment (TEE), prevalent in mobile devices, IoT edge systems, and autonomous systems. Within TrustZone, security-sensitive applications reside in a hardware-isolated secure world, protected from the normal-world's applications, OS, debugger, peripherals, and memory. However, microarchitectural side-channel vulnerabilities have been discovered on shared on-chip resources, such as caches and branch prediction unit (BPU). In this paper, we propose TrustZoneTunnel, the first Pattern History Table (PHT)-based side-channel attack on TrustZone, which is able to reveal the complete control flow of a trusted application in the secure world. We reverse-engineer the PHT indexing for ARM processors and develop key primitives for cross-world attacks, including well-controlled world-switching, PHT collision construction between two worlds, and precise PHT state-setting and checking functions. Furthermore, we introduce a novel model extraction attack against TrustZone based deep neural network, which can recover model parameters using only the side-channel leakage of vital branch instructions, obviating the need for model output or logits while prior research work requires such knowledge for model extraction.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Tianhong Xu",
        "A. A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/517a03d4025905d23928bbfdd4b1dfe595873ab3",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5e2375e3e177fdf78f63768e9c8f763898c1404e",
      "title": "Efficient Neural-Network Based Solution of Integral Equations for Electromagnetic Analysis",
      "abstract": "Existing neural network (NN) based methods for solving Maxwell's equations focus on partial differential equations (PDEs). Integral equations (IEs) are seldom studied in the machine learning framework. Furthermore, prevailing methods employ a multi-objective loss that separates the loss associated with PDEs from the related boundary conditions. In this work, we construct a physics-informed loss by integrating the loss from equations and boundary conditions, which greatly enhances the convergence performance of an NN-based solution. We also devise an NN architecture to facilitate accurate and efficient learning. Equally applicable to PDEs, we underscore the success of proposed work in solving both surface- and volume-based IEs for electromagnetic analysis.",
      "year": 2024,
      "venue": "2024 IEEE International Symposium on Antennas and Propagation and INC/USNC\u2010URSI Radio Science Meeting (AP-S/INC-USNC-URSI)",
      "authors": [
        "Runwei Zhou",
        "Dan Jiao"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/5e2375e3e177fdf78f63768e9c8f763898c1404e",
      "pdf_url": "",
      "publication_date": "2024-07-14",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "eae75f1031d15b83451e49e307d05dc44a952aab",
      "title": "Dynamical analysis and DSP implementation of 3D Hopfield neural network under dual memristive electromagnetic radiation",
      "abstract": "Numerous important biological neural activities, such as changes in their own firing patterns and information transmission between neurons, are affected to some extent by electromagnetic radiation in the external environment. To explore the impacts of two different external electromagnetic radiation stimulation on neuronal activities in a neural network, a 3D Hopfield neural network under dual memristive electromagnetic radiation (DMEMRHNN) is proposed in this paper. Firstly, two memristor models for simulating different external electromagnetic radiation are proposed and introduced into the 3D Hopfield neural network (HNN), thus constructing the DMEMRHNN. Then, the rich dynamical behavior changes of the DMEMRHNN under the influence of parameters such as electromagnetic radiation intensity are analyzed. At the same time, the coexisting attractors, state transition, and rare and interesting 4-symmetric remerging Feigenbaum tree phenomena are discovered. Finally, the simulation results of the DMEMRHNN on MATLAB are verified through DSP experimental platform.",
      "year": 2024,
      "venue": "Physica Scripta",
      "authors": [
        "Minyuan Cheng",
        "Yinghong Cao",
        "Peng Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/eae75f1031d15b83451e49e307d05dc44a952aab",
      "pdf_url": "",
      "publication_date": "2024-12-12",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5d9824e7b540953dd378f6e8a7b41da2da5234ef",
      "title": "Galerkin neural network-POD for acoustic and electromagnetic wave propagation in parametric domains",
      "abstract": "We investigate reduced order models for acoustic and electromagnetic wave problems in parametrically defined domains. The parameter-to-solution maps are approximated following the so-called Galerkin POD-NN method, which combines the construction of a reduced basis via proper orthogonal decomposition (POD) with neural networks (NNs). As opposed to the standard reduced basis method, this approach allows for the swift and efficient evaluation of reduced order solutions for any given parametric input. As is customary in the analysis of problems in random or parametrically defined domains, we start by transporting the formulation to a reference domain. This yields a parameter-dependent variational problem set on parameter-independent functional spaces. In particular, we consider affine-parametric domain transformations characterized by a high-dimensional, possibly countably infinite, parametric input. To keep the number of evaluations of the high-fidelity solutions manageable, we propose using low-discrepancy sequences to sample the parameter space efficiently. Then, we train an NN to learn the coefficients in the reduced representation. This approach completely decouples the offline and online stages of the reduced basis paradigm. Numerical results for the three-dimensional Helmholtz and Maxwell equations confirm the method\u2019s accuracy up to a certain barrier and show significant gains in online speed-up compared to the traditional Galerkin POD method.",
      "year": 2024,
      "venue": "Advances in Computational Mathematics",
      "authors": [
        "Philipp Weder",
        "Mariella Kast",
        "Fernando Henr\u00edquez",
        "J.S. Hesthaven"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/5d9824e7b540953dd378f6e8a7b41da2da5234ef",
      "pdf_url": "",
      "publication_date": "2024-06-19",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "add12a9dac301e7e6554a03a018e86a051e3c1b8",
      "title": "Coupled Electromagnetic-Thermal Analysis for Temperature-Dependent Materials with Physics-Informed Neural Networks",
      "abstract": "We present a Physics-Informed Neural Network (PINN) based method for the coupled electromagnetic-thermal analysis of microwave structures with temperature-dependent materials. Combined with the Finite-Difference-Time domain technique, the proposed approach efficiently handles the dynamic change of material parameters with temperature, without compromising accuracy. We demonstrate this method with the electromagnetic-thermal modeling of a micro-electro-mechanical switch on a coplanar waveguide. This study demonstrates the potential of employing PINNs in real-world multiphysics applications for the first time.",
      "year": 2024,
      "venue": "Intelligent Memory Systems",
      "authors": [
        "Q. Shutong",
        "C. Sarris"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/add12a9dac301e7e6554a03a018e86a051e3c1b8",
      "pdf_url": "",
      "publication_date": "2024-06-16",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a9f0a5405f75e2596ebde4b0339dcc20b3b72db3",
      "title": "Exploring the Validity of Knockoff Nets Model Stealing Attack on Vgg16 Based on Different Models",
      "abstract": "Model stealing attacks represented by the Knockoff Nets method steal the intellectual property of AI models by black-box querying. Model stealing attacks on a wide range of deep learning models have attracted widespread attention in recent years. However, there has not been any research on stealing attacks based on common models such as VGG16, ResNet18, AlexNet, etc., especially since the research on the validity of the attack on the VGG16 model is still insufficient. Therefore, in this paper, three types of models, VGG16, ResNet18, and AlexNet, are used as the models for stealing, and the Knockoff Nets method is used to carry out stealing attacks on the pre-trained model of VGG16, which is capable of cat and dog image recognition. This paper analyzes the stealing similarity, stealing model accuracy and stealing training time so as to reflect the validity of stealing. The paper shows that Knockoff Nets based on three types of models, VGG16, ResNet18, and AlexNet, are all effective against the VGG16 model stealing attack, and the more similar the architectures of the stealing model and the victim's model are, the better the stealing effect is. In addition, to a certain extent, the stealing training time and the stealing model accuracy are affected by the architecture of model used to steal. This paper reveals the validity of the Knockoff Nets model stealing attack against VGG16 based on three types of models, namely VGG16, ResNet18, and AlexNet, to provide a reference for model security protection.",
      "year": 2024,
      "venue": "Applied and Computational Engineering",
      "authors": [
        "Yunxi Hei"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a9f0a5405f75e2596ebde4b0339dcc20b3b72db3",
      "pdf_url": "https://www.ewadirect.com/proceedings/ace/article/view/17285/pdf",
      "publication_date": "2024-11-26",
      "keywords_matched": [
        "model stealing attack",
        "knockoff nets"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0c713d0c069bfe03c945bfe7f67e9176ea8bcaff",
      "title": "Enhancing Data-Free Model Stealing Attack on Robust Models",
      "abstract": "Machine Learning Model Deployment as a Service (MLaaS) has surged in popularity, offering substantial business value. However, the significant resources and costs required to train models have raised concerns about Model Stealing Attacks (MSAs), where attackers create a clone model to replicate the knowledge of a victim model without access to its parameters. In data-free MSA, attackers also lack access to the training data for the victim model. In this setting, existing MSA methods rely on Generative Adversarial Networks (GANs) to generate images to query the victim model. However, GANs are known to suffer from model collapse, resulting in limited diversity in generated images. The lack of diversity in generated images will significantly impact the accuracy of the clone model, especially in stealing robust models trained with adversarial training. Recent studies have demonstrated that Denoising Diffusion Probabilistic Models (DDPMs) outperform GANs in generating images with greater diversity. In our data-free MSA framework, using DDPM as the generator to steal robust models significantly increases the effectiveness, improving the accuracy of the clone model from 21.34% to 60.23% compared to the GANs-based approach DFME, and requires fewer queries. We further use denoise diffusion GANs to address the problem of low sampling speed of DDPM, while retaining the advantage of its high sample diversity and obtaining better results.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Jianping He",
        "Haichang Gao",
        "Yunyi Zhou"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0c713d0c069bfe03c945bfe7f67e9176ea8bcaff",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9295ae78a4a20d7d43cf84a5d68c70912df9ce6b",
      "title": "Model Extraction Attack Without Natural Images",
      "abstract": null,
      "year": 2024,
      "venue": "ACNS Workshops",
      "authors": [
        "Kota Yoshida",
        "Takeshi Fujino"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9295ae78a4a20d7d43cf84a5d68c70912df9ce6b",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d9dcd3bf3aa0262eea6e17b5bb35705cc5f0301e",
      "title": "On the Security of Privacy-Preserving Machine Learning Against Model Stealing Attacks",
      "abstract": null,
      "year": 2024,
      "venue": "Cryptology and Network Security",
      "authors": [
        "Bhuvnesh Chaturvedi",
        "Anirban Chakraborty",
        "Ayantika Chatterjee",
        "Debdeep Mukhopadhyay"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d9dcd3bf3aa0262eea6e17b5bb35705cc5f0301e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9bedd67004ef344b801365ae28c09dce28410517",
      "title": "Stolen Subwords: Importance of Vocabularies for Machine Translation Model Stealing",
      "abstract": "In learning-based functionality stealing, the attacker is trying to build a local model based on the victim's outputs. The attacker has to make choices regarding the local model's architecture, optimization method and, specifically for NLP models, subword vocabulary, such as BPE. On the machine translation task, we explore (1) whether the choice of the vocabulary plays a role in model stealing scenarios and (2) if it is possible to extract the victim's vocabulary. We find that the vocabulary itself does not have a large effect on the local model's performance. Given gray-box model access, it is possible to collect the victim's vocabulary by collecting the outputs (detokenized subwords on the output). The results of the minimum effect of vocabulary choice are important more broadly for black-box knowledge distillation.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Vil\u00e9m Zouhar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9bedd67004ef344b801365ae28c09dce28410517",
      "pdf_url": "",
      "publication_date": "2024-01-29",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "66025882080646af642638429d89a473c9569718",
      "title": "LSSMSD: defending against black-box DNN model stealing based on localized stochastic sensitivity",
      "abstract": null,
      "year": 2024,
      "venue": "International Journal of Machine Learning and Cybernetics",
      "authors": [
        "Xueli Zhang",
        "Jiale Chen",
        "Qihua Li",
        "Jianjun Zhang",
        "Wing W. Y. Ng",
        "Ting Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/66025882080646af642638429d89a473c9569718",
      "pdf_url": "",
      "publication_date": "2024-09-18",
      "keywords_matched": [
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "274ead8c75880a252b6c92908dc329b0eb5f9f3f",
      "title": "DM4Steal: Diffusion Model For Link Stealing Attack On Graph Neural Networks",
      "abstract": "Graph has become increasingly integral to the advancement of recommendation systems, particularly with the fast development of graph neural network(GNN). By exploring the virtue of rich node features and link information, GNN is designed to provide personalized and accurate suggestions. Meanwhile, the privacy leakage of GNN in such contexts has also captured special attention. Prior work has revealed that a malicious user can utilize auxiliary knowledge to extract sensitive link data of the target graph, integral to recommendation systems, via the decision made by the target GNN model. This poses a significant risk to the integrity and confidentiality of data used in recommendation system. Though important, previous works on GNN's privacy leakage are still challenged in three aspects, i.e., limited stealing attack scenarios, sub-optimal attack performance, and adaptation against defense. To address these issues, we propose a diffusion model based link stealing attack, named DM4Steal. It differs previous work from three critical aspects. (i) Generality: aiming at six attack scenarios with limited auxiliary knowledge, we propose a novel training strategy for diffusion models so that DM4Steal is transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from the retention of semantic structure in the diffusion model during the training process, DM4Steal is capable to learn the precise topology of the target graph through the GNN decision process. (iii) Adaptation: when GNN is defensive (e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling the score model multiple times to keep performance degradation to a minimum, thus DM4Steal implements successful adaptive attack on defensive GNN.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jinyin Chen",
        "Haonan Ma",
        "Haibin Zheng"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/274ead8c75880a252b6c92908dc329b0eb5f9f3f",
      "pdf_url": "",
      "publication_date": "2024-11-05",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "08e1f102344bc341e8109a5c23f78093a4c53323",
      "title": "Protecting Object Detection Models from Model Extraction Attack via Feature Space Coverage",
      "abstract": "The model extraction attack is an attack pattern aimed at stealing well-trained machine learning models' functionality or privacy information. With the gradual popularization of AI-related technologies in daily life, various well-trained models are being deployed. As a result, these models are considered valuable assets and attractive to model extraction attackers. Currently, the academic community primarily focuses on defense for model extraction attacks in the context of classification, with little attention to the more commonly used task scenario of object detection. Therefore, we propose a detection framework targeting model extraction attacks against object detection models in this paper. The framework first locates suspicious users based on feature coverage in query traffic and uses an active verification module to confirm whether the identified suspicious users are attackers. Through experiments conducted in multiple task scenarios, we validate the effectiveness and detection efficiency of the proposed method.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Zeyu Li",
        "Yuwen Pu",
        "Xuhong Zhang",
        "Yu Li",
        "Jinbao Li",
        "Shouling Ji"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/08e1f102344bc341e8109a5c23f78093a4c53323",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "264566b429bd774b7827cc1b5e4c4bd0efb84f1e",
      "title": "Preserving Accuracy While Stealing Watermarked Deep Neural Networks",
      "abstract": "The deployment of Deep Neural Networks (DNNs) as cloud services has accelerated significantly over the years. Training an application-specific DNN for cloud deployment requires substantial computational resources and costs associated with hyper-parameter tuning and model selection. To preserve Intellectual Property (IP) rights, model owners embed watermarks into publicly deployed DNNs. These trigger inputs and labels are uniquely selected and embedded into the watermarked DNN by the model owner, remaining undisclosed during deployment. If a watermarked DNN (target classifier) is stolen via white-box access and re-deployed by an adversary (pirated classifier) without securing the IP rights from the model owner, the model owner can identify their IP by sending trigger inputs to retrieve trigger labels. Typically, adversaries tamper with the model weights of the target classifier prior to deployment, which in turn reduces the utility of the well-trained DNN. The authors proposes re-deploying the target classifier without altering the model weights to preserve model utility, and using a small sample of non-identical in-distribution inputs (used for training the target classifier) to train a Siamese neural network to evade detection, at inference stage. Experimental evaluations on standard benchmark datasets- MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100- using ResNet architectures with varying triggers demonstrate that the proposed method achieves zero false positive rate (fraction of clean testing input incorrectly labelled as trigger inputs) and false negative rate (fraction of trigger inputs incorrectly labelled as clean in-distribution inputs) in nearly all cases, proving its efficacy.",
      "year": 2024,
      "venue": "International Conference on Machine Learning and Applications",
      "authors": [
        "Aritra Ray",
        "F. Firouzi",
        "Kyle Lafata",
        "Krishnendu Chakrabarty"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/264566b429bd774b7827cc1b5e4c4bd0efb84f1e",
      "pdf_url": "",
      "publication_date": "2024-12-18",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "deef9baed03e2eb53aac92a38b5cfa6317dc1019",
      "title": "A Hard-Label Cryptanalytic Extraction of Non-Fully Connected Deep Neural Networks using Side-Channel Attacks",
      "abstract": "During the past decade, Deep Neural Networks (DNNs) proved their value on a large variety of subjects. However despite their high value and public accessibility, the protection of the intellectual property of DNNs is still an issue and an emerging research field. Recent works have successfully extracted fully-connected DNNs using cryptanalytic methods in hard-label settings, proving that it was possible to copy a DNN with high fidelity, i.e., high similitude in the output predictions. However, the current cryptanalytic attacks cannot target complex, i.e., not fully connected, DNNs and are limited to special cases of neurons present in deep networks. In this work, we introduce a new end-to-end attack framework designed for model extraction of embedded DNNs with high fidelity. We describe a new black-box side-channel attack which splits the DNN in several linear parts for which we can perform cryptanalytic extraction and retrieve the weights in hard-label settings. With this method, we are able to adapt cryptanalytic extraction, for the first time, to non-fully connected DNNs, while maintaining a high fidelity. We validate our contributions by targeting several architectures implemented on a microcontroller unit, including a Multi-Layer Perceptron (MLP) of 1.7 million parameters and a shortened MobileNetv1. Our framework successfully extracts all of these DNNs with high fidelity (88.4% for the MobileNetv1 and 93.2% for the MLP). Furthermore, we use the stolen model to generate adversarial examples and achieve close to white-box performance on the victim's model (95.8% and 96.7% transfer rate).",
      "year": 2024,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Beno\u00eet Coqueret",
        "Mathieu Carbone",
        "Olivier Sentieys",
        "Gabriel Zaid"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/deef9baed03e2eb53aac92a38b5cfa6317dc1019",
      "pdf_url": "",
      "publication_date": "2024-11-15",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e57dea53d0d1f28d330a0f635232d07c37d9403f",
      "title": "2.5D Interposer HBM Signal Integrity Analysis Based on Fast MoM",
      "abstract": "High-bandwidth memory (HBM) placed side-by-side with ASIC on silicon interposer is capable of delivering the TB/s bandwidth. To maintain such high bandwidth, it is crucial to perform extensive signal integrity analysis and optimization. In this paper, a novel methodology for fast HBM signal integrity analysis and optimization is proposed, which includes accurate HBM channel model extraction and fast HBM channel simulation in both frequency domain and time domain with IBIS-AMI models to ultimately enable full signal coverage with automation.",
      "year": 2024,
      "venue": "International Conference on Information Communication and Management",
      "authors": [
        "Wenliang Dai",
        "Ping Liu",
        "Liguo Jiang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e57dea53d0d1f28d330a0f635232d07c37d9403f",
      "pdf_url": "",
      "publication_date": "2024-10-25",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b9a4b5e07b9973e968ea746159bfe88c0801f854",
      "title": "Security Concerns of Machine Learning Hardware",
      "abstract": "AI-as-a-Service (AIaaS) has been emerging with model providers deploying their models on cloud and model consumers using the model. Recently, ML models are being deployed on edge devices to improve cost and response time. The widespread usage of machine learning has made the study of security in the context of Machine Learning (ML) very critical. Model extraction attacks focuses on extracting model parameters such as weights and biases which can be used to clone a ML target model deployed on the cloud or on an edge device hardware. This paper explores different types of attacks on ML models primarily focusing on model extraction attacks on ML hardware such as scan-chain and side-channel attacks. The paper present an analysis of various such attacks and their countermeasures. Possible future directions of work are also discussed.",
      "year": 2024,
      "venue": "Asian Test Symposium",
      "authors": [
        "Nilotpola Sarma",
        "E. Bhawani",
        "E. Reddy",
        "C. Karfa"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b9a4b5e07b9973e968ea746159bfe88c0801f854",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "34c1fadafbd9e6afc26590cd06c66ed9364fac67",
      "title": "Electromagnetic Field Analysis Using Physics Informed Neural Network Considering Eddy Current",
      "abstract": "The exploration of PINN (Physics Informed Neural Network) in research is still in its nascent stages globally, with a notable dearth of studies focusing on electromagnetic field analysis. In response to this gap, this paper introduces a novel approach for eddy current analysis employing a transfer learning-based discrete differentiation method within the framework of a PINN. While discrete differentiation methods offer the advantage of low model complexity and rapid analysis, they encounter challenges in eddy current analysis due to the need for learning at each time step. This paper addresses these challenges through the application of transfer learning techniques. Our findings demonstrate that the proposed method significantly reduces the total analysis time in time-dependent scenarios compared to traditional Finite Element Method (FEM) approaches.",
      "year": 2024,
      "venue": "IEEE Conference on Electromagnetic Field Computation",
      "authors": [
        "Ji-hoon Han",
        "Jong-Hoon Park",
        "Seung-Min Song",
        "Sun-Ki Hong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/34c1fadafbd9e6afc26590cd06c66ed9364fac67",
      "pdf_url": "",
      "publication_date": "2024-06-02",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e1e53971438e42df0a29fa448b3398331d63f94a",
      "title": "Enabling Power Side-Channel Attack Simulation on Mixed-Signal Neural Network Accelerators",
      "abstract": "Due to the tremendous success of Deep Learning with neural networks (NNs) in recent years and the simultaneous leap of embedded, low-power devices (e.g. wearables, smart-phones, IoT, and smart sensors), enabling the inference of those NNs in power-constrained environments gave rise to specialized NN accelerators. One paradigm followed by many of those accelerators was the transition from digital domain computing towards performing operations in the analog domain, turning them from digital to mixed-signal NN accelerators. While power-efficiency and inference accuracy have been researched with increasing interest, security and protection against a side-channel attack (SCA) have not found much attention. However, side-channels pose a major security concern by allowing an attacker to steal valuable knowledge about proprietary NNs deployed on accelerators. In order to evaluate mixed-signal NNs accelerators concerning SCA robustness, its tendency to leak information through the side-channel needs investigation. In this work, we propose a methodology for enabling side-channel analysis of mixed-signal NNs accelerators, which shows reasonable accuracy in an early development stage. The approach enables the reuse of large portions of design sources for simulation and production while providing flexibility and fast development cycles for changes to the analog design.",
      "year": 2024,
      "venue": "Coins",
      "authors": [
        "Simon Wilhelmst\u00e4tter",
        "Joschua Conrad",
        "Devanshi Upadhyaya",
        "I. Polian",
        "M. Ortmanns"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e1e53971438e42df0a29fa448b3398331d63f94a",
      "pdf_url": "",
      "publication_date": "2024-07-29",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ed9b2ca6d5741a9af8c3f2c368cfdf68eed98a48",
      "title": "Stealing Brains: From English to Czech Language Model",
      "abstract": ": We present a simple approach for efficiently adapting pre-trained English language models to generate text in lower-resource language, specifically Czech. We propose a vocabulary swap method that leverages parallel corpora to map tokens between languages, allowing the model to retain much of its learned capabilities. Experiments conducted on a Czech translation of the TinyStories dataset demonstrate that our approach significantly outperforms baseline methods, especially when using small amounts of training data. With only 10% of the data, our method achieves a perplexity of 17.89, compared to 34.19 for the next best baseline. We aim to contribute to work in the field of cross-lingual transfer in natural language processing and we propose a simple to implement, computationally efficient method tested in a controlled environment.",
      "year": 2024,
      "venue": "International Joint Conference on Computational Intelligence",
      "authors": [
        "Petr Hyner",
        "Petr Marek",
        "D. Adamczyk",
        "Jan Hula",
        "Jan \u0160ediv\u00fd"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ed9b2ca6d5741a9af8c3f2c368cfdf68eed98a48",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "title": "APMSA: Adversarial Perturbation Against Model Stealing Attacks",
      "abstract": "Training a Deep Learning (DL) model requires proprietary data and computing-intensive resources. To recoup their training costs, a model provider can monetize DL models through Machine Learning as a Service (MLaaS). Generally, the model is deployed at the cloud, while providing a publicly accessible Application Programming Interface (API) for paid queries to obtain benefits. However, model stealing attacks have posed security threats to this model monetizing scheme as they steal the model without paying for future extensive queries. Specifically, an adversary queries a targeted model to obtain input-output pairs and thus infer the model\u2019s internal working mechanism by reverse-engineering a substitute model, which has deprived model owner\u2019s business advantage and leaked the privacy of the model. In this work, we observe that the confidence vector or the top-1 confidence returned from the model under attack (MUA) varies in a relative large degree given different queried inputs. Therefore, rich internal information of the MUA is leaked to the attacker that facilities her reconstruction of a substitute model. We thus propose to leverage adversarial confidence perturbation to hide such varied confidence distribution given different queries, consequentially against model stealing attacks (dubbed as APMSA). In other words, the confidence vectors returned now is similar for queries from a specific category, considerably reducing information leakage of the MUA. To achieve this objective, through automated optimization, we constructively add delicate noise into per input query to make its confidence close to the decision boundary of the MUA. Generally, this process is achieved in a similar means of crafting adversarial examples but with a distinction that the hard label is preserved to be the same as the queried input. This retains the inference utility (i.e., without sacrificing the inference accuracy) for normal users but bounded the leaked confidence information to the attacker in a small constrained area (i.e., close to decision boundary). The later renders greatly deteriorated accuracy of the attacker\u2019s substitute model. As the APMSA serves as a plug-in front-end and requires no change to the MUA, it is thus generic and easy to deploy. The high efficacy of APMSA is validated through experiments on datasets of CIFAR10 and GTSRB. Given a MUA model of ResNet-18 on the CIFAR10, our defense can degrade the accuracy of the stolen model by up to 15% (rendering the stolen model useless to a large extent) with 0% accuracy drop for normal user\u2019s hard-label inference request.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Jiliang Zhang",
        "Shuang Peng",
        "Yansong Gao",
        "Zhi Zhang",
        "Q. Hong"
      ],
      "citation_count": 74,
      "url": "https://www.semanticscholar.org/paper/796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d6339e02ec93158430c83f7962b9f5c11cb45d14",
      "title": "Estimating Compressive Strength of Concrete Using Neural Electromagnetic Field Optimization",
      "abstract": "Concrete compressive strength (CCS) is among the most important mechanical characteristics of this widely used material. This study develops a novel integrative method for efficient prediction of CCS. The suggested method is an artificial neural network (ANN) favorably tuned by electromagnetic field optimization (EFO). The EFO simulates a physics-based strategy, which in this work is employed to find the best contribution of the concrete parameters (i.e., cement (C), blast furnace slag (SBF), fly ash (FA1), water (W), superplasticizer (SP), coarse aggregate (AC), fine aggregate (FA2), and the age of testing (AT)) to the CCS. The same effort is carried out by three benchmark optimizers, namely the water cycle algorithm (WCA), sine cosine algorithm (SCA), and cuttlefish optimization algorithm (CFOA) to be compared with the EFO. The results show that hybridizing the ANN using the mentioned algorithms led to reliable approaches for predicting the CCS. However, comparative analysis indicates that there are appreciable distinctions between the prediction capacity of the ANNs created by the EFO and WCA vs. the SCA and CFOA. For example, the mean absolute error calculated for the testing phase of the ANN-WCA, ANN-SCA, ANN-CFOA, and ANN-EFO was 5.8363, 7.8248, 7.6538, and 5.6236, respectively. Moreover, the EFO was considerably faster than the other strategies. In short, the ANN-EFO is a highly efficient hybrid model, and can be recommended for the early prediction of the CCS. A user-friendly explainable and explicit predictive formula is also derived for the convenient estimation of the CCS.",
      "year": 2023,
      "venue": "Materials",
      "authors": [
        "M. Akbarzadeh",
        "H. Ghafourian",
        "Arsalan Anvari",
        "Ramin Pourhanasa",
        "M. Nehdi"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/d6339e02ec93158430c83f7962b9f5c11cb45d14",
      "pdf_url": "https://www.mdpi.com/1996-1944/16/11/4200/pdf?version=1685974787",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "title": "Prompt Stealing Attacks Against Text-to-Image Generation Models",
      "abstract": "Text-to-Image generation models have revolutionized the artwork design process and enabled anyone to create high-quality images by entering text descriptions called prompts. Creating a high-quality prompt that consists of a subject and several modifiers can be time-consuming and costly. In consequence, a trend of trading high-quality prompts on specialized marketplaces has emerged. In this paper, we perform the first study on understanding the threat of a novel attack, namely prompt stealing attack, which aims to steal prompts from generated images by text-to-image generation models. Successful prompt stealing attacks directly violate the intellectual property of prompt engineers and jeopardize the business model of prompt marketplaces. We first perform a systematic analysis on a dataset collected by ourselves and show that a successful prompt stealing attack should consider a prompt's subject as well as its modifiers. Based on this observation, we propose a simple yet effective prompt stealing attack, PromptStealer. It consists of two modules: a subject generator trained to infer the subject and a modifier detector for identifying the modifiers within the generated image. Experimental results demonstrate that PromptStealer is superior over three baseline methods, both quantitatively and qualitatively. We also make some initial attempts to defend PromptStealer. In general, our study uncovers a new attack vector within the ecosystem established by the popular text-to-image generation models. We hope our results can contribute to understanding and mitigating this emerging threat.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyue Shen",
        "Y. Qu",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "pdf_url": "http://arxiv.org/pdf/2302.09923",
      "publication_date": "2023-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "81cd9575100643a3463465ec19e90ee78e122f93",
      "title": "SoK: Model Inversion Attack Landscape: Taxonomy, Challenges, and Future Roadmap",
      "abstract": "A crucial module of the widely applied machine learning (ML) model is the model training phase, which involves large-scale training data, often including sensitive private data. ML models trained on these sensitive data suffer from significant privacy concerns since ML models can intentionally or unintendedly leak information about training data. Adversaries can exploit this information to perform privacy attacks, including model extraction, membership inference, and model inversion. While a model extraction attack steals and replicates a trained model functionality, and membership inference infers the data sample's inclusiveness to the training set, a model inversion attack has the goal of inferring the training data sample's sensitive attribute value or reconstructing the training sample (i.e., image/audio/text). Distinct and inconsistent characteristics of model inversion attack make this attack even more challenging and consequential, opening up model inversion attack as a more prominent and increasingly expanding research paradigm. Thereby, to flourish research in this relatively underexplored model inversion domain, we conduct the first-ever systematic literature review of the model inversion attack landscape. We characterize model inversion attacks and provide a comprehensive taxonomy based on different dimensions. We illustrate foundational perspectives emphasizing methodologies and key principles of the existing attacks and defense techniques. Finally, we discuss challenges and open issues in the existing model inversion attacks, focusing on the roadmap for future research directions.",
      "year": 2023,
      "venue": "IEEE Computer Security Foundations Symposium",
      "authors": [
        "S. Dibbo"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/81cd9575100643a3463465ec19e90ee78e122f93",
      "pdf_url": "",
      "publication_date": "2023-07-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "40b40e942db469663609ad6c1911cca235079434",
      "title": "D-DAE: Defense-Penetrating Model Extraction Attacks",
      "abstract": "Recent studies show that machine learning models are vulnerable to model extraction attacks, where the adversary builds a substitute model that achieves almost the same performance of a black-box victim model simply via querying the victim model. To defend against such attacks, a series of methods have been proposed to disrupt the query results before returning them to potential attackers, greatly degrading the performance of existing model extraction attacks.In this paper, we make the first attempt to develop a defense-penetrating model extraction attack framework, named D-DAE, which aims to break disruption-based defenses. The linchpins of D-DAE are the design of two modules, i.e., disruption detection and disruption recovery, which can be integrated with generic model extraction attacks. More specifically, after obtaining query results from the victim model, the disruption detection module infers the defense mechanism adopted by the defender. We design a meta-learning-based disruption detection algorithm for learning the fundamental differences between the distributions of disrupted and undisrupted query results. The algorithm features a good generalization property even if we have no access to the original training dataset of the victim model. Given the detected defense mechanism, the disruption recovery module tries to restore a clean query result from the disrupted query result with well-designed generative models. Our extensive evaluations on MNIST, FashionMNIST, CIFAR-10, GTSRB, and ImageNette datasets demonstrate that D-DAE can enhance the substitute model accuracy of the existing model extraction attacks by as much as 82.24% in the face of 4 state-of-the-art defenses and combinations of multiple defenses. We also verify the effectiveness of D-DAE in penetrating unknown defenses in real-world APIs hosted by Microsoft Azure and Face++.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yanjiao Chen",
        "Rui Guan",
        "Xueluan Gong",
        "Jianshuo Dong",
        "Meng Xue"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/40b40e942db469663609ad6c1911cca235079434",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "afbc7f9d4a4dcf0cf02ab3efd043904b361fa886",
      "title": "DeepTheft: Stealing DNN Model Architectures through Power Side Channel",
      "abstract": "Deep Neural Network (DNN) models are often deployed in resource-sharing clouds as Machine Learning as a Service (MLaaS) to provide inference services. To steal model architectures that are of valuable intellectual properties, a class of attacks has been proposed via different side-channel leakage, posing a serious security challenge to MLaaS.Also targeting MLaaS, we propose a new end-to-end attack, DeepTheft, to accurately recover complex DNN model architectures on general processors via the RAPL (Running Average Power Limit)-based power side channel. While unprivileged access to the RAPL has been disabled in bare-metal OSes, we observe that the RAPL is still legitimately accessible in a platform as a service, e.g., the latest docker environment of version 20.10.18 used in this work. However, an attacker can acquire only a low sampling rate (1 KHz) of the time-series energy traces from the RAPL interface, rendering existing techniques ineffective in stealing large and deep DNN models. To this end, we design a novel and generic learning-based framework consisting of a set of meta-models, based on which DeepTheft is demonstrated to have high accuracy in recovering a large number (thousands) of models architectures from different model families including the deepest ResNet152. Particularly, DeepTheft has achieved a Levenshtein Distance Accuracy of 99.75% in recovering network structures, and a weighted average F1 score of 99.60% in recovering diverse layer-wise hyperparameters. Besides, our proposed learning framework is general to other time-series side-channel signals. To validate its generalization, another existing side channel is exploited, i.e., CPU frequency. Different from RAPL, CPU frequency is accessible to unprivileged users in bare-metal OSes. By using our generic learning framework trained against CPU frequency traces, DeepTheft has shown similarly high attack performance in stealing model architectures.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yansong Gao",
        "Huming Qiu",
        "Zhi Zhang",
        "Binghui Wang",
        "Hua Ma",
        "A. Abuadbba",
        "Minhui Xue",
        "Anmin Fu",
        "Surya Nepal"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/afbc7f9d4a4dcf0cf02ab3efd043904b361fa886",
      "pdf_url": "https://arxiv.org/pdf/2309.11894",
      "publication_date": "2023-09-21",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b4149005980a11919731e6b4c1833d8b0af59424",
      "title": "Deep Neural Network Watermarking against Model Extraction Attack",
      "abstract": "Deep neural network (DNN) watermarking is an emerging technique to protect the intellectual property of deep learning models. At present, many DNN watermarking algorithms have been proposed to achieve provenance verification by embedding identify information into the internals or prediction behaviors of the host model. However, most methods are vulnerable to model extraction attacks, where attackers collect output labels from the model to train a surrogate or a replica. To address this issue, we present a novel DNN watermarking approach, named SSW, which constructs an adaptive trigger set progressively by optimizing over a pair of symmetric shadow models to enhance the robustness to model extraction. Precisely, we train a positive shadow model supervised by the prediction of the host model to mimic the behaviors of potential surrogate models. Additionally, a negative shadow model is normally trained to imitate irrelevant independent models. Using this pair of shadow models as a reference, we design a strategy to update the trigger samples appropriately such that they tend to persist in the host model and its stolen copies. Moreover, our method could well support two specific embedding schemes: embedding the watermark via fine-tuning or from scratch. Our extensive experimental results on popular datasets demonstrate that our SSW approach outperforms state-of-the-art methods against various model extraction attacks in whether trigger set classification accuracy based or hypothesis test based verification. The results also show that our method is robust to common model modification schemes including fine-tuning and model compression.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Jingxuan Tan",
        "Nan Zhong",
        "Zhenxing Qian",
        "Xinpeng Zhang",
        "Sheng Li"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/b4149005980a11919731e6b4c1833d8b0af59424",
      "pdf_url": "",
      "publication_date": "2023-10-26",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e562c8ee825ed4e1b3ec4037f6e9e1194d355f07",
      "title": "Dual Student Networks for Data-Free Model Stealing",
      "abstract": "Existing data-free model stealing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent to optimizing a lower bound on the generator's loss if we had access to the target model gradients. We show that our new optimization framework provides more accurate gradient estimation of the target model and better accuracies on benchmark classification datasets. Additionally, our approach balances improved query efficiency with training computation cost. Finally, we demonstrate that our method serves as a better proxy model for transfer-based adversarial attacks than existing data-free model stealing methods.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "James Beetham",
        "Navid Kardan",
        "A. Mian",
        "M. Shah"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/e562c8ee825ed4e1b3ec4037f6e9e1194d355f07",
      "pdf_url": "https://arxiv.org/pdf/2309.10058",
      "publication_date": "2023-09-18",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "06718e68bea215f2155bca2e08b70ad5d2aff62f",
      "title": "QUDA: Query-Limited Data-Free Model Extraction",
      "abstract": "Model extraction attack typically refers to extracting non-public information from a black-box machine learning model. Its unauthorized nature poses significant threat to intellectual property rights of the model owners. By using the well-designed queries and the predictions returned from the victim model, the adversary is able to train a clone model from scratch to obtain similar functionality as victim model. Recently, some methods have been proposed to perform model extraction attacks without using any in-distribution data (Data-free setting). Although these methods have been shown to achieve high clone accuracy, their query budgets are typically around 10 million or even exceed 20 million in some datasets, which lead to a high cost of model stealing and can be easily defended by limiting the number of queries. To illustrate the severe threats induced by model extraction attacks with limited query budget in realistic scenarios, we propose QUDA \u2013 a novel QUey-limited DAta-free model extraction attack that incorporates GAN pre-trained by public unrelated dataset to provide weak image prior and the technique of deep reinforcement learning to make query generation strategy more efficient. Compared with the state-of-the-art data-free model extraction method, QUDA achieves better results under query-limited condition (0.1M query budget) in FMNIST and CIFAR-10 datasets, and even outperforms the baseline method in most cases when QUDA uses only 10% query budget of its. QUDA issued a warning that solely relying on the limited numbers of queries or the confidentiality of training data is not reliable to protect model\u2019s security and privacy. Potential countermeasures, such as detection-based defense approach, are also provided.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Zijun Lin",
        "Ke Xu",
        "Chengfang Fang",
        "Huadi Zheng",
        "Aneez Ahmed Jaheezuddin",
        "Jie Shi"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/06718e68bea215f2155bca2e08b70ad5d2aff62f",
      "pdf_url": "",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1b0e8f3727f452d8ef13950ae61c631be8956306",
      "title": "Model Extraction Attacks Revisited",
      "abstract": "Model extraction (ME) attacks represent one major threat to Machine-Learning-as-a-Service (MLaaS) platforms by \"stealing\" the functionality of confidential machine-learning models through querying black-box APIs. Over seven years have passed since ME attacks were first conceptualized in the seminal work [75]. During this period, substantial advances have been made in both ME attacks and MLaaS platforms, raising the intriguing question: How has the vulnerability of MLaaS platforms to ME attacks been evolving? In this work, we conduct an in-depth study to answer this critical question. Specifically, we characterize the vulnerability of current, mainstream MLaaS platforms to ME attacks from multiple perspectives including attack strategies, learning techniques, surrogatemodel design, and benchmark tasks. Many of our findings challenge previously reported results, suggesting emerging patterns of ME vulnerability. Further, by analyzing the vulnerability of the same MLaaS platforms using historical datasets from the past four years, we retrospectively characterize the evolution of ME vulnerability over time, leading to a set of interesting findings. Finally, we make suggestions about improving the current practice of MLaaS in terms of attack robustness. Our study sheds light on the current state of ME vulnerability in the wild and points to several promising directions for future research.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Jiacheng Liang",
        "Ren Pang",
        "Changjiang Li",
        "Ting Wang"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/1b0e8f3727f452d8ef13950ae61c631be8956306",
      "pdf_url": "https://arxiv.org/pdf/2312.05386",
      "publication_date": "2023-12-08",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6f387d2d7c0a3afa73c7204cdbc8e49eed536de8",
      "title": "Electromagnetic-Thermal Analysis With FDTD and Physics-Informed Neural Networks",
      "abstract": "This article presents the coupling of the finite-difference time-domain (FDTD) method for electromagnetic field simulation, with a physics-informed neural network based solver for the heat equation. To this end, we employ a physics-informed U-Net instead of a numerical method to solve the heat equation. This approach enables the solution of general multiphysics problems with a single-physics numerical solver coupled with a neural network, overcoming the questions of accuracy and efficiency that are associated with interfacing multiphysics equations. By embedding the heat equation and its boundary conditions in the U-Net, we implement an unsupervised training methodology, which does not require the generation of ground-truth data. We test the proposed method with general 2-D coupled electromagnetic-thermal problems, demonstrating its accuracy and efficiency compared to standard finite-difference based alternatives.",
      "year": 2023,
      "venue": "IEEE Journal on Multiscale and Multiphysics Computational Techniques",
      "authors": [
        "Shutong Qi",
        "C. Sarris"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/6f387d2d7c0a3afa73c7204cdbc8e49eed536de8",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "460e48454b209e957b4942303507bf756c4a6a31",
      "title": "On the Feasibility of Specialized Ability Stealing for Large Language Code Models",
      "abstract": null,
      "year": 2023,
      "venue": "",
      "authors": [
        "Zongjie Li"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/460e48454b209e957b4942303507bf756c4a6a31",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "406d4e8d2df6f6b58e65016fea31004f781d93e7",
      "title": "DisGUIDE: Disagreement-Guided Data-Free Model Extraction",
      "abstract": "Recent model-extraction attacks on Machine Learning as a Service (MLaaS) systems have moved towards data-free approaches, showing the feasibility of stealing models trained with difficult-to-access data. However, these attacks are ineffective or limited due to the low accuracy of extracted models and the high number of queries to the models under attack. The high query cost makes such techniques infeasible for online MLaaS systems that charge per query.\nWe create a novel approach to get higher accuracy and query efficiency than prior data-free model extraction techniques. Specifically, we introduce a novel generator training scheme that maximizes the disagreement loss between two clone models that attempt to copy the model under attack. This loss, combined with diversity loss and experience replay, enables the generator to produce better instances to train the clone models. Our evaluation on popular datasets CIFAR-10 and CIFAR-100 shows that our approach improves the final model accuracy by up to 3.42% and 18.48% respectively. The average number of queries required to achieve the accuracy of the prior state of the art is reduced by up to 64.95%. We hope this will promote future work on feasible data-free model extraction and defenses against such attacks.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jonathan Rosenthal",
        "Eric Enouen",
        "H. Pham",
        "Lin Tan"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/406d4e8d2df6f6b58e65016fea31004f781d93e7",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/26150/25922",
      "publication_date": "2023-06-26",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4b99fb1fc7f60a16c4cc99a07d931fd79cf993e0",
      "title": "A Threshold Implementation-Based Neural Network Accelerator With Power and Electromagnetic Side-Channel Countermeasures",
      "abstract": "With the recent advancements in machine learning (ML) theory, a lot of energy-efficient neural network (NN) accelerators have been developed. However, their associated side-channel security vulnerabilities pose a major concern. There have been several proof-of-concept attacks demonstrating the extraction of their model parameters and input data. This work introduces a threshold implementation (TI) masking-based NN accelerator that secures model parameters and inputs against power and electromagnetic (EM) side-channel attacks. The 0.159 mm2 demonstration in 28 nm runs at 125 MHz at 0.95 V and limits the area and energy overhead to 64% and $5.5\\times $ , respectively, while demonstrating security even greater than 2M traces. The accelerator also secures model parameters through encryption and the inputs against horizontal power analysis (HPA) attacks.",
      "year": 2023,
      "venue": "IEEE Journal of Solid-State Circuits",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "Samuel H. Fuller",
        "A. Chandrakasan"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/4b99fb1fc7f60a16c4cc99a07d931fd79cf993e0",
      "pdf_url": "",
      "publication_date": "2023-01-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "114c2f2cc2ab8a80a6229339a0c532ff4d59caed",
      "title": "Defending against Data-Free Model Extraction by Distributionally Robust Defensive Training",
      "abstract": null,
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Zhenyi Wang",
        "Li Shen",
        "Tongliang Liu",
        "Tiehang Duan",
        "Yanjun Zhu",
        "Dongling Zhan",
        "D. Doermann",
        "Mingchen Gao"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/114c2f2cc2ab8a80a6229339a0c532ff4d59caed",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "45310a683fa761bbaa03ea9969fcf5bc7021624d",
      "title": "SCMA: A Scattering Center Model Attack on CNN-SAR Target Recognition",
      "abstract": "Convolutional neural networks (CNNs) have been widely used in synthetic aperture radar (SAR) target recognition, which can extract feature automatically. However, due to its own structural flaws, CNNs are easy to be fooled by adversarial examples, even if they have excellent performance. In this letter, a novel attack named scattering center model attack (SCMA) is designed, and its generation process does not rely on the prior knowledge of any neural network. Therefore, we can get a stable way which can be applied to any neural network. In addition, an improved scattering center model extraction method, which is the pre-part of SCMA, can filter out the useless noise to optimize the stability of attack. In the experiment, SCMA is compared with advanced attack algorithms. From the experimental results, it is clear to find that SCMA has excellent performance in terms of transfer attack success rate. Furthermore, visualization and interpretability analysis underpin the theoretical feasibility of SCMA.",
      "year": 2023,
      "venue": "IEEE Geoscience and Remote Sensing Letters",
      "authors": [
        "Weibo Qin",
        "Bo Long",
        "Feng Wang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/45310a683fa761bbaa03ea9969fcf5bc7021624d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "title": "Categorical Inference Poisoning: Verifiable Defense Against Black-Box DNN Model Stealing Without Constraining Surrogate Data and Query Times",
      "abstract": "Deep Neural Network (DNN) models have offered powerful solutions for a wide range of tasks, but the cost to develop such models is nontrivial, which calls for effective model protection. Although black-box distribution can mitigate some threats, model functionality can still be stolen via black-box surrogate attacks. Recent studies have shown that surrogate attacks can be launched in several ways, while the existing defense methods commonly assume attackers with insufficient in-distribution (ID) data and restricted attacking strategies. In this paper, we relax these constraints and assume a practical threat model in which the adversary not only has sufficient ID data and query times but also can adjust the surrogate training data labeled by the victim model. Then, we propose a two-step categorical inference poisoning (CIP) framework, featuring both poisoning for performance degradation (PPD) and poisoning for backdooring (PBD). In the first poisoning step, incoming queries are classified into ID and (out-of-distribution) OOD ones using an energy score (ES) based OOD detector, and the latter are further classified into high ES and low ES ones, which are subsequently passed to a strong and a weak PPD process, respectively. In the second poisoning step, difficult ID queries are detected by a proposed reliability score (RS) measurement and are passed to PBD. In doing so, the first step OOD poisoning leads to substantial performance degradation in surrogate models, the second step ID poisoning further embeds backdoors in them, while both can preserve model fidelity. Extensive experiments confirm that CIP can not only achieve promising performance against state-of-the-art black-box surrogate attacks like KnockoffNets and data-free model extraction (DFME) but also work well against stronger attacks with sufficient ID and deceptive data, better than the existing dynamic adversarial watermarking (DAWN) and deceptive perturbation defense methods. PyTorch code is available at https://github.com/Hatins/CIP_master.git.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Haitian Zhang",
        "Guang Hua",
        "Xinya Wang",
        "Hao Jiang",
        "Wen Yang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
      "title": "Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks",
      "abstract": "Despite the broad application of Machine Learning models as a Service (MLaaS), they are vulnerable to model stealing attacks. These attacks can replicate the model functionality by using the black-box query process without any prior knowledge of the target victim model. Existing stealing defenses add deceptive perturbations to the victim's posterior probabilities to mislead the attackers. However, these defenses are now suffering problems of high inference computational overheads and unfavorable trade-offs between benign accuracy and stealing robustness, which challenges the feasibility of deployed models in practice. To address the problems, this paper proposes Isolation and Induction (InI), a novel and effective training framework for model stealing defenses. Instead of deploying auxiliary defense modules that introduce redundant inference time, InI directly trains a defensive model by isolating the adversary's training gradient from the expected gradient, which can effectively reduce the inference computational cost. In contrast to adding perturbations over model predictions that harm the benign accuracy, we train models to produce uninformative outputs against stealing queries, which can induce the adversary to extract little useful knowledge from victim models with minimal impact on the benign performance. Extensive experiments on several visual classification datasets (e.g., MNIST and CIFAR10) demonstrate the superior robustness (up to 48% reduction on stealing accuracy) and speed (up to 25.4\u00d7 faster) of our InI over other state-of-the-art methods. Our codes can be found in https://github.com/DIG-Beihang/InI-Model-Stealing-Defense.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Jun Guo",
        "Xingyu Zheng",
        "Aishan Liu",
        "Siyuan Liang",
        "Yisong Xiao",
        "Yichao Wu",
        "Xianglong Liu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
      "pdf_url": "http://arxiv.org/pdf/2308.00958",
      "publication_date": "2023-08-02",
      "keywords_matched": [
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fb1f43004e7878c2da6ab86fb7427058a8ddedf7",
      "title": "B3: Backdoor Attacks against Black-box Machine Learning Models",
      "abstract": "Backdoor attacks aim to inject backdoors to victim machine learning models during training time, such that the backdoored model maintains the prediction power of the original model towards clean inputs and misbehaves towards backdoored inputs with the trigger. The reason for backdoor attacks is that resource-limited users usually download sophisticated models from model zoos or query the models from MLaaS rather than training a model from scratch, thus a malicious third party has a chance to provide a backdoored model. In general, the more precious the model provided (i.e., models trained on rare datasets), the more popular it is with users. In this article, from a malicious model provider perspective, we propose a black-box backdoor attack, named B3, where neither the rare victim model (including the model architecture, parameters, and hyperparameters) nor the training data is available to the adversary. To facilitate backdoor attacks in the black-box scenario, we design a cost-effective model extraction method that leverages a carefully constructed query dataset to steal the functionality of the victim model with a limited budget. As the trigger is key to successful backdoor attacks, we develop a novel trigger generation algorithm that intensifies the bond between the trigger and the targeted misclassification label through the neuron with the highest impact on the targeted label. Extensive experiments have been conducted on various simulated deep learning models and the commercial API of Alibaba Cloud Compute Service. We demonstrate that B3 has a high attack success rate and maintains high prediction accuracy for benign inputs. It is also shown that B3 is robust against state-of-the-art defense strategies against backdoor attacks, such as model pruning and NC.",
      "year": 2023,
      "venue": "ACM Transactions on Privacy and Security",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Wenbin Yang",
        "Huayang Huang",
        "Qian Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/fb1f43004e7878c2da6ab86fb7427058a8ddedf7",
      "pdf_url": "",
      "publication_date": "2023-06-22",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "14b4aff027ccf8fde0b19ac60b8e653c621aff30",
      "title": "Practical and Efficient Model Extraction of Sentiment Analysis APIs",
      "abstract": "Despite their stunning performance, developing deep learning models from scratch is a formidable task. Therefore, it popularizes Machine-Learning-as-a-Service (MLaaS), where general users can access the trained models of MLaaS providers via Application Programming Interfaces (APIs) on a pay-per-query basis. Unfortunately, the success of MLaaS is under threat from model extraction attacks, where attackers intend to extract a local model of equivalent functionality to the target MLaaS model. However, existing studies on model extraction of text analytics APIs frequently assume adversaries have strong knowledge about the victim model, like its architecture and parameters, which hardly holds in practice. Besides, since the attacker's and the victim's training data can be considerably discrepant, it is non-trivial to perform efficient model extraction. In this paper, to advance the understanding of such attacks, we propose a framework, PEEP, for practical and efficient model extraction of sentiment analysis APIs with only query access. Specifically, PEEP features a learning-based scheme, which employs out-of-domain public corpora and a novel query strategy to construct proxy training data for model extraction. Besides, PEEP introduces a greedy search algorithm to settle an appropriate architecture for the extracted model. We conducted extensive experiments with two victim models across three datasets and two real-life commercial sentiment analysis APIs. Experimental results corroborate that PEEP can consistently outperform the state-of-the-art baselines in terms of effectiveness and efficiency.",
      "year": 2023,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Weibin Wu",
        "Jianping Zhang",
        "Victor Junqiu Wei",
        "Xixian Chen",
        "Zibin Zheng",
        "Irwin King",
        "M. Lyu"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/14b4aff027ccf8fde0b19ac60b8e653c621aff30",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c65feaf2681a5676204a43101edff4d897d37ffb",
      "title": "Explanation-based data-free model extraction attacks",
      "abstract": null,
      "year": 2023,
      "venue": "World wide web (Bussum)",
      "authors": [
        "Anli Yan",
        "Ruitao Hou",
        "Hongyang Yan",
        "Xiaozhang Liu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/c65feaf2681a5676204a43101edff4d897d37ffb",
      "pdf_url": "",
      "publication_date": "2023-06-02",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "05ec4be7a3ec9dc2714f23f7921b31e3fece6c98",
      "title": "Dynamic Analysis and FPGA Implementation of a New Fractional-Order Hopfield Neural Network System under Electromagnetic Radiation",
      "abstract": "Fractional calculus research indicates that, within the field of neural networks, fractional-order systems more accurately simulate the temporal memory effects present in the human brain. Therefore, it is worthwhile to conduct an in-depth investigation into the complex dynamics of fractional-order neural networks compared to integer-order models. In this paper, we propose a magnetically controlled, memristor-based, fractional-order chaotic system under electromagnetic radiation, utilizing the Hopfield neural network (HNN) model with four neurons as the foundation. The proposed system is solved by using the Adomain decomposition method (ADM). Then, through dynamic simulations of the internal parameters of the system, rich dynamic behaviors are found, such as chaos, quasiperiodicity, direction-controllable multi-scroll, and the emergence of analogous symmetric dynamic behaviors in the system as the radiation parameters are altered, with the order remaining constant. Finally, we implement the proposed new fractional-order HNN system on a field-programmable gate array (FPGA). The experimental results show the feasibility of the theoretical analysis.",
      "year": 2023,
      "venue": "Biomimetics",
      "authors": [
        "Fei Yu",
        "Yue Lin",
        "Si Xu",
        "Wei Yao",
        "Yumba Musoya Gracia",
        "Shuo Cai"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/05ec4be7a3ec9dc2714f23f7921b31e3fece6c98",
      "pdf_url": "https://www.mdpi.com/2313-7673/8/8/559/pdf?version=1700538845",
      "publication_date": "2023-11-21",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6f682e862a230feffedbbd918deccc2425be09fb",
      "title": "An End-to-End Neural Network for Complex Electromagnetic Simulations",
      "abstract": "Although many numerical methods can accurately solve time-domain electromagnetic (EM) simulation problems, such as finite-difference time-domain (FDTD), the computational demands are usually significant for complex scenarios. In this letter, we investigate the feasibility of applying deep learning technology to accurately solving EM forward simulations. Based on an end-to-end neural network framework, a convolutional neural network is used to extract features of scatters, and a long short-term memory neural network is used to predict EM distributions. To ensure the accuracy of the framework, especially when dealing with complicated phenomena, principal component analysis is employed to compress data sets before training. Numerical experiments show that the proposed scheme can predict EM field distributions efficiently and accurately for complex scenarios containing scatters of different materials, locations, geometrical shapes, and random numbers. The average relative mean square error is around 8. 05e\u22125 for scenarios with certain number of scatters and 2.25e\u22124 for random number of scatters respectively, which outperforms other neural network frameworks. Meanwhile, compared with FDTD, the time speedup is around 1528 times.",
      "year": 2023,
      "venue": "IEEE Antennas and Wireless Propagation Letters",
      "authors": [
        "Menglin Zhai",
        "Yaobo Chen",
        "Longting Xu",
        "Wen-Yan Yin"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/6f682e862a230feffedbbd918deccc2425be09fb",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "51e6982bb24b76d018d96432e032703e7ea35ef4",
      "title": "AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against Decision Tree Models",
      "abstract": null,
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Abdullah \u00c7aglar \u00d6ks\u00fcz",
        "Anisa Halimi",
        "Erman Ayday"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/51e6982bb24b76d018d96432e032703e7ea35ef4",
      "pdf_url": "http://arxiv.org/pdf/2302.02162",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c1a2f4520dfd66119a07fd3e0754e4a9aecbc78f",
      "title": "MERCURY: An Automated Remote Side-channel Attack to Nvidia Deep Learning Accelerator",
      "abstract": "DNN accelerators have been widely deployed in many scenarios to speed up the inference process and reduce the energy consumption. One big concern about the usage of the accelerators is the confidentiality of the deployed models: model inference execution on the accelerators could leak side-channel information, which enables an adversary to preciously recover the model details. Such model extraction attacks can not only compromise the intellectual property of DNN models, but also facilitate some adversarial attacks. Although previous works have demonstrated a number of side-channel techniques to extract models from DNN accelerators, they are not practical for two reasons. (1) They only target simplified accelerator implementations, which have limited practicality in the real world. (2) They require heavy human analysis and domain knowledge. To overcome these limitations, this paper presents MERCURY, the first automated remote side-channel attack against the off-the-shelf Nvidia DNN accelerator. The key insight of MERCURY is to model the side-channel extraction process as a sequence-to-sequence problem. The adversary can leverage a time-to-digital converter (TDC) to remotely collect the power trace of the target model\u2019s inference. Then he uses a learning model to automatically recover the architecture details of the victim model from the power trace without any prior knowledge. The adversary can further use the attention mechanism to localize the leakage points that contribute most to the attack. Evaluation results indicate that MERCURY can keep the error rate of model extraction below 1%.",
      "year": 2023,
      "venue": "International Conference on Field-Programmable Technology",
      "authors": [
        "Xi-ai Yan",
        "Xiaoxuan Lou",
        "Guowen Xu",
        "Han Qiu",
        "Shangwei Guo",
        "Chip-Hong Chang",
        "Tianwei Zhang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/c1a2f4520dfd66119a07fd3e0754e4a9aecbc78f",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/171839/2/_DR_NTU_An_Automated_Remote_Side_channel_Attack_to_FPGA_based_DNN_Accelerators.pdf",
      "publication_date": "2023-08-02",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "148adb6df70218017aba770047cacb3c9e745411",
      "title": "A Desynchronization-Based Countermeasure Against Side-Channel Analysis of Neural Networks",
      "abstract": "Model extraction attacks have been widely applied, which can normally be used to recover confidential parameters of neural networks for multiple layers. Recently, side-channel analysis of neural networks allows parameter extraction even for networks with several multiple deep layers with high effectiveness. It is therefore of interest to implement a certain level of protection against these attacks. In this paper, we propose a desynchronization-based countermeasure that makes the timing analysis of activation functions harder. We analyze the timing properties of several activation functions and design the desynchronization in a way that the dependency on the input and the activation type is hidden. We experimentally verify the effectiveness of the countermeasure on a 32-bit ARM Cortex-M4 microcontroller and employ a t-test to show the side-channel information leakage. The overhead ultimately depends on the number of neurons in the fully-connected layer, for example, in the case of 4096 neurons in VGG-19, the overheads are between 2.8% and 11%.",
      "year": 2023,
      "venue": "International Conference on Cyber Security Cryptography and Machine Learning",
      "authors": [
        "J. Breier",
        "Dirmanto Jap",
        "Xiaolu Hou",
        "S. Bhasin"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/148adb6df70218017aba770047cacb3c9e745411",
      "pdf_url": "http://arxiv.org/pdf/2303.18132",
      "publication_date": "2023-03-25",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8ffd3bb7b0b26b33fd6f317052214e6e84dec291",
      "title": "Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders",
      "abstract": "Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose Bucks for Buckets (B4B), the first active defense that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task.vB4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space. To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations. This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy. Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jan Dubi'nski",
        "Stanislaw Pawlak",
        "Franziska Boenisch",
        "Tomasz Trzci'nski",
        "Adam Dziedzic"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/8ffd3bb7b0b26b33fd6f317052214e6e84dec291",
      "pdf_url": "https://arxiv.org/pdf/2310.08571",
      "publication_date": "2023-10-12",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b885e291f660df34d9f22777dc0678bdf8e0860d",
      "title": "Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data",
      "abstract": "We study design of black-box model extraction attacks that can send minimal number of queries from a publicly available dataset to a target ML model through a predictive API with an aim to create an informative and distributionally equivalent replica of the target. First, we define distributionally equivalent and Max-Information model extraction attacks, and reduce them into a variational optimisation problem. The attacker sequentially solves this optimisation problem to select the most informative queries that simultaneously maximise the entropy and reduce the mismatch between the target and the stolen models. This leads to an active sampling-based query selection algorithm, Marich, which is model-oblivious. Then, we evaluate Marich on different text and image data sets, and different models, including CNNs and BERT. Marich extracts models that achieve $\\sim 60-95\\%$ of true model's accuracy and uses $\\sim 1,000 - 8,500$ queries from the publicly available datasets, which are different from the private training datasets. Models extracted by Marich yield prediction distributions, which are $\\sim 2-4\\times$ closer to the target's distribution in comparison to the existing active sampling-based attacks. The extracted models also lead to $84-96\\%$ accuracy under membership inference attacks. Experimental results validate that Marich is query-efficient, and capable of performing task-accurate, high-fidelity, and informative model extraction.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Pratik Karmakar",
        "D. Basu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/b885e291f660df34d9f22777dc0678bdf8e0860d",
      "pdf_url": "http://arxiv.org/pdf/2302.08466",
      "publication_date": "2023-02-16",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3c103408ff825aad19d715edc01025a8c3fccdb4",
      "title": "EZClone: Improving DNN Model Extraction Attack via Shape Distillation from GPU Execution Profiles",
      "abstract": "Deep Neural Networks (DNNs) have become ubiquitous due to their performance on prediction and classification problems. However, they face a variety of threats as their usage spreads. Model extraction attacks, which steal DNNs, endanger intellectual property, data privacy, and security. Previous research has shown that system-level side-channels can be used to leak the architecture of a victim DNN, exacerbating these risks. We propose two DNN architecture extraction techniques catering to various threat models. The first technique uses a malicious, dynamically linked version of PyTorch to expose a victim DNN architecture through the PyTorch profiler. The second, called EZClone, exploits aggregate (rather than time-series) GPU profiles as a side-channel to predict DNN architecture, employing a simple approach and assuming little adversary capability as compared to previous work. We investigate the effectiveness of EZClone when minimizing the complexity of the attack, when applied to pruned models, and when applied across GPUs. We find that EZClone correctly predicts DNN architectures for the entire set of PyTorch vision architectures with 100% accuracy. No other work has shown this degree of architecture prediction accuracy with the same adversarial constraints or using aggregate side-channel information. Prior work has shown that, once a DNN has been successfully cloned, further attacks such as model evasion or model inversion can be accelerated significantly.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jonah O'Brien Weiss",
        "Tiago A. O. Alves",
        "S. Kundu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/3c103408ff825aad19d715edc01025a8c3fccdb4",
      "pdf_url": "http://arxiv.org/pdf/2304.03388",
      "publication_date": "2023-04-06",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "aa4824feda684fd8d1cedb5361574cc4cff35d75",
      "title": "LibSteal: Model Extraction Attack Towards Deep Learning Compilers by Reversing DNN Binary Library",
      "abstract": ": The need for Deep Learning (DL) based services has rapidly increased in the past years. As part of the trend, the privatization of Deep Neural Network (DNN) models has become increasingly popular. The authors give customers or service providers direct access to their created models and let them deploy models on devices or infrastructure out of the control of the authors. Meanwhile, the emergence of DL Compilers makes it possible to compile a DNN model into a lightweight binary for faster inference, which is attractive to many stakeholders. However, distilling the essence of a model into a binary that is free to be examined by untrusted parties creates a chance to leak essential information. With only DNN binary library, it is possible to extract neural network architecture using reverse engineering. In this paper, we present LibSteal . This framework can leak DNN architecture information by reversing the binary library generated from the DL Compiler, which is similar to or even equivalent to the original. The evaluation shows that LibSteal can efficiently steal the architecture information of victim DNN models. After training the extracted models with the same hyper-parameter, we can achieve accuracy comparable to that of the original models.",
      "year": 2023,
      "venue": "International Conference on Evaluation of Novel Approaches to Software Engineering",
      "authors": [
        "Jinquan Zhang",
        "Pei Wang",
        "Dinghao Wu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/aa4824feda684fd8d1cedb5361574cc4cff35d75",
      "pdf_url": "https://doi.org/10.5220/0011754900003464",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4ae98576016b691dfda5a78d0a88d19e8ce15103",
      "title": "Holistic Implicit Factor Evaluation of Model Extraction Attacks",
      "abstract": "Model extraction attacks (MEAs) allow adversaries to replicate a surrogate model analogous to the target model's decision pattern. While several attacks and defenses have been studied in-depth, the underlying reasons behind our susceptibility to them often remain unclear. Analyzing these implication influence factors helps to promote secure deep learning (DL) systems, it requires studying extraction attacks in various scenarios to determine the success of different attacks and the hallmarks of DLs. However, understanding, implementing, and evaluating even a single attack requires extremely high technical effort, making it impractical to study the vast number of unique extraction attack scenarios. To this end, we present a first-of-its-kind holistic evaluation of implication factors for MEAs which relies on the attack process abstracted from state-of-the-art MEAs. Specifically, we concentrate on four perspectives. we consider the impact of the task accuracy, model architecture, and robustness of the target model on MEAs, as well as the impact of the model architecture of the surrogate model on MEAs. Our empirical evaluation includes an ablation study over sixteen model architectures and four image datasets. Surprisingly, our study shows that improving the robustness of the target model via adversarial training is more vulnerable to model extraction attacks.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Anli Yan",
        "Hongyang Yan",
        "Li Hu",
        "Xiaozhang Liu",
        "Teng Huang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/4ae98576016b691dfda5a78d0a88d19e8ce15103",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7b6db013d28e72374f301f758f432545a92b22fb",
      "title": "DivTheft: An Ensemble Model Stealing Attack by Divide-and-Conquer",
      "abstract": "Recently, model stealing attacks are widely studied but most of them are focused on stealing a single non-discrete model, e.g., neural networks. For ensemble models, these attacks are either non-executable or suffer from intolerant performance degradation due to the complex model structure (multiple sub-models) and the discreteness possessed by the sub-model (e.g., decision trees). To overcome the bottleneck, this paper proposes a divide-and-conquer strategy called DivTheft to formulate the model stealing attack to common ensemble models by combining active learning (AL). Specifically, based on the boosting learning concept, we divide a hard ensemble model stealing task into multiple simpler ones about single sub-model stealing. Then, we adopt AL to conquer the data-free sub-model stealing task. During the process, the current AL algorithm easily causes the stolen model to be biased because of ignoring the past useful memories. Thus, DivTheft involves a newly designed uncertainty sampling scheme to filter reusable samples from the previously used ones. Experiments show that compared with the prior work, DivTheft can save almost 50% queries while ensuring a competitive agreement rate to the victim model.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Zhuo Ma",
        "Xinjing Liu",
        "Yang Liu",
        "Ximeng Liu",
        "Zhan Qin",
        "Kui Ren"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/7b6db013d28e72374f301f758f432545a92b22fb",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "aa4acbad4d6a3a8195d70c174564df752a5e1ba5",
      "title": "MeaeQ: Mount Model Extraction Attacks with Efficient Queries",
      "abstract": "We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based sampling strategies on publicly available, unannotated data sources. However, these methods often result in selected queries that lack task relevance and data diversity, leading to limited success in achieving satisfactory results with low query costs. In this paper, we propose MeaeQ (Model extraction attack with efficient Queries), a straightforward yet effective method to address these issues. Specifically, we initially utilize a zero-shot sequence inference classifier, combined with API service information, to filter task-relevant data from a public text corpus instead of a problem domain-specific dataset. Furthermore, we employ a clustering-based data reduction technique to obtain representative data as queries for the attack. Extensive experiments conducted on four benchmark datasets demonstrate that MeaeQ achieves higher functional similarity to the victim model than baselines while requiring fewer queries. Our code is available at https://github.com/C-W-D/MeaeQ.",
      "year": 2023,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Chengwei Dai",
        "Minxuan Lv",
        "Kun Li",
        "Wei Zhou"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/aa4acbad4d6a3a8195d70c174564df752a5e1ba5",
      "pdf_url": "",
      "publication_date": "2023-10-21",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "13f08d0ed26a48bb4fa16951e2dbbd87d0ba4797",
      "title": "Defense Against Model Extraction Attacks on Recommender Systems",
      "abstract": "The robustness of recommender systems has become a prominent topic within the research community. Numerous adversarial attacks have been proposed, but most of them rely on extensive prior knowledge, such as all the white-box attacks or most of the black-box attacks which assume that certain external knowledge is available. Among these attacks, the model extraction attack stands out as a promising and practical method, involving training a surrogate model by repeatedly querying the target model. However, there is a significant gap in the existing literature when it comes to defending against model extraction attacks on recommender systems. In this paper, we introduce Gradient-based Ranking Optimization (GRO), which is the first defense strategy designed to counter such attacks. We formalize the defense as an optimization problem, aiming to minimize the loss of the protected target model while maximizing the loss of the attacker's surrogate model. Since top-k ranking lists are non-differentiable, we transform them into swap matrices which are instead differentiable. These swap matrices serve as input to a student model that emulates the surrogate model's behavior. By back-propagating the loss of the student model, we obtain gradients for the swap matrices. These gradients are used to compute a swap loss, which maximizes the loss of the student model. We conducted experiments on three benchmark datasets to evaluate the performance of GRO, and the results demonstrate its superior effectiveness in defending against model extraction attacks.",
      "year": 2023,
      "venue": "Web Search and Data Mining",
      "authors": [
        "Sixiao Zhang",
        "Hongzhi Yin",
        "Hongxu Chen",
        "Cheng Long"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/13f08d0ed26a48bb4fa16951e2dbbd87d0ba4797",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3616855.3635751",
      "publication_date": "2023-10-25",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "faf8e2bc0cd3d33ca3c6fb80fc44d8832fa2cb1e",
      "title": "Multi-scroll Hopfield neural network under electromagnetic radiation and its brain-like coupling synchronization",
      "abstract": "Multi-scroll attractors have attracted attention because of their more complex topological structures and artificially controllable attractor structures. This paper proposes a new nonvolatile magnetic-controlled memristor and uses it to simulate the effect of membrane flux changes caused by neuronal exposure to electromagnetic radiation. A series of complex chaotic phenomena are found by plotting phase diagrams, bifurcation diagrams, attractor domains and 01 tests, including multi-scroll chaotic attractors controlled by memristors, symmetric bifurcation behavior, coexistence phenomena enhanced by initial offset. The mechanisms behind them are explained through equilibrium point analysis. A dual memristive HNN (MHNN) coupling synchronization model is proposed to simulate the synchronization between regions within the human brain. The Lyapunov function of the error is constructed to prove that this coupling synchronization scheme is ultimately bounded. The feasibility of this synchronization scheme is verified by establishing a Simulink model and conducting simulation experiments.",
      "year": 2023,
      "venue": "Frontiers of Physics",
      "authors": [
        "Sen Fu",
        "Xia Wang",
        "Haiyang Gu",
        "Xiaojing Cao",
        "Z. Yao"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/faf8e2bc0cd3d33ca3c6fb80fc44d8832fa2cb1e",
      "pdf_url": "https://www.frontiersin.org/articles/10.3389/fphy.2023.1252568/pdf",
      "publication_date": "2023-08-25",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "23faead2015ba1e36a2c0c535f987c0b36ba8534",
      "title": "Data-Free Hard-Label Robustness Stealing Attack",
      "abstract": "The popularity of Machine Learning as a Service (MLaaS) has led to increased concerns about Model Stealing Attacks (MSA), which aim to craft a clone model by querying MLaaS. Currently, most research on MSA assumes that MLaaS can provide soft labels and that the attacker has a proxy dataset with a similar distribution. However, this fails to encapsulate the more practical scenario where only hard labels are returned by MLaaS and the data distribution remains elusive. Furthermore, most existing work focuses solely on stealing the model accuracy, neglecting the model robustness, while robustness is essential in security-sensitive scenarios, e.g, face-scan payment. Notably, improving model robustness often necessitates the use of expensive techniques such as adversarial training, thereby further making stealing robustness a more lucrative prospect. In response to these identified gaps, we introduce a novel Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which enables the stealing of both model accuracy and robustness by simply querying hard labels of the target model without the help of any natural data. Comprehensive experiments demonstrate the effectiveness of our method. The clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51% against AutoAttack, which are only 4.71% and 8.40% lower than the target model on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is available at: https://github.com/LetheSec/DFHL-RS-Attack.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Xiaojian \\ Yuan",
        "Kejiang Chen",
        "Wen Huang",
        "Jie Zhang",
        "Weiming Zhang",
        "Neng H. Yu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/23faead2015ba1e36a2c0c535f987c0b36ba8534",
      "pdf_url": "",
      "publication_date": "2023-12-10",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "18918a72fda197ea02671a13c49a95d6b95fc0f3",
      "title": "AUTOLYCUS: Exploiting Explainable Artificial Intelligence (XAI) for Model Extraction Attacks against Interpretable Models",
      "abstract": "Explainable Artificial Intelligence (XAI) aims to uncover the decision-making processes of AI models. However, the data used for such explanations can pose security and privacy risks. Existing literature identifies attacks on machine learning models, including membership inference, model inversion, and model extraction attacks. These attacks target either the model or the training data, depending on the settings and parties involved. XAI tools can increase the vulnerability of model extraction attacks, which is a concern when model owners prefer black-box access, thereby keeping model parameters and architecture private. To exploit this risk, we propose AUTOLYCUS, a novel retraining (learning) based model extraction attack framework against interpretable models under black-box settings. As XAI tools, we exploit Local Interpretable Model-Agnostic Explanations (LIME) and Shapley values (SHAP) to infer decision boundaries and create surrogate models that replicate the functionality of the target model. LIME and SHAP are mainly chosen for their realistic yet information-rich explanations, coupled with their extensive adoption, simplicity, and usability. We evaluate AUTOLYCUS on six machine learning datasets, measuring the accuracy and similarity of the surrogate model to the target model. The results show that AUTOLYCUS is highly effective, requiring significantly fewer queries compared to state-of-the-art attacks, while maintaining comparable accuracy and similarity. We validate its performance and transferability on multiple interpretable ML models, including decision trees, logistic regression, naive bayes, and k-nearest neighbor. Additionally, we show the resilience of AUTOLYCUS against proposed countermeasures.",
      "year": 2023,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Abdullah \u00c7aglar \u00d6ks\u00fcz",
        "Anisa Halimi",
        "Erman Ayday"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/18918a72fda197ea02671a13c49a95d6b95fc0f3",
      "pdf_url": "",
      "publication_date": "2023-02-04",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "eca3d9bca53842a53b65594762397e583901c437",
      "title": "Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models",
      "abstract": "Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of the most significant bits with about 1500 crafted inputs. These information enable to efficiently train a substitute model, with only 8% of the training dataset, that reaches high fidelity and near identical accuracy level than the victim model.",
      "year": 2023,
      "venue": "ESORICS Workshops",
      "authors": [
        "Kevin Hector",
        "Pierre-Alain Mo\u00ebllic",
        "Mathieu Dumont",
        "J. Dutertre"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eca3d9bca53842a53b65594762397e583901c437",
      "pdf_url": "https://arxiv.org/pdf/2308.16703",
      "publication_date": "2023-08-31",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2b805ba343b54c03ffc637cf9d80a65d68d7ddbd",
      "title": "Deep-Learning Model Extraction Through Software-Based Power Side-Channel",
      "abstract": "Deep learning (DL) techniques have been increasingly applied across various applications, facing a growing number of security threats. One such threat is model extraction, an attack that steals the Intellectual Property of DL models, either by recovering the same functionality or retrieving high-fidelity models. Current model extraction methods can be categorized as learning-based or cryptanalytic, with the latter relying on model queries and computational methods to recover parameters. However, these are limited to shallow neural networks and are computationally prohibitive for deeper DL models. In this paper, we propose leveraging software-based power analysis, specifically the Intel Running Average Power Limit (RAPL) technique, for DL model extraction. RAPL allows us to measure power leakage of the most popular activation function, ReLU, through a software interface. Consequently, the ReLU branch direction can be leaked in the software power side-channel, a vulnerability common in many state-of-the-art DL frameworks. We introduce a novel methodology for model extraction Algorithm from input gradient assisted by side channel information. We implement our attack on the oneDNN framework, the most popular library on Intel processors. Compared to prior work, our model extraction, assisted by the software power side-channel, only requires 0.8% of the queries to retrieve as-layer MLP. We also successfully apply our method to a common Convolutional Neural Network (CNN) - Lenet-5. To the best of our knowledge, this is the first work that extracts CNN models with more than 5 layers based solely on queries and software.",
      "year": 2023,
      "venue": "2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)",
      "authors": [
        "Xiang Zhang",
        "A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2b805ba343b54c03ffc637cf9d80a65d68d7ddbd",
      "pdf_url": "",
      "publication_date": "2023-10-28",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a0a7b1aabe2f14696b15bac592b0ad5743ef0b85",
      "title": "Data-Free Model Extraction Attacks in the Context of Object Detection",
      "abstract": "A significant number of machine learning models are vulnerable to model extraction attacks, which focus on stealing the models by using specially curated queries against the target model. This task is well accomplished by using part of the training data or a surrogate dataset to train a new model that mimics a target model in a white-box environment. In pragmatic situations, however, the target models are trained on private datasets that are inaccessible to the adversary. The data-free model extraction technique replaces this problem when it comes to using queries artificially curated by a generator similar to that used in Generative Adversarial Nets. We propose for the first time, to the best of our knowledge, an adversary black box attack extending to a regression problem for predicting bounding box coordinates in object detection. As part of our study, we found that defining a loss function and using a novel generator setup is one of the key aspects in extracting the target model. We find that the proposed model extraction method achieves significant results by using reasonable queries. The discovery of this object detection vulnerability will support future prospects for securing such models.",
      "year": 2023,
      "venue": "International Conference on Virtual Storytelling",
      "authors": [
        "Harshit Shah",
        "G. Aravindhan",
        "Pavan Kulkarni",
        "Yuvaraj Govidarajulu",
        "Manojkumar Somabhai Parmar"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a0a7b1aabe2f14696b15bac592b0ad5743ef0b85",
      "pdf_url": "https://arxiv.org/pdf/2308.05127",
      "publication_date": "2023-08-09",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8504a0c28bd68d820ba6e4e2102ee3e7ebf57df0",
      "title": "Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers",
      "abstract": "Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction, the complexity of the attack is relatively low and we highlight the urgent need for practicable protections that could fit the strong memory and latency requirements of such platforms.",
      "year": 2023,
      "venue": "Smart Card Research and Advanced Application Conference",
      "authors": [
        "Raphael Joud",
        "Pierre-Alain Mo\u00ebllic",
        "S. Ponti\u00e9",
        "J. Rigaud"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/8504a0c28bd68d820ba6e4e2102ee3e7ebf57df0",
      "pdf_url": "",
      "publication_date": "2023-11-02",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "34bed407d65517ed2c8b98bab3a33da175677c59",
      "title": "A Plot is Worth a Thousand Words: Model Information Stealing Attacks via Scientific Plots",
      "abstract": "Building advanced machine learning (ML) models requires expert knowledge and many trials to discover the best architecture and hyperparameter settings. Previous work demonstrates that model information can be leveraged to assist other attacks, such as membership inference, generating adversarial examples. Therefore, such information, e.g., hyperparameters, should be kept confidential. It is well known that an adversary can leverage a target ML model's output to steal the model's information. In this paper, we discover a new side channel for model information stealing attacks, i.e., models' scientific plots which are extensively used to demonstrate model performance and are easily accessible. Our attack is simple and straightforward. We leverage the shadow model training techniques to generate training data for the attack model which is essentially an image classifier. Extensive evaluation on three benchmark datasets shows that our proposed attack can effectively infer the architecture/hyperparameters of image classifiers based on convolutional neural network (CNN) given the scientific plot generated from it. We also reveal that the attack's success is mainly caused by the shape of the scientific plots, and further demonstrate that the attacks are robust in various scenarios. Given the simplicity and effectiveness of the attack method, our study indicates scientific plots indeed constitute a valid side channel for model information stealing attacks. To mitigate the attacks, we propose several defense mechanisms that can reduce the original attacks' accuracy while maintaining the plot utility. However, such defenses can still be bypassed by adaptive attacks.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Boyang Zhang",
        "Xinlei He",
        "Yun Shen",
        "Tianhao Wang",
        "Yang Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/34bed407d65517ed2c8b98bab3a33da175677c59",
      "pdf_url": "http://arxiv.org/pdf/2302.11982",
      "publication_date": "2023-02-23",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2fd52a3544dc0e3a480d12af01bb978c4d1a59fc",
      "title": "A Taxonomic Survey of Model Extraction Attacks",
      "abstract": "A model extraction attack aims to clone a machine learning target model deployed in the cloud solely by querying the target in a black-box manner. Once a clone is obtained it is possible to launch further attacks with the aid of the local model. In this survey, we analyze existing approaches and present a taxonomic overview of this field based on several important aspects that affect attack efficiency and performance. We present both early works and recently explored directions. We conclude with an analysis of future directions based on recent developments in machine learning methodology.",
      "year": 2023,
      "venue": "Computer Science Symposium in Russia",
      "authors": [
        "Didem Gen\u00e7",
        "Mustafa \u00d6zuysal",
        "E. Tomur"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2fd52a3544dc0e3a480d12af01bb978c4d1a59fc",
      "pdf_url": "",
      "publication_date": "2023-07-31",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "98ba77eb485e9d10c3f56baac5ff862f59fadbec",
      "title": "Scalable Scan-Chain-Based Extraction of Neural Network Models",
      "abstract": "Scan chains have greatly improved hardware testability while introducing security breaches for confidential data. Scan-chain attacks have extended their scope from cryptoprocessors to AI edge devices. The recently proposed scan-chain-based neural network (NN) model extraction attack (lCCAD 2021) made it possible to achieve fine-grained extraction and is multiple orders of magnitude more efficient both in queries and accuracy than its coarse-grained mathematical counterparts. However, both query formulation complexity and constraint solver failures increase drastically with network depth/size. We demonstrate a more powerful adversary, who is capable of improving scalability while maintaining accuracy, by relaxing high-fidelity constraints to formulate an approximate-fidelity-based layer-constrained least-squares extraction using random queries. We conduct our extraction attack on neural network inference topologies of different depths and sizes, targeting the MNIST digit recognition task. The results show that our method outperforms the scan-chain attack proposed in ICCAD 2021 by an average increase in the extracted neural network's functional accuracy of \u2248 32% and 2\u20133 orders of reduction in queries. Furthermore, we demonstrated that our attack is highly effective even in the presence of countermeasures against adversarial samples.",
      "year": 2023,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Shui Jiang",
        "S. Potluri",
        "Tsungyi Ho"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/98ba77eb485e9d10c3f56baac5ff862f59fadbec",
      "pdf_url": "",
      "publication_date": "2023-04-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3f7a77e8437be83b4cb2810f1de0a6e6b45fe6d7",
      "title": "Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection",
      "abstract": "Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-Supervised learning to minimize costs. However, in the black-box setting, these approaches may select sub-optimal samples as they train only one thief model. Depending on the thief model\u2019s capacity and the data it was pretrained on, the model might even select noisy samples that harm the learning process. In this work, we explore the usage of an ensemble of deep learning models as our thief model. We call our attack Army of Thieves(AOT) as we train multiple models with varying complexities to leverage the crowd\u2019s wisdom. Based on the ensemble\u2019s collective decision, uncertain samples are selected for querying, while the most confident samples are directly included in the training data. Our approach is the first one to utilize an ensemble of thief models to perform model extraction. We outperform the base approaches of existing state-of-the-art methods by at least 3% and achieve a 21% higher adversarial sample transferability than previous work for models trained on the CIFAR-10 dataset. Code is available at: https://github.com/akshitjindal1/AOT_WACV.",
      "year": 2023,
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "authors": [
        "Akshit Jindal",
        "Vikram Goyal",
        "Saket Anand",
        "Chetan Arora"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3f7a77e8437be83b4cb2810f1de0a6e6b45fe6d7",
      "pdf_url": "https://arxiv.org/pdf/2311.04588",
      "publication_date": "2023-11-08",
      "keywords_matched": [
        "model stealing attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c2e1a7c575e0a10c5493b5390b4c6ce321c6cf8d",
      "title": "Defense against ML-based Power Side-channel Attacks on DNN Accelerators with Adversarial Attacks",
      "abstract": "Artificial Intelligence (AI) hardware accelerators have been widely adopted to enhance the efficiency of deep learning applications. However, they also raise security concerns regarding their vulnerability to power side-channel attacks (SCA). In these attacks, the adversary exploits unintended communication channels to infer sensitive information processed by the accelerator, posing significant privacy and copyright risks to the models. Advanced machine learning algorithms are further employed to facilitate the side-channel analysis and exacerbate the privacy issue of AI accelerators. Traditional defense strategies naively inject execution noise to the runtime of AI models, which inevitably introduce large overheads. In this paper, we present AIAShield, a novel defense methodology to safeguard FPGA-based AI accelerators and mitigate model extraction threats via power-based SCAs. The key insight of AIAShield is to leverage the prominent adversarial attack technique from the machine learning community to craft delicate noise, which can significantly obfuscate the adversary's side-channel observation while incurring minimal overhead to the execution of the protected model. At the hardware level, we design a new module based on ring oscillators to achieve fine-grained noise generation. At the algorithm level, we repurpose Neural Architecture Search to worsen the adversary's extraction results. Extensive experiments on the Nvidia Deep Learning Accelerator (NVDLA) demonstrate that AIAShield outperforms existing solutions with excellent transferability.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Xiaobei Yan",
        "Chip Hong Chang",
        "Tianwei Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/c2e1a7c575e0a10c5493b5390b4c6ce321c6cf8d",
      "pdf_url": "",
      "publication_date": "2023-12-07",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1a2fd1461e9160246ed51190a455916bb9d48fb3",
      "title": "Bits to BNNs: Reconstructing FPGA ML-IP with Joint Bitstream and Side-Channel Analysis",
      "abstract": "Energy-efficient hardware acceleration platforms for edge deployment of artificial intelligence (AI) and machine learning (ML) applications has been an ongoing research endeavor. Many efforts have focused on optimizing the algorithms and compute structures for use in resource-constrained hardware such as field-programmable gate arrays (FPGAs). Indeed, the difficult nature of crafting the best model makes the ML model itself a valuable intellectual property (IP) asset. This can be problematic, as the IP can now be exposed to an attacker through physical interfaces, enabling threats from side-channel analysis (SCA) attacks. One of the more devastating attacks is the model extraction attack, which threatens piracy and cloning of the valuable IP. While the problem of SCA-based model extraction on FPGA-deployed neural networks has been well-studied, it does not capture the full picture of what vulnerabilities may be present in those platforms. In this paper, we demonstrate how bitstream analysis can be used to obtain neural network parameters and connectivity information from block RAMs (BRAMs). We leverage the knowledge gleaned from the bitstream to mount a power SCA attack to further refine the network reconstruction effort. This is the first method that has approached the problem of ML-IP theft from the angle of FPGA bitstream analysis and suggests that further work is needed to improve security assurance for edge intelligence.",
      "year": 2023,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Brooks Olney",
        "Robert Karam"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/1a2fd1461e9160246ed51190a455916bb9d48fb3",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1365039961e03ac532b63cf4dbd99bbe5352ec0a",
      "title": "A Model Stealing Attack Against Multi-Exit Networks",
      "abstract": "Compared to traditional neural networks with a single output channel, a multi-exit network has multiple exits that allow for early outputs from the model's intermediate layers, thus significantly improving computational efficiency while maintaining similar main task accuracy. Existing model stealing attacks can only steal the model's utility while failing to capture its output strategy, i.e., a set of thresholds used to determine from which exit to output. This leads to a significant decrease in computational efficiency for the extracted model, thereby losing the advantage of multi-exit networks. In this paper, we propose the first model stealing attack against multi-exit networks to extract both the model utility and the output strategy. We employ Kernel Density Estimation to analyze the target model's output strategy and use performance loss and strategy loss to guide the training of the extracted model. Furthermore, we design a novel output strategy search algorithm to maximize the consistency between the victim model and the extracted model's output behaviors. In experiments across multiple multi-exit networks and benchmark datasets, our method always achieves accuracy and efficiency closest to the victim models.",
      "year": 2023,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Pan Li",
        "Peizhuo Lv",
        "Kai Chen",
        "Yuling Cai",
        "Fan Xiang",
        "Shengzhi Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1365039961e03ac532b63cf4dbd99bbe5352ec0a",
      "pdf_url": "",
      "publication_date": "2023-05-23",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "252cc0c47b76f804a2e839773a90e7a389289695",
      "title": "SNATCH: Stealing Neural Network Architecture from ML Accelerator in Intelligent Sensors",
      "abstract": "The use of Machine Learning (ML) models executing on ML Accelerators (MLA) in Intelligent sensors for feature extraction has garnered substantial interest. The Neural Network (NN) architecture implemented of MLA are intellectual property for the vendors. Along with improved power-efficiency and reduced bandwidth, the hardware based ML models embedded in the sensor also provides additional security against cyber-attacks on the ML. In this paper, we introduce an attack referred as SNATCH which uses a profiling-based side channel attack (SCA) that aims to steal the NN architecture executing on a digital MLA (Deep Learning Processing Unit (DPU) IP by Xilinx). We use electromagnetic side channel leakage from a clone device to create a profiler and then attack the victim's device to steal the NN architecture. Stealing the ML model undermines the intellectual property rights of the vendors of a sensor. Further, it also allows an adversary to mount critical Denial of Service and misuse attack.",
      "year": 2023,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Sudarshan Sharma",
        "U. Kamal",
        "Jianming Tong",
        "Tushar Krishna",
        "S. Mukhopadhyay"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/252cc0c47b76f804a2e839773a90e7a389289695",
      "pdf_url": "",
      "publication_date": "2023-10-29",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c2a10426d91b95197f8489699026326bddb0eabc",
      "title": "MEAOD: Model Extraction Attack against Object Detectors",
      "abstract": "The widespread use of deep learning technology across various industries has made deep neural network models highly valuable and, as a result, attractive targets for potential attackers. Model extraction attacks, particularly query-based model extraction attacks, allow attackers to replicate a substitute model with comparable functionality to the victim model and present a significant threat to the confidentiality and security of MLaaS platforms. While many studies have explored threats of model extraction attacks against classification models in recent years, object detection models, which are more frequently used in real-world scenarios, have received less attention. In this paper, we investigate the challenges and feasibility of query-based model extraction attacks against object detection models and propose an effective attack method called MEAOD. It selects samples from the attacker-possessed dataset to construct an efficient query dataset using active learning and enhances the categories with insufficient objects. We additionally improve the extraction effectiveness by updating the annotations of the query dataset. According to our gray-box and black-box scenarios experiments, we achieve an extraction performance of over 70% under the given condition of a 10k query budget.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zeyu Li",
        "Chenghui Shi",
        "Yuwen Pu",
        "Xuhong Zhang",
        "Yu Li",
        "Jinbao Li",
        "Shouling Ji"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c2a10426d91b95197f8489699026326bddb0eabc",
      "pdf_url": "",
      "publication_date": "2023-12-22",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7d0593c25bcd11f26e8deaa97d18da3314f7af48",
      "title": "Rethink before Releasing your Model: ML Model Extraction Attack in EDA",
      "abstract": "Machine learning (ML)-based techniques for electronic design automation (EDA) have boosted the performance of modern integrated circuits (ICs). Such achievement makes ML model to be of importance for the EDA industry. In addition, ML models for EDA are widely considered having high development cost because of the time-consuming and complicated training data generation process. Thus, confidentiality protection for EDA models is a critical issue. However, an adversary could apply model extraction attacks to steal the model in the sense of achieving the comparable performance to the victim's model. As model extraction attacks have posed great threats to other application domains, e.g., computer vision and natural language process, in this paper, we study model extraction attacks for EDA models under two real-world scenarios. It is the first work that (1) introduces model extraction attacks on EDA models and (2) proposes two attack methods against the unlimited and limited query budget scenarios. Our results show that our approach can achieve competitive performance with the well-trained victim model without any performance degradation. Based on the results, we demonstrate that model extraction attacks truly threaten the EDA model privacy and hope to raise concerns about ML security issues in EDA.",
      "year": 2023,
      "venue": "Asia and South Pacific Design Automation Conference",
      "authors": [
        "Chen-Chia Chang",
        "Jingyu Pan",
        "Zhiyao Xie",
        "Jiangkun Hu",
        "Yiran Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7d0593c25bcd11f26e8deaa97d18da3314f7af48",
      "pdf_url": "https://doi.org/10.1145/3566097.3567896",
      "publication_date": "2023-01-16",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0005c9691c8c299476d201d0a5a3c86b49593fac",
      "title": "High-frequency Matters: An Overwriting Attack and defense for Image-processing Neural Network Watermarking",
      "abstract": "In recent years, there has been significant advancement in the field of model watermarking techniques. However, the protection of image-processing neural networks remains a challenge, with only a limited number of methods being developed. The objective of these techniques is to embed a watermark in the output images of the target generative network, so that the watermark signal can be detected in the output of a surrogate model obtained through model extraction attacks. This promising technique, however, has certain limits. Analysis of the frequency domain reveals that the watermark signal is mainly concealed in the high-frequency components of the output. Thus, we propose an overwriting attack that involves forging another watermark in the output of the generative network. The experimental results demonstrate the efficacy of this attack in sabotaging existing watermarking schemes for image-processing networks, with an almost 100% success rate. To counter this attack, we devise an adversarial framework for the watermarking network. The framework incorporates a specially designed adversarial training step, where the watermarking network is trained to defend against the overwriting network, thereby enhancing its robustness. Additionally, we observe an overfitting phenomenon in the existing watermarking method, which can render it ineffective. To address this issue, we modify the training process to eliminate the overfitting problem.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Chi Liu",
        "Shui Yu",
        "Wanlei Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0005c9691c8c299476d201d0a5a3c86b49593fac",
      "pdf_url": "http://arxiv.org/pdf/2302.08637",
      "publication_date": "2023-02-17",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "title": "Model Stealing Attacks and Defenses: Where Are We Now?",
      "abstract": "The success of deep learning in many application domains has been nothing short of dramatic. This has brought the spotlight onto security and privacy concerns with machine learning (ML). One such concern is the threat of model theft. I will discuss work on exploring the threat of model theft, especially in the form of \u201cmodel extraction attacks\u201d \u2014 when a model is made available to customers via an inference interface, a malicious customer can use repeated queries to this interface and use the information gained to construct a surrogate model. I will also discuss possible countermeasures, focusing on deterrence mechanisms that allow for model ownership resolution (MOR) based on watermarking or fingerprinting. In particular, I will discuss the robustness of MOR schemes. I will touch on the issue of conflicts that arise when protection mechanisms for multiple different threats need to be applied simultaneously to a given ML model, using MOR techniques as a case study. This talk is based on work done with my students and collaborators, including Buse Atli Tekgul, Jian Liu, Mika Juuti, Rui Zhang, Samuel Marchal, and Sebastian Szyller. The work was funded in part by Intel Labs in the context of the Private AI consortium.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "N. Asokan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "pdf_url": "",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "model stealing attack",
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "598acab0fdbf2a4c77e05e953498521a1a9f208f",
      "title": "Elevating Defenses: Bridging Adversarial Training and Watermarking for Model Resilience",
      "abstract": "Machine learning models are being used in an increasing number of critical applications; thus, securing their integrity and ownership is critical. Recent studies observed that adversarial training and watermarking have a conflicting interaction. This work introduces a novel framework to integrate adversarial training with watermarking techniques to fortify against evasion attacks and provide confident model verification in case of intellectual property theft. We use adversarial training together with adversarial watermarks to train a robust watermarked model. The key intuition is to use a higher perturbation budget to generate adversarial watermarks compared to the budget used for adversarial training, thus avoiding conflict. We use the MNIST and Fashion-MNIST datasets to evaluate our proposed technique on various model stealing attacks. The results obtained consistently outperform the existing baseline in terms of robustness performance and further prove the resilience of this defense against pruning and fine-tuning removal attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Janvi Thakkar",
        "Giulio Zizzo",
        "S. Maffeis"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/598acab0fdbf2a4c77e05e953498521a1a9f208f",
      "pdf_url": "",
      "publication_date": "2023-12-21",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0d7139b54040b34c6a64187622f8b8038775ff45",
      "title": "On the Limitations of Model Stealing with Uncertainty Quantification Models",
      "abstract": "Model stealing aims at inferring a victim model's functionality at a fraction of the original training cost. While the goal is clear, in practice the model's architecture, weight dimension, and original training data can not be determined exactly, leading to mutual uncertainty during stealing. In this work, we explicitly tackle this uncertainty by generating multiple possible networks and combining their predictions to improve the quality of the stolen model. For this, we compare five popular uncertainty quantification models in a model stealing task. Surprisingly, our results indicate that the considered models only lead to marginal improvements in terms of label agreement (i.e., fidelity) to the stolen model. To find the cause of this, we inspect the diversity of the model's prediction by looking at the prediction variance as a function of training iterations. We realize that during training, the models tend to have similar predictions, indicating that the network diversity we wanted to leverage using uncertainty quantification models is not (high) enough for improvements on the model stealing task.",
      "year": 2023,
      "venue": "The European Symposium on Artificial Neural Networks",
      "authors": [
        "David Pape",
        "Sina D\u00e4ubener",
        "Thorsten Eisenhofer",
        "A. E. Cin\u00e0",
        "Lea Sch\u00f6nherr"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0d7139b54040b34c6a64187622f8b8038775ff45",
      "pdf_url": "https://arxiv.org/pdf/2305.05293",
      "publication_date": "2023-05-09",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "725b22aeeb00baa7f9b4aed0f12f5183067ec1d8",
      "title": "A Fast Prediction Method for the Electromagnetic Response of the LTE-R System Based on a PSO-BP Cascade Neural Network Model",
      "abstract": "In this paper, a fast prediction model of the electromagnetic response of the LTE-R (Long Term Evolution for Railway) communication system based on a cascade neural network is developed to quickly analyze the impact of pantograph arcing on the LTE-R system during vehicle operation. In order to obtain the coupling disturbance level of the LTE-R antenna port, this model uses the cascade neural network based on the PSO-BP (Particle Swarm Optimization of Back Propagation Neural Network) algorithm to quickly solve the coupling coefficient of the pantograph arcing measurement probe and the antenna port. A two-stage BP neural network model is used to train both the simulation data and measurement data, and the results are validated by field tests. Finally, the antenna port coupling interference is added to the LTE-R system to analyze the impact of pantograph arcing on the quality of communication transmission. Analysis and tests demonstrate that pantograph arcing can lead to an increase in the BLER of LTE-R systems at 400 MHz, affecting system performance and transmission efficiency.",
      "year": 2023,
      "venue": "Applied Sciences",
      "authors": [
        "Xiao He",
        "Y. Wen",
        "Dan Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/725b22aeeb00baa7f9b4aed0f12f5183067ec1d8",
      "pdf_url": "https://www.mdpi.com/2076-3417/13/11/6640/pdf?version=1685446251",
      "publication_date": "2023-05-30",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "title": "Model Stealing Attack against Recommender System",
      "abstract": "Recent studies have demonstrated the vulnerability of recommender systems to data privacy attacks. However, research on the threat to model privacy in recommender systems, such as model stealing attacks, is still in its infancy. Some adversarial attacks have achieved model stealing attacks against recommender systems, to some extent, by collecting abundant training data of the target model (target data) or making a mass of queries. In this paper, we constrain the volume of available target data and queries and utilize auxiliary data, which shares the item set with the target data, to promote model stealing attacks. Although the target model treats target and auxiliary data differently, their similar behavior patterns allow them to be fused using an attention mechanism to assist attacks. Besides, we design stealing functions to effectively extract the recommendation list obtained by querying the target model. Experimental results show that the proposed methods are applicable to most recommender systems and various scenarios and exhibit excellent attack performance on multiple datasets.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhihao Zhu",
        "Rui Fan",
        "Chenwang Wu",
        "Yi Yang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ad26add46cae7797de1d638b84a0fa4ad1743a60",
      "title": "FMSA: a meta-learning framework-based fast model stealing attack technique against intelligent network intrusion detection systems",
      "abstract": "Intrusion detection systems are increasingly using machine learning. While machine learning has shown excellent performance in identifying malicious traffic, it may increase the risk of privacy leakage. This paper focuses on implementing a model stealing attack on intrusion detection systems. Existing model stealing attacks are hard to implement in practical network environments, as they either need private data of the victim dataset or frequent access to the victim model. In this paper, we propose a novel solution called Fast Model Stealing Attack (FMSA) to address the problem in the field of model stealing attacks. We also highlight the risks of using ML-NIDS in network security. First, meta-learning frameworks are introduced into the model stealing algorithm to clone the victim model in a black-box state. Then, the number of accesses to the target model is used as an optimization term, resulting in minimal queries to achieve model stealing. Finally, adversarial training is used to simulate the data distribution of the target model and achieve the recovery of privacy data. Through experiments on multiple public datasets, compared to existing state-of-the-art algorithms, FMSA reduces the number of accesses to the target model and improves the accuracy of the clone model on the test dataset to 88.9% and the similarity with the target model to 90.1%. We can demonstrate the successful execution of model stealing attacks on the ML-NIDS system even with protective measures in place to limit the number of anomalous queries.",
      "year": 2023,
      "venue": "Cybersecurity",
      "authors": [
        "Kaisheng Fan",
        "Weizhe Zhang",
        "Guangrui Liu",
        "Hui He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/ad26add46cae7797de1d638b84a0fa4ad1743a60",
      "pdf_url": "https://cybersecurity.springeropen.com/counter/pdf/10.1186/s42400-023-00171-y",
      "publication_date": "2023-08-04",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e49363bd733bed0016e57170c31c7b3dc4dc1714",
      "title": "NNLeak: An AI-Oriented DNN Model Extraction Attack through Multi-Stage Side Channel Analysis",
      "abstract": "Side channel analysis (SCA) attacks have become emerging threats to AI algorithms and deep neural network (DNN) models. However, most existing SCA attacks focus on extracting models deployed on embedded devices, such as microcontrollers. Accurate SCA attacks on extracting DNN models deployed on AI accelerators are largely missing, leaving researchers with an (improper) assumption that DNNs on AI accelerators may be immune to SCA attacks due to their complexity. In this paper, we propose a novel method, namely NNLeak to extract complete DNN models on FPGA-based AI accelerators. To achieve this goal, NNLeak first exploits simple power analysis (SPA) to identify model architecture. Then a multi-stage correlation power analysis (CPA) is designed to recover model weights accurately. Finally, NNLeak determines the activation functions of DNN models through an AI-oriented classifier. The efficacy of NNLeak is validated on FPGA implementations of two DNN models, including multilayer perceptron (MLP) and LeNet. Experimental results show that NNLeak can successfully extract complete DNN models within 2000 power traces.",
      "year": 2023,
      "venue": "Asian Hardware-Oriented Security and Trust Symposium",
      "authors": [
        "Ya Gao",
        "Haocheng Ma",
        "Mingkai Yan",
        "Jiaji He",
        "Yiqiang Zhao",
        "Yier Jin"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/e49363bd733bed0016e57170c31c7b3dc4dc1714",
      "pdf_url": "",
      "publication_date": "2023-12-13",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2cd2ba46e7b6195521d02606aac5e2cda751b271",
      "title": "Efficient Model Extraction by Data Set Stealing, Balancing, and Filtering",
      "abstract": "Model extraction replicates the functionality of machine learning models deployed as a service. Recently, generative adversarial networks (GANs)-based methods have achieved remarkable performance in data-free model extraction. However, previous methods generate random data in every training batch, resulting in slow convergence and redundant queries. We propose to tackle the task with a much simpler paradigm. Specifically, we steal a data set with GAN before training the clone model rather than during every training batch. Benefiting from full use of the generated data, the proposed paradigm needs less training time and query cost. To improve the class distribution of data, a balancing strategy is applied. Furthermore, the balanced data set is filtered based on adversarial robustness for better quality. Combining the above strategies, we propose an efficient model extraction by data set stealing, balancing, and filtering (DSBF). Experiments on three widely used data sets show that DSBF outperforms previous methods while converging faster and costing fewer queries.",
      "year": 2023,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Panpan Yang",
        "Qinglong Wu",
        "Xinming Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/2cd2ba46e7b6195521d02606aac5e2cda751b271",
      "pdf_url": "",
      "publication_date": "2023-12-15",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "42a2e32a77ffa1ea1671c1543ee0a71164375305",
      "title": "Exploring and Exploiting Data-Free Model Stealing",
      "abstract": null,
      "year": 2023,
      "venue": "ECML/PKDD",
      "authors": [
        "Chi Hong",
        "Jiyue Huang",
        "Robert Birke",
        "Lydia Y. Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/42a2e32a77ffa1ea1671c1543ee0a71164375305",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "title": "Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks",
      "abstract": "Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a \u00d737 faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.",
      "year": 2023,
      "venue": "International Conference on Machine Learning and Applications",
      "authors": [
        "Kacem Khaled",
        "Mouna Dhaouadi",
        "F. Magalh\u00e3es",
        "G. Nicolescu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "pdf_url": "",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8e5b27857ea5ede4927d6c673ef795cb23b59071",
      "title": "Stolen Risks of Models with Security Properties",
      "abstract": "Verifiable robust machine learning, as a new trend of ML security defense, enforces security properties (e.g., Lipschitzness, Monotonicity) on machine learning models and achieves satisfying accuracy-security trade-off. Such security properties identify a series of evasion strategies of ML security attackers and specify logical constraints on their effects on a classifier (e.g., the classifier is monotonically increasing along some feature dimensions). However, little has been done so far to understand the side effect of those security properties on the model privacy. In this paper, we aim at better understanding the privacy impacts on security properties of robust ML models. Particularly, we report the first measurement study to investigate the model stolen risks of robust models satisfying four security properties (i.e., LocalInvariance, Lipschitzness, SmallNeighborhood, and Monotonicity). Our findings bring to light the factors that influence model stealing attacks and defense performance on models trained with security properties. In addition, to train an ML model satisfying goals in accuracy, security, and privacy, we propose a novel technique, called BoundaryFuzz, which introduces a privacy property into verifiable robust training frameworks to defend against model stealing attacks on robust models. Experimental results demonstrate the defense effectiveness of BoundaryFuzz.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yue Qin",
        "Zhuoqun Fu",
        "Chuyun Deng",
        "Xiaojing Liao",
        "Jia Zhang",
        "Haixin Duan"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8e5b27857ea5ede4927d6c673ef795cb23b59071",
      "pdf_url": "",
      "publication_date": "2023-11-15",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4bda132d5df256a41326ca03d3b569bfe926734f",
      "title": "Metric Learning-Based Neural Network Model for Electromagnetic Compatibility Fault Diagnosis: An Application Study",
      "abstract": "With the growing prevalence of electronic equipment and the increasing severity of the electromagnetic environment, the likelihood of electromagnetic compatibility failures is on the rise. As a result, the difficulty of diagnosing EMC faults is also increasing. However, by employing neural networks in deep learning for EMC fault diagnosis, we can simplify and streamline the process of feature extraction and similarity analysis. Compared to traditional artificial feature extraction methods, neural networks can learn to measure the similarity between features more efficiently, resulting in more accurate diagnoses. To train the model, we obtain response data from each port of the electronic equipment system in a high radio frequency environment and pair it with the corresponding equipment fault status. However, due to the limited availability of labeled data, conventional neural networks are susceptible to overfitting. Therefore, we use a neural network model that is well-suited for few-shot learning, which is based on a metric learning approach. This approach enables the model to learn from a small amount of labeled data, making it more effective in diagnosing EMC faults.",
      "year": 2023,
      "venue": "2023 5th International Conference on Electronic Engineering and Informatics (EEI)",
      "authors": [
        "Xiangguo Shen",
        "Zhongyuan Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4bda132d5df256a41326ca03d3b569bfe926734f",
      "pdf_url": "",
      "publication_date": "2023-06-30",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "897571a2c302fb6f243f91b35caa0a2159b255c8",
      "title": "Intelligent electromagnetic mapping via physics driven and neural networks on frequency selective surfaces",
      "abstract": "The high mapping efficiency between various structures and electromagnetic (EM) properties of frequency selective surfaces (FSSs) is the state-of-the-art in the EM community. The most straightforward approaches for beam analysis depend on measurements and conventional EM calculation methods, which are inefficient and time-consuming. Equivalent circuit models (ECMs) with excellent intuitiveness and simplicity have been put forward extensively. Despite several applications, bottlenecks in ECM still exist, i.e. the application scope is restricted to narrow bands and specific structures, which is triggered by the ignorance of EM nonlinear coupling. In this study, for the first time, a lightweight physical model based on neural network (ECM-NN) is proposed , which exhibits great physical interpretability and spatial generalization abilities. The nonlinear mapping relationship between structure and beam behavior is interpreted by corresponding simulations. Specifically, two deep parametric factors obtained by multi-layer perceptron networks are introduced to serve as the core of lightweight strategies and compensate for the absence of nonlinearity. Experimental results of single square loop (SL) and double SL indicate that compared with related works, better agreements of the frequency responses and resonant frequencies are achieved with ECM-NN in broadband (0\u201330 GHz) as well as oblique incident angles (0\u00b0\u201360\u00b0). The average accuracy of the mapping is higher than 98.6%. The findings of this study provide a novel strategy for further studies of complex FSSs.",
      "year": 2023,
      "venue": "Journal of Physics D: Applied Physics",
      "authors": [
        "Wuxia Miao",
        "Lamei Zhang",
        "B. Zou",
        "Ye Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/897571a2c302fb6f243f91b35caa0a2159b255c8",
      "pdf_url": "",
      "publication_date": "2023-03-07",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2a87d871e849e270cda90057b709b35977f3d7d1",
      "title": "Determination of Electromagnetic properties of Concrete Using Microwave Non-Destructive testing techniques and Arti\ufffdcial neural networks",
      "abstract": null,
      "year": 2023,
      "venue": "",
      "authors": [
        "Madhuri Guntamukkala",
        "G. Vidya",
        "Venkat Lute",
        "Parul Mathur"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/2a87d871e849e270cda90057b709b35977f3d7d1",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9cdce5c92985967c2bd34aacdd46b1490bf8565c",
      "title": "BarraCUDA: Edge GPUs do Leak DNN Weights",
      "abstract": "Over the last decade, applications of neural networks (NNs) have spread to various aspects of our lives. A large number of companies base their businesses on building products that use neural networks for tasks such as face recognition, machine translation, and self-driving cars. Much of the intellectual property underpinning these products is encoded in the exact parameters of the neural networks. Consequently, protecting these is of utmost priority to businesses. At the same time, many of these products need to operate under a strong threat model, in which the adversary has unfettered physical control of the product. In this work, we present BarraCUDA, a novel attack on general purpose Graphic Processing Units (GPUs) that can extract parameters of neural networks running on the popular Nvidia Jetson Nano device. BarraCUDA uses correlation electromagnetic analysis to recover parameters of real-world convolutional neural networks.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "P\u00e9ter Horv\u00e1th",
        "Lukasz Chmielewski",
        "L\u00e9o Weissbart",
        "L. Batina",
        "Y. Yarom"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/9cdce5c92985967c2bd34aacdd46b1490bf8565c",
      "pdf_url": "",
      "publication_date": "2023-12-12",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3166f8c8e00f2cd9dc5a903009a4f50d176d9f8a",
      "title": "Data-Free Model Stealing Attack Based on Denoising Diffusion Probabilistic Model",
      "abstract": "Data-free model stealing (MS) attacks use synthetic samples to query a target model and train a substitute model to fit the target model\u2019s predictions, avoiding strong dependence on real datasets used by model developers. However, the existing data-free MS attack methods still have a big gap in generating high-quality query samples for high-precision MS attacks. In this paper, we construct the DDPM-optimized generator to generate data, in which a residual network-like structure is designed to fuse data to synthesize query samples. Our method further improves the quantity and quality of synthetic query samples, and effectively reduces the number of queries to the target model. The results show that the proposed method achieves superior performance compared to state-of-the-art methods.",
      "year": 2023,
      "venue": "2023 IEEE Smart World Congress (SWC)",
      "authors": [
        "Guofeng Gao",
        "Xiaodong Wang",
        "Zhiqiang Wei",
        "Jinghai Ai"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3166f8c8e00f2cd9dc5a903009a4f50d176d9f8a",
      "pdf_url": "",
      "publication_date": "2023-08-28",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9b1d486d20cb915ee42d19e556c3587298eec5d6",
      "title": "Model Stealing Attacks On FHE-based Privacy-Preserving Machine Learning through Adversarial Examples",
      "abstract": null,
      "year": 2023,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Bhuvnesh Chaturvedi",
        "Anirban Chakraborty",
        "Ayantika Chatterjee",
        "Debdeep Mukhopadhyay"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9b1d486d20cb915ee42d19e556c3587298eec5d6",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ecb4dff5160be86c97f27d9be1c0b74472af127b",
      "title": "Sniffer: A Novel Model Type Detection System against Machine-Learning-as-a-Service Platforms",
      "abstract": "\n Recent works explore several attacks against Machine-Learning-as-a-Service (MLaaS) platforms (e.g., the model stealing attack), allegedly posing potential real-world threats beyond viability in laboratories. However, hampered by\n model-type-sensitive\n , most of the attacks can hardly break mainstream real-world MLaaS platforms. That is, many MLaaS attacks are designed against only one certain type of model, such as tree models or neural networks. As the black-box MLaaS interface hides model type info, the attacker cannot choose a proper attack method with confidence, limiting the attack performance. In this paper, we demonstrate a system, named Sniffer, that is capable of making model-type-sensitive attacks \"great again\" in real-world applications. Specifically, Sniffer consists of four components: Generator, Querier, Probe, and Arsenal. The first two components work for preparing attack samples. Probe, as the most characteristic component in Sniffer, implements a series of self-designed algorithms to determine the type of models hidden behind the black-box MLaaS interfaces. With model type info unraveled, an optimum method can be selected from Arsenal (containing multiple attack methods) to accomplish its attack. Our demonstration shows how the audience can interact with Sniffer in a web-based interface against five mainstream MLaaS platforms.\n",
      "year": 2023,
      "venue": "Proceedings of the VLDB Endowment",
      "authors": [
        "Zhuo Ma",
        "Yilong Yang",
        "Bin Xiao",
        "Yang Liu",
        "Xinjing Liu",
        "Tong Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ecb4dff5160be86c97f27d9be1c0b74472af127b",
      "pdf_url": "",
      "publication_date": "2023-08-01",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8b50f09560108d2f79381e2f5ff9146a1079a29c",
      "title": "Deep-neural-network-based Electromagnetic Analysis and Optimal Design of Fractional-slot Brushless DC Motor for High Torque Robot Joints",
      "abstract": "Fractional-slot brushless DC motors (FS-BLDCMs) have the advantages of high torque density and low cogging torque for robot joints. However, finite element analysis (FEA) of the FS-BLDCMs causes time consumption, which obstructs the progress on finding optimal electromagnetic characteristics of the FS-BLDCMs. This paper presents a novel design method to improve the FS-BLDCM motor characteristics and improve the computation efficiency by the deep neural network (DNN). The FS-BLDCM motor performance is optimized by the genetic algorithm and validated by finite element analysis. The computation time between the finite element analysis (FEA) and the deep neural network (DNN) is compared. The result shows the efficiency of the deep neural network.",
      "year": 2023,
      "venue": "2023 3rd International Conference on Electrical Engineering and Mechatronics Technology (ICEEMT)",
      "authors": [
        "Anguo Liu",
        "Fei Meng",
        "Hengzai Hu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8b50f09560108d2f79381e2f5ff9146a1079a29c",
      "pdf_url": "",
      "publication_date": "2023-07-21",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee761742bda631b619a728ba9819dbddc1922a3c",
      "title": "A Study on Electromagnetic Field Analysis Considering Geometry Variation Using Physics-Informed Neural Network",
      "abstract": "The problem with finite element analysis is that each analysis is run separately, so any variation in geometry requires a new analysis. When performing finite element analysis for generating fault diagnosis data, the frequent geometry variations that occur to consider numerous fault levels lead to a long time. In this paper, to solve these problems, an analysis using a physics-informed neural network is proposed to solve the differential equation that was solved by a numerical method using a neural network. Transfer learning is used, which enables fast analysis based on the analysis experience before the geometry is varied. In addition, nonlinear magnetic material characteristics and electronic systems in the saturated region are analyzed to evaluate whether the physics-informed neural network can cope with some numerical analysis problem.",
      "year": 2023,
      "venue": "International Conference on Electrical Machines and Systems",
      "authors": [
        "Ji-hoon Han",
        "Eui-Jin Choi",
        "Sun-Ki Hong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ee761742bda631b619a728ba9819dbddc1922a3c",
      "pdf_url": "",
      "publication_date": "2023-11-05",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d79a93189a8acc8306504ed20a48fec9934897d9",
      "title": "Electromagnetic Field Model of Tubular Permanent Magnet Synchronous Linear Motor Based on Deep Transfer Neural Network",
      "abstract": "During high-speed operation, tubular permanent magnet linear motors with slotted stators often experience large fluctuations in thrust, overheating of the coils and irreversible demagnetization of the permanent magnets. For the first two cases, this paper proposes a deep transfer neural network (DTNN)-based electromagnetic field model for TPMLM, the specific implementation steps of which include: (1) Using different geometric parameters of TPMLM as input and average thrust, thrust fluctuation and coil copper loss as output, finite element analysis (FEA) and analytical method (AM) are used to provide the electromagnetic parameter dataset of the motor respectively, and these two datasets are used as the source and target domains for transfer learning; (2) Based on the features of the sample dataset, the source tasks corresponding to the source domain are pre-trained using DTNN, and the target tasks corresponding to the target domain are fitted by fine-tuning the model parameters. Then, in order to verify the accuracy of the proposed model, this paper compares the output prediction results with some non-parametric models, such as Random Forest (RF), Support Vector Machine (SVM), and Deep Neural Network (DNN), by dividing the target domain dataset with different scales. The results show that the best prediction results are obtained from the DTNN model, which fully combines the accuracy of FEA and the efficiency of AM, and shows better generalization ability in the case of insufficient real training data.",
      "year": 2023,
      "venue": "ACM Cloud and Autonomic Computing Conference",
      "authors": [
        "Kai Zhu",
        "Tao Wu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d79a93189a8acc8306504ed20a48fec9934897d9",
      "pdf_url": "",
      "publication_date": "2023-11-17",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7899de0fae6d036acce5ef82d6e6937008dd1758",
      "title": "Enhancing Neural Network Performance for Problems in the Physical Sciences: Applications to Electromagnetic Signal Source Localization",
      "abstract": "Neural networks are widely employed in many domains such as graph analysis, image recognition, and social networks. They are less commonly used in the physical sciences, yet the physical sciences provide many rich problems which can benefit from the careful application of neural networks. Problems in the physical sciences have different challenges when compared to problems in classic neural network domains. One such problem is the scarcity of real-world data in many physical problems. In our signal processing application, the real-world data is time-consuming and expensive to collect. In this paper, we detail methods for improving neural network performance in a small-data domain for determining source localization of electromagnetic signals. These methods can also be leveraged in other real-world problems. Our key findings include five data-related results we leveraged to achieve an RMSE error of 0.78km on our source localization efforts.",
      "year": 2023,
      "venue": "Asilomar Conference on Signals, Systems and Computers",
      "authors": [
        "Maria Barger",
        "Evan C. Witz",
        "Randy C. Paffenroth"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7899de0fae6d036acce5ef82d6e6937008dd1758",
      "pdf_url": "",
      "publication_date": "2023-10-29",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "82bf68c3e3aa8b2f8ccea0842e43cbe39a5b2dff",
      "title": "Model Stealing Attack against Graph Classification with Authenticity, Uncertainty and Diversity",
      "abstract": "Recent research demonstrates that GNNs are vulnerable to the model stealing attack, a nefarious endeavor geared towards duplicating the target model via query permissions. However, they mainly focus on node classification tasks, neglecting the potential threats entailed within the domain of graph classification tasks. Furthermore, their practicality is questionable due to unreasonable assumptions, specifically concerning the large data requirements and extensive model knowledge. To this end, we advocate following strict settings with limited real data and hard-label awareness to generate synthetic data, thereby facilitating the stealing of the target model. Specifically, following important data generation principles, we introduce three model stealing attacks to adapt to different actual scenarios: MSA-AU is inspired by active learning and emphasizes the uncertainty to enhance query value of generated samples; MSA-AD introduces diversity based on Mixup augmentation strategy to alleviate the query inefficiency issue caused by over-similar samples generated by MSA-AU; MSA-AUD combines the above two strategies to seamlessly integrate the authenticity, uncertainty, and diversity of the generated samples. Finally, extensive experiments consistently demonstrate the superiority of the proposed methods in terms of concealment, query efficiency, and stealing performance.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhihao Zhu",
        "Chenwang Wu",
        "Rui Fan",
        "Yi Yang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/82bf68c3e3aa8b2f8ccea0842e43cbe39a5b2dff",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3c26202058ff573620e70e340c9c8cafb2094678",
      "title": "Model Extraction Attacks on DistilBERT",
      "abstract": null,
      "year": 2023,
      "venue": "Tiny Papers @ ICLR",
      "authors": [
        "Amro Salman",
        "Ayman Saeed",
        "Khalid Elmadani",
        "S. Babiker"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3c26202058ff573620e70e340c9c8cafb2094678",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "78c6861c798e06b5651f45594eb6c8da43708e08",
      "title": "GNMS: A novel method for model stealing based on GAN",
      "abstract": "Many well-performing models are currently deployed on the cloud to provide machine Learning as a service (MLaaS). However, these models are susceptible to Model Stealing Attacks, where attackers can access the model\u2019s functionality, parameters, and internal structure in a black-box. As a result, data-free model stealing methods have gained popularity due to their higher accuracy and not requiring real data. Previous data-free model stealing methods have mainly focused on single scenarios and limited model and dataset variations. In this paper, we introduce a novel generalized network model Stealing method (GNMS), which is suitable for both benchmark and transfer models, achieving high model stealing accuracy across various scenarios. We pre-train generative adversarial network (GAN) using publicly available datasets and efficiently steal model functionality by training a student model with the pre-trained generator and the discriminator. Adversarial samples and the generated image dataset are also used to explore the model\u2019s decision boundaries. During the training of the clone model, we train two clone models to minimize the differences with the target model further. We employ a contrastive learning approach to encourage the models to learn meaningful feature representations by distinguishing between similar and dissimilar data points, thereby enhancing the model\u2019s accuracy. We achieve a model stealing accuracy of 73.02% and 72.93% on more complex datasets CIFAR100 and Caltech101. Surpass the latest DisGUIDE by 3.55% and 2.61%.",
      "year": 2023,
      "venue": "International Conference on Advanced Cloud and Big Data",
      "authors": [
        "Moxuan Zeng",
        "Yangzhong Wang",
        "Yangming Zhang",
        "Jun Niu",
        "Yuqing Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/78c6861c798e06b5651f45594eb6c8da43708e08",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f95c674b837576be6698eda1f6ef89cf2d8c37c4",
      "title": "An attack framework for stealing black-box based on active semi-supervised learning",
      "abstract": "Neural network models are commonly used as black-box services, but they are vulnerable to model stealing attacks, where an attacker can train a substitute model with similar performance to the original model by exploiting limited information related to the target model. This can cause significant losses to the owner of the target model and pose a serious security risk. To advance our understanding of neural networks and promote the evolution of model protection mechanisms, we conducted in-depth research on neural network model stealing attacks. In this paper, we propose a black-box stealing attack framework that combines active and semi-supervised learning, even if the target black-box only provides hard-label output, an effective attack can be achieved, generating a substitute model with the same functionality as the black-box. The framework involves selectively querying the most informative samples for black-box labeling using active learning, which significantly reduces the workload of querying the black-box and enables to achieve better performance with fewer training samples. We also apply semi-supervised learning to leverage the abundance of unlabeled data and further improve model performance. We evaluated our method on various data sets and proved that the stealing ability of our method was significantly higher than 3.86%~26.64% other methods when faced with hardlabel black-box with the same number of queries, which can achieve effective black-box function stealing.",
      "year": 2023,
      "venue": "6th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE 2023)",
      "authors": [
        "Lijun Gao",
        "Yuting Wang",
        "Wenjun Liu",
        "Kai Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f95c674b837576be6698eda1f6ef89cf2d8c37c4",
      "pdf_url": "",
      "publication_date": "2023-08-16",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4612f541d3a548bc1b87e42b82e61d37bb8cd66b",
      "title": "Model Extraction Attacks Against Reinforcement Learning Based Controllers",
      "abstract": "We introduce the problem of model-extraction attacks in cyber-physical systems in which an attacker attempts to estimate (or extract) the feedback controller of the system. Extracting (or estimating) the controller provides an unmatched edge to attackers since it allows them to predict the future control actions of the system and plan their attack accordingly. Hence, it is important to understand the ability of the attackers to perform such an attack. In this paper, we focus on the setting when a Deep Neural Network (DNN) controller is trained using Reinforcement Learning (RL) algorithms and is used to control a stochastic system. We play the role of the attacker that aims to estimate such an unknown DNN controller, and we propose a two-phase algorithm. In the first phase, also called the offline phase, the attacker uses side-channel information about the RL-reward function and the system dynamics to identify a set of candidate estimates of the unknown DNN. In the second phase, also called the online phase, the attacker observes the behavior of the unknown DNN and uses these observations to shortlist the set of final policy estimates. We provide theoretical analysis of the error between the unknown DNN and the estimated one. We also provide numerical results showing the effectiveness of the proposed algorithm.",
      "year": 2023,
      "venue": "IEEE Conference on Decision and Control",
      "authors": [
        "Momina Sajid",
        "Yanning Shen",
        "Yasser Shoukry"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4612f541d3a548bc1b87e42b82e61d37bb8cd66b",
      "pdf_url": "http://arxiv.org/pdf/2304.13090",
      "publication_date": "2023-04-25",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e0662b9113b25a29473a6f5083bc5e15c04abbad",
      "title": "Data-Driven Deep Convolutional Neural Networks for Electromagnetic Field Estimation of Dry-type Transformer",
      "abstract": "This paper aims to estimate the electromagnetic field distribution in a simplified transformer through two-dimensional (2-D) finite element analysis. We create two datasets, namely the original dataset (OD) to simulate grayscale images and the new dataset (ND) which incorporates distinct physical properties of materials. These datasets are helpful to investigate the impact of different data types on deep learning. In order to enhance the accuracy of estimation, we compare the performances of two convolutional neural network (CNN) architectures: U-net and its improved version, U-Resnet (which incorporates residual blocks from ResNet). Additionally, we introduce a specialized loss function, Add-RMSE, which is better suited for dense regression problems, thus improving the prediction accuracy. The effectiveness of our proposed method is validated through test experiments, where we analyze the estimation results obtained.",
      "year": 2023,
      "venue": "IEEE International Conference on Applied Superconductivity and Electromagnetic Devices",
      "authors": [
        "Yifan Chen",
        "Qingxin Yang",
        "Yongjian Li",
        "Hao Zhang",
        "Changgeng Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e0662b9113b25a29473a6f5083bc5e15c04abbad",
      "pdf_url": "",
      "publication_date": "2023-10-27",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "71ba257fa70f6d8cc5485c5cd8026b8935211814",
      "title": "Semantic Awareness Model For Binary Operator Code",
      "abstract": "With the promotion of artificial intelligence in various industries, there have been some organizations and individuals using various means to attack it. Common types of attacks against models include adversarial sample attacks, data poisoning attacks, and model stealing attacks. The above attack methods require a certain understanding of the structure of the model, so it becomes a challenge to restore the category of binary operators from the model.Binary code similarity detection (BCSD) has important applications in code checking, vulnerability detection, and malicious co de analysis. Due to the lack of syntax structure information in binary operator code, it is difficult to determine the type of operator. Recent research has focused on using deep learning models to understand the semantics and structural information of binary code to achieve better results. Recent research has shown that deep learning models, especially natural language processing models, can comprehend the semantics of binary code. In this paper, we propose a method for identifying binary operators which uses a sequence-aware model and a structure-aware model to model binary operators. It combines CNN semantics and Transformer semantics for classification. The evaluation shows that our method can achieve good performance in binary operator classification.",
      "year": 2023,
      "venue": "2023 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)",
      "authors": [
        "Haichao Gao",
        "Liming Fang",
        "Yang Li",
        "Minghui Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/71ba257fa70f6d8cc5485c5cd8026b8935211814",
      "pdf_url": "",
      "publication_date": "2023-11-03",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4fc5f31a38a19e3acc9501aeb3006810eec0e767",
      "title": "FedRolex: Model-Heterogeneous Federated Learning with Rolling Sub-Model Extraction",
      "abstract": "Most cross-device federated learning (FL) studies focus on the model-homogeneous setting where the global server model and local client models are identical. However, such constraint not only excludes low-end clients who would otherwise make unique contributions to model training but also restrains clients from training large models due to on-device resource bottlenecks. In this work, we propose FedRolex, a partial training (PT)-based approach that enables model-heterogeneous FL and can train a global server model larger than the largest client model. At its core, FedRolex employs a rolling sub-model extraction scheme that allows different parts of the global server model to be evenly trained, which mitigates the client drift induced by the inconsistency between individual client models and server model architectures. We show that FedRolex outperforms state-of-the-art PT-based model-heterogeneous FL methods (e.g. Federated Dropout) and reduces the gap between model-heterogeneous and model-homogeneous FL, especially under the large-model large-dataset regime. In addition, we provide theoretical statistical analysis on its advantage over Federated Dropout and evaluate FedRolex on an emulated real-world device distribution to show that FedRolex can enhance the inclusiveness of FL and boost the performance of low-end devices that would otherwise not benefit from FL. Our code is available at: https://github.com/AIoT-MLSys-Lab/FedRolex",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Samiul Alam",
        "Luyang Liu",
        "Ming Yan",
        "Mi Zhang"
      ],
      "citation_count": 192,
      "url": "https://www.semanticscholar.org/paper/4fc5f31a38a19e3acc9501aeb3006810eec0e767",
      "pdf_url": "http://arxiv.org/pdf/2212.01548",
      "publication_date": "2022-12-03",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d405b58a8f465d5ba2e91f9541e09760904c11a8",
      "title": "I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences",
      "abstract": "Machine-Learning-as-a-Service (MLaaS) has become a widespread paradigm, making even the most complex Machine Learning models available for clients via, e.g., a pay-per-query principle. This allows users to avoid time-consuming processes of data collection, hyperparameter tuning, and model training. However, by giving their customers access to the (predictions of their) models, MLaaS providers endanger their intellectual property such as sensitive training data, optimised hyperparameters, or learned model parameters. In some cases, adversaries can create a copy of the model with (almost) identical behaviour using the the prediction labels only. While many variants of this attack have been described, only scattered defence strategies that address isolated threats have been proposed. To arrive at a comprehensive understanding why these attacks are successful and how they could be holistically defended against, a thorough systematisation of the field of model stealing is necessary. We address this by categorising and comparing model stealing attacks, assessing their performance, and exploring corresponding defence techniques in different settings. We propose a taxonomy for attack and defence approaches and provide guidelines on how to select the right attack or defence strategy based on the goal and available resources. Finally, we analyse which defences are rendered less effective by current attack strategies.",
      "year": 2022,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "A. Rauber"
      ],
      "citation_count": 138,
      "url": "https://www.semanticscholar.org/paper/d405b58a8f465d5ba2e91f9541e09760904c11a8",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3595292",
      "publication_date": "2022-06-16",
      "keywords_matched": [
        "model stealing attack",
        "stealing machine learning model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "35ade8553de7259a5e8105bd20a160f045f9d112",
      "title": "Towards Data-Free Model Stealing in a Hard Label Setting",
      "abstract": "Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adversary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels. In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query budget. We propose a novel GAN-based framework11Project Page: https://sites.google.com/view/dfms-hl that trains the student and generator in tandem to steal the model effectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim's gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-of-the-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scalability of Model Stealing in a restricted access setting on a 100 class dataset as well.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Sunandini Sanyal",
        "Sravanti Addepalli",
        "R. Venkatesh Babu"
      ],
      "citation_count": 100,
      "url": "https://www.semanticscholar.org/paper/35ade8553de7259a5e8105bd20a160f045f9d112",
      "pdf_url": "https://arxiv.org/pdf/2204.11022",
      "publication_date": "2022-04-23",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7da6c9273a14eb8681824d0c3ee84e05366c5627",
      "title": "Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders",
      "abstract": "Self-supervised representation learning techniques have been developing rapidly to make full use of unlabeled images. They encode images into rich features that are oblivious to downstream tasks. Behind their revolutionary representation power, the requirements for dedicated model designs and a massive amount of computation resources expose image encoders to the risks of potential model stealing attacks - a cheap way to mimic the well-trained encoder performance while circumventing the demanding requirements. Yet conventional attacks only target supervised classifiers given their predicted labels and/or posteriors, which leaves the vulnerability of unsupervised encoders unexplored. In this paper, we first instantiate the conventional stealing attacks against encoders and demonstrate their severer vulnerability compared with downstream classifiers. To better leverage the rich representation of encoders, we further propose Cont-Steal, a contrastive-learning-based attack, and validate its improved stealing effectiveness in various experiment settings. As a takeaway, we appeal to our community's attention to the intellectual property protection of representation learning techniques, especially to the defenses against encoder stealing attacks like ours.11See our code in https://github.com/zeyangsha/Cont-Steal.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zeyang Sha",
        "Xinlei He",
        "Ning Yu",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/7da6c9273a14eb8681824d0c3ee84e05366c5627",
      "pdf_url": "https://arxiv.org/pdf/2201.07513",
      "publication_date": "2022-01-19",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0a58e101142efdd9dd653b41d504152e940096be",
      "title": "Are You Stealing My Model? Sample Correlation for Fingerprinting Deep Neural Networks",
      "abstract": "An off-the-shelf model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting aims to verify whether a suspect model is stolen from the victim model, which gains more and more attention nowadays. Previous methods always leverage the transferable adversarial examples as the model fingerprint, which is sensitive to adversarial defense or transfer learning scenarios. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-w that selects wrongly classified normal samples as model inputs and calculates the mean correlation among their model outputs. To reduce the training time, we further develop SAC-m that selects CutMix Augmented samples as model inputs, without the need for training the surrogate models or generating adversarial examples. Extensive results validate that SAC successfully defends against various model stealing attacks, even including adversarial training or transfer learning, and detects the stolen models with the best performance in terms of AUC across different datasets and model architectures. The codes are available at https://github.com/guanjiyang/SAC.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jiyang Guan",
        "Jian Liang",
        "R. He"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/0a58e101142efdd9dd653b41d504152e940096be",
      "pdf_url": "https://arxiv.org/pdf/2210.15427",
      "publication_date": "2022-10-21",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "274dbb98c63cdd282eb86b0338bdc3c5dfd9b904",
      "title": "Dataset Inference for Self-Supervised Models",
      "abstract": "Self-supervised models are increasingly prevalent in machine learning (ML) since they reduce the need for expensively labeled data. Because of their versatility in downstream applications, they are increasingly used as a service exposed via public APIs. At the same time, these encoder models are particularly vulnerable to model stealing attacks due to the high dimensionality of vector representations they output. Yet, encoders remain undefended: existing mitigation strategies for stealing attacks focus on supervised learning. We introduce a new dataset inference defense, which uses the private training set of the victim encoder model to attribute its ownership in the event of stealing. The intuition is that the log-likelihood of an encoder's output representations is higher on the victim's training data than on test data if it is stolen from the victim, but not if it is independently trained. We compute this log-likelihood using density estimation models. As part of our evaluation, we also propose measuring the fidelity of stolen encoders and quantifying the effectiveness of the theft detection without involving downstream tasks; instead, we leverage mutual information and distance measurements. Our extensive empirical results in the vision domain demonstrate that dataset inference is a promising direction for defending self-supervised models against model stealing.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Adam Dziedzic",
        "Haonan Duan",
        "Muhammad Ahmad Kaleem",
        "Nikita Dhawan",
        "Jonas Guan",
        "Yannis Cattan",
        "Franziska Boenisch",
        "Nicolas Papernot"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/274dbb98c63cdd282eb86b0338bdc3c5dfd9b904",
      "pdf_url": "http://arxiv.org/pdf/2209.09024",
      "publication_date": "2022-09-16",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f3886b3c22675da54b0d55a5bc16754e3399c979",
      "title": "DualCF: Efficient Model Extraction Attack from Counterfactual Explanations",
      "abstract": "Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.",
      "year": 2022,
      "venue": "Conference on Fairness, Accountability and Transparency",
      "authors": [
        "Yongjie Wang",
        "Hangwei Qian",
        "C. Miao"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/f3886b3c22675da54b0d55a5bc16754e3399c979",
      "pdf_url": "https://arxiv.org/pdf/2205.06504",
      "publication_date": "2022-05-13",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2ee4127a2a6aab51a03305d8a564693181bc6424",
      "title": "Increasing the Cost of Model Extraction with Calibrated Proof of Work",
      "abstract": "In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.",
      "year": 2022,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Adam Dziedzic",
        "Muhammad Ahmad Kaleem",
        "Y. Lu",
        "Nicolas Papernot"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/2ee4127a2a6aab51a03305d8a564693181bc6424",
      "pdf_url": "",
      "publication_date": "2022-01-23",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "af72f901d7a0f2edca55ee008c8893ae93b09971",
      "title": "How to Steer Your Adversary: Targeted and Efficient Model Stealing Defenses with Gradient Redirection",
      "abstract": "Model stealing attacks present a dilemma for public machine learning APIs. To protect financial investments, companies may be forced to withhold important information about their models that could facilitate theft, including uncertainty estimates and prediction explanations. This compromise is harmful not only to users but also to external transparency. Model stealing defenses seek to resolve this dilemma by making models harder to steal while preserving utility for benign users. However, existing defenses have poor performance in practice, either requiring enormous computational overheads or severe utility trade-offs. To meet these challenges, we present a new approach to model stealing defenses called gradient redirection. At the core of our approach is a provably optimal, efficient algorithm for steering an adversary's training updates in a targeted manner. Combined with improvements to surrogate networks and a novel coordinated defense strategy, our gradient redirection defense, called GRAD${}^2$, achieves small utility trade-offs and low computational overhead, outperforming the best prior defenses. Moreover, we demonstrate how gradient redirection enables reprogramming the adversary with arbitrary behavior, which we hope will foster work on new avenues of defense.",
      "year": 2022,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Mantas Mazeika",
        "B. Li",
        "David A. Forsyth"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/af72f901d7a0f2edca55ee008c8893ae93b09971",
      "pdf_url": "https://arxiv.org/pdf/2206.14157",
      "publication_date": "2022-06-28",
      "keywords_matched": [
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8bdb27ba98f457549bb7e03d6aa2d5c54a4de79e",
      "title": "On the Difficulty of Defending Self-Supervised Learning against Model Extraction",
      "abstract": "Self-Supervised Learning (SSL) is an increasingly popular ML paradigm that trains models to transform complex inputs into representations without relying on explicit labels. These representations encode similarity structures that enable efficient learning of multiple downstream tasks. Recently, ML-as-a-Service providers have commenced offering trained SSL models over inference APIs, which transform user inputs into useful representations for a fee. However, the high cost involved to train these models and their exposure over APIs both make black-box extraction a realistic security threat. We thus explore model stealing attacks against SSL. Unlike traditional model extraction on classifiers that output labels, the victim models here output representations; these representations are of significantly higher dimensionality compared to the low-dimensional prediction scores output by classifiers. We construct several novel attacks and find that approaches that train directly on a victim's stolen representations are query efficient and enable high accuracy for downstream models. We then show that existing defenses against model extraction are inadequate and not easily retrofitted to the specificities of SSL.",
      "year": 2022,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Adam Dziedzic",
        "Nikita Dhawan",
        "Muhammad Ahmad Kaleem",
        "Jonas Guan",
        "Nicolas Papernot"
      ],
      "citation_count": 31,
      "url": "https://www.semanticscholar.org/paper/8bdb27ba98f457549bb7e03d6aa2d5c54a4de79e",
      "pdf_url": "http://arxiv.org/pdf/2205.07890",
      "publication_date": "2022-05-16",
      "keywords_matched": [
        "model stealing attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a4c748225360d89a6b03e1f277daddb64f429bcc",
      "title": "Imitated Detectors: Stealing Knowledge of Black-box Object Detectors",
      "abstract": "Deep neural networks have shown great potential in many practical applications, yet their knowledge is at the risk of being stolen via exposed services (\\eg APIs). In contrast to the commonly-studied classification model extraction, there exist no studies on the more challenging object detection task due to the sufficiency and efficiency of problem domain data collection. In this paper, we for the first time reveal that black-box victim object detectors can be easily replicated without knowing the model structure and training data. In particular, we treat it as black-box knowledge distillation and propose a teacher-student framework named Imitated Detector to transfer the knowledge of the victim model to the imitated model. To accelerate the problem domain data construction, we extend the problem domain dataset by generating synthetic images, where we apply the text-image generation process and provide short text inputs consisting of object categories and natural scenes; to promote the feedback information, we aim to fully mine the latent knowledge of the victim model by introducing an iterative adversarial attack strategy, where we feed victim models with transferable adversarial examples making victim provide diversified predictions with more information. Extensive experiments on multiple datasets in different settings demonstrate that our approach achieves the highest model extraction accuracy and outperforms other model stealing methods by large margins in the problem domain dataset. Our codes can be found at \\urlhttps://github.com/LiangSiyuan21/Imitated-Detectors.",
      "year": 2022,
      "venue": "ACM Multimedia",
      "authors": [
        "Siyuan Liang",
        "Aishan Liu",
        "Jiawei Liang",
        "Longkang Li",
        "Yang Bai",
        "Xiaochun Cao"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/a4c748225360d89a6b03e1f277daddb64f429bcc",
      "pdf_url": "",
      "publication_date": "2022-10-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fa8e84e2091105083f3b697158073f8896c7fb4c",
      "title": "Hybrid Annealing Method Based on subQUBO Model Extraction With Multiple Solution Instances",
      "abstract": "Ising machines are expected to solve combinatorial optimization problems efficiently by representing them as Ising models or equivalent quadratic unconstrained binary optimization (QUBO) models . However, upper bound exists on the computable problem size due to the hardware limitations of Ising machines. This paper propose a new hybrid annealing method based on partial QUBO extraction, called subQUBO model extraction, with multiple solution instances. For a given QUBO model, the proposed method obtains <inline-formula><tex-math notation=\"LaTeX\">$N_I$</tex-math><alternatives><mml:math><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq1-3138629.gif\"/></alternatives></inline-formula> quasi-optimal solutions (quasi-ground-state solutions) in some way using a classical computer. The solutions giving these quasi-optimal solutions are called <italic>solution instances</italic>. We extract a size-limited subQUBO model as follows based on a strong theoretical background: we randomly select <inline-formula><tex-math notation=\"LaTeX\">$N_S$</tex-math><alternatives><mml:math><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq2-3138629.gif\"/></alternatives></inline-formula> <inline-formula><tex-math notation=\"LaTeX\">$(N_S<N_I)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo><</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"atobe-ieq3-3138629.gif\"/></alternatives></inline-formula> solution instances among them and focus on a particular binary variable <inline-formula><tex-math notation=\"LaTeX\">$x_i$</tex-math><alternatives><mml:math><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq4-3138629.gif\"/></alternatives></inline-formula> in the <inline-formula><tex-math notation=\"LaTeX\">$N_S$</tex-math><alternatives><mml:math><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq5-3138629.gif\"/></alternatives></inline-formula> solution instances. If <inline-formula><tex-math notation=\"LaTeX\">$x_i$</tex-math><alternatives><mml:math><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq6-3138629.gif\"/></alternatives></inline-formula> value is much <italic>varied</italic> over <inline-formula><tex-math notation=\"LaTeX\">$N_S$</tex-math><alternatives><mml:math><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq7-3138629.gif\"/></alternatives></inline-formula> solution instances, it is included in the subQUBO model; otherwise, it is not. We find a (quasi-)ground-state solution of the extracted subQUBO model using an Ising machine and add it as a new solution instance. By repeating this process, we can finally obtain a (quasi-)ground-state solution of the original QUBO model. Experimental evaluations confirm that the proposed method can obtain better quasi-ground-state solution than existing methods for large-sized QUBO models.",
      "year": 2022,
      "venue": "IEEE transactions on computers",
      "authors": [
        "Yuta Atobe",
        "Masashi Tawada",
        "N. Togawa"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/fa8e84e2091105083f3b697158073f8896c7fb4c",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/12/9880482/09664360.pdf",
      "publication_date": "2022-10-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "89f080275bf088de3140625d6ee7c4ffa7a368b9",
      "title": "Reverse Engineering Neural Network Folding with Remote FPGA Power Analysis",
      "abstract": "Specialized hardware accelerators in the form of FPGAs are widely being used for neural network implementations. By that, they also become the target of power analysis attacks that try to reverse engineer the embedded secret information, in the form of model parameters. However, most of these attacks assume rather simple implementations, not realistic frameworks. Layer folding is used in such accelerators to optimize the network under given area constraints with various degrees of parallel and sequential operations. In this paper, we show that folding does mislead existing power side-channel attacks on frameworks such as FINN. We show how we can extract the folding parameters successfully and use that information to subsequently also recover the number of neurons\u2013something not reliably possible without knowing the folding information. Following the methodologies of both profiling side-channel attacks and machine learning, our approach can extract the amount of neurons with 98% accuracy on a test device, compared to 44-79% accuracy based on related work under the same test conditions and datasets. Furthermore, we show how a classifier that is based on regression can detect previously unknown parameters, which has not been shown before. To verify our results under different environmental conditions, we test the target device in a climate chamber under various temperature ranges and still reach accuracies of at least 93%.",
      "year": 2022,
      "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines",
      "authors": [
        "Vincent Meyers",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/89f080275bf088de3140625d6ee7c4ffa7a368b9",
      "pdf_url": "",
      "publication_date": "2022-05-15",
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e8572d9722e992a770b7c00a8419dda17297a9da",
      "title": "Side-Channel Fuzzy Analysis-Based AI Model Extraction Attack With Information-Theoretic Perspective in Intelligent IoT",
      "abstract": "Accessibility to smart devices provides opportunities for side-channel attacks (SCAs) on artificial intelligent (AI) models in the intelligent Internet of Things (IoT). However, the existing literature exposes some shortcomings: 1) incapability of quantifying and analyzing the leaked information through side channels of the intelligent IoT and 2) inability to devise efficient and accurate SCA algorithms. To address these challenges, we propose a side-channel fuzzy analysis-empowered AI model extraction attack in the intelligent IoT. First, the integrated AI model extraction framework is proposed, including power trace-based structure, execution time-based metaparameters, and hierarchical weight extractions. Then, we develop the information theory-based analysis for the AI model extraction via SCA. We derive a mutual information-enabled quantification method, theoretical lower/upper bounds of information leakage, and the minimum number of attack queries to obtain accurate weights. Furthermore, a fuzzy gray correlation-based multiple-microspace parallel SCA algorithm is proposed to extract model weights in the intelligent IoT. Based on the established information-theoretic analysis model, the proposed fuzzy gray correlation-based SCA algorithm obtains high-precision AI weights. Experimental results, consisting of simulation and real-world experiments, verify that the developed analysis method with the information-theoretic perspective is feasible and demonstrate that the designed fuzzy gray correlation-based SCA algorithm is effective for AI model extraction.",
      "year": 2022,
      "venue": "IEEE transactions on fuzzy systems",
      "authors": [
        "Qianqian Pan",
        "Jun Wu",
        "A. Bashir",
        "Jianhua Li",
        "Jie Wu"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/e8572d9722e992a770b7c00a8419dda17297a9da",
      "pdf_url": "",
      "publication_date": "2022-11-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bb6cf8210bb8035e71557dfb45d0170d909ced1f",
      "title": "Towards explainable model extraction attacks",
      "abstract": "One key factor able to boost the applications of artificial intelligence (AI) in security\u2010sensitive domains is to leverage them responsibly, which is engaged in providing explanations for AI. To date, a plethora of explainable artificial intelligence (XAI) has been proposed to help users interpret model decisions. However, given its data\u2010driven nature, the explanation itself is potentially susceptible to a high risk of exposing privacy. In this paper, we first show that the existing XAI is vulnerable to model extraction attacks and then present an XAI\u2010aware dual\u2010task model extraction attack (DTMEA). DTMEA can attack a target model with explanation services, that is, it can extract both the classification and explanation tasks of the target model. More specifically, the substitution model extracted by DTMEA is a multitask learning architecture, consisting of a sharing layer and two task\u2010specific layers for classification and explanation. To reveal which explanation technologies are more vulnerable to expose privacy information, we conduct an empirical evaluation of four major explanation types in the benchmark data set. Experimental results show that the attack accuracy of DTMEA outperforms the predicted\u2010only method with up to 1.25%, 1.53%, 9.25%, and 7.45% in MNIST, Fashion\u2010MNIST, CIFAR\u201010, and CIFAR\u2010100, respectively. By exposing the potential threats on explanation technologies, our research offers the insights to develop effective tools that are able to trade off security\u2010sensitive relationships.",
      "year": 2022,
      "venue": "International Journal of Intelligent Systems",
      "authors": [
        "Anli Yan",
        "Ruitao Hou",
        "Xiaozhang Liu",
        "Hongyang Yan",
        "Teng Huang",
        "Xianmin Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/bb6cf8210bb8035e71557dfb45d0170d909ced1f",
      "pdf_url": "",
      "publication_date": "2022-09-08",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "14df6adb35c1e13d69d9f8c61f12c07bd7294ecf",
      "title": "MExMI: Pool-based Active Model Extraction Crossover Membership Inference",
      "abstract": null,
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yaxin Xiao",
        "Qingqing Ye",
        "Haibo Hu",
        "Huadi Zheng",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/14df6adb35c1e13d69d9f8c61f12c07bd7294ecf",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0bdf3349040e5b803e8ca3a9c2cacbd9c840b747",
      "title": "Clairvoyance: Exploiting Far-field EM Emanations of GPU to \"See\" Your DNN Models through Obstacles at a Distance",
      "abstract": "Deep neural networks (DNNs) are becoming increasingly popular in real-world applications, and they are considered valuable assets of enterprises. In recent years, a number of model extraction attacks have been formulated that can be mounted to successfully steal proprietary DNN models. Nevertheless, previous model extraction attacks require either logical access to the target models or physical access to the victim machines, and thus are not suitable for performing model stealing in scenarios where an outside attacker is in the proximity but at a distance.In this paper, we propose a new model extraction attack named Clairvoyance that exploits certain far-field electromagnetic signals emanated from a GPU to steal DNN models at a distance of several meters away from the victim machine even with some obstacles in-between. Using Clairvoyance, an attacker can effectively deduce DNN architectures (e.g., the number of layers and their types) and layer configurations (e.g., the number of kernels, sizes of layers, and sizes of strides). We use several case studies (e.g., VGG and ResNet) to demonstrate its effectiveness.",
      "year": 2022,
      "venue": "2022 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Sisheng Liang",
        "Zihao Zhan",
        "Fan Yao",
        "Long Cheng",
        "Zhenkai Zhang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/0bdf3349040e5b803e8ca3a9c2cacbd9c840b747",
      "pdf_url": "",
      "publication_date": "2022-05-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8b2fb5e135323f8c69f11515ea3aceec86e6b66e",
      "title": "Stealthy Inference Attack on DNN via Cache-based Side-Channel Attacks",
      "abstract": "The advancement of deep neural networks (DNNs) motivates the deployment in various domains, including image classification, disease diagnoses, voice recognition, etc. Since some tasks that DNN undertakes are very sensitive, the label information is confidential and contains a commercial value or critical privacy. This paper demonstrates that DNNs also bring a new security threat, leading to the leakage of label information of input instances for the DNN models. In particular, we leverage the cache-based side-channel attack (SCA), i.e., Flush-Reload on the DNN (victim) models, to observe the execution of computation graphs, and create a database of them for building a classifier that the attacker can use to decide the label information of (unknown) input instances for victim models. Then we deploy the cache-based SCA on the same host machine with victim models and deduce the labels with the attacker's classification model to compromise the privacy and confidentiality of victim models. We explore different settings and classification techniques to achieve a high attack success rate of stealing label information from the victim models. Additionally, we consider two attacking scenarios: binary attacking identifies specific sensitive labels and others while multi-class attacking targets recognize all classes victim DNNs provide. Last, we implement the attack on both static DNN models with identical architectures for all inputs and dynamic DNN models with an adaptation of architectures for different inputs to demonstrate the vast existence of the proposed attack, including DenseNet 121, DenseNet 169, VGG 16, VGG 19, MobileNet v1, and MobileNet v2. Our experiment exhibits that MobileNet v1 is the most vulnerable one with 99% and 75.6% attacking success rates for binary and multi-class attacking scenarios, respectively.",
      "year": 2022,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Han Wang",
        "Syed Mahbub Hafiz",
        "Kartik Patwari",
        "Chen-Nee Chuah",
        "Zubair Shafiq",
        "H. Homayoun"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/8b2fb5e135323f8c69f11515ea3aceec86e6b66e",
      "pdf_url": "",
      "publication_date": "2022-03-14",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "83b9d7b9121ed02f7d75b294905cebe26938c6f7",
      "title": "Real-Time Identification of Natural Gas Pipeline Leakage Apertures Based on Lightweight Residual Convolutional Neural Network",
      "abstract": "Deep-learning techniques have been widely used in pipeline leakage aperture identification. However, most are designed and implemented for offline data, with problems such as large parameters, high memory consumption, and poor noise immunity. To solve the problem, this article presents a lightweight residual convolutional neural network (L-Resnet) applied to a real-time detection platform to achieve real-time identification of pipeline leakage apertures. First, based on the depth separable technique, two different separable residual modules are constructed to realize the feature extraction of signals; then, a more efficient activation function is applied to the high-dimensional space to enhance the nonlinear capability of the model; after that, a lightweight attention mechanism is used to weight the features to distinguish the importance of different features; finally, the classification results are obtained by a classifier. The real-time detection platform consists of Jetson Nano, the signal acquisition module, and the processing circuit. The results indicated that the method could accurately identify the pipeline leakage apertures in real time. Moreover, the number of parameters is only 14.71 kb, and the model has good computing efficiency and robustness compared to other methods.",
      "year": 2022,
      "venue": "IEEE Sensors Journal",
      "authors": [
        "Xiufang Wang",
        "Yuan Liu",
        "Chunlei Jiang",
        "Yueming Li",
        "Hongbo Bi"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/83b9d7b9121ed02f7d75b294905cebe26938c6f7",
      "pdf_url": "",
      "publication_date": "2022-12-15",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "33158daa5df8197872a06e415f2f277b026d9988",
      "title": "High-Fidelity Model Extraction Attacks via Remote Power Monitors",
      "abstract": "This paper shows the first side-channel attack on neural network (NN) IPs through a remote power monitor. We demonstrate that a remote monitor implemented with time-to-digital converters can be exploited to steal the weights from a hardware implementation of NN inference. Such an attack alleviates the need to have physical access to the target device and thus expands the attack vector to multi-tenant cloud FPGA platforms. Our results quantify the effectiveness of the attack on an FPGA implementation of NN inference and compare it to an attack with physical access. We demonstrate that it is indeed possible to extract the weights using DPA with 25000 traces if the SNR is sufficient. The paper, therefore, motivates secure virtualization-to protect the confidentiality of high-valued NN model IPs in multi-tenant execution environments, platform developers need to employ strong countermeasures against physical side-channel attacks.",
      "year": 2022,
      "venue": "International Conference on Artificial Intelligence Circuits and Systems",
      "authors": [
        "Anuj Dubey",
        "Emre Karabulut",
        "Amro Awad",
        "Aydin Aysu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/33158daa5df8197872a06e415f2f277b026d9988",
      "pdf_url": "",
      "publication_date": "2022-06-13",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4c57688fde64650fe767a2ba57341520595b5b13",
      "title": "A Practical Introduction to Side-Channel Extraction of Deep Neural Network Parameters",
      "abstract": "Model extraction is a major threat for embedded deep neural network models that leverages an extended attack surface. Indeed, by physically accessing a device, an adversary may exploit side-channel leakages to extract critical information of a model (i.e., its architecture or internal parameters). Different adversarial objectives are possible including a fidelity-based scenario where the architecture and parameters are precisely extracted (model cloning). We focus this work on software implementation of deep neural networks embedded in a high-end 32-bit microcontroller (Cortex-M7) and expose several challenges related to fidelity-based parameters extraction through side-channel analysis, from the basic multiplication operation to the feed-forward connection through the layers. To precisely extract the value of parameters represented in the single-precision floating point IEEE-754 standard, we propose an iterative process that is evaluated with both simulations and traces from a Cortex-M7 target. To our knowledge, this work is the first to target such an high-end 32-bit platform. Importantly, we raise and discuss the remaining challenges for the complete extraction of a deep neural network model, more particularly the critical case of biases.",
      "year": 2022,
      "venue": "Smart Card Research and Advanced Application Conference",
      "authors": [
        "Raphael Joud",
        "Pierre-Alain Mo\u00ebllic",
        "S. Ponti\u00e9",
        "J. Rigaud"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/4c57688fde64650fe767a2ba57341520595b5b13",
      "pdf_url": "https://arxiv.org/pdf/2211.05590",
      "publication_date": "2022-11-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "44b068abddec3e0162670ca15ae1eeb2b247ee72",
      "title": "Model Stealing Defense against Exploiting Information Leak through the Interpretation of Deep Neural Nets",
      "abstract": "Model stealing techniques allow adversaries to create attack models that mimic the functionality of black-box machine learning models, querying only class membership or probability outcomes. Recently, interpretable AI is getting increasing attention, to enhance our understanding of AI models, provide additional information for diagnoses, or satisfy legal requirements. However, it has been recently reported that providing such additional information can make AI models more vulnerable to model stealing attacks. In this paper, we propose DeepDefense, the first defense mechanism that protects an AI model against model stealing attackers exploiting both class probabilities and interpretations. DeepDefense uses a misdirection model to hide the critical information of the original model against model stealing attacks, with minimal degradation on both the class probability and the interpretability of prediction output. DeepDefense is highly applicable for any model stealing scenario since it makes minimal assumptions about the model stealing adversary. In our experiments, DeepDefense shows significantly higher defense performance than the existing state-of-the-art defenses on various datasets and interpreters.",
      "year": 2022,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Jeonghyun Lee",
        "Sungmin Han",
        "Sangkyun Lee"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/44b068abddec3e0162670ca15ae1eeb2b247ee72",
      "pdf_url": "https://www.ijcai.org/proceedings/2022/0100.pdf",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8119ab9eb8974693705bde0fa074439da40fda96",
      "title": "HDLock: exploiting privileged encoding to protect hyperdimensional computing models against IP stealing",
      "abstract": "Hyperdimensional Computing (HDC) is facing infringement issues due to straightforward computations. This work, for the first time, raises a critical vulnerability of HDC --- an attacker can reverse engineer the entire model, only requiring the unindexed hypervector memory. To mitigate this attack, we propose a defense strategy, namely HDLock, which significantly increases the reasoning cost of encoding. Specifically, HDLock adds extra feature hypervector combination and permutation in the encoding module. Compared to the standard HDC model, a two-layer-key HDLock can increase the adversarial reasoning complexity by 10 order of magnitudes without inference accuracy loss, with only 21% latency overhead.",
      "year": 2022,
      "venue": "Design Automation Conference",
      "authors": [
        "Shijin Duan",
        "Shaolei Ren",
        "Xiaolin Xu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/8119ab9eb8974693705bde0fa074439da40fda96",
      "pdf_url": "",
      "publication_date": "2022-03-18",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5639133170aa4f4a598994518d34a9b375494876",
      "title": "A Systematic View of Model Leakage Risks in Deep Neural Network Systems",
      "abstract": "As deep neural networks (DNNs) continue to find applications in ever more domains, the exact nature of the neural network architecture becomes an increasingly sensitive subject, due to either intellectual property protection or risks of adversarial attacks. While prior work has explored aspects of the risk associated with model leakage, exactly which parts of the model are most sensitive and how one infers the full architecture of the DNN when nothing is known about the structure a priori are problems that have been left unexplored. In this paper we address this gap, first by presenting a schema for reasoning about model leakage holistically, and then by proposing and quantitatively evaluating DeepSniffer, a novel learning-based model extraction framework that uses no prior knowledge of the victim model. DeepSniffer is robust to architectural and system noises introduced by the complex memory hierarchy and diverse run-time system optimizations. Taking GPU platforms as a showcase, DeepSniffer performs model extraction by learning both the architecture-level execution features of kernels and the inter-layer temporal association information introduced by the common practice of DNN design. We demonstrate that DeepSniffer works experimentally in the context of an off-the-shelf Nvidia GPU platform running a variety of DNN models and that the extracted models significantly improve attempts at crafting adversarial inputs. The DeepSniffer project has been released in https://github.com/xinghu7788/DeepSniffer.",
      "year": 2022,
      "venue": "IEEE transactions on computers",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Xiaobing Chen",
        "Lei Deng",
        "Yu Ji",
        "Yufei Ding",
        "Zidong Du",
        "Qi Guo",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/5639133170aa4f4a598994518d34a9b375494876",
      "pdf_url": "https://doi.org/10.1109/tc.2022.3148235",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a0a63d6c89b5925591da1384413d660a4a3e49a6",
      "title": "DNNCloak: Secure DNN Models Against Memory Side-channel Based Reverse Engineering Attacks",
      "abstract": "As deep neural networks (DNN) expand their attention into various domains and the high cost of training a model, the structure of a DNN model has become a valuable intellectual property and needs to be protected. However, reversing DNN models by exploiting side-channel leakage has been demonstrated in various ways. Even if the model is encrypted and the processing hardware units are trusted, the attacker can still extract the model\u2019s structure and critical parameters through side channels, potentially posing significant commercial risks. In this paper, we begin by analyzing representative memory side-channel attacks on DNN models and identifying the primary causes of leakage. We also find that the full encryption used to protect model parameters could add extensive overhead. Based on our observations, we propose DNNCloak, a lightweight and secure framework aiming at mitigating reverse engineering attacks on common DNN architectures. DNNCloak includes a set of obfuscation schemes that increase the difficulty of reverse-engineering the DNN structure. Additionally, DNNCloak reduces the overhead of full weights encryption with an efficient matrix permutation scheme, resulting in reduced memory access time and enhanced security against retraining attacks on the model parameters. At last, we show how DNNCloak can defend DNN models from side-channel attacks effectively, with minimal performance overhead.",
      "year": 2022,
      "venue": "ICCD",
      "authors": [
        "Yuezhi Che",
        "Rujia Wang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/a0a63d6c89b5925591da1384413d660a4a3e49a6",
      "pdf_url": "",
      "publication_date": "2022-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dba696ce4856d7fad6a51847a8997145050eb9c3",
      "title": "GAME: Generative-Based Adaptive Model Extraction Attack",
      "abstract": null,
      "year": 2022,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Yi Xie",
        "Mengdie Huang",
        "Xiaoyu Zhang",
        "Changyu Dong",
        "W. Susilo",
        "Xiaofeng Chen"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/dba696ce4856d7fad6a51847a8997145050eb9c3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "53bb321ffc1864f9ed3fc689085f8bed943971a3",
      "title": "DynaMarks: Defending Against Deep Learning Model Extraction Using Dynamic Watermarking",
      "abstract": "The functionality of a deep learning (DL) model can be stolen via model extraction where an attacker obtains a surrogate model by utilizing the responses from a prediction API of the original model. In this work, we propose a novel watermarking technique called DynaMarks to protect the intellectual property (IP) of DL models against such model extraction attacks in a black-box setting. Unlike existing approaches, DynaMarks does not alter the training process of the original model but rather embeds watermark into a surrogate model by dynamically changing the output responses from the original model prediction API based on certain secret parameters at inference runtime. The experimental outcomes on Fashion MNIST, CIFAR-10, and ImageNet datasets demonstrate the efficacy of DynaMarks scheme to watermark surrogate models while preserving the accuracies of the original models deployed in edge devices. In addition, we also perform experiments to evaluate the robustness of DynaMarks against various watermark removal strategies, thus allowing a DL model owner to reliably prove model ownership.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Abhishek Chakraborty",
        "Daniel Xing",
        "Yuntao Liu",
        "Ankur Srivastava"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/53bb321ffc1864f9ed3fc689085f8bed943971a3",
      "pdf_url": "http://arxiv.org/pdf/2207.13321",
      "publication_date": "2022-07-27",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4908fdc1feff153269670f0f8aac837346042553",
      "title": "Demystifying Arch-hints for Model Extraction: An Attack in Unified Memory System",
      "abstract": "The deep neural network (DNN) models are deemed confidential due to their unique value in expensive training efforts, privacy-sensitive training data, and proprietary network characteristics. Consequently, the model value raises incentive for adversary to steal the model for profits, such as the representative model extraction attack. Emerging attack can leverage timing-sensitive architecture-level events (i.e., Arch-hints) disclosed in hardware platforms to extract DNN model layer information accurately. In this paper, we take the first step to uncover the root cause of such Arch-hints and summarize the principles to identify them. We then apply these principles to emerging Unified Memory (UM) management system and identify three new Arch-hints caused by UM's unique data movement patterns. We then develop a new extraction attack, UMProbe. We also create the first DNN benchmark suite in UM and utilize the benchmark suite to evaluate UMProbe. Our evaluation shows that UMProbe can extract the layer sequence with an accuracy of 95% for almost all victim test models, which thus calls for more attention to the DNN security in UM system.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Zhendong Wang",
        "Xiaoming Zeng",
        "Xulong Tang",
        "Danfeng Zhang",
        "Xingbo Hu",
        "Yang Hu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/4908fdc1feff153269670f0f8aac837346042553",
      "pdf_url": "http://arxiv.org/pdf/2208.13720",
      "publication_date": "2022-08-29",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d1e08aa5c411179d72cfb971767f53ce8b9ce4ff",
      "title": "A ThreshoId-ImpIementation-Based Neural-Network Accelerator Securing Model Parameters and Inputs Against Power Side-Channel Attacks",
      "abstract": "Neural network (NN) hardware accelerators are being widely deployed on low-power loT nodes for energy-efficient decision making. Embedded NN implementations can use locally stored proprietary models, and may operate over private inputs (e.g., health monitors with patient-specific biomedical classifiers [6]), which must not be disclosed. Side-channel attacks (SCA) are a major concern in embedded systems where physical access to the operating hardware can allow attackers to recover secret data by exploiting information leakage through power consumption, timing and electromagnetic emissions [1, 7, 8]. As shown in Fig. 34.3.1, SCA on embedded NN implementations can reveal the model parameters [9] as well as the inputs [10]. To address these concerns, we present an energy - efficient ASlC solution for protecting both the model parameters and the input data against power-based SCA.",
      "year": 2022,
      "venue": "IEEE International Solid-State Circuits Conference",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "Samuel H. Fuller",
        "A. Chandrakasan"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/d1e08aa5c411179d72cfb971767f53ce8b9ce4ff",
      "pdf_url": "",
      "publication_date": "2022-02-20",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e0d3dbcc5819e70dd23deba8f0f00648fede12de",
      "title": "Fault Diagnosis of Tower Grounding Conductor Based on the Electromagnetic Measurement and Neural Network",
      "abstract": "The performance of the tower grounding conductor is very important for the safe and reliable operation of the power transmission system. Under the current excitation, the grounding conductor in different states will produce different magnetic fields on the earth\u2019s surface. By measuring the surface magnetic fields, the fault types and location of the grounding conductor can be evaluated. However, the process of traditional manual analysis is extremely complex. This article proposes a fault diagnosis method for tower grounding conductors based on deep learning to replace the manual diagnosis. The earth\u2019s surface magnetic field dataset generated by the grounding conductor consists of simulation data and experimental data. For practical application, the fine-tuning method is proposed to improve the diagnosis performance of the 1-D-convolutional neural network (1-D-CNN). Compared with the original 1-D-CNN model, the results demonstrate that the proposed fine-tuning 1-D-CNN has improved the diagnostic ability of the tower grounding conductor. For the six kinds of faults of rectangular tower grounding conductor, the average classification accuracy of the fine-tuning 1-D-CNN model reaches 80.50% when only the classifier and dense connection layers are trained. The proposed method can be used in the fault diagnosis of tower grounding conductors, which has significant potential applications in detecting and maintaining tower grounding conductors.",
      "year": 2022,
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": [
        "Caijiang Lu",
        "T. Zhang",
        "Shaoheng Sun",
        "Zhongqing Cao",
        "Mingyong Xin",
        "Guoqiang Fu",
        "Tao Wang",
        "Xi Wang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/e0d3dbcc5819e70dd23deba8f0f00648fede12de",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8a435e15c39efebf6cad521c15fff50fcf7e0bfb",
      "title": "Model Extraction Attack and Defense on Deep Generative Models",
      "abstract": "The security issues of machine learning have aroused much attention and model extraction attack is one of them. The definition of model extraction attack is that an adversary can collect data through query access to a victim model and train a substitute model with it in order to steal the functionality of the target model. At present, most of the related work has focused on the research of model extraction attack against discriminative models while this paper pays attention to deep generative models. First, considering the difference of an adversary` goals, the attacks are taxonomized into two different types: accuracy extraction attack and fidelity extraction attack and the effect is evaluated by 1-NN accuracy. Attacks among three main types of deep generative models and the influence of the number of queries are also researched. Finally, this paper studies different defensive techniques to safeguard the models according to their architectures.",
      "year": 2022,
      "venue": "Journal of Physics: Conference Series",
      "authors": [
        "Sheng Liu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/8a435e15c39efebf6cad521c15fff50fcf7e0bfb",
      "pdf_url": "https://doi.org/10.1088/1742-6596/2189/1/012024",
      "publication_date": "2022-02-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "54d3e2764c3445c89fbaa9c684c5f5d03cb44254",
      "title": "Play the Imitation Game: Model Extraction Attack against Autonomous Driving Localization",
      "abstract": "The security of the Autonomous Driving (AD) system has been gaining researchers\u2019 and public\u2019s attention recently. Given that AD companies have invested a huge amount of resources in developing their AD models, e.g., localization models, these models, especially their parameters, are important intellectual property and deserve strong protection. In this work, we examine whether the confidentiality of production-grade Multi-Sensor Fusion (MSF) models, in particular, Error-State Kalman Filter (ESKF), can be stolen from an outside adversary. We propose a new model extraction attack called TaskMaster that can infer the secret ESKF parameters under black-box assumption. In essence, TaskMaster trains a substitutional ESKF model to recover the parameters, by observing the input and output to the targeted AD system. To precisely recover the parameters, we combine a set of techniques, like gradient-based optimization, search-space reduction and multi-stage optimization. The evaluation result on real-world vehicle sensor dataset shows that TaskMaster is practical. For example, with 25 seconds AD sensor data for training, the substitutional ESKF model reaches centimeter-level accuracy, comparing with the ground-truth model.",
      "year": 2022,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Qifan Zhang",
        "Junjie Shen",
        "Mingtian Tan",
        "Zhe Zhou",
        "Zhou Li",
        "Qi Alfred Chen",
        "Haipeng Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/54d3e2764c3445c89fbaa9c684c5f5d03cb44254",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3564625.3567977",
      "publication_date": "2022-12-05",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f74980d18e194246618eca55faad7858fe57d08c",
      "title": "Leveraging Ferroelectric Stochasticity and In-Memory Computing for DNN IP Obfuscation",
      "abstract": "With the emergence of the Internet of Things (IoT), deep neural networks (DNNs) are widely used in different domains, such as computer vision, healthcare, social media, and defense. The hardware-level architecture of a DNN can be built using an in-memory computing-based design, which is loaded with the weights of a well-trained DNN model. However, such hardware-based DNN systems are vulnerable to model stealing attacks where an attacker reverse-engineers (REs) and extracts the weights of the DNN model. In this work, we propose an energy-efficient defense technique that combines a ferroelectric field effect transistor (FeFET)-based reconfigurable physically unclonable function (PUF) with an in-memory FeFET XNOR to thwart model stealing attacks. We leverage the inherent stochasticity in the FE domains to build a PUF that helps to corrupt the neural network\u2019s (NN) weights when an adversarial attack is detected. We showcase the efficacy of the proposed defense scheme by performing experiments on graph-NNs (GNNs), a particular type of DNN. The proposed defense scheme is a first of its kind that evaluates the security of GNNs. We investigate the effect of corrupting the weights on different layers of the GNN on the accuracy degradation of the graph classification application for two specific error models of corrupting the FeFET-based PUFs and five different bioinformatics datasets. We demonstrate that our approach successfully degrades the inference accuracy of the graph classification by corrupting any layer of the GNN after a small rewrite pulse.",
      "year": 2022,
      "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits",
      "authors": [
        "Likhitha Mankali",
        "N. Rangarajan",
        "Swetaki Chatterjee",
        "Shubham Kumar",
        "Y. Chauhan",
        "O. Sinanoglu",
        "Hussam Amrouch"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/f74980d18e194246618eca55faad7858fe57d08c",
      "pdf_url": "https://doi.org/10.1109/jxcdc.2022.3217043",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2a7c3ecb7a424480687b3fd1c6f4f0b0a04b100a",
      "title": "PUFs Physical Learning: Accelerating the Enrollment via Delay-Based Model Extraction",
      "abstract": "The introduction of Physical Unclonable Functions (PUFs) has been originally motivated by their ability to resist physical attacks, particularly in anti-counterfeiting scenarios. In these one-way functions, machine learning, cryptanalysis, and side-channel attacks are common attack vectors threatening the promised PUF's property of unclonability. These attacks often emulate a PUF by employing a large number of Challenge-Response Pairs (CRPs). Some solutions to defeat such attacks are based on a protocol, where a model of the underlying PUF primitives should be extracted during the enrollment phase. In this article, we introduce a novel physical cloning approach applicable to FPGA-based implementations, which allows extracting the PUF's unique physical characteristics with a few number of Challenge-Response Pairs (CRPs), that increases only linearly for a higher number of PUF components. Indeed, our proposed approach significantly accelerates the enrollment phase and makes complex enrollment protocols feasible. Our core idea relies on an on-chip delay sensor, which can be realized by ordinary FPGA components, measuring the unique characteristic of the PUF elements. We demonstrate the feasibility of our introduced technique by practical experiments on different FPGA platforms, cloning a couple of (complex) PUF constructions, i.e., XOR APUF, iPUF, composed of delay-based Arbiter PUFs.",
      "year": 2022,
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": [
        "Anita Aghaie",
        "Maik Ender",
        "A. Moradi"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2a7c3ecb7a424480687b3fd1c6f4f0b0a04b100a",
      "pdf_url": "",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "cf95400543d5018168d8949e1f08805c72b6c162",
      "title": "Bias magnetic characteristic analysis and condition identification of transformers under DC bias magnetism conditions based on electromagnetic vibration and convolutional neural network",
      "abstract": null,
      "year": 2022,
      "venue": "Journal of Magnetism and Magnetic Materials",
      "authors": [
        "Wang Guo",
        "Xingmou Liu",
        "You Ma",
        "Yongming Yang",
        "ammad jadoo"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/cf95400543d5018168d8949e1f08805c72b6c162",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1761a6879ea8d5224aabe0e63424042dc3d901d8",
      "title": "Enhance Model Stealing Attack via Label Refining",
      "abstract": "With machine learning models being increasingly deployed, model stealing attacks have raised an increasing interest. Extracting decision-based models is a more challenging task with the information of class similarity missing. In this paper, we propose a novel and effective model stealing method as Label Refining via Feature Distance (LRFD), to re-dig the class similarity. Specifically, since the information of class similarity can be represented by the distance between samples from different classes in the feature space, we design a soft label construction module inspired by the prototype learning, and transfer the knowledge in the soft label to the substitution model. Extensive experiments conducted on four widely-used datasets consistently demonstrate that our method yields a model with significantly greater functional similarity to the victim model.",
      "year": 2022,
      "venue": "International Conference on the Software Process",
      "authors": [
        "Yixu Wang",
        "Xianming Lin"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1761a6879ea8d5224aabe0e63424042dc3d901d8",
      "pdf_url": "",
      "publication_date": "2022-04-15",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c082d0fdc703fd06f99a93f4683416f987d335ff",
      "title": "Privacy-Preserving DNN Model Authorization against Model Theft and Feature Leakage",
      "abstract": "Today\u2019s intelligent services are built on well-trained deep neural network (DNN) models, which usually require large private datasets along with a high cost for model training. It consequently makes the model providers cherish the pre-trained DNN models and only distribute them to authorized users. However, malicious users can steal these valuable models for abuse, illegal copy and redistribution. Attackers can also extract private features from even authorized models to leak partial training datasets. They both violate privacy. Existing techniques from secure community attempt to avoid parameter leakage during model authorization but yet cannot solve privacy issues sufficiently. In this paper, we propose a privacy-preserving model authorization approach, AgAuth, to resist the aforementioned privacy threats. We devise a novel scheme called Information-Agnostic Conversion (IAC) for forwarding procedure to eliminate residual features in model parameters. Based on it, we then propose Inference-on-Ciphertext (CiFer) mechanism for DNN reasoning, which includes three stages in each forwarding. The Encrypt phase first converts the proprietary model parameters to demonstrate uniform distribution. The Forward stage per-forms forwarding function without decryption at authorized side. Specifically, this stage just computes over ciphertext. The Decrypt phase finally recovers the information-agnostic outputs to informative output tensor for real-world services. In addition, we implement a prototype and conduct extensive experiments to evaluate its performance. The qualitative and quantitative results demonstrate that our solution AgAuth is privacy-preserving to defend against model theft and feature leakage, without accuracy loss or notable performance decrease.",
      "year": 2022,
      "venue": "ICC 2022 - IEEE International Conference on Communications",
      "authors": [
        "Qiushi Li",
        "Ju Ren",
        "Yuezhi Zhou",
        "Yaoxue Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c082d0fdc703fd06f99a93f4683416f987d335ff",
      "pdf_url": "",
      "publication_date": "2022-05-16",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1e49811d8f55244c2cd7e657ba3cebc5e5ed6dd5",
      "title": "A Survey on Side-Channel-based Reverse Engineering Attacks on Deep Neural Networks",
      "abstract": "Hardware side-channels have been exploited to leak sensitive information. With the emergence of deep learning, their hardware platforms have also been scrutinized for side-channel information leakage. It has been shown that the structure, weights, and input samples of deep neural networks (DNN) can all be the victim of reverse engineering attacks that rely on side-channel information leakage. In this paper, we survey existing work on hardware side-channel-based reverse engineering attacks on DNNs as well as the countermeasures.",
      "year": 2022,
      "venue": "International Conference on Artificial Intelligence Circuits and Systems",
      "authors": [
        "Yuntao Liu",
        "Michael Zuzak",
        "Daniel Xing",
        "Isaac McDaniel",
        "Priya Mittu",
        "Olsan Ozbay",
        "Abir Akib",
        "Ankur Srivastava"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1e49811d8f55244c2cd7e657ba3cebc5e5ed6dd5",
      "pdf_url": "",
      "publication_date": "2022-06-13",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16b149e4472f863cfee5999644e37c216900cd01",
      "title": "A Framework for Understanding Model Extraction Attack and Defense",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as-a-Service applications, where prediction services based on well-trained models are offered to users via pay-per-query. The lack of a defense mechanism can impose a high risk on the privacy of the server's model since an adversary could efficiently steal the model by querying only a few `good' data points. The interplay between a server's defense and an adversary's attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user's view and privacy from an adversary's view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the `equilibrium' between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results will be demonstrated by examples and experiments.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/16b149e4472f863cfee5999644e37c216900cd01",
      "pdf_url": "https://arxiv.org/pdf/2206.11480",
      "publication_date": "2022-06-23",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
      "title": "MEGA: Model Stealing via Collaborative Generator-Substitute Networks",
      "abstract": "Deep machine learning models are increasingly deployedin the wild for providing services to users. Adversaries maysteal the knowledge of these valuable models by trainingsubstitute models according to the inference results of thetargeted deployed models. Recent data-free model stealingmethods are shown effective to extract the knowledge of thetarget model without using real query examples, but they as-sume rich inference information, e.g., class probabilities andlogits. However, they are all based on competing generator-substitute networks and hence encounter training instability.In this paper we propose a data-free model stealing frame-work,MEGA, which is based on collaborative generator-substitute networks and only requires the target model toprovide label prediction for synthetic query examples. Thecore of our method is a model stealing optimization con-sisting of two collaborative models (i) the substitute modelwhich imitates the target model through the synthetic queryexamples and their inferred labels and (ii) the generatorwhich synthesizes images such that the confidence of thesubstitute model over each query example is maximized. Wepropose a novel coordinate descent training procedure andanalyze its convergence. We also empirically evaluate thetrained substitute model on three datasets and its applicationon black-box adversarial attacks. Our results show that theaccuracy of our trained substitute model and the adversarialattack success rate over it can be up to 33% and 40% higherthan state-of-the-art data-free black-box attacks.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Chi Hong",
        "Jiyue Huang",
        "L. Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
      "pdf_url": "",
      "publication_date": "2022-01-31",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b7c49323aaa05f3732d0c43767e659c39169f724",
      "title": "Understanding Model Extraction Games",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as- a-Service applications, where prediction services based on well- trained models are offered to users via the pay-per-query scheme. However, the lack of a defense mechanism can impose a high risk on the privacy of the server\u2019s model since an adversary could efficiently steal the model by querying only a few \u2018good\u2019 data points. The game between a server\u2019s defense and an adversary\u2019s attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user\u2019s view and privacy from an adversary\u2019s view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the \u2018equilibrium\u2019 between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results are demonstrated by examples and empirical experiments.",
      "year": 2022,
      "venue": "International Conference on Trust, Privacy and Security in Intelligent Systems and Applications",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b7c49323aaa05f3732d0c43767e659c39169f724",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6482f6bf11a3b1c6f9c9f1ceaeddfcbebd68262c",
      "title": "Radar signal recognition based on deep convolutional neural network in complex electromagnetic environment",
      "abstract": "To solve the problem that tradition signal recognition algorithms cannot effectively recognize the contaminated and diverse radar signals in complex and variable Electronic Warfare (EW) environment, a new recognition method based on deep convolutional neural network (CNN) and time-frequency (TF) analysis is proposed. Firstly, the TF images of radar signals are extracted as the inputs to the CNN model. Then, a new network, called CNN-TF, is constructed to analyze these time-frequency images and use the robustness of CNN to suppress the noise interference. Thirdly, a complete and diverse signal librai7 is constructed based on the complex EW environment, and the librai7 is used to train and test CNN-TF. Finally, trained CNN-TF will be used for signal recognition. Simulation results show that the proposed algorithm not only improves the performance of signal recognition, but also has excellent anti-noise performance, which makes the proposed algorithm adapt to the complex and variable electronic warfare environment.",
      "year": 2022,
      "venue": "Annual Conference on Information Sciences and Systems",
      "authors": [
        "Zhang Qi",
        "Yewei Chen",
        "Yuan Liu",
        "Anqi Xu",
        "Li Li",
        "Jianpu Li"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/6482f6bf11a3b1c6f9c9f1ceaeddfcbebd68262c",
      "pdf_url": "",
      "publication_date": "2022-11-02",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "32d8e4dd6b800ba50d9224ae007707b345c7e711",
      "title": "Recovering the Weights of Convolutional Neural Network via Chosen Pixel Horizontal Power Analysis",
      "abstract": null,
      "year": 2022,
      "venue": "Wireless Algorithms, Systems, and Applications",
      "authors": [
        "Sihan He",
        "Weibin Wu",
        "Yanbin Li",
        "Lu Zhou",
        "Liming Fang",
        "Zhe Liu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/32d8e4dd6b800ba50d9224ae007707b345c7e711",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f1f33c9654649947d2dffb02c12fc36c5fd5116e",
      "title": "Model Extraction Attack against Self-supervised Speech Models",
      "abstract": "Self-supervised learning (SSL) speech models generate meaningful representations of given clips and achieve incredible performance across various downstream tasks. Model extraction attack (MEA) often refers to an adversary stealing the functionality of the victim model with only query access. In this work, we study the MEA problem against SSL speech model with a small number of queries. We propose a two-stage framework to extract the model. In the first stage, SSL is conducted on the large-scale unlabeled corpus to pre-train a small speech model. Secondly, we actively sample a small portion of clips from the unlabeled corpus and query the target model with these clips to acquire their representations as labels for the small model's second-stage training. Experiment results show that our sampling methods can effectively extract the target model without knowing any information about its model architecture.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Tsung-Yuan Hsu",
        "Chen-An Li",
        "Tung-Yu Wu",
        "Hung-yi Lee"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f1f33c9654649947d2dffb02c12fc36c5fd5116e",
      "pdf_url": "https://arxiv.org/pdf/2211.16044",
      "publication_date": "2022-11-29",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d62cb1d14a12b128e2d45b22b4239dd6417780a",
      "title": "Matryoshka: Stealing Functionality of Private ML Data by Hiding Models in Model",
      "abstract": "In this paper, we present a novel insider attack called Matryoshka, which employs an irrelevant scheduled-to-publish DNN model as a carrier model for covert transmission of multiple secret models which memorize the functionality of private ML data stored in local data centers. Instead of treating the parameters of the carrier model as bit strings and applying conventional steganography, we devise a novel parameter sharing approach which exploits the learning capacity of the carrier model for information hiding. Matryoshka simultaneously achieves: (i) High Capacity -- With almost no utility loss of the carrier model, Matryoshka can hide a 26x larger secret model or 8 secret models of diverse architectures spanning different application domains in the carrier model, neither of which can be done with existing steganography techniques; (ii) Decoding Efficiency -- once downloading the published carrier model, an outside colluder can exclusively decode the hidden models from the carrier model with only several integer secrets and the knowledge of the hidden model architecture; (iii) Effectiveness -- Moreover, almost all the recovered models have similar performance as if it were trained independently on the private data; (iv) Robustness -- Information redundancy is naturally implemented to achieve resilience against common post-processing techniques on the carrier before its publishing; (v) Covertness -- A model inspector with different levels of prior knowledge could hardly differentiate a carrier model from a normal model.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Xudong Pan",
        "Yifan Yan",
        "Sheng Zhang",
        "Mi Zhang",
        "Min Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3d62cb1d14a12b128e2d45b22b4239dd6417780a",
      "pdf_url": "http://arxiv.org/pdf/2206.14371",
      "publication_date": "2022-06-29",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2841458166d23bd76abd8f2a03b8d57c99522d44",
      "title": "Model Stealing Attack based on Sampling and Weighting",
      "abstract": null,
      "year": 2022,
      "venue": "",
      "authors": [
        "\u8bba\u6587 \u57fa\u4e8e\u91c7\u6837\u548c\u52a0\u6743\u635f\u5931\u51fd\u6570\u7684\u6a21\u578b\u7a83\u53d6\u653b\u51fb\u65b9\u6cd5 \u738b\u71a0\u65ed"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2841458166d23bd76abd8f2a03b8d57c99522d44",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f7b70da96899fdac65153e172e426c5b39d2bde5",
      "title": "Detecting Data-Free Model Stealing",
      "abstract": null,
      "year": 2022,
      "venue": "",
      "authors": [
        "Ashley Borum",
        "James Beetham",
        "Dr. Niels Da",
        "Vitoria Lobo",
        "Dr. Mubarak Shah"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f7b70da96899fdac65153e172e426c5b39d2bde5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
      "title": "Verify Deep Learning Models Ownership via Preset Embedding",
      "abstract": "A well-trained deep neural network (DNNs) requires massive computing resources and data, therefore it belongs to the model owners\u2019 Intellectual Property (IP). Recent works have shown that the model can be stolen by the adversary without any training data or internal parameters of the model. Currently, there were some defense methods to resist it, by increasing the cost of model stealing attack or detecting the theft afterwards.In this paper, We propose a method to determine theft by detecting whether the victim\u2019s preset embedding exists in the adversary model. Firstly, we convert some training images into grayscale images as embedding and inject them to the training set. Then, we train a binary classifier to determine whether the model is stolen from the victim. The main intuition behind our approach is that the stolen model should contain embedded knowledge learned by the victim model. Our results demonstrate that our method is effective in defending against different types of model theft methods.",
      "year": 2022,
      "venue": "2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)",
      "authors": [
        "Wenxuan Yin",
        "Hai-feng Qian"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e3bc283af5746de4ede17c123d39e02e819edcab",
      "title": "Characterizing Side-Channel Leakage of DNN Classifiers though Performance Counters",
      "abstract": "Rapid advancements in Deep Neural Networks (DNN) have led to their deployment in a wide range of com-mercial applications. DNN classifiers are powerful tools that drive a broad spectrum of important applications, from image recognition to autonomous vehicles. Like other applications, they have been shown to be vulnerable to side-channel information leakage. There have been several proof-of-concept attacks demon-strating the extraction of their model parameters and input data. However, no prior study has examined the possibility of using side-channels to extract the DNN classifier's decision or output. In this initial study, we aim to understand if there exists a correlation between the output class selected by a classifier and side-channel information collected while running the inference process on a CPU. Our initial evaluation shows that with the proposed approach it is possible to accurately recover the output class for model inputs via multiple side-channels: primarily power, but also branch mispredictions and cache misses.",
      "year": 2022,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Saikat Majumdar",
        "Mohammad Hossein Samavatian",
        "R. Teodorescu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e3bc283af5746de4ede17c123d39e02e819edcab",
      "pdf_url": "",
      "publication_date": "2022-06-27",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1921489a2801f053d0906079f0015ba6e68dd505",
      "title": "DIET-SNN: A Low-Latency Spiking Neural Network With Direct Input Encoding and Leakage and Threshold Optimization",
      "abstract": "Bioinspired spiking neural networks (SNNs), operating with asynchronous binary signals (or spikes) distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. The state-of-the-art SNNs suffer from high inference latency, resulting from inefficient input encoding and suboptimal settings of the neuron parameters (firing threshold and membrane leak). We propose DIET-SNN, a low-latency deep spiking network trained with gradient descent to optimize the membrane leak and the firing threshold along with other network parameters (weights). The membrane leak and threshold of each layer are optimized with end-to-end backpropagation to achieve competitive accuracy at reduced latency. The input layer directly processes the analog pixel values of an image without converting it to spike train. The first convolutional layer converts analog inputs into spikes where leaky-integrate-and-fire (LIF) neurons integrate the weighted inputs and generate an output spike when the membrane potential crosses the trained firing threshold. The trained membrane leak selectively attenuates the membrane potential, which increases activation sparsity in the network. The reduced latency combined with high activation sparsity provides massive improvements in computational efficiency. We evaluate DIET-SNN on image classification tasks from CIFAR and ImageNet datasets on VGG and ResNet architectures. We achieve top-1 accuracy of 69% with five timesteps (inference latency) on the ImageNet dataset with $12\\times $ less compute energy than an equivalent standard artificial neural network (ANN). In addition, DIET-SNN performs 20\u2013 $500\\times $ faster inference compared to other state-of-the-art SNN models.",
      "year": 2021,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Nitin Rathi",
        "K. Roy"
      ],
      "citation_count": 289,
      "url": "https://www.semanticscholar.org/paper/1921489a2801f053d0906079f0015ba6e68dd505",
      "pdf_url": "",
      "publication_date": "2021-10-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "74f6e70fd9a945b517e6495920a1267c01842bd4",
      "title": "DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories",
      "abstract": "Recent advancements in Deep Neural Networks (DNNs) have enabled widespread deployment in multiple security-sensitive domains. The need for resource-intensive training and the use of valuable domain-specific training data have made these models the top intellectual property (IP) for model owners. One of the major threats to DNN privacy is model extraction attacks where adversaries attempt to steal sensitive information in DNN models. In this work, we propose an advanced model extraction framework DeepSteal that steals DNN weights remotely for the first time with the aid of a memory side-channel attack. Our proposed DeepSteal comprises two key stages. Firstly, we develop a new weight bit information extraction method, called HammerLeak, through adopting the rowhammer-based fault technique as the information leakage vector. HammerLeak leverages several novel system-level techniques tailored for DNN applications to enable fast and efficient weight stealing. Secondly, we propose a novel substitute model training algorithm with Mean Clustering weight penalty, which leverages the partial leaked bit information effectively and generates a substitute prototype of the target victim model. We evaluate the proposed model extraction framework on three popular image datasets (e.g., CIFAR-10/100/GTSRB) and four DNN architectures (e.g., ResNet-18/34/Wide-ResNetNGG-11). The extracted substitute model has successfully achieved more than 90% test accuracy on deep residual networks for the CIFAR-10 dataset. Moreover, our extracted substitute model could also generate effective adversarial input samples to fool the victim model. Notably, it achieves similar performance (i.e., ~1-2% test accuracy under attack) as white-box adversarial input attack (e.g., PGD/Trades).",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "A. S. Rakin",
        "Md Hafizul Islam Chowdhuryy",
        "Fan Yao",
        "Deliang Fan"
      ],
      "citation_count": 137,
      "url": "https://www.semanticscholar.org/paper/74f6e70fd9a945b517e6495920a1267c01842bd4",
      "pdf_url": "http://arxiv.org/pdf/2111.04625",
      "publication_date": "2021-11-08",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "title": "Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!",
      "abstract": "Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.",
      "year": 2021,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Xuanli He",
        "L. Lyu",
        "Qiongkai Xu",
        "Lichao Sun"
      ],
      "citation_count": 111,
      "url": "https://www.semanticscholar.org/paper/16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "pdf_url": "https://aclanthology.org/2021.naacl-main.161.pdf",
      "publication_date": "2021-03-18",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "45ea6495958f04c1f02de1741c952dac1154d4f4",
      "title": "UnSplit: Data-Oblivious Model Inversion, Model Stealing, and Label Inference Attacks against Split Learning",
      "abstract": "Training deep neural networks often forces users to work in a distributed or outsourced setting, accompanied with privacy concerns. Split learning aims to address this concern by distributing the model among a client and a server. The scheme supposedly provides privacy, since the server cannot see the clients' models and inputs. We show that this is not true via two novel attacks. (1) We show that an honest-but-curious split learning server, equipped only with the knowledge of the client neural network architecture, can recover the input samples and obtain a functionally similar model to the client model, without being detected. (2) We show that if the client keeps hidden only the output layer of the model to ''protect'' the private labels, the honest-but-curious server can infer the labels with perfect accuracy. We test our attacks using various benchmark datasets and against proposed privacy-enhancing extensions to split learning. Our results show that plaintext split learning can pose serious risks, ranging from data (input) privacy to intellectual property (model parameters), and provide no more than a false sense of security.",
      "year": 2021,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Ege Erdogan",
        "Alptekin K\u00fcp\u00e7\u00fc",
        "A. E. Cicek"
      ],
      "citation_count": 98,
      "url": "https://www.semanticscholar.org/paper/45ea6495958f04c1f02de1741c952dac1154d4f4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3559613.3563201",
      "publication_date": "2021-08-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f2eb05b91e4fed91626c961372d7d189211aa552",
      "title": "Memristive electromagnetic induction effects on Hopfield neural network",
      "abstract": "Due to the existence of membrane potential differences, the electromagnetic induction flows can be induced in the interconnected neurons of Hopfield neural network (HNN). To express the induction flows, this paper presents a unified memristive HNN model using hyperbolic-type memristors to link neurons. By employing theoretical analysis along with multiple numerical methods, we explore the electromagnetic induction effects on the memristive HNN with three neurons. Three cases are classified and discussed. When using one memristor to link two neurons bidirectionally, the coexisting bifurcation behaviors and extreme events are disclosed with respect to the memristor coupling strength. When using two memristors to link three neurons, the antimonotonicity phenomena of periodic and chaotic bubbles are yielded, and the initial-related extreme events are emerged. When using three memristors to link three neurons end to end, the extreme events owning prominent riddled basins of attraction are demonstrated. In addition, we develop the printed circuit board (PCB)-based hardware experiments by synthesizing the memristive HNN, and the experimental results well confirm the memristive electromagnetic induction effects. Certainly, the PCB-based implementation will benefit the integrated circuit design for large-scale Hopfield neural network in the future.",
      "year": 2021,
      "venue": "Nonlinear dynamics",
      "authors": [
        "Chengjie Chen",
        "Fuhong Min",
        "Yunzhen Zhang",
        "B. Bao"
      ],
      "citation_count": 95,
      "url": "https://www.semanticscholar.org/paper/f2eb05b91e4fed91626c961372d7d189211aa552",
      "pdf_url": "https://www.researchsquare.com/article/rs-722277/latest.pdf",
      "publication_date": "2021-07-20",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "864cf501b5c04bfcdc836a9cf1909a51ac1d2a99",
      "title": "Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction",
      "abstract": "We investigate whether model extraction can be used to \u2018steal\u2019 the weights of sequential recommender systems, and the potential threats posed to victims of such attacks. This type of risk has attracted attention in image and text classification, but to our knowledge not in recommender systems. We argue that sequential recommender systems are subject to unique vulnerabilities due to the specific autoregressive regimes used to train them. Unlike many existing recommender attackers, which assume the dataset used to train the victim model is exposed to attackers, we consider a data-free setting, where training data are not accessible. Under this setting, we propose an API-based model extraction method via limited-budget synthetic data generation and knowledge distillation. We investigate state-of-the-art models for sequential recommendation and show their vulnerability under model extraction and downstream attacks. We perform attacks in two stages. (1) Model extraction: given different types of synthetic data and their labels retrieved from a black-box recommender, we extract the black-box model to a white-box model via distillation. (2) Downstream attacks: we attack the black-box model with adversarial samples generated by the white-box recommender. Experiments show the effectiveness of our data-free model extraction and downstream attacks on sequential recommenders in both profile pollution and data poisoning settings.",
      "year": 2021,
      "venue": "ACM Conference on Recommender Systems",
      "authors": [
        "Zhenrui Yue",
        "Zhankui He",
        "Huimin Zeng",
        "Julian McAuley"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/864cf501b5c04bfcdc836a9cf1909a51ac1d2a99",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3460231.3474275",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7026ec6c12f325aec884ff802812c3c80319f668",
      "title": "Model Stealing Attacks Against Inductive Graph Neural Networks",
      "abstract": "Many real-world data come in the form of graphs. Graph neural networks (GNNs), a new family of machine learning (ML) models, have been proposed to fully leverage graph data to build powerful applications. In particular, the inductive GNNs, which can generalize to unseen data, become mainstream in this direction. Machine learning models have shown great potential in various tasks and have been deployed in many real-world scenarios. To train a good model, a large amount of data as well as computational resources are needed, leading to valuable intellectual property. Previous research has shown that ML models are prone to model stealing attacks, which aim to steal the functionality of the target models. However, most of them focus on the models trained with images and texts. On the other hand, little attention has been paid to models trained with graph data, i.e., GNNs. In this paper, we fill the gap by proposing the first model stealing attacks against inductive GNNs. We systematically define the threat model and propose six attacks based on the adversary\u2019s background knowledge and the responses of the target models. Our evaluation on six benchmark datasets shows that the proposed model stealing attacks against GNNs achieve promising performance.1",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yun Shen",
        "Xinlei He",
        "Yufei Han",
        "Yang Zhang"
      ],
      "citation_count": 79,
      "url": "https://www.semanticscholar.org/paper/7026ec6c12f325aec884ff802812c3c80319f668",
      "pdf_url": "https://arxiv.org/pdf/2112.08331",
      "publication_date": "2021-12-15",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bfb6e56602b658fdabaaa66987ebf685a8139892",
      "title": "Defending against Model Stealing via Verifying Embedded External Features",
      "abstract": "Obtaining a well-trained model involves expensive data collection and training procedures, therefore the model is a valuable intellectual property. Recent studies revealed that adversaries can `steal' deployed models even when they have no training samples and can not get access to the model parameters or structures. Currently, there were some defense methods to alleviate this threat, mostly by increasing the cost of model stealing. In this paper, we explore the defense from another angle by verifying whether a suspicious model contains the knowledge of defender-specified external features. Specifically, we embed the external features by tempering a few training samples with style transfer. We then train a meta-classifier to determine whether a model is stolen from the victim. This approach is inspired by the understanding that the stolen models should contain the knowledge of features learned by the victim model. We examine our method on both CIFAR-10 and ImageNet datasets. Experimental results demonstrate that our method is effective in detecting different types of model stealing simultaneously, even if the stolen model is obtained via a multi-stage stealing process. The codes for reproducing main results are available at Github (https://github.com/zlh-thu/StealingVerification).",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Linghui Zhu",
        "Yiming Li",
        "Xiaojun Jia",
        "Yong Jiang",
        "Shutao Xia",
        "Xiaochun Cao"
      ],
      "citation_count": 77,
      "url": "https://www.semanticscholar.org/paper/bfb6e56602b658fdabaaa66987ebf685a8139892",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/20036/19795",
      "publication_date": "2021-12-07",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "03df500a29fb8717662ea5626afd9e2b22b5d14c",
      "title": "Stealing Neural Network Structure Through Remote FPGA Side-Channel Analysis",
      "abstract": "Deep Neural Network (DNN) models have been extensively developed by companies for a wide range of applications. The development of a customized DNN model with great performance requires costly investments, and its structure (layers and hyper-parameters) is considered intellectual property and holds immense value. However, in this paper, we found the model secret is vulnerable when a cloud-based FPGA accelerator executes it. We demonstrate an end-to-end attack based on remote power side-channel analysis and machine-learning-based secret inference against different DNN models. The evaluation result shows that an attacker can reconstruct the layer and hyper-parameter sequence at over 90% accuracy using our method, which can significantly reduce her model development workload. We believe the threat presented by our attack is tangible, and new defense mechanisms should be developed against this threat.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Yicheng Zhang",
        "Rozhin Yasaei",
        "Hao Chen",
        "Zhou Li",
        "M. A. Faruque"
      ],
      "citation_count": 77,
      "url": "https://www.semanticscholar.org/paper/03df500a29fb8717662ea5626afd9e2b22b5d14c",
      "pdf_url": "",
      "publication_date": "2021-02-17",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "cd709a11dfb2308b0d2ffc779261165e92c49e2b",
      "title": "Dynamics analysis, hardware implementation and engineering applications of novel multi-style attractors in a neural network under electromagnetic radiation",
      "abstract": null,
      "year": 2021,
      "venue": "Chaos, Solitons & Fractals",
      "authors": [
        "Fei Yu",
        "Hui Shen",
        "ZiNan Zhang",
        "Yuanyuan Huang",
        "Shuo Cai",
        "Sichun Du"
      ],
      "citation_count": 67,
      "url": "https://www.semanticscholar.org/paper/cd709a11dfb2308b0d2ffc779261165e92c49e2b",
      "pdf_url": "",
      "publication_date": "2021-11-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5beafcd6c0222a2f45863058280873c1ef088ec4",
      "title": "Simulating Unknown Target Models for Query-Efficient Black-box Attacks",
      "abstract": "Many adversarial attacks have been proposed to investigate the security issues of deep neural networks. In the black-box setting, current model stealing attacks train a substitute model to counterfeit the functionality of the target model. However, the training requires querying the target model. Consequently, the query complexity remains high, and such attacks can be defended easily. This study aims to train a generalized substitute model called \"Simulator\", which can mimic the functionality of any unknown target model. To this end, we build the training data with the form of multiple tasks by collecting query sequences generated during the attacks of various existing networks. The learning process uses a mean square error-based knowledge-distillation loss in the meta-learning to minimize the difference between the Simulator and the sampled networks. The meta-gradients of this loss are then computed and accumulated from multiple tasks to update the Simulator and subsequently improve generalization. When attacking a target model that is unseen in training, the trained Simulator can accurately simulate its functionality using its limited feedback. As a result, a large fraction of queries can be transferred to the Simulator, thereby reducing query complexity. Results of the comprehensive experiments conducted using the CIFAR-10, CIFAR-100, and TinyImageNet datasets demonstrate that the proposed approach reduces query complexity by several orders of magnitude compared to the baseline method. The implementation source code is released online 1.",
      "year": 2021,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Chen Ma",
        "Li Chen",
        "Junhai Yong"
      ],
      "citation_count": 57,
      "url": "https://www.semanticscholar.org/paper/5beafcd6c0222a2f45863058280873c1ef088ec4",
      "pdf_url": "https://arxiv.org/pdf/2009.00960",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f4847ab95c43b9d3c214d494ce3852473bf297e4",
      "title": "MEGEX: Data-Free Model Extraction Attack Against Gradient-Based Explainable AI",
      "abstract": "Explainable AI encourages machine learning applications in the real world, whereas data-free model extraction attacks (DFME), in which an adversary steals a trained machine learning model by creating input queries with generative models instead of collecting training data, have attracted attention as a serious threat. In this paper, we propose MEGEX, a data-free model extraction attack against explainable AI that provides gradient-based explanations for inference results, and investigate whether the gradient-based explanations increase the vulnerability to the data-free model extraction attacks. In MEGEX, an adversary leverages explanations by Vanilla Gradient as derivative values for training a generative model. We prove that MEGEX is identical to white-box data-free knowledge distillation, whereby the adversary can train the generative model with the exact gradients. Our experiments show that the adversary in MEGEX can steal highly accurate models - 0.98\u00d7, 0.91\u00d7, and 0.96\u00d7 the victim model accuracy on SVHN, Fashion-MNIST, and CIFAR-10 datasets given 1.5M, 5M, 20M queries, respectively. In addition, we also apply sophisticated gradient-based explanations, i.e., SmoothGrad and Integrated Gradients, to MEGEX. The experimental results indicate that these explanations are potential countermeasures to MEGEX. We also found that the accuracy of the model stolen by the adversary depends on the diversity of query inputs by the generative model.",
      "year": 2021,
      "venue": "SecTL@AsiaCCS",
      "authors": [
        "T. Miura",
        "Toshiki Shibahara",
        "Naoto Yanai"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/f4847ab95c43b9d3c214d494ce3852473bf297e4",
      "pdf_url": "https://arxiv.org/pdf/2107.08909",
      "publication_date": "2021-07-19",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "18da8b08b89646dc18d6734cac6d0222c9239cbf",
      "title": "InverseNet: Augmenting Model Extraction Attacks with Training Data Inversion",
      "abstract": "Cloud service providers, including Google, Amazon, and Alibaba, have now launched machine-learning-as-a-service (MLaaS) platforms, allowing clients to access sophisticated cloud-based machine learning models via APIs. Unfortunately, however, the commercial value of these models makes them alluring targets for theft, and their strategic position as part of the IT infrastructure of many companies makes them an enticing springboard for conducting further adversarial attacks. In this paper, we put forth a novel and effective attack strategy, dubbed InverseNet, that steals the functionality of black-box cloud-based models with only a small number of queries. The crux of the innovation is that, unlike existing model extraction attacks that rely on public datasets or adversarial samples, InverseNet constructs inversed training samples to increase the similarity between the extracted substitute model and the victim model. Further, only a small number of data samples with high confidence scores (rather than an entire dataset) are used to reconstruct the inversed dataset, which substantially reduces the attack cost. Extensive experiments conducted on three simulated victim models and Alibaba Cloud's commercially-available API demonstrate that InverseNet yields a model with significantly greater functional similarity to the victim model than the current state-of-the-art attacks at a substantially lower query budget.",
      "year": 2021,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Wenbin Yang",
        "Guanghao Mei",
        "Qian Wang"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/18da8b08b89646dc18d6734cac6d0222c9239cbf",
      "pdf_url": "https://www.ijcai.org/proceedings/2021/0336.pdf",
      "publication_date": "2021-08-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "57d5abaa047a0401874383628c646918527d4df2",
      "title": "Monitoring-Based Differential Privacy Mechanism Against Query Flooding-Based Model Extraction Attack",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this article, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.",
      "year": 2021,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Haonan Yan",
        "Xiaoguang Li",
        "Hui Li",
        "Jiamin Li",
        "Wenhai Sun",
        "Fenghua Li"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/57d5abaa047a0401874383628c646918527d4df2",
      "pdf_url": "",
      "publication_date": "2021-03-29",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3b357acb6c807bc694372c773e507cdcef27d974",
      "title": "Leaky Nets: Recovering Embedded Neural Network Models and Inputs Through Simple Power and Timing Side-Channels\u2014Attacks and Defenses",
      "abstract": "With the recent advancements in machine learning theory, many commercial embedded microprocessors use neural network (NN) models for a variety of signal processing applications. However, their associated side-channel security vulnerabilities pose a major concern. There have been several proof-of-concept attacks demonstrating the extraction of their model parameters and input data. But, many of these attacks involve specific assumptions, have limited applicability, or pose huge overheads to the attacker. In this work, we study the side-channel vulnerabilities of embedded NN implementations by recovering their parameters using timing-based information leakage and simple power analysis side-channel attacks. We demonstrate our attacks on popular microcontroller platforms over networks of different precisions, such as floating point, fixed point, and binary networks. We are able to successfully recover not only the model parameters but also the inputs for the above networks. Countermeasures against timing-based attacks are implemented and their overheads are analyzed.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "A. Chandrakasan"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/3b357acb6c807bc694372c773e507cdcef27d974",
      "pdf_url": "",
      "publication_date": "2021-02-23",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5179302656cf46cfbcc9cce2b70d4b0c758f7d7c",
      "title": "Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack",
      "abstract": "Previous studies have verified that the functionality of black-box models can be stolen with full probability outputs. However, under the more practical hard-label setting, we observe that existing methods suffer from catastrophic performance degradation. We argue this is due to the lack of rich information in the probability prediction and the overfitting caused by hard labels. To this end, we propose a novel hard-label model stealing method termed \\emph{black-box dissector}, which consists of two erasing-based modules. One is a CAM-driven erasing strategy that is designed to increase the information capacity hidden in hard labels from the victim model. The other is a random-erasing-based self-knowledge distillation module that utilizes soft labels from the substitute model to mitigate overfitting. Extensive experiments on four widely-used datasets consistently demonstrate that our method outperforms state-of-the-art methods, with an improvement of at most $8.27\\%$. We also validate the effectiveness and practical potential of our method on real-world APIs and defense methods. Furthermore, our method promotes other downstream tasks, \\emph{i.e.}, transfer adversarial attacks.",
      "year": 2021,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yixu Wang",
        "Jie Li",
        "Hong Liu",
        "Yongjian Wu",
        "Rongrong Ji"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/5179302656cf46cfbcc9cce2b70d4b0c758f7d7c",
      "pdf_url": "http://arxiv.org/pdf/2105.00623",
      "publication_date": "2021-05-03",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d167f82f1f1bec0dd173fe3173053eebddae05b8",
      "title": "SEAT: Similarity Encoder by Adversarial Training for Detecting Model Extraction Attack Queries",
      "abstract": "Given black-box access to the prediction API, model extraction attacks can steal the functionality of models deployed in the cloud. In this paper, we introduce the SEAT detector, which detects black-box model extraction attacks so that the defender can terminate malicious accounts. SEAT has a similarity encoder trained by adversarial training. Using the similarity encoder, SEAT detects accounts that make queries that indicate a model extraction attack in progress and cancels these accounts. We evaluate our defense against existing model extraction attacks and against new adaptive attacks introduced in this paper. Our results show that even against adaptive attackers, SEAT increases the cost of model extraction attacks by 3.8 times to 16 times.",
      "year": 2021,
      "venue": "AISec@CCS",
      "authors": [
        "Zhanyuan Zhang",
        "Yizheng Chen",
        "David A. Wagner"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/d167f82f1f1bec0dd173fe3173053eebddae05b8",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3474369.3486863",
      "publication_date": "2021-11-15",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a7f5460b2f2b1a215064e9a0669dd3d43a382af9",
      "title": "Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks",
      "abstract": "Model extraction attacks aim to duplicate a machine learning model through query access to a target model. Early studies mainly focus on discriminative models. Despite the success, model extraction attacks against generative models are less well explored. In this paper, we systematically study the feasibility of model extraction attacks against generative adversarial networks (GANs). Specifically, we first define fidelity and accuracy on model extraction attacks against GANs. Then we study model extraction attacks against GANs from the perspective of fidelity extraction and accuracy extraction, according to the adversary\u2019s goals and background knowledge. We further conduct a case study where the adversary can transfer knowledge of the extracted model which steals a state-of-the-art GAN trained with more than 3 million images to new domains to broaden the scope of applications of model extraction attacks. Finally, we propose effective defense techniques to safeguard GANs, considering a trade-off between the utility and security of GAN models.",
      "year": 2021,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/a7f5460b2f2b1a215064e9a0669dd3d43a382af9",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3485832.3485838",
      "publication_date": "2021-12-06",
      "keywords_matched": [
        "model extraction attack",
        "stealing machine learning model",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "645e890034126dfc08484d3509b3493e85fbd603",
      "title": "Stateful Detection of Model Extraction Attacks",
      "abstract": "Machine-Learning-as-a-Service providers expose machine learning (ML) models through application programming interfaces (APIs) to developers. Recent work has shown that attackers can exploit these APIs to extract good approximations of such ML models, by querying them with samples of their choosing. We propose VarDetect, a stateful monitor that tracks the distribution of queries made by users of such a service, to detect model extraction attacks. Harnessing the latent distributions learned by a modified variational autoencoder, VarDetect robustly separates three types of attacker samples from benign samples, and successfully raises an alarm for each. Further, with VarDetect deployed as an automated defense mechanism, the extracted substitute models are found to exhibit poor performance and transferability, as intended. Finally, we demonstrate that even adaptive attackers with prior knowledge of the deployment of VarDetect, are detected by it.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Kanade",
        "S. Shevade"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/645e890034126dfc08484d3509b3493e85fbd603",
      "pdf_url": "",
      "publication_date": "2021-07-12",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "aa6389843232b205bf6d494f00f2cfdeaa633cd8",
      "title": "Thief, Beware of What Get You There: Towards Understanding Model Extraction Attack",
      "abstract": "Model extraction increasingly attracts research attentions as keeping commercial AI models private can retain a competitive advantage. In some scenarios, AI models are trained proprietarily, where neither pre-trained models nor sufficient in-distribution data is publicly available. Model extraction attacks against these models are typically more devastating. Therefore, in this paper, we empirically investigate the behaviors of model extraction under such scenarios. We find the effectiveness of existing techniques significantly affected by the absence of pre-trained models. In addition, the impacts of the attacker's hyperparameters, e.g. model architecture and optimizer, as well as the utilities of information retrieved from queries, are counterintuitive. We provide some insights on explaining the possible causes of these phenomena. With these observations, we formulate model extraction attacks into an adaptive framework that captures these factors with deep reinforcement learning. Experiments show that the proposed framework can be used to improve existing techniques, and show that model extraction is still possible in such strict scenarios. Our research can help system designers to construct better defense strategies based on their scenarios.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Xinyi Zhang",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/aa6389843232b205bf6d494f00f2cfdeaa633cd8",
      "pdf_url": "",
      "publication_date": "2021-04-13",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "cb47e5eafb9c57351aaab630b2307ff6d0c73188",
      "title": "Chaos control and analysis of fractional order neural network under electromagnetic radiation",
      "abstract": null,
      "year": 2021,
      "venue": "",
      "authors": [
        "F. Allehiany",
        "E. Mahmoud",
        "L. S. Jahanzaib",
        "P. Trikha",
        "Hammad Alotaibi"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/cb47e5eafb9c57351aaab630b2307ff6d0c73188",
      "pdf_url": "https://doi.org/10.1016/j.rinp.2020.103786",
      "publication_date": "2021-02-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "13d626a08050930fbf1cb072938d9de87e742fee",
      "title": "Extraction of Binarized Neural Network Architecture and Secret Parameters Using Side-Channel Information",
      "abstract": "In recent years, neural networks have been applied to various applications. To speed up the evaluation, a method using binarized network weights has been introduced, facilitating extremely efficient hardware implementation. Using electromagnetic (EM) side-channel analysis techniques, this study presents a framework of model extraction from practical binarized neural network (BNN) hardware. The target BNN hardware is generated and synthesized using open-source and commercial high-level synthesis tools GUINNESS and Xilinx SDSoC, respectively. With the hardware implemented on an up-to-date FPGA chip, we demonstrate how the layers can be identified from a single EM trace measured during the network evaluation, and we also demonstrate how an attacker may use side-channel attacks to recover secret weights used in the network.",
      "year": 2021,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Ville Yli-M\u00e4yry",
        "Akira Ito",
        "N. Homma",
        "S. Bhasin",
        "Dirmanto Jap"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/13d626a08050930fbf1cb072938d9de87e742fee",
      "pdf_url": "",
      "publication_date": "2021-05-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b408a5e1a60c3afd5954613516126b6c512a7804",
      "title": "Model Extraction and Defenses on Generative Adversarial Networks",
      "abstract": "Model extraction attacks aim to duplicate a machine learning model through query access to a target model. Early studies mainly focus on discriminative models. Despite the success, model extraction attacks against generative models are less well explored. In this paper, we systematically study the feasibility of model extraction attacks against generative adversarial networks (GANs). Specifically, we first define accuracy and fidelity on model extraction attacks against GANs. Then we study model extraction attacks against GANs from the perspective of accuracy extraction and fidelity extraction, according to the adversary's goals and background knowledge. We further conduct a case study where an adversary can transfer knowledge of the extracted model which steals a state-of-the-art GAN trained with more than 3 million images to new domains to broaden the scope of applications of model extraction attacks. Finally, we propose effective defense techniques to safeguard GANs, considering a trade-off between the utility and security of GAN models.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/b408a5e1a60c3afd5954613516126b6c512a7804",
      "pdf_url": "",
      "publication_date": "2021-01-06",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b2212da1f8c968a60f70227623678a41fb63c3cf",
      "title": "Toward Invisible Adversarial Examples Against DNN-Based Privacy Leakage for Internet of Things",
      "abstract": "Deep neural networks (DNNs) can be utilized maliciously for compromising the privacy stored in electronic devices, e.g., identifying the images stored in a mobile phone connected to the Internet of Things (IoT). However, recent studies demonstrated that DNNs are vulnerable to adversarial examples, which are artificially designed perturbations in the original samples for misleading DNNs. Adversarial examples can be used to protect the DNN-based privacy leakage in mobile phones by replacing the photos with adversarial examples. To avoid affecting the normal use of photos, the adversarial examples need to be highly similar to original images. To handle a large number of photos stored in the devices at a proper time, the time efficiency of a method needs to be high enough. Previous methods cannot do well on both sides. In this article, we propose a broad class of selective gradient sign iterative algorithms to make adversarial examples useful in protecting the privacy of photos in IoT devices. By neglecting the unimportant image pixels in the iterative process of attacks according to the sort of first-order partial derivative, we control the optimization direction meticulously to reduce image distortions of adversarial examples without leveraging high time-consuming tricks. Extensive experimental results show that the proposed methods successfully fool the neural network classifiers for the image classification task with a small change in the visual effects and consume little calculating time simultaneously.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xuyang Ding",
        "Shuai Zhang",
        "Mengkai Song",
        "Xiaocong Ding",
        "Fagen Li"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/b2212da1f8c968a60f70227623678a41fb63c3cf",
      "pdf_url": "",
      "publication_date": "2021-01-15",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0b498f9f76310da0ccb0cce7ae26ee1039769b89",
      "title": "Stealing Neural Network Models through the Scan Chain: A New Threat for ML Hardware",
      "abstract": "Stealing trained machine learning (ML) models is a new and growing concern due to the model's development cost. Existing work on ML model extraction either applies a mathematical attack or exploits hardware vulnerabilities such as side-channel leakage. This paper shows a new style of attack, for the first time, on ML models running on embedded devices by abusing the scan-chain infrastructure. We illustrate that having course-grained scan-chain access to non-linear layer outputs is sufficient to steal ML models. To that end, we propose a novel small-signal analysis inspired attack that applies small perturbations into the input signals, identifies the quiescent operating points and, selectively activates certain neurons. We then couple this with a Linear Constraint Satisfaction based approach to efficiently extract model parameters such as weights and biases. We conduct our attack on neural network inference topologies defined in earlier works, and we automate our attack. The results show that our attack outperforms mathematical model extraction proposed in CRYPTO 2020, USENIX 2020, and ICML 2020 by an increase in accuracy of $2^{20.7}\\times, 2^{50.7}\\times$, and $2^{33.9}\\times$, respectively, and a reduction in queries by $2^{6.5}\\times, 2^{4.6}\\times$, and $2^{14.2}\\times$, respectively.",
      "year": 2021,
      "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "authors": [
        "S. Potluri",
        "Aydin Aysu"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/0b498f9f76310da0ccb0cce7ae26ee1039769b89",
      "pdf_url": "",
      "publication_date": "2021-11-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0242acadf4940cf94596b54f7fd6fb75af7bb20d",
      "title": "Beyond Model Extraction: Imitation Attack for Black-Box NLP APIs",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Qiongkai Xu",
        "Xuanli He",
        "L. Lyu",
        "Lizhen Qu",
        "Gholamreza Haffari"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0242acadf4940cf94596b54f7fd6fb75af7bb20d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c09eaf8e58fabd2371294b26a37376be960546a2",
      "title": "Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Sebastian Szyller",
        "Vasisht Duddu",
        "Tommi Grondahl",
        "N. Asokan"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/c09eaf8e58fabd2371294b26a37376be960546a2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0e7081f0f76faabda1f9c6497fc9a83b1c051c71",
      "title": "Ownership Verification of DNN Architectures via Hardware Cache Side Channels",
      "abstract": "Deep Neural Networks (DNN) are gaining higher commercial values in computer vision applications, e.g., image classification, video analytics, etc. This calls for urgent demands of the intellectual property (IP) protection of DNN models. In this paper, we present a novel watermarking scheme to achieve the ownership verification of DNN architectures. Existing works all embedded watermarks into the model parameters while treating the architecture as public property. These solutions were proven to be vulnerable by an adversary to detect or remove the watermarks. In contrast, we claim the model architectures as an important IP for model owners, and propose to implant watermarks into the architectures. We design new algorithms based on Neural Architecture Search (NAS) to generate watermarked architectures, which are unique enough to represent the ownership, while maintaining high model usability. Such watermarks can be extracted via side-channel-based model extraction techniques with high fidelity. We conduct comprehensive experiments on watermarked CNN models for image classification tasks and the experimental results show our scheme has negligible impact on the model performance, and exhibits strong robustness against various model transformations and adaptive attacks.",
      "year": 2021,
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "authors": [
        "Xiaoxuan Lou",
        "Shangwei Guo",
        "Jiwei Li",
        "Tianwei Zhang"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/0e7081f0f76faabda1f9c6497fc9a83b1c051c71",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/159773/2/main.pdf",
      "publication_date": "2021-02-06",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "43c6ffef36a8acda9da688a0fca324abdb64498b",
      "title": "Model Reverse-Engineering Attack against Systolic-Array-Based DNN Accelerator Using Correlation Power Analysis",
      "abstract": "SUMMARY A model extraction attack is a security issue in deep neural networks (DNNs). Information on a trained DNN model is an attractive target for an adversary not only in terms of intellectual property but also of security. Thus, an adversary tries to reveal the sensitive information contained in the trained DNN model from machine-learning services. Previous studies on model extraction attacks assumed that the victim provides a machine-learning cloud service and the adversary accesses the service through formal queries. However, when a DNN model is implemented on an edge device, adversaries can physically access the device and try to reveal the sensitive information contained in the implemented DNN model. We call these physical model extraction attacks model reverse-engineering (MRE) attacks to distinguish them from attacks on cloud services. Power side-channel analyses are often used in MRE attacks to reveal the internal operation from power consumption or electromagnetic leakage. Previous studies, including ours, evaluated MRE attacks against several types of DNN processors with power side-channel analyses. In this paper, information leakage from a systolic array which is used for the matrix multiplication unit in the DNN processors is evaluated. We utilized correlation power analysis (CPA) for the MRE attack and reveal weight parameters of a DNN model from the systolic array. Two types of the systolic array were implemented on \ufb01eld-programmable gate array (FPGA) to demonstrate that CPA reveals weight parameters from those systolic arrays. In addition, we applied an extended analysis approach called \u201cchain CPA\u201d for robust CPA analysis against the systolic arrays. Our experimental results indicate that an adversary can reveal trained model parameters from a DNN accelerator even if the DNN model parameters in the o \ufb00 -chip bus are protected with data encryption. Countermeasures against side-channel leaks will be important for implementing a DNN accelerator on a FPGA or application-speci\ufb01c integrated circuit (ASIC).",
      "year": 2021,
      "venue": "IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences",
      "authors": [
        "Kota Yoshida",
        "M. Shiozaki",
        "S. Okura",
        "Takaya Kubota",
        "T. Fujino"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/43c6ffef36a8acda9da688a0fca324abdb64498b",
      "pdf_url": "https://doi.org/10.1587/transfun.2020cip0024",
      "publication_date": "2021-01-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2ce7526221d6f4c1ef3d9a745e147ba70678d857",
      "title": "Hyperparameters optimization of neural network using improved particle swarm optimization for modeling of electromagnetic inverse problems",
      "abstract": "Abstract Optimization of hyperparameters of artificial neural network (ANN) usually involves a trial and error approach which is not only computationally expensive but also fails to predict a near-optimal solution most of the time. To design a better optimized ANN model, evolutionary algorithms are widely utilized to determine hyperparameters. This work proposes hyperparameters optimization of the ANN model using an improved particle swarm optimization (IPSO) algorithm. The different ANN hyperparameters considered are a number of hidden layers, neurons in each hidden layer, activation function, and training function. The proposed technique is validated using inverse modeling of two meander line electromagnetic bandgap unit cells and a slotted ultra-wideband antenna loaded with EBG structures. Three other evolutionary algorithms viz. hybrid PSO, conventional PSO, and genetic algorithm are also adopted for the hyperparameter optimization of the ANN models for comparative analysis. Performances of all the models are evaluated using quantitative assessment parameters viz. mean square error, mean absolute percentage deviation, and coefficient of determination (R2). The comparative investigation establishes the accurate and efficient prediction capability of the ANN models tuned using IPSO compared to other evolutionary algorithms.",
      "year": 2021,
      "venue": "International journal of microwave and wireless technologies",
      "authors": [
        "Debanjali Sarkar",
        "T. Khan",
        "Fazal Ahmed Talukdar"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/2ce7526221d6f4c1ef3d9a745e147ba70678d857",
      "pdf_url": "",
      "publication_date": "2021-12-17",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "373936d00c4a357579c4d375de0ce439e4e54d5f",
      "title": "Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs",
      "abstract": "The collection and availability of big data, combined with advances in pre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive performance of modern natural language processing tasks, ranging from text classification to text generation. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. However, BERT-based APIs have exhibited a series of security and privacy vulnerabilities. For example, prior work has exploited the security issues of the BERT-based APIs through the adversarial examples crafted by the extracted model. However, the privacy leakage problems of the BERT-based APIs through the extracted model have not been well studied. On the other hand, due to the high capacity of BERT-based APIs, the fine-tuned model is easy to be overlearned, but what kind of information can be leaked from the extracted model remains unknown. In this work, we bridge this gap by first presenting an effective model extraction attack, where the adversary can practically steal a BERT-based API (the target/victim model) by only querying a limited number of queries. We further develop an effective attribute inference attack which can infer the sensitive attribute of the training data used by the BERT-based APIs. Our extensive experiments on benchmark datasets under various realistic settings validate the potential vulnerabilities of BERT-based APIs. Moreover, we demonstrate that two promising defense methods become ineffective against our attacks, which calls for more effective defense methods.",
      "year": 2021,
      "venue": "",
      "authors": [
        "Chen Chen",
        "Xuanli He",
        "Lingjuan Lyu",
        "Fangzhao Wu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/373936d00c4a357579c4d375de0ce439e4e54d5f",
      "pdf_url": "",
      "publication_date": "2021-05-23",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3813c369accb4b87b5882eec943511fe2e8b4767",
      "title": "Killing Two Birds with One Stone: Stealing Model and Inferring Attribute from BERT-based APIs",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Lingjuan Lyu",
        "Xuanli He",
        "Fangzhao Wu",
        "Lichao Sun"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/3813c369accb4b87b5882eec943511fe2e8b4767",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c092ed52816d6060929bbb40021dbfdc88ba28b7",
      "title": "SCANet: Securing the Weights With Superparamagnetic-MTJ Crossbar Array Networks",
      "abstract": "Deep neural networks (DNNs) form a critical infrastructure supporting various systems, spanning from the iPhone neural engine to imaging satellites and drones. The design of these neural cores is often proprietary or a military secret. Nevertheless, they remain vulnerable to model replication attacks that seek to reverse engineer the network\u2019s synaptic weights. In this article, we propose SCANet (Superparamagnetic-MTJ Crossbar Array Networks), a novel defense mechanism against such model stealing attacks by utilizing the innate stochasticity in superparamagnets. When used as the synapse in DNNs, superparamagnetic magnetic tunnel junctions (s-MTJs) are shown to be significantly more secure than prior memristor-based solutions. The thermally induced telegraphic switching in the s-MTJs is robust and uncontrollable, thus thwarting the attackers from obtaining sensitive data from the network. Using a mixture of both superparamagnetic and conventional MTJs in the neural network (NN), the designer can optimize the time period between the weight updation and the power consumed by the system. Furthermore, we propose a modified NN architecture that can prevent replication attacks while minimizing power consumption. We investigate the effect of the number of layers in the deep network and the number of neurons in each layer on the sharpness of accuracy degradation when the network is under attack. We also explore the efficacy of SCANet in real-time scenarios, using a case study on object detection.",
      "year": 2021,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Dinesh Rajasekharan",
        "N. Rangarajan",
        "Satwik Patnaik",
        "O. Sinanoglu",
        "Y. Chauhan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/c092ed52816d6060929bbb40021dbfdc88ba28b7",
      "pdf_url": "",
      "publication_date": "2021-12-15",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "25800c4599b2e834899a180d77e093fb9282d97e",
      "title": "HODA: Hardness-Oriented Detection of Model Extraction Attacks",
      "abstract": "Model extraction attacks exploit the target model\u2019s prediction API to create a surrogate model, allowing the adversary to steal or reconnoiter the functionality of the target model in the black-box setting. Several recent studies have shown that a data-limited adversaries with no or limited access to the samples from the target model\u2019s training data distribution, can employ synthesized or semantically similar samples to conduct model extraction attacks. In this paper, we introduce the concept of hardness degree to characterize sample difficulty based on the concept of learning speed. The hardness degree of a sample depends on the epoch number at which the predicted label for that sample converges. We investigate the hardness degree of samples and demonstrate that the hardness degree histogram of a data-limited adversary\u2019s sample sequence is differs significantly from that of benign users\u2019 sample sequences. We propose Hardness-Oriented Detection Approach (HODA) to detect the sample sequences of model extraction attacks. Our results indicate that HODA can effectively detect model extraction attack sequences with a high success rate, using only 100 monitored samples. It outperforms all previously proposed methods for model extraction detection.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "A. M. Sadeghzadeh",
        "Amir Mohammad Sobhanian",
        "F. Dehghan",
        "R. Jalili"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/25800c4599b2e834899a180d77e093fb9282d97e",
      "pdf_url": "https://arxiv.org/pdf/2106.11424",
      "publication_date": "2021-06-21",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0d2e0d3c8fb930a9a577ec60152c219995cc6b10",
      "title": "Neural Network Stealing via Meltdown",
      "abstract": "Deep learning services are now deployed in various fields on top of cloud infrastructures. In such cloud environment, virtualization technology provides logically independent and isolated computing space for each tenant. However, recent studies demonstrate that by leveraging vulnerabilities of virtualization techniques and shared processor architectures in the cloud system, various side-channels can be established between cloud tenants. In this paper, we propose a novel attack scenario that can steal internal information of deep learning models by exploiting the Meltdown vulnerability in a multitenant system environment. On the basis of our experiment, the proposed attack method could extract internal information of a TensorFlow deep learning service with 92.875% accuracy and 1.325kB/s extraction speed.",
      "year": 2021,
      "venue": "International Conference on Information Networking",
      "authors": [
        "Hoyong Jeong",
        "Dohyun Ryu",
        "Junbeom Hur"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0d2e0d3c8fb930a9a577ec60152c219995cc6b10",
      "pdf_url": "",
      "publication_date": "2021-01-13",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "title": "Model Extraction and Adversarial Attacks on Neural Networks using Switching Power Information",
      "abstract": "Artificial neural networks (ANNs) have gained significant popularity in the last decade for solving narrow AI problems in domains such as healthcare, transportation, and defense. As ANNs become more ubiquitous, it is imperative to understand their associated safety, security, and privacy vulnerabilities. Recently, it has been shown that ANNs are susceptible to a number of adversarial evasion attacks--inputs that cause the ANN to make high-confidence misclassifications despite being almost indistinguishable from the data used to train and test the network. This work explores to what degree finding these examples maybe aided by using side-channel information, specifically switching power consumption, of hardware implementations of ANNs. A black-box threat scenario is assumed, where an attacker has access to the ANN hardware's input, outputs, and topology, but the trained model parameters are unknown. Then, a surrogate model is trained to have similar functional (i.e. input-output mapping) and switching power characteristics as the oracle (black-box) model. Our results indicate that the inclusion of power consumption data increases the fidelity of the model extraction by up to 30 percent based on a mean square error comparison of the oracle and surrogate weights. However, transferability of adversarial examples from the surrogate to the oracle model was not significantly affected.",
      "year": 2021,
      "venue": "International Conference on Artificial Neural Networks",
      "authors": [
        "Tommy Li",
        "Cory E. Merkel"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "pdf_url": "",
      "publication_date": "2021-06-15",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "beaa48a90464b1d9df6d42566a42c7326cc75826",
      "title": "Shuffling Countermeasure against Power Side-Channel Attack for MLP with Software Implementation",
      "abstract": null,
      "year": 2021,
      "venue": "International Conference on Electrical and Control Engineering",
      "authors": [
        "Y. Nozaki",
        "M. Yoshikawa"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/beaa48a90464b1d9df6d42566a42c7326cc75826",
      "pdf_url": "",
      "publication_date": "2021-12-17",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6981af0aa5d7bee0c1f0c3c685824077140f6be1",
      "title": "First to Possess His Statistics: Data-Free Model Extraction Attack on Tabular Data",
      "abstract": "Model extraction attacks are a kind of attacks where an adversary obtains a machine learning model whose performance is comparable with one of the victim model through queries and their results. This paper presents a novel model extraction attack, named TEMPEST, applicable on tabular data under a practical data-free setting. Whereas model extraction is more challenging on tabular data due to normalization, TEMPEST no longer needs initial samples that previous attacks require; instead, it makes use of publicly available statistics to generate query samples. Experiments show that our attack can achieve the same level of performance as the previous attacks. Moreover, we identify that the use of mean and variance as statistics for query generation and the use of the same normalization process as the victim model can improve the performance of our attack. We also discuss a possibility whereby TEMPEST is executed in the real world through an experiment with a medical diagnosis dataset. We plan to release the source code for reproducibility and a reference to subsequent works.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Masataka Tasumi",
        "Kazuki Iwahana",
        "Naoto Yanai",
        "Katsunari Shishido",
        "Toshiya Shimizu",
        "Yuji Higuchi",
        "I. Morikawa",
        "Jun Yajima"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/6981af0aa5d7bee0c1f0c3c685824077140f6be1",
      "pdf_url": "",
      "publication_date": "2021-09-30",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "73ac60e0f1fa8bbe7de599df66d2cbaef9a9b268",
      "title": "Tenet: A Neural Network Model Extraction Attack in Multi-core Architecture",
      "abstract": "As neural networks (NNs) are being widely deployed in many cloud-oriented systems for safety-critical tasks, the privacy and security of NNs become significant concerns to users in the cloud platform that shares the computation infrastructure such as memory resource. In this work, we observed that the memory timing channel in the shared memory of cloud multi-core architecture poses the risk of network model information leakage. Based on the observation, we propose a learning-based method to steal the model architecture of the NNs by exploiting the memory timing channel without any high-level privilege or physical access. We first trained an end-to-end measurement network offline to learn the relation between memory timing information and NNs model architecture. Then, we performed an online attack and reconstructed the target model using the prediction from the measurement network. We evaluated the proposed attack method on a multi-core architecture simulator. The experimental results show that our learning-based attack method can reconstruct the target model with high accuracy and improve the adversarial attack success rate by 42.4%.",
      "year": 2021,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Chengsi Gao",
        "Bing Li",
        "Ying Wang",
        "Weiwei Chen",
        "Lei Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/73ac60e0f1fa8bbe7de599df66d2cbaef9a9b268",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3453688.3461512",
      "publication_date": "2021-06-22",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "44cc3c0d5a80fecd1b6285c73f79986852135162",
      "title": "ML-Stealer: Stealing Prediction Functionality of Machine Learning Models with Mere Black-Box Access",
      "abstract": "Machine Learning (ML) models are progressively deployed in many real-world applications to perform a wide range of tasks, but are exposed to the security and privacy threats which aim to infer the details and even steal the functionality of the ML models. Despite extensive attacking efforts which rely on white-box or gray-box access, how to perform attacks with black-box access continues to be elusive. Aspiring to fill this gap, we move one step further and present ML-Stealer that can steal the functionality of any type of ML models with mere black-box access. With two algorithm designs, namely, synthetic data generation and replica model construction, ML-Stealer can construct a deep neural network (DNN)-based replica model which has the similar prediction functionality to the victim ML model. ML-Stealer does not require any knowledge about the victim model, nor does it enforce the access to statistical information or samples of the victim's training data. Experiment results demonstrate that ML-Stealer can achieve the consistent prediction results with the victim model of an averaged testing accuracy of 85.6%, and up to 93.6% at best.",
      "year": 2021,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Gaoyang Liu",
        "Shijie Wang",
        "Borui Wan",
        "Zekun Wang",
        "Chen Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/44cc3c0d5a80fecd1b6285c73f79986852135162",
      "pdf_url": "",
      "publication_date": "2021-10-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "130ce87d6141db6954d50d43b402583f7682ead8",
      "title": "Data-Driven Electromagnetic Scalar Field Estimation of a Patch Antenna Using Deep Convolutional Neural Network",
      "abstract": "Artificial neural network (ANN) is emerging as an alternative approach for numerical electromagnetic (EM) antenna modeling. In this paper, we focus on predicting the near-field properties of antennas using a data-driven approach, and exploiting the universal function approximator feature of the ANNs. Specifically, we demonstrate the use of a convolutional neural network (CNN) to estimate the surface current on a patch antenna. This field analysis prediction allows a pathway for the prediction of near-field and far-field properties and has the potential to replace full EM modeling. Based on our results, our neural network predicted with only an 11% error on average the desired surface currents, showing promise for the proposed approach.",
      "year": 2021,
      "venue": "2021 IEEE International Symposium on Antennas and Propagation and USNC-URSI Radio Science Meeting (APS/URSI)",
      "authors": [
        "Md Rayhan Khan",
        "C. Zekios",
        "S. Bhardwaj",
        "S. Georgakopoulos"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/130ce87d6141db6954d50d43b402583f7682ead8",
      "pdf_url": "",
      "publication_date": "2021-12-04",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8283479256ea1d54d98ed00bb53caba38f2f688e",
      "title": "Side-Channel Analysis-Based Model Extraction on Intelligent CPS: An Information Theory Perspective",
      "abstract": "The intelligent cyber-physical system (CPS) has been applied in various fields, covering multiple critical infras-tructures and human daily life support areas. CPS Security is a major concern and of critical importance, especially the security of the intelligent control component. Side-channel analysis (SCA) is the common threat exploiting the weaknesses in system operation to extract information of the intelligent CPS. However, existing literature lacks the systematic theo-retical analysis of the side-channel attacks on the intelligent CPS, without the ability to quantify and measure the leaked information. To address these issues, we propose the SCA-based model extraction attack on intelligent CPS. First, we design an efficient and novel SCA-based model extraction framework, including the threat model, hierarchical attack process, and the multiple micro-space parallel search enabled weight extraction algorithm. Secondly, an information theory-empowered analy-sis model for side-channel attacks on intelligent CPS is built. We propose a mutual information-based quantification method and derive the capacity of side-channel attacks on intelligent CPS, formulating the amount of information leakage through side channels. Thirdly, we develop the theoretical bounds of the leaked information over multiple attack queries based on the data processing inequality and properties of entropy. These convergence bounds provide theoretical means to estimate the amount of information leaked. Finally, experimental evaluation, including real-world experiments, demonstrates the effective-ness of the proposed SCA-based model extraction algorithm and the information theory-based analysis method in intelligent CPS.",
      "year": 2021,
      "venue": "2021 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",
      "authors": [
        "Qianqian Pan",
        "Jun Wu",
        "Xi Lin",
        "Jianhua Li"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8283479256ea1d54d98ed00bb53caba38f2f688e",
      "pdf_url": "",
      "publication_date": "2021-12-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "87af159e1eaa99d17b9b6aff6781f2342e8ff385",
      "title": "Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Models",
      "abstract": "Machine learning models are typically made available to potential client users via inference APIs. Model extraction attacks occur when a malicious client uses information gleaned from queries to the inference API of a victim model $F_V$ to build a surrogate model $F_A$ with comparable functionality. Recent research has shown successful model extraction of image classification, and natural language processing models. In this paper, we show the first model extraction attack against real-world generative adversarial network (GAN) image translation models. We present a framework for conducting such attacks, and show that an adversary can successfully extract functional surrogate models by querying $F_V$ using data from the same domain as the training data for $F_V$. The adversary need not know $F_V$'s architecture or any other information about it beyond its intended task. We evaluate the effectiveness of our attacks using three different instances of two popular categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo (image style transfer), and (3) Super-Resolution (super resolution). Using standard performance metrics for GANs, we show that our attacks are effective. Furthermore, we conducted a large scale (125 participants) user study on Selfie-to-Anime and Monet-to-Photo to show that human perception of the images produced by $F_V$ and $F_A$ can be considered equivalent, within an equivalence bound of Cohen's d = 0.3. Finally, we show that existing defenses against model extraction attacks (watermarking, adversarial examples, poisoning) do not extend to image translation models.",
      "year": 2021,
      "venue": "",
      "authors": [
        "Sebastian Szyller",
        "Vasisht Duddu",
        "Tommi Grondahl",
        "Nirmal Asokan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/87af159e1eaa99d17b9b6aff6781f2342e8ff385",
      "pdf_url": "",
      "publication_date": "2021-04-26",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "89e225f6d755599d14d25580315d60725977ca62",
      "title": "Fast Prediction for Electromagnetic Shielding Effectiveness of Ground-Via Distribution of SiP by Convolutional Neural Network",
      "abstract": "Ground vias are often used as an essential shielding structure in System-in-Package (SiP) to suppress electromagnetic leakage. In this paper, a new method based on Convolutional Neural Network (CNN) is adopted to predict the shielding effectiveness of ground-via distributions. In particular, the suitability of the pooling layer is studied separately. Furthermore, the appropriateness of using the convolutional layer is proved by a comparative experiment with a Deep Neural Network (DNN) model. The method proposed in this paper shows good accuracy in electromagnetic leakage prediction and has universal value in similar prediction tasks. Compared with traditional analysis methods, the proposed CNN model has a significant time advantage, making it possible to use other optimization algorithms to replace the artificial shielding structure design.",
      "year": 2021,
      "venue": "2021 13th Global Symposium on Millimeter-Waves & Terahertz (GSMM)",
      "authors": [
        "Zheming Gu",
        "Tuomin Tao",
        "Erping Li"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/89e225f6d755599d14d25580315d60725977ca62",
      "pdf_url": "",
      "publication_date": "2021-05-23",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "46582b5600668089180542b725a2868692bcce87",
      "title": "An extraction attack on image recognition model using VAE-kdtree model",
      "abstract": "This paper proposes a black box extraction attack model on pre-trained image classifiers to rebuild a functionally equivalent model with high similarity. Common model extraction attacks use a large number of training samples to feed the target classifier which is time-consuming with redundancy. The attack results have a high dependency on the selected training samples and the target model. The extracted model may only get part of crucial features because of inappropriate sample selection. To eliminate these uncertainties, we proposed the VAE-kdtree attack model which eliminates the high dependency between selected training samples and the target model. It can not only save redundant computation, but also extract critical boundaries more accurately in image classification. This VAE-kdtree model has shown to achieve around 90% similarity on MNIST and around 80% similarity on MNIST-Fashion with a target Convolutional Network Model and a target Support Vector Machine Model. The performance of this VAE-kdtree model could be further improved by adopting higher dimension space of the kdtree.",
      "year": 2021,
      "venue": "Other Conferences",
      "authors": [
        "Tianqi Wen",
        "Haibo Hu",
        "Huadi Zheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/46582b5600668089180542b725a2868692bcce87",
      "pdf_url": "",
      "publication_date": "2021-03-13",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6ac1497696276d63b01b4589eadc4cdbfe50f827",
      "title": "Leakage Reuse for Energy Efficient Near-Memory Computing of Heterogeneous DNN Accelerators",
      "abstract": "The exploration of custom deep neural network (DNN) accelerators for highly energy constrained edge devices with on-device intelligence is gaining traction in the research community. Despite the superior throughput and performance of custom accelerators as compared to CPUs or GPUs, the energy efficiency and versatility of state-of-the-art DNN accelerators is constrained due to a) the storage and movement of a large volume of data and b) the limited scope of monolithic architectures, where the entire accelerator executes only a single model at any given time. In this paper, a multi-voltage domain heterogeneous DNN accelerator is proposed that executes multiple models simultaneously with different power-performance operating points. The proposed architecture concurrently implements near-memory computing and leakage reuse, where the leakage current of idle memory banks within each processing element is utilized to deliver current to the adjacently placed multiply-and-accumulate (MAC) units. The proposed architecture and circuit techniques are evaluated with SPICE simulation in a 65 nm CMOS technology. The simulation results indicate that the proposed heterogeneous architecture with leakage reuse results in an energy efficiency of 3.27 tera-operations per second per watt (TOPS/W) as compared to a conventional monolithic and single voltage domain architecture that exhibits an energy efficiency of 0.0458 TOPS/W. In addition, the proposed accelerator that implements the leakage reuse technique on only half of the memory elements storing the weights reduces the power consumption of the sub-arrays of processing elements by 26% (99.4 mW) as compared to an accelerator that does not apply leakage reuse.",
      "year": 2021,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Md. Shazzad Hossain",
        "I. Savidis"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6ac1497696276d63b01b4589eadc4cdbfe50f827",
      "pdf_url": "https://doi.org/10.1109/jetcas.2021.3121687",
      "publication_date": "2021-12-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "37454d318bd3300a96db6f61164d19c46e117e36",
      "title": "Electromagnetic Modeling of Microstrip Patch Antennas: A Neural Network Approach",
      "abstract": "This paper aims to elucidate a design procedure that can help solve both the synthesis and analysis parts of an electromagnetic microstrip patch antenna problem. Unlike numerical conventional modeling methods or maybe even analytical methods, artificial neural network are way faster and easier to perform due to their robust correlation and high accuracy. They are also computationally inexpensive when compared to tradition analytical and conventional methods alike. Due to their brevity and highly flexible nature, they are easy to train and work on and can be expanded or modelled in different ways as per the use case owing to their plasticity. Conventional methods are not only time consuming put also require high computational capacity unlike the neural network perspective which has been proposed here. Hence, this paper gives a brief overview of the same and also includes a brief background and basic working of artificial neural networks, activation functions, Radial Basis functions, etc. from both the analysis and synthesis perspective from a RF designers viewpoint. Unlike previous soft computing neural network approaches like back propagation and regression techniques to achieve the same purpose, this paper uses a custom designed Radial Basis Network due to its proven history of computational inexpensiveness and high accuracy and validates its accuracy through the results shown. Circuit design and yield optimizations using a method of moments(MoM) based electromagnetic simulation software was also done to verify accuracy and to feed the ground truth to the network. This neural network was implemented in python due to its vast resource library and ease of implementation.",
      "year": 2021,
      "venue": "2021 6th International Conference for Convergence in Technology (I2CT)",
      "authors": [
        "R. S",
        "N. Gupta"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/37454d318bd3300a96db6f61164d19c46e117e36",
      "pdf_url": "",
      "publication_date": "2021-04-02",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1759582a0b818cdb05dabdb8cb5349230011dcaa",
      "title": "A Novel Neural Network Cell Method for Solving Nonlinear Electromagnetic Problems",
      "abstract": "Effective analysis of nonlinear electromagnetic fields is essential for the accurate modeling of electromagnetic devices, such as transformers, generators, and motors. This paper proposes a novel approach of coupled neural network (NN) and cell method (CM) or NNCM for solving nonlinear electromagnetic problems with ferromagnetic domains. While the topologically linear relations of the cell complexes are mathematically assembled through a transformation in the Tonti diagram by the CM, and the constitutive nonlinear magnetic relations are dealt with by partially connected NN for the fast prediction of the permeability distribution inside the ferromagnetic domain. Since the construction of NN is directly related to the grid connections, a partially connected NN structure with a small number of neurons can reduce the computational cost of the training process. By using a compact NN, the proposed NNCM can effectively eliminate the time consuming iterations for determining the nonlinear permeability distribution, and improve the computational efficiency significantly. The NNCM is employed to analyze the transient electromagnetic field distribution inside a cylindrical ferromagnetic core. The results are compared with those obtained by the traditional iterative CM, which determines the nonlinear permeability distribution by lengthy numerical iterations, to verify the feasibility and effectiveness of the proposed NNCM.",
      "year": 2021,
      "venue": "Advanced Theory and Simulations",
      "authors": [
        "G. Zhu",
        "Longnv Li",
        "Weinong Fu",
        "Ming Xue",
        "Tao Liu",
        "Jianguo Zhu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/1759582a0b818cdb05dabdb8cb5349230011dcaa",
      "pdf_url": "",
      "publication_date": "2021-10-13",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b9f14b52cf9c01e651a4c8e9b04344e9c3929ddc",
      "title": "Model Extraction and Adversarial Attacks on Neural Networks Model Extraction and Adversarial Attacks on Neural Networks Using Side-Channel Information Using Side-Channel Information",
      "abstract": null,
      "year": 2021,
      "venue": "",
      "authors": [
        "Tommy Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b9f14b52cf9c01e651a4c8e9b04344e9c3929ddc",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bf617161968220f3c48751aa68ccc38df0961f0e",
      "title": "CNN-based Tree Model Extraction",
      "abstract": "We propose a method for the segmentation and structural reconstruction of tree stems and branches in cluttered environments. We use single images of monocular cameras and convolutional neural networks for the segmentation, centerline, and contour detection of trees (trunks and branches), and a deterministic approach to build tree model graphs, defining the positions of vertices and edges, on the points of segments' centerlines. Compared to previous stochastic methods, this approach, tested on a synthetic dataset, gives better segmentation accuracy with a significantly smaller computational complexity.",
      "year": 2021,
      "venue": "International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications",
      "authors": [
        "K. Alaya",
        "L. Cz\u00fani"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bf617161968220f3c48751aa68ccc38df0961f0e",
      "pdf_url": "",
      "publication_date": "2021-09-22",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "abbb0fd559ade70265f4f528df094fbbd8ae2040",
      "title": "Entangled Watermarks as a Defense against Model Extraction",
      "abstract": "Machine learning involves expensive data collection and training procedures. Model owners may be concerned that valuable intellectual property can be leaked if adversaries mount model extraction attacks. Because it is difficult to defend against model extraction without sacrificing significant prediction accuracy, watermarking leverages unused model capacity to have the model overfit to outlier input-output pairs, which are not sampled from the task distribution and are only known to the defender. The defender then demonstrates knowledge of the input-output pairs to claim ownership of the model at inference. The effectiveness of watermarks remains limited because they are distinct from the task distribution and can thus be easily removed through compression or other forms of knowledge transfer. \nWe introduce Entangled Watermarking Embeddings (EWE). Our approach encourages the model to learn common features for classifying data that is sampled from the task distribution, but also data that encodes watermarks. An adversary attempting to remove watermarks that are entangled with legitimate data is also forced to sacrifice performance on legitimate data. Experiments on MNIST, Fashion-MNIST, and Google Speech Commands validate that the defender can claim model ownership with 95% confidence after less than 10 queries to the stolen copy, at a modest cost of 1% accuracy in the defended model's performance.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Hengrui Jia",
        "Christopher A. Choquette-Choo",
        "Nicolas Papernot"
      ],
      "citation_count": 259,
      "url": "https://www.semanticscholar.org/paper/abbb0fd559ade70265f4f528df094fbbd8ae2040",
      "pdf_url": "",
      "publication_date": "2020-02-27",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4d548fd21aad60e3052455e22b7a57cc1f06e3c3",
      "title": "CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples",
      "abstract": "Cloud-based Machine Learning as a Service (MLaaS) is gradually gaining acceptance as a reliable solution to various real-life scenarios. These services typically utilize Deep Neural Networks (DNNs) to perform classification and detection tasks and are accessed through Application Programming Interfaces (APIs). Unfortunately, it is possible for an adversary to steal models from cloud-based platforms, even with black-box constraints, by repeatedly querying the public prediction API with malicious inputs. In this paper, we introduce an effective and efficient black-box attack methodology that extracts largescale DNN models from cloud-based platforms with near-perfect performance. In comparison to existing attack methods, we significantly reduce the number of queries required to steal the target model by incorporating several novel algorithms, including active learning, transfer learning, and adversarial attacks. During our experimental evaluations, we validate our proposed model for conducting theft attacks on various commercialized MLaaS platforms hosted by Microsoft, Face++, IBM, Google and Clarifai. Our results demonstrate that the proposed method can easily reveal/steal large-scale DNN models from these cloud platforms. The proposed attack method can also be used to accurately evaluates the robustness of DNN based MLaaS classifiers against theft attacks.",
      "year": 2020,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Honggang Yu",
        "Kaichen Yang",
        "Teng Zhang",
        "Yun-Yun Tsai",
        "Tsung-Yi Ho",
        "Yier Jin"
      ],
      "citation_count": 183,
      "url": "https://www.semanticscholar.org/paper/4d548fd21aad60e3052455e22b7a57cc1f06e3c3",
      "pdf_url": "https://doi.org/10.14722/ndss.2020.24178",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d5ffa58133940646d1339c2610cb35f27442e0d3",
      "title": "MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation",
      "abstract": "High quality Machine Learning (ML) models are often considered valuable intellectual property by companies. Model Stealing (MS) attacks allow an adversary with blackbox access to a ML model to replicate its functionality by training a clone model using the predictions of the target model for different inputs. However, best available existing MS attacks fail to produce a high-accuracy clone without access to the target dataset or a representative dataset necessary to query the target model. In this paper, we show that preventing access to the target dataset is not an adequate defense to protect a model. We propose MAZE \u2013 a data-free model stealing attack using zeroth-order gradient estimation that produces high-accuracy clones. In contrast to prior works, MAZE uses only synthetic data created using a generative model to perform MS.Our evaluation with four image classification models shows that MAZE provides a normalized clone accuracy in the range of 0.90\u00d7 to 0.99\u00d7, and outperforms even the recent attacks that rely on partial data (JBDA, clone accuracy 0.13\u00d7 to 0.69\u00d7) and on surrogate data (KnockoffNets, clone accuracy 0.52\u00d7 to 0.97\u00d7). We also study an extension of MAZE in the partial-data setting, and develop MAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97\u00d7 to 1.0\u00d7) and reduces the query budget required for the attack by 2\u00d7-24\u00d7.",
      "year": 2020,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "S. Kariyappa",
        "A. Prakash",
        "Moinuddin K. Qureshi"
      ],
      "citation_count": 168,
      "url": "https://www.semanticscholar.org/paper/d5ffa58133940646d1339c2610cb35f27442e0d3",
      "pdf_url": "https://arxiv.org/pdf/2005.03161",
      "publication_date": "2020-05-06",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "109ad71af2ffce01b60852f8141ea91be6eed9e1",
      "title": "DeepSniffer: A DNN Model Extraction Framework Based on Learning Architectural Hints",
      "abstract": "As deep neural networks (DNNs) continue their reach into a wide range of application domains, the neural network architecture of DNN models becomes an increasingly sensitive subject, due to either intellectual property protection or risks of adversarial attacks. Previous studies explore to leverage architecture-level events disposed in hardware platforms to extract the model architecture information. They pose the following limitations: requiring a priori knowledge of victim models, lacking in robustness and generality, or obtaining incomplete information of the victim model architecture. Our paper proposes DeepSniffer, a learning-based model extraction framework to obtain the complete model architecture information without any prior knowledge of the victim model. It is robust to architectural and system noises introduced by the complex memory hierarchy and diverse run-time system optimizations. The basic idea of DeepSniffer is to learn the relation between extracted architectural hints (e.g., volumes of memory reads/writes obtained by side-channel or bus snooping attacks) and model internal architectures. Taking GPU platforms as a show case, DeepSniffer conducts model extraction by learning both the architecture-level execution features of kernels and the inter-layer temporal association information introduced by the common practice of DNN design. We demonstrate that DeepSniffer works experimentally in the context of an off-the-shelf Nvidia GPU platform running a variety of DNN models. The extracted models are directly helpful to the attempting of crafting adversarial inputs. Our experimental results show that DeepSniffer achieves a high accuracy of model extraction and thus improves the adversarial attack success rate from 14.6%$\\sim$25.5% (without network architecture knowledge) to 75.9% (with extracted network architecture). The DeepSniffer project has been released in Github.",
      "year": 2020,
      "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Shuangchen Li",
        "Lei Deng",
        "Pengfei Zuo",
        "Yu Ji",
        "Xinfeng Xie",
        "Yufei Ding",
        "Chang Liu",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 153,
      "url": "https://www.semanticscholar.org/paper/109ad71af2ffce01b60852f8141ea91be6eed9e1",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3373376.3378460",
      "publication_date": "2020-03-09",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "186377b9098efc726b8f1bda7e4f8aa2ed7bafa5",
      "title": "Cryptanalytic Extraction of Neural Network Models",
      "abstract": "We argue that the machine learning problem of model extraction is actually a cryptanalytic problem in disguise, and should be studied as such. Given oracle access to a neural network, we introduce a differential attack that can efficiently steal the parameters of the remote model up to floating point precision. Our attack relies on the fact that ReLU neural networks are piecewise linear functions, and thus queries at the critical points reveal information about the model parameters. \nWe evaluate our attack on multiple neural network models and extract models that are 2^20 times more precise and require 100x fewer queries than prior work. For example, we extract a 100,000 parameter neural network trained on the MNIST digit recognition task with 2^21.5 queries in under an hour, such that the extracted model agrees with the oracle on all inputs up to a worst-case error of 2^-25, or a model with 4,000 parameters in 2^18.5 queries with worst-case error of 2^-40.4. Code is available at this https URL.",
      "year": 2020,
      "venue": "Annual International Cryptology Conference",
      "authors": [
        "Nicholas Carlini",
        "Matthew Jagielski",
        "Ilya Mironov"
      ],
      "citation_count": 149,
      "url": "https://www.semanticscholar.org/paper/186377b9098efc726b8f1bda7e4f8aa2ed7bafa5",
      "pdf_url": "",
      "publication_date": "2020-03-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d8f64187b447d1b64f69f541dbbaec71bd79d205",
      "title": "ActiveThief: Model Extraction Using Active Learning and Unannotated Public Data",
      "abstract": "Machine learning models are increasingly being deployed in practice. Machine Learning as a Service (MLaaS) providers expose such models to queries by third-party developers through application programming interfaces (APIs). Prior work has developed model extraction attacks, in which an attacker extracts an approximation of an MLaaS model by making black-box queries to it. We design ActiveThief \u2013 a model extraction framework for deep neural networks that makes use of active learning techniques and unannotated public datasets to perform model extraction. It does not expect strong domain knowledge or access to annotated data on the part of the attacker. We demonstrate that (1) it is possible to use ActiveThief to extract deep classifiers trained on a variety of datasets from image and text domains, while querying the model with as few as 10-30% of samples from public datasets, (2) the resulting model exhibits a higher transferability success rate of adversarial examples than prior work, and (3) the attack evades detection by the state-of-the-art model extraction detection method, PRADA.",
      "year": 2020,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Shukla",
        "Aditya Kanade",
        "S. Shevade",
        "V. Ganapathy"
      ],
      "citation_count": 145,
      "url": "https://www.semanticscholar.org/paper/d8f64187b447d1b64f69f541dbbaec71bd79d205",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/5432/5288",
      "publication_date": "2020-04-03",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7a3d5f1887fa908a5df303f37ec4450998caafac",
      "title": "DeepEM: Deep Neural Networks Model Recovery through EM Side-Channel Information Leakage",
      "abstract": "Neural Network (NN) accelerators are currently widely deployed in various security-crucial scenarios, including image recognition, natural language processing and autonomous vehicles. Due to economic and privacy concerns, the hardware implementations of structures and designs inside NN accelerators are usually inaccessible to the public. However, these accelerators still tend to leak crucial information through Electromagnetic (EM) side channels in addition to timing and power information. In this paper, we propose an effective and efficient model stealing attack against current popular large-scale NN accelerators deployed on hardware platforms through side-channel information. Specifically, the proposed attack approach contains two stages: 1) Inferring the underlying network architecture through EM sidechannel information; 2) Estimating the parameters, especially the weights, through a margin-based, adversarial active learning method. The experimental results show that the proposed attack approach can accurately recover the large-scale NN through EM side-channel information leakages. Overall, our attack highlights the importance of masking EM traces for large-scale NN accelerators in real-world applications.",
      "year": 2020,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Honggang Yu",
        "Haocheng Ma",
        "Kaichen Yang",
        "Yiqiang Zhao",
        "Yier Jin"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/7a3d5f1887fa908a5df303f37ec4450998caafac",
      "pdf_url": "",
      "publication_date": "2020-12-07",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d0d8c912b20d2d081672f07b6926b620950a39c8",
      "title": "Leaky DNN: Stealing Deep-Learning Model Secret with GPU Context-Switching Side-Channel",
      "abstract": "Machine learning has been attracting strong interests in recent years. Numerous companies have invested great efforts and resources to develop customized deep-learning models, which are their key intellectual properties. In this work, we investigate to what extent the secret of deep-learning models can be inferred by attackers. In particular, we focus on the scenario that a model developer and an adversary share the same GPU when training a Deep Neural Network (DNN) model. We exploit the GPU side-channel based on context-switching penalties. This side-channel allows us to extract the fine-grained structural secret of a DNN model, including its layer composition and hyper-parameters. Leveraging this side-channel, we developed an attack prototype named MosConS, which applies LSTM-based inference models to identify the structural secret. Our evaluation of MosConS shows the structural information can be accurately recovered. Therefore, we believe new defense mechanisms should be developed to protect training against the GPU side-channel.",
      "year": 2020,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Junyin Wei",
        "Yicheng Zhang",
        "Zhe Zhou",
        "Zhou Li",
        "M. A. Faruque"
      ],
      "citation_count": 108,
      "url": "https://www.semanticscholar.org/paper/d0d8c912b20d2d081672f07b6926b620950a39c8",
      "pdf_url": "",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ab9f91fe93b59bf44811e6fc1fcdf1b61cf9a5c9",
      "title": "Now You See Me (CME): Concept-based Model Extraction",
      "abstract": "Deep Neural Networks (DNNs) have achieved remarkable performance on a range of tasks. A key step to further empowering DNN-based approaches is improving their explainability. In this work we present CME: a concept-based model extraction framework, used for analysing DNN models via concept-based extracted models. Using two case studies (dSprites, and Caltech UCSD Birds), we demonstrate how CME can be used to (i) analyse the concept information learned by a DNN model (ii) analyse how a DNN uses this concept information when predicting output labels (iii) identify key concept information that can further improve DNN predictive performance (for one of the case studies, we showed how model accuracy can be improved by over 14%, using only 30% of the available concepts).",
      "year": 2020,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Dmitry Kazhdan",
        "B. Dimanov",
        "M. Jamnik",
        "Pietro Lio'",
        "Adrian Weller"
      ],
      "citation_count": 81,
      "url": "https://www.semanticscholar.org/paper/ab9f91fe93b59bf44811e6fc1fcdf1b61cf9a5c9",
      "pdf_url": "",
      "publication_date": "2020-10-25",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "935c1f02735f02a0d86443cec8b9fc33d98a627c",
      "title": "Physics-Informed Deep Neural Networks for Transient Electromagnetic Analysis",
      "abstract": "In this paper, we propose a deep neural network based model to predict the time evolution of field values in transient electrodynamics. The key component of our model is a recurrent neural network, which learns representations of long-term spatial-temporal dependencies in the sequence of its input data. We develop an encoder-recurrent-decoder architecture, which is trained with finite difference time domain simulations of plane wave scattering from distributed, perfect electric conducting objects. We demonstrate that, the trained network can emulate a transient electrodynamics problem with more than 17 times speed-up in simulation time compared to traditional finite difference time domain solvers.",
      "year": 2020,
      "venue": "IEEE Open Journal of Antennas and Propagation",
      "authors": [
        "Oameed Noakoasteen",
        "Shu Wang",
        "Z. Peng",
        "C. Christodoulou"
      ],
      "citation_count": 72,
      "url": "https://www.semanticscholar.org/paper/935c1f02735f02a0d86443cec8b9fc33d98a627c",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8566058/8821524/09158400.pdf",
      "publication_date": "2020-08-04",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7ea18b512074acc10d6a4025cb479955ba295f2d",
      "title": "Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realisation",
      "abstract": "Machine learning models are shown to face a severe threat from Model Extraction Attacks, where a well-trained private model owned by a service provider can be stolen by an attacker pretending as a client. Unfortunately, prior works focus on the models trained over the Euclidean space, e.g., images and texts, while how to extract a GNN model that contains a graph structure and node features is yet to be explored. In this paper, for the first time, we comprehensively investigate and develop model extraction attacks against GNN models. We first systematically formalise the threat modelling in the context of GNN model extraction and classify the adversarial threats into seven categories by considering different background knowledge of the attacker, e.g., attributes and/or neighbour connections of the nodes obtained by the attacker. Then we present detailed methods which utilise the accessible knowledge in each threat to implement the attacks. By evaluating over three real-world datasets, our attacks are shown to extract duplicated models effectively, i.e., 84% - 89% of the inputs in the target domain have the same output predictions as the victim model.",
      "year": 2020,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Bang Wu",
        "Xiangwen Yang",
        "Shirui Pan",
        "Xingliang Yuan"
      ],
      "citation_count": 65,
      "url": "https://www.semanticscholar.org/paper/7ea18b512074acc10d6a4025cb479955ba295f2d",
      "pdf_url": "",
      "publication_date": "2020-10-24",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d4a679c685de885ab98b7c1f2378e26a255de490",
      "title": "Model Extraction Attacks and Defenses on Cloud-Based Machine Learning Models",
      "abstract": "Machine learning models have achieved state-of-the-art performance in various fields, from image classification to speech recognition. However, such models are trained with a large amount of sensitive training data, and are typically computationally expensive to build. As a result, many cloud providers (e.g., Google) have launched machine-learning-as-a-service, which helps clients benefit from the sophisticated cloud-based machine learning models via accessing public APIs. Such a business paradigm significantly expedites and simplifies the development circles. Unfortunately, the commercial value of such cloud-based machine learning models motivates attackers to conduct model extraction attacks for free use or as a springboard to conduct other attacks (e.g., craft adversarial examples in black-box settings). In this article, we conduct a thorough investigation of existing approaches to model extraction attacks and defenses on cloud-based models. We classify the state-of-the-art attack schemes into two categories based on whether the attacker aims to steal the property (i.e., parameters, hyperparameters, and architecture) or the functionality of the model. We also categorize defending schemes into two groups based on whether the scheme relies on output disturbance or query observation. We not only present a detailed survey of each method, but also demonstrate the comparison of both attack and defense approaches via experiments. We highlight several future directions in both model extraction attacks and its defenses, which shed light on possible avenues for further studies.",
      "year": 2020,
      "venue": "IEEE Communications Magazine",
      "authors": [
        "Xueluan Gong",
        "Qian Wang",
        "Yanjiao Chen",
        "Wang Yang",
        "Xinye Jiang"
      ],
      "citation_count": 59,
      "url": "https://www.semanticscholar.org/paper/d4a679c685de885ab98b7c1f2378e26a255de490",
      "pdf_url": "",
      "publication_date": "2020-12-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "983f1e19ba55aa97bd20086ca7ad5cf4e436a37a",
      "title": "Model extraction from counterfactual explanations",
      "abstract": "Post-hoc explanation techniques refer to a posteriori methods that can be used to explain how black-box machine learning models produce their outcomes. Among post-hoc explanation techniques, counterfactual explanations are becoming one of the most popular methods to achieve this objective. In particular, in addition to highlighting the most important features used by the black-box model, they provide users with actionable explanations in the form of data instances that would have received a different outcome. Nonetheless, by doing so, they also leak non-trivial information about the model itself, which raises privacy issues. In this work, we demonstrate how an adversary can leverage the information provided by counterfactual explanations to build high-fidelity and high-accuracy model extraction attacks. More precisely, our attack enables the adversary to build a faithful copy of a target model by accessing its counterfactual explanations. The empirical evaluation of the proposed attack on black-box models trained on real-world datasets demonstrates that they can achieve high-fidelity and high-accuracy extraction even under low query budgets.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "U. A\u00efvodji",
        "Alexandre Bolot",
        "S. Gambs"
      ],
      "citation_count": 56,
      "url": "https://www.semanticscholar.org/paper/983f1e19ba55aa97bd20086ca7ad5cf4e436a37a",
      "pdf_url": "",
      "publication_date": "2020-09-03",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a7abac76ec8aea31ee595af45d733bd462d7f35b",
      "title": "Stealing Deep Reinforcement Learning Models for Fun and Profit",
      "abstract": "This paper presents the first model extraction attack against Deep Reinforcement Learning (DRL), which enables an external adversary to precisely recover a black-box DRL model only from its interaction with the environment. Model extraction attacks against supervised Deep Learning models have been widely studied. However, those techniques cannot be applied to the reinforcement learning scenario due to DRL models' high complexity, stochasticity and limited observable information. We propose a novel methodology to overcome the above challenges. The key insight of our approach is that the process of DRL model extraction is equivalent to imitation learning, a well-established solution to learn sequential decision-making policies. Based on this observation, our methodology first builds a classifier to reveal the training algorithm family of the targeted black-box DRL model only based on its predicted actions, and then leverages state-of-the-art imitation learning techniques to replicate the model from the identified algorithm family. Experimental results indicate that our methodology can effectively recover the DRL models with high fidelity and accuracy. We also demonstrate two use cases to show that our model extraction attack can (1) significantly improve the success rate of adversarial attacks, and (2) steal DRL models stealthily even they are protected by DNN watermarks. These pose a severe threat to the intellectual property and privacy protection of DRL applications.",
      "year": 2020,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Kangjie Chen",
        "Tianwei Zhang",
        "Xiaofei Xie",
        "Yang Liu"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/a7abac76ec8aea31ee595af45d733bd462d7f35b",
      "pdf_url": "https://ink.library.smu.edu.sg/sis_research/7110",
      "publication_date": "2020-06-09",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8765853a2d7027dfe91d650462d7431552bd7315",
      "title": "ES Attack: Model Stealing Against Deep Neural Networks Without Data Hurdles",
      "abstract": "Deep neural networks (DNNs) have become the essential components for various commercialized machine learning services, such as Machine Learning as a Service (MLaaS). Recent studies show that machine learning services face severe privacy threats - well-trained DNNs owned by MLaaS providers can be stolen through public APIs, namely model stealing attacks. However, most existing works undervalued the impact of such attacks, where a successful attack has to acquire confidential training data or auxiliary data regarding the victim DNN. In this paper, we propose ES Attack, a novel model stealing attack without any data hurdles. By using heuristically generated synthetic data, ES Attack iteratively trains a substitute model and eventually achieves a functionally equivalent copy of the victim DNN. The experimental results reveal the severity of ES Attack: i) ES Attack successfully steals the victim model without data hurdles, and ES Attack even outperforms most existing model stealing attacks using auxiliary data in terms of model accuracy; ii) most countermeasures are ineffective in defending ES Attack; iii) ES Attack facilitates further attacks relying on the stolen model.",
      "year": 2020,
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
      "authors": [
        "Xiaoyong Yuan",
        "Lei Ding",
        "Lan Zhang",
        "Xiaolin Li",
        "D. Wu"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/8765853a2d7027dfe91d650462d7431552bd7315",
      "pdf_url": "https://doi.org/10.1109/tetci.2022.3147508",
      "publication_date": "2020-09-21",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1f53fe2f4458bde77735482552dc6818ae5eb30f",
      "title": "GANRED: GAN-based Reverse Engineering of DNNs via Cache Side-Channel",
      "abstract": "In recent years, deep neural networks (DNN) have become an important type of intellectual property due to their high performance on various classification tasks. As a result, DNN stealing attacks have emerged. Many attack surfaces have been exploited, among which cache timing side-channel attacks are hugely problematic because they do not need physical probing or direct interaction with the victim to estimate the DNN model. However, existing cache-side-channel-based DNN reverse engineering attacks rely on analyzing the binary code of the DNN library that must be shared between the attacker and the victim in the main memory. In reality, the DNN library code is often inaccessible because 1) the code is proprietary, or 2) memory sharing has been disabled by the operating system. In our work, we propose GANRED, an attack approach based on the generative adversarial nets (GAN) framework which utilizes cache timing side-channel information to accurately recover the structure of DNNs without memory sharing or code access. The benefit of GANRED is four-fold. 1) There is no need for DNN library code analysis. 2) No shared main memory segment between the victim and the attacker is needed. 3) Our attack locates the exact structure of the victim model, unlike existing attacks which only narrow down the structure search space. 4) Our attack efficiently scales to deeper DNNs, exhibiting only linear growth in the number of layers in the victim DNN.",
      "year": 2020,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Yuntao Liu",
        "Ankur Srivastava"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/1f53fe2f4458bde77735482552dc6818ae5eb30f",
      "pdf_url": "",
      "publication_date": "2020-11-09",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "729fbe386e31e36ed5108ee276a6db223176a33d",
      "title": "Information Leakage by Model Weights on Federated Learning",
      "abstract": "Federated learning aggregates data from multiple sources while protecting privacy, which makes it possible to train efficient models in real scenes. However, although federated learning uses encrypted security aggregation, its decentralised nature makes it vulnerable to malicious attackers. A deliberate attacker can subtly control one or more participants and upload malicious model parameter updates, but the aggregation server cannot detect it due to encrypted privacy protection. Based on these problems, we find a practical and novel security risk in the design of federal learning. We propose an attack for conspired malicious participants to adjust the training data strategically so that the weight of a certain dimension in the aggregation model will rise or fall with a pattern. The trend of weights or parameters in the aggregation model forms meaningful signals, which is the risk of information leakage. The leakage is exposed to other participants in this federation but only available for participants who reach an agreement with the malicious participant, i.e., the receiver must be able to understand patterns of changes in weights. The attack effect is evaluated and verified on open-source code and data sets.",
      "year": 2020,
      "venue": "PPMLP@CCS",
      "authors": [
        "Xiaoyun Xu",
        "Jingzheng Wu",
        "Mutian Yang",
        "Tianyue Luo",
        "Xu Duan",
        "Weiheng Li",
        "Yanjun Wu",
        "Bin Wu"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/729fbe386e31e36ed5108ee276a6db223176a33d",
      "pdf_url": "",
      "publication_date": "2020-11-09",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "db3f6b0581d2cc23756e3fec1076a4399e97a0d8",
      "title": "Model Extraction Attacks on Recurrent Neural Networks",
      "abstract": ": Model extraction attacks are an attack in which an adversary utilizes a query access to the target model to obtain a new model whose performance is equivalent to the target model e \ufb03 ciently, i.e., fewer datasets and computational resources than those of the target model. Existing works have dealt with only simple deep neural networks (DNNs), e.g., only three layers, as targets of model extraction attacks, and hence are not aware of the e \ufb00 ectiveness of recurrent neural networks (RNNs) in dealing with time-series data. In this work, we shed light on the threats of model extraction attacks on RNNs. We discuss whether a model with a higher accuracy can be extracted with a simple RNN from a long short-term memory (LSTM), which is a more complicated and powerful type of RNN. Speci\ufb01cally, we tackle the following problems. First, in case of a classi\ufb01cation task, such as image recognition, extraction of an RNN model without \ufb01nal outputs from an LSTM model is presented by utilizing outputs halfway through the sequence. Next, in case of a regression task such as weather forecasting, a new attack by newly con\ufb01guring a loss function is presented. We conduct experiments on our model extraction attacks on an RNN and an LSTM trained with publicly available academic datasets. We then show that a model with a higher accuracy can be extracted e \ufb03 ciently, especially through con\ufb01guring a loss function and a more complex architecture di \ufb00 erent from the target model.",
      "year": 2020,
      "venue": "Journal of Information Processing",
      "authors": [
        "Tatsuya Takemura",
        "Naoto Yanai",
        "T. Fujiwara"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/db3f6b0581d2cc23756e3fec1076a4399e97a0d8",
      "pdf_url": "https://doi.org/10.2197/ipsjjip.28.1010",
      "publication_date": null,
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9b0758a89df635cded669bc5f9e3b9e2fa93d093",
      "title": "Effect of Microwave on Changes of Gallic Acid and Resveratrol in a Model Extraction Solution",
      "abstract": null,
      "year": 2020,
      "venue": "Food and Bioprocess Technology",
      "authors": [
        "Jiang-Feng Yuan",
        "Ting-ting Wang",
        "Da\u2010hong Wang",
        "Guo-Hua Zhou",
        "Guo-Xin Zou",
        "Yan Wang",
        "Mingjing Gong",
        "Bin Zhang"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/9b0758a89df635cded669bc5f9e3b9e2fa93d093",
      "pdf_url": "",
      "publication_date": "2020-05-04",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "634d65bba4efc94b17dcc4a60fd0bb2de29ac2c4",
      "title": "MARLeME: A Multi-Agent Reinforcement Learning Model Extraction Library",
      "abstract": "Multi-Agent Reinforcement Learning (MARL) encompasses a powerful class of methodologies that have been applied in a wide range of fields. An effective way to further empower these methodologies is to develop approaches and tools that could expand their interpretability and explainability. In this work, we introduce MARLeME: a MARL model extraction library, designed to improve explainability of MARL systems by approximating them with symbolic models. Symbolic models offer a high degree of interpretability, well-defined properties, and verifiable behaviour. Consequently, they can be used to inspect and better understand the underlying MARL systems and corresponding MARL agents, as well as to replace all/some of the agents that are particularly safety and security critical. In this work, we demonstrate how MARLeME can be applied to two well-known case studies (Cooperative Navigation and RoboCup Takeaway), using extracted models based on Abstract Argumentation.",
      "year": 2020,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Dmitry Kazhdan",
        "Z. Shams",
        "Pietro Lio"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/634d65bba4efc94b17dcc4a60fd0bb2de29ac2c4",
      "pdf_url": "http://arxiv.org/pdf/2004.07928",
      "publication_date": "2020-04-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c774bb2b6cb1108773a0e86aff0fc00003c7397a",
      "title": "Finite\u2010state model extraction and visualization from Java program execution",
      "abstract": "Finite\u2010state models are extensively used for discrete systems and they have also been adopted for the analysis and verification of concurrent systems. Programs that have a repetitive cycle, such as event\u2010driven servers and controllers, lend themselves to finite\u2010state modeling. In this article, we use the term model extraction to refer to the construction of a finite\u2010state model from an execution trace of a Java program and a set of key attributes, that is, a subset of the fields of the objects in the program execution. By choosing different sets of attributes, different finite\u2010state models (or views) of the execution can be obtained. Such models aid program comprehension and they can also be used in debugging a program. We present algorithms for model extraction and also for model abstraction in order to reduce the size of the extracted models so that they are amenable to visualization. For long executions, we show how to minimize the overhead of execution trace collection through a bytecode instrumentation technique; and, for large models, which are not amenable to visualization, we show how key properties of the extracted model can be checked against declarative specifications. We have implemented our techniques in the context of JIVE, an Eclipse plugin that supports runtime visualization and analysis of Java program executions. We illustrate our techniques through a collection of case studies of varying size and complexity, from classic problems of concurrency control to a medium\u2010size protocol for authorization (OAuth2.0 protocol) to a large\u2010scale software that underlies web applications (Apache Tomcat server).",
      "year": 2020,
      "venue": "Software, Practice & Experience",
      "authors": [
        "Jevitha K. P.",
        "Swaminathan Jayaraman",
        "B. Jayaraman",
        "Sethumadhavan M"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/c774bb2b6cb1108773a0e86aff0fc00003c7397a",
      "pdf_url": "",
      "publication_date": "2020-10-11",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "92c6a2f7e178337fb99c695ae856b9377e8e72a5",
      "title": "Model Extraction Attacks against Recurrent Neural Networks",
      "abstract": "Model extraction attacks are a kind of attacks in which an adversary obtains a new model, whose performance is equivalent to that of a target model, via query access to the target model efficiently, i.e., fewer datasets and computational resources than those of the target model. Existing works have dealt with only simple deep neural networks (DNNs), e.g., only three layers, as targets of model extraction attacks, and hence are not aware of the effectiveness of recurrent neural networks (RNNs) in dealing with time-series data. In this work, we shed light on the threats of model extraction attacks against RNNs. We discuss whether a model with a higher accuracy can be extracted with a simple RNN from a long short-term memory (LSTM), which is a more complicated and powerful RNN. Specifically, we tackle the following problems. First, in a case of a classification problem, such as image recognition, extraction of an RNN model without final outputs from an LSTM model is presented by utilizing outputs halfway through the sequence. Next, in a case of a regression problem. such as in weather forecasting, a new attack by newly configuring a loss function is presented. We conduct experiments on our model extraction attacks against an RNN and an LSTM trained with publicly available academic datasets. We then show that a model with a higher accuracy can be extracted efficiently, especially through configuring a loss function and a more complex architecture different from the target model.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Tatsuya Takemura",
        "Naoto Yanai",
        "T. Fujiwara"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/92c6a2f7e178337fb99c695ae856b9377e8e72a5",
      "pdf_url": "",
      "publication_date": "2020-02-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "31bfc3d4c51534a0c27ea9928423c53deb1ff9f0",
      "title": "MEME: Generating RNN Model Explanations via Model Extraction",
      "abstract": "Recurrent Neural Networks (RNNs) have achieved remarkable performance on a range of tasks. A key step to further empowering RNN-based approaches is improving their explainability and interpretability. In this work we present MEME: a model extraction approach capable of approximating RNNs with interpretable models represented by human-understandable concepts and their interactions. We demonstrate how MEME can be applied to two multivariate, continuous data case studies: Room Occupation Prediction, and In-Hospital Mortality Prediction. Using these case-studies, we show how our extracted models can be used to interpret RNNs both locally and globally, by approximating RNN decision-making via interpretable concept interactions.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Dmitry Kazhdan",
        "B. Dimanov",
        "M. Jamnik",
        "Pietro Lio'"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/31bfc3d4c51534a0c27ea9928423c53deb1ff9f0",
      "pdf_url": "",
      "publication_date": "2020-12-13",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "03a0a36bc355e71e758285da4fc7c89948c41997",
      "title": "A 1.02-\u03bcW STT-MRAM-Based DNN ECG Arrhythmia Monitoring SoC With Leakage-Based Delay MAC Unit",
      "abstract": "A low-power STT-MRAM-based mixed-mode electrocardiogram (ECG) arrhythmia monitoring SoC is proposed. The proposed SoC consists of 1-MB STT-MRAM, leakage-based delay multiply-and-accumulation (MAC) unit (LDMAC), and ECG analog front end (AFE). ResNet structure with 16 1-D convolution layers and max-pooling layers is adopted for the ECG arrhythmia detection with weight reusing and partial sum reusing scheme. A nonvolatile 1-MB STT-MRAM enables deep neural network (DNN) inference to achieve higher area efficiency, lower power consumption without external memory access. The proposed mixed-mode LDMAC consumes only 4.11-nW MAC power by reusing leakage current. The proposed SoC is fabricated in 28-nm FDSOI process with 7.29-mm2 area. It demonstrates ECG arrhythmia detection with 85.1% accuracy, which is the highest score reported, and the lowest power consumption of 1.02 ${\\mu }\\text{W}$ .",
      "year": 2020,
      "venue": "IEEE Solid-State Circuits Letters",
      "authors": [
        "Kyoung-Rog Lee",
        "Jihoon Kim",
        "Changhyeon Kim",
        "Donghyeon Han",
        "Juhyoung Lee",
        "Jinsu Lee",
        "Hongsik Jeong",
        "H. Yoo"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/03a0a36bc355e71e758285da4fc7c89948c41997",
      "pdf_url": "",
      "publication_date": "2020-09-18",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c9c7498580d105ebac92ed80ec6698bb6dbeb76f",
      "title": "Simple Electromagnetic Analysis Against Activation Functions of Deep Neural Networks",
      "abstract": null,
      "year": 2020,
      "venue": "ACNS Workshops",
      "authors": [
        "Go Takatoi",
        "T. Sugawara",
        "K. Sakiyama",
        "Y. Li"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/c9c7498580d105ebac92ed80ec6698bb6dbeb76f",
      "pdf_url": "",
      "publication_date": "2020-10-19",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7282e208d9063a460ad0306103a76557387322f6",
      "title": "Practical Side-Channel Based Model Extraction Attack on Tree-Based Machine Learning Algorithm",
      "abstract": null,
      "year": 2020,
      "venue": "ACNS Workshops",
      "authors": [
        "Dirmanto Jap",
        "Ville Yli-M\u00e4yry",
        "Akira Ito",
        "Rei Ueno",
        "S. Bhasin",
        "N. Homma"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/7282e208d9063a460ad0306103a76557387322f6",
      "pdf_url": "",
      "publication_date": "2020-10-19",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "57099cf852807c652fd55274a3ce4a8c9777ef56",
      "title": "Neural Model Stealing Attack to Smart Mobile Device on Intelligent Medical Platform",
      "abstract": "To date, the Medical Internet of Things (MIoT) technology has been recognized and widely applied due to its convenience and practicality. The MIoT enables the application of machine learning to predict diseases of various kinds automatically and accurately, assisting and facilitating effective and efficient medical treatment. However, the MIoT are vulnerable to cyberattacks which have been constantly advancing. In this paper, we establish a MIoT platform and demonstrate a scenario where a trained Convolutional Neural Network (CNN) model for predicting lung cancer complicated with pulmonary embolism can be attacked. First, we use CNN to build a model to predict lung cancer complicated with pulmonary embolism and obtain high detection accuracy. Then, we build a copycat model using only a small amount of data labeled by the target network, aiming to steal the established prediction model. Experimental results prove that the stolen model can also achieve a relatively high prediction outcome, revealing that the copycat network could successfully copy the prediction performance from the target network to a large extent. This also shows that such a prediction model deployed on MIoT devices can be stolen by attackers, and effective prevention strategies are open questions for researchers.",
      "year": 2020,
      "venue": "Wireless Communications and Mobile Computing",
      "authors": [
        "Liqiang Zhang",
        "Guanjun Lin",
        "Bixuan Gao",
        "Zhibao Qin",
        "Yonghang Tai",
        "Jun Zhang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/57099cf852807c652fd55274a3ce4a8c9777ef56",
      "pdf_url": "https://downloads.hindawi.com/journals/wcmc/2020/8859489.pdf",
      "publication_date": "2020-11-26",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f9f49997c404e386610289202a7c3812c6cbfe34",
      "title": "Differentially Private Machine Learning Model against Model Extraction Attack",
      "abstract": "Machine learning model is vulnerable to model extraction attacks since the attackers can send plenty of queries to infer the hyperparameters of the machine learning model thus stealing confidential information of the learning models. Therefore, there is a urgent need to defend against such an attack. Differential privacy is a promising technique to protect the valuable information. We propose a differential privacy-based method applied in the linear neural network to obfuscate the output of the machine learning model. The security and utility issue of injecting a noise layer to the linear neural network is mathematically analyzed. The experiment results show that our proposed method can lower the attacker's extraction rate while keeping high utility.",
      "year": 2020,
      "venue": "2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",
      "authors": [
        "Zelei Cheng",
        "Zuotian Li",
        "Jiwei Zhang",
        "Shuhan Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f9f49997c404e386610289202a7c3812c6cbfe34",
      "pdf_url": "",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "82e2e111d2a81fe8efaabec44b5d7d3c74cfa95d",
      "title": "Stealing Your Data from Compressed Machine Learning Models",
      "abstract": "Machine learning models have been widely deployed in many real-world tasks. When a non-expert data holder wants to use a third-party machine learning service for model training, it is critical to preserve the confidentiality of the training data. In this paper, we for the first time explore the potential privacy leakage in a scenario that a malicious ML provider offers data holder customized training code including model compression which is essential in practical deployment The provider is unable to access the training process hosted by the secured third party, but could inquire models when they are released in public. As a result, adversary can extract sensitive training data with high quality even from these deeply compressed models that are tailored for resource-limited devices. Our investigation shows that existing compressions like quantization, can serve as a defense against such an attack, by degrading the model accuracy and memorized data quality simultaneously. To overcome this defense, we take an initial attempt to design a simple but stealthy quantized correlation encoding attack flow from an adversary perspective. Three integrated components-data pre-processing, layer-wise data-weight correlation regularization, data-aware quantization, are developed accordingly. Extensive experimental results show that our framework can preserve the evasiveness and effectiveness of stealing data from compressed models.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Nuo Xu",
        "Qi Liu",
        "Tao Liu",
        "Zihao Liu",
        "Xiaochen Guo",
        "Wujie Wen"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/82e2e111d2a81fe8efaabec44b5d7d3c74cfa95d",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "title": "Model Stealing Defense with Hybrid Fuzzy Models: Work-in-Progress",
      "abstract": "With increasing applications of Deep Neural Networks (DNNs) to edge computing systems, security issues have received more attentions. Particularly, model stealing attack is one of the biggest challenge to the privacy of models. To defend against model stealing attack, we propose a novel protection architecture with fuzzy models. Each fuzzy model is designed to generate wrong predictions corresponding to a particular category. In addition\u2019 we design a special voting strategy to eliminate the systemic errors, which can destroy the dark knowledge in predictions at the same time. Preliminary experiments show that our method substantially decreases the clone model's accuracy (up to 20%) without loss of inference accuracy for benign users.",
      "year": 2020,
      "venue": "International Conference on Hardware/Software Codesign and System Synthesis",
      "authors": [
        "Zicheng Gong",
        "Wei Jiang",
        "Jinyu Zhan",
        "Ziwei Song"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "pdf_url": "",
      "publication_date": "2020-09-20",
      "keywords_matched": [
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9356a216e5ae48c0007b66687a3da47fc2bea834",
      "title": "Perturbing Inputs to Prevent Model Stealing",
      "abstract": "We show how perturbing inputs to machine learning services (ML-service) deployed in the cloud can protect against model stealing attacks. In our formulation, there is an ML-service that receives inputs from users and returns the output of the model. There is an attacker that is interested in learning the parameters of the ML-service. We use the linear and logistic regression models to illustrate how strategically adding noise to the inputs fundamentally alters the attacker\u2019s estimation problem. We show that even with infinite samples, the attacker would not be able to recover the true model parameters. We focus on characterizing the trade-off between the error in the attacker\u2019s estimate of the parameters with the error in the ML-service\u2019s output.",
      "year": 2020,
      "venue": "IEEE Conference on Communications and Network Security",
      "authors": [
        "J. Grana"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9356a216e5ae48c0007b66687a3da47fc2bea834",
      "pdf_url": "https://arxiv.org/pdf/2005.05823",
      "publication_date": "2020-05-12",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b97bc90c7106b6c44ffaa0c0ce81a67134524e89",
      "title": "A Low-Cost Image Encryption Method to Prevent Model Stealing of Deep Neural Network",
      "abstract": "Model stealing attack may happen by stealing useful data transmitted from embedded end to server end for an artificial intelligent systems. In this paper, we are interested in preventing model stealing of neural network for resource-constrained systems. We propose an Image Encryption based on Class Activation Map (IECAM) to encrypt information before transmitting in embedded end. According to class activation map, IECAM chooses certain key areas of the image to be encrypted with the purpose of reducing the model stealing risk of neural network. With partly encrypted information, IECAM can greatly reduce the time overheads of encryption/decryption in both embedded and server ends, especially for big size images. The experimental results demonstrate that our method can significantly reduce time overheads of encryption/decryption and the risk of model stealing compared with traditional methods.",
      "year": 2020,
      "venue": "J. Circuits Syst. Comput.",
      "authors": [
        "Wei Jiang",
        "Zicheng Gong",
        "Jinyu Zhan",
        "Zhiyuan He",
        "Weijia Pan"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/b97bc90c7106b6c44ffaa0c0ce81a67134524e89",
      "pdf_url": "",
      "publication_date": "2020-05-28",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a2923ace209e6193effa49a5c254b29d77b2ee0d",
      "title": "Stealing Black-Box Functionality Using The Deep Neural Tree Architecture",
      "abstract": "This paper makes a substantial step towards cloning the functionality of black-box models by introducing a Machine learning (ML) architecture named Deep Neural Trees (DNTs). This new architecture can learn to separate different tasks of the black-box model, and clone its task-specific behavior. We propose to train the DNT using an active learning algorithm to obtain faster and more sample-efficient training. In contrast to prior work, we study a complex \"victim\" black-box model based solely on input-output interactions, while at the same time the attacker and the victim model may have completely different internal architectures. The attacker is a ML based algorithm whereas the victim is a generally unknown module, such as a multi-purpose digital chip, complex analog circuit, mechanical system, software logic or a hybrid of these. The trained DNT module not only can function as the attacked module, but also provides some level of explainability to the cloned model due to the tree-like nature of the proposed architecture.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Daniel Teitelman",
        "I. Naeh",
        "Shie Mannor"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a2923ace209e6193effa49a5c254b29d77b2ee0d",
      "pdf_url": "",
      "publication_date": "2020-02-23",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d6a0da1403eb183a24541ac5636eb7a44223b7dc",
      "title": "Special-purpose Model Extraction Attacks: Stealing Coarse Model with Fewer Queries",
      "abstract": "Model extraction (ME) attacks have been shown to cause financial losses for Machine-Learning-as-a-Service (MLaaS) providers. Attackers steal ML models on MLaaS platforms by building substitute models using queries to and responses from MLaaS platforms. The ML models targeted by attackers are called targeted models. In previous studies, researchers have assumed that attackers build substitute models that classify the same number of classes as targeted ones, which classify thousands or millions of classes to meet users' diverse expectations. We call such models general-purpose models. In fact, attackers can monetize stolen models if they accurately distinguish some classes from others. We call such models special-purpose models. For instance, a model that detects vehicles is useful for collision avoidance systems, and a model that detects wild animals is useful to drive them away from agricultural land. In this work, we investigate a threat of special-purpose ME attacks that steal special-purpose models. Our experimental results show that attackers can build an accurate special-purpose model, which achieves an 80% f-measure, with as few as 100 queries in the worst case. We discuss the difficulty in preventing the attacks with previously proposed defense methods and point out the necessity of a new defense method.",
      "year": 2020,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Rina Okada",
        "Zen Ishikura",
        "Toshiki Shibahara",
        "Satoshi Hasegawa"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d6a0da1403eb183a24541ac5636eb7a44223b7dc",
      "pdf_url": "",
      "publication_date": "2020-12-01",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "550ae0ee2a43d419ea92b899505b96a64eaa213e",
      "title": "Analysis of Electromagnetic Band Gap Structure using Artificial Neural Network for UWB Applications",
      "abstract": "In this paper, a planar meander-line electromagnetic band-gap (EBG) structure for UWB applications is proposed. The EBG resonating at 7.5 GHz is designed and analyzed using artificial neural network (ANN) technique. The resonant frequency and the corresponding |S11| are predicted using geometrical parameters of the EBG unit cell. The proposed ANN model with single hidden layer is trained using Levenberg-Marquardt (LM) algorithm. The network is trained using different sets of hidden neurons in the hidden layer and activation function, in order to select the optimum ones. The least mean square error (MSE) is obtained when the network is trained using log-sigmoid transfer function with 40 hidden neurons in the hidden layer. Also, performance of the ANN model is validated using other statistical parameters viz. mean absolute percentage error (MAPE), and correlation coefficient (CC).",
      "year": 2020,
      "venue": "2020 Advanced Communication Technologies and Signal Processing (ACTS)",
      "authors": [
        "Debanjali Sarkar",
        "T. Khan",
        "F. Talukdar"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/550ae0ee2a43d419ea92b899505b96a64eaa213e",
      "pdf_url": "",
      "publication_date": "2020-12-04",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "30d6ef1b7d590094b62f38e97603122f65aa0d09",
      "title": "Model Extraction",
      "abstract": null,
      "year": 2020,
      "venue": "S-Parameters for Signal Integrity",
      "authors": [],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/30d6ef1b7d590094b62f38e97603122f65aa0d09",
      "pdf_url": "",
      "publication_date": "2020-02-06",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "91ffcaa35fd6690ff799298aa75bf9a42199a834",
      "title": "Conformational changes of \u03b2-thalassemia major hemoglobin and oxidative status of plasma after in vitro exposure to extremely low-frequency electromagnetic fields: An artificial neural network analysis",
      "abstract": "ABSTRACT Electromagnetic fields (EMF) can generate reactive oxygen species and induce oxidative modifications. We investigated the effects of extremely low-frequency electromagnetic fields (ELF-EMF) on oxidative status of plasma and erythrocytes in \u03b2-thalassemia major patients and design artificial neural networks (ANN) for evaluating the oxyHb concentration. Blood samples were obtained from age and sex-matched healthy donors (n = 12) and major \u03b2-thalassemia patients (n = 12) and subjected to 0.5 and 1 mT and 50 Hz of EMF. Plasma oxidative status was estimated after 1 and 2 h exposure to ELE-EMF. Structural changes of plasma proteins were investigated by Native PAGE and SDS-PAGE. Moreover; multilayer perceptron (MLP) method was applied for designing a feed forward ANN model to predict the impact of these oxidative and antioxidative parameters on oxyHb concentration. Two hour exposure to ELF-EMF induced significant oxidative changes on major \u03b2-thalassemia samplesElectrophoretic profiles showed two high molecular weight (HMW) protein aggregates in plasma samples from healthy donors and major \u03b2-thalassemia patients. According to our ANN design, the main predictors of oxyHb concentration were optical density of Hb at 542, 340, 569, 630, 577, and 420 nm and metHb and hemichrome (HC) concentration. Accuracy of the proposed ANN model was shown by predicted by observed chart (y = 1.3 + 0.96x, R2 = 0.942), sum of squares errors (SSR), and relative errors (RE). Our results showed the detailed effects of ELF-EMF on Hb structure and oxidative balance of plasma in major \u03b2-thalassemia patients and significance of ANN analysis during normal and pathologic conditions.",
      "year": 2020,
      "venue": "Electromagnetic Biology and Medicine",
      "authors": [
        "Saeideh Rahmani",
        "Hadi Ansarihadipour",
        "M. Bayatiani",
        "A. Khosrowbeygi",
        "S. Babaei",
        "Yousef Rasmi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/91ffcaa35fd6690ff799298aa75bf9a42199a834",
      "pdf_url": "",
      "publication_date": "2020-10-23",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs",
      "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Kalpesh Krishna",
        "Gaurav Singh Tomar",
        "Ankur P. Parikh",
        "Nicolas Papernot",
        "Mohit Iyyer"
      ],
      "citation_count": 225,
      "url": "https://www.semanticscholar.org/paper/ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "pdf_url": "",
      "publication_date": "2019-10-27",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "da10f79f983fd4fbd589ed7ffa68d33964841443",
      "title": "Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks",
      "abstract": "High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker's error rate up to a factor of 85$\\times$ with minimal impact on the utility for benign users.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 183,
      "url": "https://www.semanticscholar.org/paper/da10f79f983fd4fbd589ed7ffa68d33964841443",
      "pdf_url": "",
      "publication_date": "2019-06-26",
      "keywords_matched": [
        "model stealing attack",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e6c55623cf56a70659e612eb6a741a4de7545ba7",
      "title": "Defending Against Model Stealing Attacks With Adaptive Misinformation",
      "abstract": "Deep Neural Networks (DNNs) are susceptible to model stealing attacks, which allows a data-limited adversary with no knowledge of the training dataset to clone the functionality of a target model, just by using black-box query access. Such attacks are typically carried out by querying the target model using inputs that are synthetically generated or sampled from a surrogate dataset to construct a labeled dataset. The adversary can use this labeled dataset to train a clone model, which achieves a classification accuracy comparable to that of the target model. We propose \"Adaptive Misinformation\" to defend against such model stealing attacks. We identify that all existing model stealing attacks invariably query the target model with Out-Of-Distribution (OOD) inputs. By selectively sending incorrect predictions for OOD queries, our defense substantially degrades the accuracy of the attacker's clone model (by up to 40%), while minimally impacting the accuracy (<0.5%) for benign users. Compared to existing defenses, our defense has a significantly better security vs accuracy trade-off and incurs minimal computational overhead.",
      "year": 2019,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "S. Kariyappa",
        "Moinuddin K. Qureshi"
      ],
      "citation_count": 123,
      "url": "https://www.semanticscholar.org/paper/e6c55623cf56a70659e612eb6a741a4de7545ba7",
      "pdf_url": "https://arxiv.org/pdf/1911.07100",
      "publication_date": "2019-11-16",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "555b847ee9c39c12d5764f4e11b888b331a89cfb",
      "title": "Defending Against Neural Network Model Stealing Attacks Using Deceptive Perturbations",
      "abstract": "Machine learning architectures are readily available, but obtaining the high quality labeled data for training is costly. Pre-trained models available as cloud services can be used to generate this costly labeled data, and would allow an attacker to replicate trained models, effectively stealing them. Limiting the information provided by cloud based models by omitting class probabilities has been proposed as a means of protection but significantly impacts the utility of the models. In this work, we illustrate how cloud based models can still provide useful class probability information for users, while significantly limiting the ability of an adversary to steal the model. Our defense perturbs the model's final activation layer, slightly altering the output probabilities. This forces the adversary to discard the class probabilities, requiring significantly more queries before they can train a model with comparable performance. We evaluate our defense under diverse scenarios and defense aware attacks. Our evaluation shows our defense can degrade the accuracy of the stolen model at least 20%, or increase the number of queries required by an adversary 64 fold, all with a negligible decrease in the protected model accuracy.",
      "year": 2019,
      "venue": "2019 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Taesung Lee",
        "Ben Edwards",
        "Ian Molloy",
        "D. Su"
      ],
      "citation_count": 106,
      "url": "https://www.semanticscholar.org/paper/555b847ee9c39c12d5764f4e11b888b331a89cfb",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8834415/8844588/08844598.pdf",
      "publication_date": "2019-05-19",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6854c4c8dfca1de802f2662db62deb7ba2bc10b2",
      "title": "MaskedNet: The First Hardware Inference Engine Aiming Power Side-Channel Protection",
      "abstract": "Differential Power Analysis (DPA) has been an active area of research for the past two decades to study the attacks for extracting secret information from cryptographic implementations through power measurements and their defenses. The research on power side-channels have so far predominantly focused on analyzing implementations of ciphers such as AES, DES, RSA, and recently post-quantum cryptography primitives (e.g., lattices). Meanwhile, machine-learning applications are becoming ubiquitous with several scenarios where the Machine Learning Models are Intellectual Properties requiring confidentiality. Expanding side-channel analysis to Machine Learning Model extraction, however, is largely unexplored. This paper expands the DPA framework to neural-network classifiers. First, it shows DPA attacks during inference to extract the secret model parameters such as weights and biases of a neural network. Second, it proposes the first countermeasures against these attacks by augmenting masking. The resulting design uses novel masked components such as masked adder trees for fully-connected layers and masked Rectifier Linear Units for activation functions. On a SAKURA-X FPGA board, experiments show that the first-order DPA attacks on the unprotected implementation can succeed with only 200 traces and our protection respectively increases the latency and area-cost by $ 2.8\\times$ and $2.3\\times$.",
      "year": 2019,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Anuj Dubey",
        "Rosario Cammarota",
        "Aydin Aysu"
      ],
      "citation_count": 91,
      "url": "https://www.semanticscholar.org/paper/6854c4c8dfca1de802f2662db62deb7ba2bc10b2",
      "pdf_url": "https://arxiv.org/pdf/1910.13063",
      "publication_date": "2019-10-29",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8bedaca9ebb1d055046643802683d013fd1b2da9",
      "title": "BDPL: A Boundary Differentially Private Layer Against Machine Learning Model Extraction Attacks",
      "abstract": null,
      "year": 2019,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Huadi Zheng",
        "Qingqing Ye",
        "Haibo Hu",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/8bedaca9ebb1d055046643802683d013fd1b2da9",
      "pdf_url": "",
      "publication_date": "2019-09-23",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "304ac5d62f2888e94078715413c40cf00b58ac4f",
      "title": "Efficiently Stealing your Machine Learning Models",
      "abstract": "Machine Learning as a Service (MLaaS) is a growing paradigm in the Machine Learning (ML) landscape. More and more ML models are being uploaded to the cloud and made accessible from all over the world. Creating good ML models, however, can be expensive and the used data is often sensitive. Recently, Secure Multi-Party Computation (SMPC) protocols for MLaaS have been proposed, which protect sensitive user data and ML models at the expense of substantially higher computation and communication than plaintext evaluation. In this paper, we show that for a subset of ML models used in MLaaS, namely Support Vector Machines (SVMs) and Support Vector Regression Machines (SVRs) which have found many applications to classifying multimedia data such as texts and images, it is possible for adversaries to passively extract the private models even if they are protected by SMPC, using known and newly devised model extraction attacks. We show that our attacks are not only theoretically possible but also practically feasible and cheap, which makes them lucrative to financially motivated attackers such as competitors or customers. We perform model extraction attacks on the homomorphic encryption-based protocol for privacy-preserving SVR-based indoor localization by Zhang et al. (International Workshop on Security 2016). We show that it is possible to extract a highly accurate model using only 854 queries with the estimated cost of $0.09 on the Amazon ML platform, and our attack would take only 7 minutes over the Internet. Also, we perform our model extraction attacks on SVM and SVR models trained on publicly available state-of-the-art ML datasets.",
      "year": 2019,
      "venue": "WPES@CCS",
      "authors": [
        "R. Reith",
        "T. Schneider",
        "Oleksandr Tkachenko"
      ],
      "citation_count": 61,
      "url": "https://www.semanticscholar.org/paper/304ac5d62f2888e94078715413c40cf00b58ac4f",
      "pdf_url": "https://encrypto.de/papers/RST19.pdf",
      "publication_date": "2019-11-11",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7542385f3672cfee43adc964a90efb0b19f6b9fa",
      "title": "Deep Neural Network Attribution Methods for Leakage Analysis and Symmetric Key Recovery",
      "abstract": null,
      "year": 2019,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Benjamin Hettwer",
        "Stefan Gehrer",
        "Tim G\u00fcneysu"
      ],
      "citation_count": 55,
      "url": "https://www.semanticscholar.org/paper/7542385f3672cfee43adc964a90efb0b19f6b9fa",
      "pdf_url": "",
      "publication_date": "2019-08-12",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b2fc15dcd0f223b06be7195fee16364c260433fa",
      "title": "An Approach for Process Model Extraction by Multi-grained Text Classification",
      "abstract": "Process model extraction (PME) is a recently emerged interdiscipline between natural language processing (NLP) and business process management (BPM), which aims to extract process models from textual descriptions. Previous process extractors heavily depend on manual features and ignore the potential relations between clues of different text granularities. In this paper, we formalize the PME task into the multi-grained text classification problem, and propose a hierarchical neural network to effectively model and extract multi-grained information without manually-defined procedural features. Under this structure, we accordingly propose the coarse-to-fine (grained) learning mechanism, training multi-grained tasks in coarse-to-fine grained order to share the high-level knowledge for the low-level tasks. To evaluate our approach, we construct two multi-grained datasets from two different domains and conduct extensive experiments from different dimensions. The experimental results demonstrate that our approach outperforms the state-of-the-art methods with statistical significance and further investigations demonstrate its effectiveness.",
      "year": 2019,
      "venue": "International Conference on Advanced Information Systems Engineering",
      "authors": [
        "Chen Qian",
        "L. Wen",
        "Akhil Kumar",
        "Leilei Lin",
        "Li Lin",
        "Zan Zong",
        "Shuang Li",
        "Jianmin Wang"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/b2fc15dcd0f223b06be7195fee16364c260433fa",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-49435-3_17.pdf",
      "publication_date": "2019-05-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "703f6008899db58f8c2bf454f6e146efb1df38e3",
      "title": "Extraction of Complex DNN Models: Real Threat or Boogeyman?",
      "abstract": "Recently, machine learning (ML) has introduced advanced solutions to many domains. Since ML models provide business advantage to model owners, protecting intellectual property of ML models has emerged as an important consideration. Confidentiality of ML models can be protected by exposing them to clients only via prediction APIs. However, model extraction attacks can steal the functionality of ML models using the information leaked to clients through the results returned via the API. In this work, we question whether model extraction is a serious threat to complex, real-life ML models. We evaluate the current state-of-the-art model extraction attack (Knockoff nets) against complex models. We reproduce and confirm the results in the original paper. But we also show that the performance of this attack can be limited by several factors, including ML model architecture and the granularity of API response. Furthermore, we introduce a defense based on distinguishing queries used for Knockoff nets from benign queries. Despite the limitations of the Knockoff nets, we show that a more realistic adversary can effectively steal complex ML models and evade known defenses.",
      "year": 2019,
      "venue": "Communications in Computer and Information Science",
      "authors": [
        "B. Atli",
        "Sebastian Szyller",
        "Mika Juuti",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/703f6008899db58f8c2bf454f6e146efb1df38e3",
      "pdf_url": "",
      "publication_date": "2019-10-11",
      "keywords_matched": [
        "model extraction attack",
        "model extraction",
        "knockoff nets"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e36c9f2c5c4791cf504760988de33b39a013373f",
      "title": "Neural Network Model Extraction Attacks in Edge Devices by Hearing Architectural Hints",
      "abstract": "As neural networks continue their reach into nearly every aspect of software operations, the details of those networks become an increasingly sensitive subject. Even those that deploy neural networks embedded in physical devices may wish to keep the inner working of their designs hidden -- either to protect their intellectual property or as a form of protection from adversarial inputs. The specific problem we address is how, through heavy system stack, given noisy and imperfect memory traces, one might reconstruct the neural network architecture including the set of layers employed, their connectivity, and their respective dimension sizes. Considering both the intra-layer architecture features and the inter-layer temporal association information introduced by the DNN design empirical experience, we draw upon ideas from speech recognition to solve this problem. We show that off-chip memory address traces and PCIe events provide ample information to reconstruct such neural network architectures accurately. We are the first to propose such accurate model extraction techniques and demonstrate an end-to-end attack experimentally in the context of an off-the-shelf Nvidia GPU platform with full system stack. Results show that the proposed techniques achieve a high reverse engineering accuracy and improve the one's ability to conduct targeted adversarial attack with success rate from 14.6\\%$\\sim$25.5\\% (without network architecture knowledge) to 75.9\\% (with extracted network architecture).",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Lei Deng",
        "Shuangchen Li",
        "Xinfeng Xie",
        "Yu Ji",
        "Yufei Ding",
        "Chang Liu",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/e36c9f2c5c4791cf504760988de33b39a013373f",
      "pdf_url": "",
      "publication_date": "2019-03-10",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ce90e2faf697bff599ea8cac9dd47bda9911abf4",
      "title": "Performance Analysis and Dynamic Evolution of Deep Convolutional Neural Network for Electromagnetic Inverse Scattering",
      "abstract": "The solution of electromagnetic (EM) inverse scattering problems is hindered by challenges, such as ill-posedness, nonlinearity, and high computational costs. Recently, deep learning was shown to be a promising tool in addressing these challenges. In particular, it is possible to establish a connection between a deep convolutional neural network (CNN) and iterative solution methods of EM inverse scattering. This led to the development of an efficient CNN-based solution to EM inverse problems, termed DeepNIS. It has been shown that DeepNIS can outperform conventional inverse scattering solution methods in terms of both the image quality and computational time. In this letter, we evaluate the DeepNIS performance as a function of the number of layers using structure similarity index measure and mean square error metrics. In addition, we probe the dynamic evolution behavior of DeepNIS by examining its near-isometry property and show that, after a proper training stage, the proposed CNN has near-optimal stability properties.",
      "year": 2019,
      "venue": "IEEE Antennas and Wireless Propagation Letters",
      "authors": [
        "Lianlin Li",
        "Longgang Wang",
        "F. Teixeira"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/ce90e2faf697bff599ea8cac9dd47bda9911abf4",
      "pdf_url": "https://arxiv.org/pdf/1901.02610",
      "publication_date": "2019-01-09",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "abcad78d2dba846e7c02ab853f86fbb258ad9952",
      "title": "An Active Learning Approach for Improving the Accuracy of Automated Domain Model Extraction",
      "abstract": "Domain models are a useful vehicle for making the interpretation and elaboration of natural-language requirements more precise. Advances in natural-language processing (NLP) have made it possible to automatically extract from requirements most of the information that is relevant to domain model construction. However, alongside the relevant information, NLP extracts from requirements a significant amount of information that is superfluous (not relevant to the domain model). Our objective in this article is to develop automated assistance for filtering the superfluous information extracted by NLP during domain model extraction. To this end, we devise an active-learning-based approach that iteratively learns from analysts\u2019 feedback over the relevance and superfluousness of the extracted domain model elements and uses this feedback to provide recommendations for filtering superfluous elements. We empirically evaluate our approach over three industrial case studies. Our results indicate that, once trained, our approach automatically detects an average of \u2248 45% of the superfluous elements with a precision of \u2248 96%. Since precision is very high, the automatic recommendations made by our approach are trustworthy. Consequently, analysts can dispose of a considerable fraction \u2013 nearly half \u2013 of the superfluous elements with minimal manual work. The results are particularly promising, as they should be considered in light of the non-negligible subjectivity that is inherently tied to the notion of relevance.",
      "year": 2019,
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "authors": [
        "Chetan Arora",
        "M. Sabetzadeh",
        "S. Nejati",
        "L. Briand"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/abcad78d2dba846e7c02ab853f86fbb258ad9952",
      "pdf_url": "https://orbilu.uni.lu/bitstream/10993/37054/1/TOSEM_ASNB.pdf",
      "publication_date": "2019-01-09",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b82d52b95040c0682b7a8baa9254eae79b068505",
      "title": "Adversarial Model Extraction on Graph Neural Networks",
      "abstract": "Along with the advent of deep neural networks came various methods of exploitation, such as fooling the classifier or contaminating its training data. Another such attack is known as model extraction, where provided API access to some black box neural network, the adversary extracts the underlying model. This is done by querying the model in such a way that the underlying neural network provides enough information to the adversary to be reconstructed. While several works have achieved impressive results with neural network extraction in the propositional domain, this problem has not yet been considered over the relational domain, where data samples are no longer considered to be independent and identically distributed (iid). Graph Neural Networks (GNNs) are a popular deep learning framework to perform machine learning tasks over relational data. In this work, we formalize an instance of GNN extraction, present a solution with preliminary results, and discuss our assumptions and future directions.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "David DeFazio",
        "Arti Ramesh"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/b82d52b95040c0682b7a8baa9254eae79b068505",
      "pdf_url": "",
      "publication_date": "2019-12-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e8a58e29056a1db21dd8cf5bcb17eaf4db648aaf",
      "title": "Prediction Poisoning: Utility-Constrained Defenses Against Model Stealing Attacks",
      "abstract": null,
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/e8a58e29056a1db21dd8cf5bcb17eaf4db648aaf",
      "pdf_url": "",
      "publication_date": "2019-06-26",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0135f83ab34d66668306b2ad8207d028da2539e4",
      "title": "MimosaNet: An Unrobust Neural Network Preventing Model Stealing",
      "abstract": "Deep Neural Networks are robust to minor perturbations of the learned network parameters and their minor modifications do not change the overall network response significantly. This allows space for model stealing, where a malevolent attacker can steal an already trained network, modify the weights and claim the new network his own intellectual property. In certain cases this can prevent the free distribution and application of networks in the embedded domain. In this paper, we propose a method for creating an equivalent version of an already trained fully connected deep neural network that can prevent network stealing: namely, it produces the same responses and classification accuracy, but it is extremely sensitive to weight changes.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "K\u00e1lm\u00e1n Szentannai",
        "Jalal Al-Afandi",
        "A. Horv\u00e1th"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/0135f83ab34d66668306b2ad8207d028da2539e4",
      "pdf_url": "",
      "publication_date": "2019-07-02",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-29"
    },
    {
      "paper_id": "ec2de6ca1857b6ec36f8aba1b940281d6f0a6246",
      "title": "The Feasibility of Deep Learning Use for Adversarial Model Extraction in the Cybersecurity Domain",
      "abstract": null,
      "year": 2019,
      "venue": "Ideal",
      "authors": [
        "M. Chora\u015b",
        "M. Pawlicki",
        "R. Kozik"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/ec2de6ca1857b6ec36f8aba1b940281d6f0a6246",
      "pdf_url": "",
      "publication_date": "2019-11-14",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a8a80de402cef8a41a85e1220e24818fc010e6f8",
      "title": "Quantifying (Hyper) Parameter Leakage in Machine Learning",
      "abstract": "Machine Learning models are extensively used for various multimedia applications and are offered to users as a blackbox service on the Cloud on a pay-per-query basis. Such blackbox models are commercially valuable to adversaries, making them vulnerable to extraction attacks that reverse engineer the proprietary model thereby violating the model privacy and Intellectual Property. Extraction attacks proposed in the literature are empirically evaluated and lack a theoretical framework to measure the information leaked under such attacks. In this work, we propose a novel model-agnostic probabilistic framework, AIRAVATA, to quantify information leakage using partial knowledge and limited evidences from model extraction attacks. This framework captures the fact that extracting the exact target model is difficult due to experimental uncertainty while inferring model hyperparameters and stochastic nature of training for stealing the target model functionality. We use Bayesian Networks to capture uncertainty in estimating the target model under various extraction attacks based on the subjective notion of probability. We validate the proposed framework under different adversary assumptions commonly adopted in the literature to reason about the attack efficacy. This provides a practical tool to identify the best attack combination which maximises the knowledge extracted (or information leaked) from the target model and estimate the relative threats from different attacks.",
      "year": 2019,
      "venue": "IEEE International Conference on Multimedia Big Data",
      "authors": [
        "Vasisht Duddu",
        "D. V. Rao"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a8a80de402cef8a41a85e1220e24818fc010e6f8",
      "pdf_url": "https://arxiv.org/pdf/1910.14409",
      "publication_date": "2019-10-31",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7488bd1a98fd5d366a9076287e014321bbb17674",
      "title": "Deep Neural Network Based Prediction of Leak-Off Pressure in Offshore Norway",
      "abstract": "\n Leak-off pressure (LOP) is an important parameter to determine a weight of drilling mud and in-situ horizontal stresses. When the well pressure become higher than the LOP, it can cause a wellbore instability during drilling, such as a mud loss. Thus, accurate prediction of LOP is important for safe and economical drilling for the oil and gas industry. In this study, we present a novel prediction model for the leak-off pressure (LOP) offshore Norway. The model uses a deep neural network (DNN) applied on a public wellbore database provided by the Norwegian Petroleum Directorate (NPD).\n We used a Python-based web scrapping tool to collect data from more than 6400 wells (1800 exploration wells and 4600 development wells) from the NPD factpages. Then, we analyzed the collected data to investigate impacts of spatial and regional factors on the collected LOPs. The DNN model was structured to predict the leak off pressure offshore Norway using open source libraries Keras and Tensor Flow. The model tests have various hidden layers (i.e. 3, 5, and 10 layers). In order to avoid overfitting, we specified an early-stop algorithm. In our study, we took 80% of the data as the training set keeping the remaining 20% to test the model. In total, the database consists of around 3000 leak-off pressure data from about 1800 exploration wells, and grouped in geographical area (North Sea, Norwegian Sea, Barents Sea groups).\n The LOPs of the North Sea and the Norwegian Sea show a bi-linear trend with depth. The LOPs that are measured from deeper than 2-3 km below sea level show clear a deviation in trend, with a steeper increase compared to the shallower section. The steeper part of the bi-linear trend at greated sub-surface depths can be related to a coupling with tectonic stresses from base rocks. The data from the Barents Sea shows more scattered LOP compared to the other regions offshore Norway. The scattered data seem to relate to the complex geological history on the Barents sea. In general, the accuracy of the prediction increases with the number of hidden layers. However, when the number of the hidden layer exceed 5, there was no significant improvement in the accuracy of prediction. The validation test shows relatively good prediction of LOP with an MAE (Mean Absolute Error) of less than 0.07 even for areas experiencing complex geological history such as the deep subsurface of the Norwegian Sea and the shallow subsurface of the Barents sea.\n This study clearly demonstrates how a data-driven approach combined with machine learning algorithms can provide hidden patterns of not only LOP itself but also the additional information about the lithology, the stress history and the geographical frequency of exploration.",
      "year": 2019,
      "venue": "Day 2 Tue, May 07, 2019",
      "authors": [
        "J. Choi",
        "E. Skurtveit",
        "L. Grande"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7488bd1a98fd5d366a9076287e014321bbb17674",
      "pdf_url": "",
      "publication_date": "2019-04-26",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a391a74b82dbb86b120c78976f21546c08d7116e",
      "title": "Analysis of Information Leakage from MCU using Neural Network",
      "abstract": "Electromagnetic leakage from an operating microcontroller unit (MCU) can be intercepted and analyzed to deduce the instructions being processed. This is helpful to understanding eavesdropping and to later protecting against it. In this work, harnessing a deep neural network, we analyze the massive electromagnetic (EM) leakage information to extract the instructions run in a microcontroller unit (MCU). An EM leakage acquisition environment is built, and the leakage signal of an MCU is collected. A multi-layer convolutional neural network is constructed for the side channel analysis and identification. The recognition accuracy is over 95% in the training phase, and more than 75% in the prediction phase. This experiment proves that the electromagnetic leakage emitted can be collected by appropriate methods and analyzed by deep learning technology. It can effectively deduce the instructions running in an MCU, which lays a foundation for the protection against eavesdropping.",
      "year": 2019,
      "venue": "International Workshop on Electromagnetic Compatibility of Integrated Circuits",
      "authors": [
        "Siping Gao",
        "Yingkai Guo",
        "Z. Aung",
        "Yongxin Guo"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a391a74b82dbb86b120c78976f21546c08d7116e",
      "pdf_url": "",
      "publication_date": "2019-10-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "089c6224cfbcf5c18b63564eb65001c7c42a7acf",
      "title": "Knockoff Nets: Stealing Functionality of Black-Box Models",
      "abstract": "Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such ``victim'' models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we study complex victim blackbox models, and an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a ``knockoff'' with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as show that a reasonable knockoff of an image analysis API could be created for as little as $30.",
      "year": 2018,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 587,
      "url": "https://www.semanticscholar.org/paper/089c6224cfbcf5c18b63564eb65001c7c42a7acf",
      "pdf_url": "http://arxiv.org/pdf/1812.02766",
      "publication_date": "2018-12-06",
      "keywords_matched": [
        "knockoff nets"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4582e2350e4822834dcf266522690722dd4430d4",
      "title": "PRADA: Protecting Against DNN Model Stealing Attacks",
      "abstract": "Machine learning (ML) applications are increasingly prevalent. Protecting the confidentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to find transferable adversarial examples that can evade classification by the original model. Access to the model can be restricted to be only via well-defined prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API. In this paper, we describe new model extraction attacks using novel approaches for generating synthetic queries, and optimizing training hyperparameters. Our attacks outperform state-of-the-art model extraction in terms of transferability of both targeted and non-targeted adversarial examples (up to +29-44 percentage points, pp), and prediction accuracy (up to +46 pp) on two datasets. We provide take-aways on how to perform effective model extraction attacks. We then propose PRADA, the first step towards generic and effective detection of DNN model extraction attacks. It analyzes the distribution of consecutive API queries and raises an alarm when this distribution deviates from benign behavior. We show that PRADA can detect all prior model extraction attacks with no false positives.",
      "year": 2018,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Mika Juuti",
        "Sebastian Szyller",
        "A. Dmitrenko",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 474,
      "url": "https://www.semanticscholar.org/paper/4582e2350e4822834dcf266522690722dd4430d4",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8790377/8806708/08806737.pdf",
      "publication_date": "2018-05-07",
      "keywords_matched": [
        "model stealing attack",
        "model extraction attack",
        "model extraction",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "669a04d8bbf8d530a8d6e1900f8b63dd906b4050",
      "title": "I Know What You See: Power Side-Channel Attack on Convolutional Neural Network Accelerators",
      "abstract": "Deep learning has become the de-facto computational paradigm for various kinds of perception problems, including many privacy-sensitive applications such as online medical image analysis. No doubt to say, the data privacy of these deep learning systems is a serious concern. Different from previous research focusing on exploiting privacy leakage from deep learning models, in this paper, we present the first attack on the implementation of deep learning models. To be specific, we perform the attack on an FPGA-based convolutional neural network accelerator and we manage to recover the input image from the collected power traces without knowing the detailed parameters in the neural network. For the MNIST dataset, our power side-channel attack is able to achieve up to 89% recognition accuracy.",
      "year": 2018,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Lingxiao Wei",
        "Yannan Liu",
        "Bo Luo",
        "Yu LI",
        "Qiang Xu"
      ],
      "citation_count": 217,
      "url": "https://www.semanticscholar.org/paper/669a04d8bbf8d530a8d6e1900f8b63dd906b4050",
      "pdf_url": "https://arxiv.org/pdf/1803.05847",
      "publication_date": "2018-03-05",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "35c863e151e47b6dbf6356e9a1abaa2a0eeab3fc",
      "title": "Exploring Connections Between Active Learning and Model Extraction",
      "abstract": "Machine learning is being increasingly used by individuals, research institutions, and corporations. This has resulted in the surge of Machine Learning-as-a-Service (MLaaS) - cloud services that provide (a) tools and resources to learn the model, and (b) a user-friendly query interface to access the model. However, such MLaaS systems raise privacy concerns such as model extraction. In model extraction attacks, adversaries maliciously exploit the query interface to steal the model. More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface. This attack was introduced by Tramer et al. at the 2016 USENIX Security Symposium, where practical attacks for various models were shown. We believe that better understanding the efficacy of model extraction attacks is paramount to designing secure MLaaS systems. To that end, we take the first step by (a) formalizing model extraction and discussing possible defense strategies, and (b) drawing parallels between model extraction and established area of active learning. In particular, we show that recent advancements in the active learning domain can be used to implement powerful model extraction attacks, and investigate possible defense strategies.",
      "year": 2018,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Varun Chandrasekaran",
        "Kamalika Chaudhuri",
        "Irene Giacomelli",
        "S. Jha",
        "Songbai Yan"
      ],
      "citation_count": 176,
      "url": "https://www.semanticscholar.org/paper/35c863e151e47b6dbf6356e9a1abaa2a0eeab3fc",
      "pdf_url": "",
      "publication_date": "2018-11-05",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f5014e34ed13191082cd20cc279ca4cc9adee84f",
      "title": "Stealing Neural Networks via Timing Side Channels",
      "abstract": "Deep learning is gaining importance in many applications. However, Neural Networks face several security and privacy threats. This is particularly significant in the scenario where Cloud infrastructures deploy a service with Neural Network model at the back end. Here, an adversary can extract the Neural Network parameters, infer the regularization hyperparameter, identify if a data point was part of the training data, and generate effective transferable adversarial examples to evade classifiers. This paper shows how a Neural Network model is susceptible to timing side channel attack. In this paper, a black box Neural Network extraction attack is proposed by exploiting the timing side channels to infer the depth of the network. Although, constructing an equivalent architecture is a complex search problem, it is shown how Reinforcement Learning with knowledge distillation can effectively reduce the search space to infer a target model. The proposed approach has been tested with VGG architectures on CIFAR10 data set. It is observed that it is possible to reconstruct substitute models with test accuracy close to the target models and the proposed approach is scalable and independent of type of Neural Network architectures.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Vasisht Duddu",
        "D. Samanta",
        "D. V. Rao",
        "V. Balas"
      ],
      "citation_count": 146,
      "url": "https://www.semanticscholar.org/paper/f5014e34ed13191082cd20cc279ca4cc9adee84f",
      "pdf_url": "",
      "publication_date": "2018-12-31",
      "keywords_matched": [
        "neural network extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "42155d5c8c6e46ef26a0d7212123128a364b07a0",
      "title": "Defending Against Model Stealing Attacks Using Deceptive Perturbations",
      "abstract": null,
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Taesung Lee",
        "Ben Edwards",
        "Ian Molloy",
        "D. Su"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/42155d5c8c6e46ef26a0d7212123128a364b07a0",
      "pdf_url": "",
      "publication_date": "2018-05-31",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e003d8021ba35b842fc75c5b854d37f82db91ed5",
      "title": "A Complete Workflow for Automatic Forward Kinematics Model Extraction of Robotic Total Stations Using the Denavit-Hartenberg Convention",
      "abstract": "Development and verification of real-time algorithms for robotic total stations usually require hard-ware-in-the-loop approaches, which can be complex and time-consuming. Simulator-in-the-loop can be used instead, but the design of a simulation environment and sufficient detailed modeling of the hardware are required. Typically, device specification and calibration data are provided by the device manufacturers and are used by the device drivers. However, geometric models of robotic total stations cannot be used directly with existing ro-botic simulators. Model details are often treated as company secrets, and no source code of device drivers is available to the public. In this paper, we present a complete workflow for automatic geometric model extraction of robotic total stations using the Denavit-Hartenberg convention. We provide a complete set of Denavit-Hartenberg parameters for an exemplary ro-botic total station. These parameters can be used in existing robotic simulators without modifications. Furthermore, we analyze the difference between the extracted geometric model, the calibrated model, which is used by the device drivers, and the standard spherical representation for 3D point measurements of the device.",
      "year": 2018,
      "venue": "Journal of Intelligent and Robotic Systems",
      "authors": [
        "Christoph Klug",
        "D. Schmalstieg",
        "T. Gloor",
        "Clemens Arth"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/e003d8021ba35b842fc75c5b854d37f82db91ed5",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10846-018-0931-4.pdf",
      "publication_date": "2018-09-21",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2cb2f7c824a089422ac210b3f77d2e078b017f66",
      "title": "Model Extraction and Active Learning",
      "abstract": null,
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Varun Chandrasekaran",
        "Kamalika Chaudhuri",
        "Irene Giacomelli",
        "S. Jha",
        "Songbai Yan"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/2cb2f7c824a089422ac210b3f77d2e078b017f66",
      "pdf_url": "",
      "publication_date": "2018-11-05",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7e7157625088e62582f45409cb1704844ddb5535",
      "title": "Poisoning Machine Learning Based Wireless IDSs via Stealing Learning Model",
      "abstract": null,
      "year": 2018,
      "venue": "Wireless Algorithms, Systems, and Applications",
      "authors": [
        "Pan Li",
        "Wentao Zhao",
        "Qiang Liu",
        "Xiao Liu",
        "Linyuan Yu"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/7e7157625088e62582f45409cb1704844ddb5535",
      "pdf_url": "",
      "publication_date": "2018-06-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "93ee45cc18b168f4e51481f9b95516d9556e8cef",
      "title": "Prediction of the Force on a Projectile in an Electromagnetic Launcher Coil with Multilayer Neural Network",
      "abstract": "The force on the projectile in the electromagnetic launchers varies according to the the excitation value and the position of the projectile in the winding. In this study, 3D model of coil and projectile used in electromagnetic launchers have been created and analyzed by finite element method. The force characteristic on the projectile has been obtained by changing the excitation value of the winding and the position of the projectile using parametric solution method. In finite element analysis, more accurate analysis can be performed by defining smaller solution steps. However, the analysis time is prolonged due to the increase in the number of variables. Taking into consideration the duration of analysis, the force prediction has been carried out using multilayer neural network models consisting of one hidden layer and two hidden layers. Successful results have been obtained in the force prediction studies with multilayer neural networks.",
      "year": 2018,
      "venue": "Sakarya University Journal of Computer and Information Sciences",
      "authors": [
        "A. Dalcal\u0131",
        "Onursal \u00c7etin",
        "C. Ocak",
        "Feyzullah Temurta\u015f"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/93ee45cc18b168f4e51481f9b95516d9556e8cef",
      "pdf_url": "http://saucis.sakarya.edu.tr/en/download/article-file/600139",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "68564f11e79c19195d0e82854a0aa156d7764922",
      "title": "Interpreting Blackbox Models via Model Extraction",
      "abstract": "Interpretability has become incredibly important as machine learning is increasingly used to inform consequential decisions. We propose to construct global explanations of complex, blackbox models in the form of a decision tree approximating the original model---as long as the decision tree is a good approximation, then it mirrors the computation performed by the blackbox model. We devise a novel algorithm for extracting decision tree explanations that actively samples new training points to avoid overfitting. We evaluate our algorithm on a random forest to predict diabetes risk and a learned controller for cart-pole. Compared to several baselines, our decision trees are both substantially more accurate and equally or more interpretable based on a user study. Finally, we describe several insights provided by our interpretations, including a causal issue validated by a physician.",
      "year": 2017,
      "venue": "arXiv.org",
      "authors": [
        "Osbert Bastani",
        "Carolyn Kim",
        "Hamsa Bastani"
      ],
      "citation_count": 181,
      "url": "https://www.semanticscholar.org/paper/68564f11e79c19195d0e82854a0aa156d7764922",
      "pdf_url": "",
      "publication_date": "2017-05-23",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "181fe8787937dbf7abf886042855be1bc6149f80",
      "title": "Model Extraction Warning in MLaaS Paradigm",
      "abstract": "Machine learning models deployed on the cloud are susceptible to several security threats including extraction attacks. Adversaries may abuse a model's prediction API to steal the model thus compromising model confidentiality, privacy of training data, and revenue from future query payments. This work introduces a model extraction monitor that quantifies the extraction status of models by continually observing the API query and response streams of users. We present two novel strategies that measure either the information gain or the coverage of the feature space spanned by user queries to estimate the learning rate of individual and colluding adversaries. Both approaches have low computational overhead and can easily be offered as services to model owners to warn them against state of the art extraction attacks. We demonstrate empirical performance results of these approaches for decision tree and neural network models using open source datasets and BigML MLaaS platform.",
      "year": 2017,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "M. Kesarwani",
        "B. Mukhoty",
        "V. Arya",
        "S. Mehta"
      ],
      "citation_count": 153,
      "url": "https://www.semanticscholar.org/paper/181fe8787937dbf7abf886042855be1bc6149f80",
      "pdf_url": "https://arxiv.org/pdf/1711.07221",
      "publication_date": "2017-11-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a9007cf4c4ec7e4fc956bead7008a3605451de49",
      "title": "Interpretability via Model Extraction",
      "abstract": "The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.",
      "year": 2017,
      "venue": "arXiv.org",
      "authors": [
        "Osbert Bastani",
        "Carolyn Kim",
        "Hamsa Bastani"
      ],
      "citation_count": 133,
      "url": "https://www.semanticscholar.org/paper/a9007cf4c4ec7e4fc956bead7008a3605451de49",
      "pdf_url": "",
      "publication_date": "2017-06-29",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "090b9f33bf878f7630e7715c1cd2897df6d89fbd",
      "title": "Symbolic Model Extraction for Web Application Verification",
      "abstract": null,
      "year": 2017,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Ivan Bocic",
        "T. Bultan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/090b9f33bf878f7630e7715c1cd2897df6d89fbd",
      "pdf_url": "",
      "publication_date": "2017-05-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8d1192d8f79827f00fa3efe392e5b4a165f5ef8d",
      "title": "Assessing the Performance of Automated Model Extraction Rules",
      "abstract": null,
      "year": 2017,
      "venue": "Integrated Spatial Databases",
      "authors": [
        "Jorge Echeverr\u00eda",
        "Francisca P\u00e9rez",
        "\u00d3. Pastor",
        "Carlos Cetina"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/8d1192d8f79827f00fa3efe392e5b4a165f5ef8d",
      "pdf_url": "https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1150&context=isd2014",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "963098553570745043ba198ffb12996f1a17ab95",
      "title": "A neural network identifier for electromagnetic thermotherapy systems",
      "abstract": null,
      "year": 2017,
      "venue": "",
      "authors": [
        "C. Tai",
        "Wei-Cheng Wang",
        "Yuan Jui Hsu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/963098553570745043ba198ffb12996f1a17ab95",
      "pdf_url": "",
      "publication_date": "2017-03-14",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e",
      "title": "Stealing Machine Learning Models via Prediction APIs",
      "abstract": "Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (\"predictive analytics\") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. \nThe tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., \"steal\") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures.",
      "year": 2016,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Florian Tram\u00e8r",
        "Fan Zhang",
        "A. Juels",
        "M. Reiter",
        "Thomas Ristenpart"
      ],
      "citation_count": 1947,
      "url": "https://www.semanticscholar.org/paper/8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e",
      "pdf_url": "",
      "publication_date": "2016-08-10",
      "keywords_matched": [
        "model extraction attack",
        "stealing machine learning model",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4bcd67d7dd2f5fd18986c8c28f254e9a36af6fe7",
      "title": "A Reference Architecture for Online Performance Model Extraction in Virtualized Environments",
      "abstract": null,
      "year": 2016,
      "venue": "ICPE Companion",
      "authors": [
        "Simon Spinner",
        "J. Walter",
        "Samuel Kounev"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/4bcd67d7dd2f5fd18986c8c28f254e9a36af6fe7",
      "pdf_url": "",
      "publication_date": "2016-03-12",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3547c3fefa6bfdf07b71893a7d2894189e397529",
      "title": "High Dimensional Electromagnetic Interference Signal Clustering Based On SOM Neural Network",
      "abstract": "In this paper, we study the spectral characteristics and global representations of strongly nonlinear, non-stationary electromagnetic interferences (EMI), which is of great significance in analysing the mathematical modelling of electromagnetic capability (EMC) for a large scale integrated system. We firstly propose to use Self-Organizing Feature Map Neural Network (SOM) to cluster EMI signals. To tackle with the high dimensionality of EMI signals, we combine the dimension reduction and clustering approaches, and find out the global features of different interference factors, in order to finally provide precise mathematical simulation models for EMC design, analysis, forecasting and evaluation. Experimental results have demonstrated the validity and effectiveness of the proposed method.",
      "year": 2016,
      "venue": "",
      "authors": [
        "Hongyi Li",
        "Di Zhao",
        "S. Xu",
        "Pidong Wang",
        "Jiaxin Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3547c3fefa6bfdf07b71893a7d2894189e397529",
      "pdf_url": "https://doi.org/10.7251/els1620027l",
      "publication_date": "2016-07-15",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "185f579ac2b43f7adba2b7ccf5a429052bf062f1",
      "title": "Context-Specific Metabolic Model Extraction Based on Regularized Least Squares Optimization",
      "abstract": "Genome-scale metabolic models have proven highly valuable in investigating cell physiology. Recent advances include the development of methods to extract context-specific models capable of describing metabolism under more specific scenarios (e.g., cell types). Yet, none of the existing computational approaches allows for a fully automated model extraction and determination of a flux distribution independent of user-defined parameters. Here we present RegrEx, a fully automated approach that relies solely on context-specific data and \u21131-norm regularization to extract a context-specific model and to provide a flux distribution that maximizes its correlation to data. Moreover, the publically available implementation of RegrEx was used to extract 11 context-specific human models using publicly available RNAseq expression profiles, Recon1 and also Recon2, the most recent human metabolic model. The comparison of the performance of RegrEx and its contending alternatives demonstrates that the proposed method extracts models for which both the structure, i.e., reactions included, and the flux distributions are in concordance with the employed data. These findings are supported by validation and comparison of method performance on additional data not used in context-specific model extraction. Therefore, our study sets the ground for applications of other regularization techniques in large-scale metabolic modeling.",
      "year": 2015,
      "venue": "PLoS ONE",
      "authors": [
        "Semid\u00e1n Robaina Est\u00e9vez",
        "Z. Nikoloski"
      ],
      "citation_count": 55,
      "url": "https://www.semanticscholar.org/paper/185f579ac2b43f7adba2b7ccf5a429052bf062f1",
      "pdf_url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0131875&type=printable",
      "publication_date": "2015-07-09",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "41b409788d637d25b28fced1411ecc80a5b46b29",
      "title": "Nitrate and Sulfate Estimations in Water Sources Using a Planar Electromagnetic Sensor Array and Artificial Neural Network Method",
      "abstract": null,
      "year": 2015,
      "venue": "IEEE Sensors Journal",
      "authors": [
        "A. N. M. Nor",
        "M. Faramarzi",
        "M. Yunus",
        "S. Ibrahim"
      ],
      "citation_count": 49,
      "url": "https://www.semanticscholar.org/paper/41b409788d637d25b28fced1411ecc80a5b46b29",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "62835b529bce481e0e51128463c4176a087c9cdc",
      "title": "A new nonlinear model extraction methodology for GaN HEMTs subject to trapping effects",
      "abstract": null,
      "year": 2015,
      "venue": "2015 IEEE MTT-S International Microwave Symposium",
      "authors": [
        "L. Nunes",
        "J. M. Gomes",
        "P. Cabral",
        "J. Pedro"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/62835b529bce481e0e51128463c4176a087c9cdc",
      "pdf_url": "",
      "publication_date": "2015-05-17",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "58341bdbc16d5b0f4624e1109c670b74a1268421",
      "title": "FinFET Centric Variability-Aware Compact Model Extraction and Generation Technology Supporting DTCO",
      "abstract": null,
      "year": 2015,
      "venue": "IEEE Transactions on Electron Devices",
      "authors": [
        "Xingsheng Wang",
        "B. Cheng",
        "D. Reid",
        "A. Pender",
        "P. Asenov",
        "C. Millar",
        "A. Asenov"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/58341bdbc16d5b0f4624e1109c670b74a1268421",
      "pdf_url": "https://eprints.gla.ac.uk/108434/1/108434.pdf",
      "publication_date": "2015-08-14",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "be73148fe884f5e824cffb5046781524eb6a602e",
      "title": "Room model extraction device, room model extraction system, room model extraction program, and room model extraction method",
      "abstract": null,
      "year": 2015,
      "venue": "",
      "authors": [
        "\u82f1\u4e4b \u7802\u7530"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/be73148fe884f5e824cffb5046781524eb6a602e",
      "pdf_url": "",
      "publication_date": "2015-03-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "605e8d2f41607c531a632ed928bc3997ea016567",
      "title": "Generalized framework for context-specific metabolic model extraction methods",
      "abstract": "Genome-scale metabolic models (GEMs) are increasingly applied to investigate the physiology not only of simple prokaryotes, but also eukaryotes, such as plants, characterized with compartmentalized cells of multiple types. While genome-scale models aim at including the entirety of known metabolic reactions, mounting evidence has indicated that only a subset of these reactions is active in a given context, including: developmental stage, cell type, or environment. As a result, several methods have been proposed to reconstruct context-specific models from existing genome-scale models by integrating various types of high-throughput data. Here we present a mathematical framework that puts all existing methods under one umbrella and provides the means to better understand their functioning, highlight similarities and differences, and to help users in selecting a most suitable method for an application.",
      "year": 2014,
      "venue": "Frontiers in Plant Science",
      "authors": [
        "Semid\u00e1n Robaina Est\u00e9vez",
        "Z. Nikoloski"
      ],
      "citation_count": 82,
      "url": "https://www.semanticscholar.org/paper/605e8d2f41607c531a632ed928bc3997ea016567",
      "pdf_url": "https://www.frontiersin.org/articles/10.3389/fpls.2014.00491/pdf",
      "publication_date": "2014-09-19",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "25d378877b59eb00f838f277fb4cb4529c9b0153",
      "title": "Coupled electromagnetic/thermal machine design optimization based on finite element analysis with application of artificial neural network",
      "abstract": null,
      "year": 2014,
      "venue": "European Conference on Cognitive Ergonomics",
      "authors": [
        "Wenying Jiang",
        "T. Jahns"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/25d378877b59eb00f838f277fb4cb4529c9b0153",
      "pdf_url": "",
      "publication_date": "2014-11-13",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d84c0d4dbab81757490213742b1e75dc258bd3ba",
      "title": "Feature model extraction from large collections of informal product descriptions",
      "abstract": null,
      "year": 2013,
      "venue": "ESEC/FSE 2013",
      "authors": [
        "J. Davril",
        "Edouard Delfosse",
        "N. Hariri",
        "M. Acher",
        "J. Cleland-Huang",
        "P. Heymans"
      ],
      "citation_count": 158,
      "url": "https://www.semanticscholar.org/paper/d84c0d4dbab81757490213742b1e75dc258bd3ba",
      "pdf_url": "https://hal.inria.fr/hal-00859475/file/fse13main-id221-p-18686-final.pdf",
      "publication_date": "2013-08-18",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e9c3fad3993b9942cc0523e6d064fe3d420ff07e",
      "title": "Ontology-Based Partial Building Information Model Extraction",
      "abstract": null,
      "year": 2013,
      "venue": "Journal of computing in civil engineering",
      "authors": [
        "Le Zhang",
        "R. Issa"
      ],
      "citation_count": 97,
      "url": "https://www.semanticscholar.org/paper/e9c3fad3993b9942cc0523e6d064fe3d420ff07e",
      "pdf_url": "",
      "publication_date": "2013-11-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    }
  ]
}