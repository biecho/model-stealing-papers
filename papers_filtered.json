{
  "updated": "2026-01-05",
  "total": 403,
  "keywords": [
    "model stealing",
    "model extraction",
    "model theft",
    "steal model",
    "stealing model",
    "extract model",
    "model stealing attack",
    "model extraction attack",
    "neural network extraction attack",
    "knockoff nets",
    "knockoff net",
    "copycat CNN",
    "copycat model",
    "imitation attack",
    "clone model",
    "cloning attack",
    "stealing machine learning",
    "steal ML model",
    "steal ML models",
    "steal neural network",
    "DNN model stealing",
    "DNN extraction",
    "stealing deep learning",
    "LLM stealing",
    "LLM extraction",
    "stealing language model",
    "stealing functionality",
    "functionality stealing",
    "black-box model stealing",
    "blackbox model extraction",
    "model stealing defense",
    "model extraction defense",
    "prevent model stealing",
    "protect model extraction",
    "side-channel model extraction",
    "side-channel neural network",
    "timing attack neural network",
    "cache attack DNN",
    "power analysis neural network",
    "electromagnetic neural network",
    "DNN weights leakage",
    "neural network weight extraction",
    "reverse engineer neural network",
    "reverse engineering DNN",
    "cryptanalytic extraction neural",
    "API model extraction",
    "query-based model stealing",
    "prediction API stealing"
  ],
  "seed_papers": [
    "Tram\u00e8r 2016 - Stealing ML Models",
    "Knockoff Nets 2018",
    "Thieves on Sesame Street 2019",
    "CloudLeak 2020",
    "ActiveThief 2020",
    "Cryptanalytic Extraction 2020",
    "DeepSniffer 2020",
    "Stealing via Timing Side Channels 2018",
    "PRADA 2018",
    "Entangled Watermarks 2020",
    "Prediction Poisoning 2019",
    "Survey: Stealing ML Models 2022"
  ],
  "note": "Filtered to include only papers primarily about model stealing/extraction",
  "papers": [
    {
      "paper_id": "eead229e073ee9f1221144e60ee2d90a7174eaf2",
      "title": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns",
      "abstract": "Large Language Models (LLMs) have raised increasing concerns about their misuse in generating hate speech. Among all the efforts to address this issue, hate speech detectors play a crucial role. However, the effectiveness of different detectors against LLM-generated hate speech remains largely unknown. In this paper, we propose HateBench, a framework for benchmarking hate speech detectors on LLM-generated hate speech. We first construct a hate speech dataset of 7,838 samples generated by six widely-used LLMs covering 34 identity groups, with meticulous annotations by three labelers. We then assess the effectiveness of eight representative hate speech detectors on the LLM-generated dataset. Our results show that while detectors are generally effective in identifying LLM-generated hate speech, their performance degrades with newer versions of LLMs. We also reveal the potential of LLM-driven hate campaigns, a new threat that LLMs bring to the field of hate speech detection. By leveraging advanced techniques like adversarial attacks and model stealing attacks, the adversary can intentionally evade the detector and automate hate campaigns online. The most potent adversarial attack achieves an attack success rate of 0.966, and its attack efficiency can be further improved by $13-21\\times$ through model stealing attacks with acceptable attack performance. We hope our study can serve as a call to action for the research community and platform moderators to fortify defenses against these emerging threats.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyue Shen",
        "Yixin Wu",
        "Y. Qu",
        "Michael Backes",
        "Savvas Zannettou",
        "Yang Zhang"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/eead229e073ee9f1221144e60ee2d90a7174eaf2",
      "pdf_url": "",
      "publication_date": "2025-01-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "37ef78b60cb09e136e13df735fc58ade8348d671",
      "title": "Finding the PISTE: Towards Understanding Privacy Leaks in Vertical Federated Learning Systems",
      "abstract": "Vertical Federated Learning (VFL) is a collaborative learning paradigm where participants share the same sample space while splitting the feature space. In VFL, local participants host their bottom models for feature extraction and collaboratively train a classifier by exchanging intermediate results with the server owning the labels. Both local training data and bottom models contain privacy-sensitive information and are considered the intellectual property of each participant, and thus should be protected by the design of VFL. Our study exposes the fundamental susceptibility of VFL systems to privacy leaks, which arise from the collaboration between the server and clients during both training and testing. Based on our findings, we propose PISTE, a model-agnostic framework of privacy stealing attacks against VFL. PISTE delivers three privacy inference attacks, i.e., model stealing, data reconstruction, and property inference attacks on five benchmark datasets and four different model architectures. We further discuss four potential countermeasures. Experimental results show that all of them cannot prevent all three privacy stealing attacks in PISTE. In summary, our study demonstrates the inherent yet rarely uncovered vulnerability of VFL on leaking data and model privacy.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Xiangru Xu",
        "Wei Wang",
        "Zheng Chen",
        "Bin Wang",
        "Chao Li",
        "Li Duan",
        "Zhen Han",
        "Yufei Han"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/37ef78b60cb09e136e13df735fc58ade8348d671",
      "pdf_url": "",
      "publication_date": "2025-03-01",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7044d075fba8c188f716a25618d522c808a67a96",
      "title": "A Survey of Model Extraction Attacks and Defenses in Distributed Computing Environments",
      "abstract": "Model Extraction Attacks (MEAs) threaten modern machine learning systems by enabling adversaries to steal models, exposing intellectual property and training data. With the increasing deployment of machine learning models in distributed computing environments, including cloud, edge, and federated learning settings, each paradigm introduces distinct vulnerabilities and challenges. Without a unified perspective on MEAs across these distributed environments, organizations risk fragmented defenses, inadequate risk assessments, and substantial economic and privacy losses. This survey is motivated by the urgent need to understand how the unique characteristics of cloud, edge, and federated deployments shape attack vectors and defense requirements. We systematically examine the evolution of attack methodologies and defense mechanisms across these environments, demonstrating how environmental factors influence security strategies in critical sectors such as autonomous vehicles, healthcare, and financial services. By synthesizing recent advances in MEAs research and discussing the limitations of current evaluation practices, this survey provides essential insights for developing robust and adaptive defense strategies. Our comprehensive approach highlights the importance of integrating protective measures across the entire distributed computing landscape to ensure the secure deployment of machine learning models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/7044d075fba8c188f716a25618d522c808a67a96",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model extraction",
        "steal model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f94fe2776e4258650baffb9b0100518076aacdad",
      "title": "Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment",
      "abstract": "Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on image classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-Steal), the first stealing attack against medical MLLMs. ADA-Steal relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yaling Shen",
        "Zhixiong Zhuang",
        "Kun Yuan",
        "Maria-Irina Nicolae",
        "N. Navab",
        "N. Padoy",
        "Mario Fritz"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f94fe2776e4258650baffb9b0100518076aacdad",
      "pdf_url": "",
      "publication_date": "2025-02-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "253a59d979d560456c2984742466b796a983da0e",
      "title": "ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have gained traction in Graph-based Machine Learning as a Service (GMLaaS) platforms, yet they remain vulnerable to graph-based model extraction attacks (MEAs), where adversaries reconstruct surrogate models by querying the victim model. Existing defense mechanisms, such as watermarking and fingerprinting, suffer from poor real-time performance, susceptibility to evasion, or reliance on post-attack verification, making them inadequate for handling the dynamic characteristics of graph-based MEA variants. To address these limitations, we propose ATOM, a novel real-time MEA detection framework tailored for GNNs. ATOM integrates sequential modeling and reinforcement learning to dynamically detect evolving attack patterns, while leveraging k-core embedding to capture the structural properties, enhancing detection precision. Furthermore, we provide theoretical analysis to characterize query behaviors and optimize detection strategies. Extensive experiments on multiple real-world datasets demonstrate that ATOM outperforms existing approaches in detection performance, maintaining stable across different time steps, thereby offering a more effective defense mechanism for GMLaaS environments. Our source code is available at https://github.com/LabRAI/ATOM.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Zhan Cheng",
        "Bolin Shen",
        "Tianming Sha",
        "Yuan Gao",
        "Shibo Li",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/253a59d979d560456c2984742466b796a983da0e",
      "pdf_url": "",
      "publication_date": "2025-03-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "95b993a3b281c920589fb5b158ff07009ff628b9",
      "title": "CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable utility across diverse applications, and their growing complexity has made Machine Learning as a Service (MLaaS) a viable platform for scalable deployment. However, this accessibility also exposes GNN to serious security threats, most notably model extraction attacks (MEAs), in which adversaries strategically query a deployed model to construct a high-fidelity replica. In this work, we evaluate the vulnerability of GNNs to MEAs and explore their potential for cost-effective model acquisition in non-adversarial research settings. Importantly, adaptive node querying strategies can also serve a critical role in research, particularly when labeling data is expensive or time-consuming. By selectively sampling informative nodes, researchers can train high-performing GNNs with minimal supervision, which is particularly valuable in domains such as biomedicine, where annotations often require expert input. To address this, we propose a node querying strategy tailored to a highly practical yet underexplored scenario, where bulk queries are prohibited, and only a limited set of initial nodes is available. Our approach iteratively refines the node selection mechanism over multiple learning cycles, leveraging historical feedback to improve extraction efficiency. Extensive experiments on benchmark graph datasets demonstrate our superiority over comparable baselines on accuracy, fidelity, and F1 score under strict query-size constraints. These results highlight both the susceptibility of deployed GNNs to extraction attacks and the promise of ethical, efficient GNN acquisition methods to support low-resource research environments.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zebin Wang",
        "Menghan Lin",
        "Bolin Shen",
        "Ken Anderson",
        "Molei Liu",
        "Tianxi Cai",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/95b993a3b281c920589fb5b158ff07009ff628b9",
      "pdf_url": "",
      "publication_date": "2025-06-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "26be7a4a13776ac194912a70e97783bf2e587c24",
      "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models",
      "abstract": "Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt-targeted attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/26be7a4a13776ac194912a70e97783bf2e587c24",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ef83ddc62cfc6a7caa37ea87a985eed7a9fede85",
      "title": "Balancing Security and Efficiency in GAI-Driven Semantic Communication: Challenges, Solutions, and Future Paths",
      "abstract": "The convergence of artificial intelligence (AI) and wireless communications has driven the emergence of semantic communication (SC), a paradigm that prioritizes context-aware semantic exchange over traditional bit-level transmission. Although enhancing efficiency and task-specific reliability, this advancing capability is accompanied by significant security challenges that remain underexplored. In this paper, we provide an overview of security challenges in SC systems, with a particular focus on the confidentiality, integrity, and availability of the wireless transmission and generative AI (GAI) models. To defend against risks of model confidentiality compromise and semantic feature leakage, we propose a solution integrating trusted execution environments (TEEs) for secure model inference and adversarial cryptography for the protection of semantics over realistic wireless channels. Test results show it achieves close-to-black-box attack resistance in model stealing effectiveness, and the BLEU scores of eavesdropping attackers are effectively reduced to below 0.1 across various SNR levels. Finally, we discuss potential open issues and solutions for enhancing the SC security, paving the way for future research in this critical area. The proposed framework demonstrates promising results in enhancing both model and data confidentiality, contributing to the development of secure SC systems for 6G networks.",
      "year": 2025,
      "venue": "IEEE Network",
      "authors": [
        "Qianyun Zhang",
        "Jiting Shi",
        "Weihao Zeng",
        "Xinyu Xu",
        "Zhenyu Guan",
        "Shufeng Li",
        "Zhijing Qin"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/ef83ddc62cfc6a7caa37ea87a985eed7a9fede85",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4c167947e7fe8fd78118504627930f1935b11924",
      "title": "MER-Inspector: Assessing Model Extraction Risks from An Attack-Agnostic Perspective",
      "abstract": "Information leakage issues in machine learning-based Web applications have attracted increasing attention. While the risk of data privacy leakage has been rigorously analyzed, the theory of model function leakage, known as Model Extraction Attacks (MEAs), has not been well studied. In this paper, we are the first to understand MEAs theoretically from an attack-agnostic perspective and to propose analytical metrics for evaluating model extraction risks. By using the Neural Tangent Kernel (NTK) theory, we formulate the linearized MEA as a regularized kernel classification problem and then derive the fidelity gap and generalization error bounds of the attack performance. Based on these theoretical analyses, we propose a new theoretical metric called Model Recovery Complexity (MRC), which measures the distance of weight changes between the victim and surrogate models to quantify risk. Additionally, we find that victim model accuracy, which shows a strong positive correlation with model extraction risk, can serve as an empirical metric. By integrating these two metrics, we propose a framework, namely Model Extraction Risk Inspector (MER-Inspector), to compare the extraction risks of models under different model architectures by utilizing relative metric values. We conduct extensive experiments on 16 model architectures and 5 datasets. The experimental results demonstrate that the proposed metrics have a high correlation with model extraction risks, and MER-Inspector can accurately compare the extraction risks of any two models with up to 89.58%.",
      "year": 2025,
      "venue": "The Web Conference",
      "authors": [
        "Xinwei Zhang",
        "Haibo Hu",
        "Qingqing Ye",
        "Li Bai",
        "Huadi Zheng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4c167947e7fe8fd78118504627930f1935b11924",
      "pdf_url": "",
      "publication_date": "2025-04-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a1dbeef30b37363111eb815ff7fd2a0b8e7da83c",
      "title": "Adversarial Autoencoder based Model Extraction Attacks for Collaborative DNN Inference at Edge",
      "abstract": "Deep neural networks (DNNs) are influencing a wide range of applications from safety-critical to security-sensitive use cases. In many such use cases, the DNN inference process relies on distributed systems involving IoT devices and edge/cloud severs as participants where a pre-trained DNN model is partitioned/split onto multiple parts and the participants collaboratively execute them. However, often such collaboration requires dynamic DNN partitioning information to be exchanged among the participants over unsecured network or via relays/hops which can lead to novel privacy vulnerabilities. In this paper, we propose a DNN model extraction attack that exploits such vulnerabilities to not only extract the original input data, but also reconstruct the entire victim DNN model. Specifically, the proposed attack model utilizes extracted/leaked data and adversarial autoencoders to generate and train a shadow model that closely mimics the behavior of the original victim model. The proposed attack is query-free and does not require the attacker to have any prior information about the victim model and input data. Using an IoT-edge hardware testbed running collaborative DNN inference, we demonstrate the effectiveness of the proposed attack model in extracting the victim model with high levels of certainty across many realistic scenarios.",
      "year": 2025,
      "venue": "IEEE/IFIP Network Operations and Management Symposium",
      "authors": [
        "Manal Zneit",
        "Xiaojie Zhang",
        "Motahare Mounesan",
        "S. Debroy"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a1dbeef30b37363111eb815ff7fd2a0b8e7da83c",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0d3384cb78be25ed28a3544f175cab59236093dd",
      "title": "HoneypotNet: Backdoor Attacks Against Model Extraction",
      "abstract": "Model extraction attacks are one type of inference-time attacks that approximate the functionality and performance of a black-box victim model by launching a certain number of queries to the model and then leveraging the model's predictions to train a substitute model. These attacks pose severe security threats to production models and MLaaS platforms and could cause significant monetary losses to the model owners. A body of work has proposed to defend machine learning models against model extraction attacks, including both active defense methods that modify the model's outputs or increase the query overhead to avoid extraction and passive defense methods that detect malicious queries or leverage watermarks to perform post-verification. In this work, we introduce a new defense paradigm called attack as defense which modifies the model's output to be poisonous such that any malicious users that attempt to use the output to train a substitute model will be poisoned. To this end, we propose a novel lightweight backdoor attack method dubbed HoneypotNet that replaces the classification layer of the victim model with a honeypot layer and then fine-tunes the honeypot layer with a shadow model (to simulate model extraction) via bi-level optimization to modify its output to be poisonous while remaining the original performance. We empirically demonstrate on four commonly used benchmark datasets that HoneypotNet can inject backdoors into substitute models with a high success rate. The injected backdoor not only facilitates ownership verification but also disrupts the functionality of substitute models, serving as a significant deterrent to model extraction attacks.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yixu Wang",
        "Tianle Gu",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0d3384cb78be25ed28a3544f175cab59236093dd",
      "pdf_url": "",
      "publication_date": "2025-01-02",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f961eb51aed4d1db5b246ac4cc842e0faa820ca2",
      "title": "TSQP: Safeguarding Real-Time Inference for Quantization Neural Networks on Edge Devices",
      "abstract": "Quantization Neural Networks (QNNs) has been widely adopted in resource-constrained edge devices due to their real-time capabilities and low resource requirement. However, concerns have arisen regarding that deployed models are white-box available to model thefts. To address this issue, TEE-shielded secure inference has been introduced as a secure and efficient solution. Nevertheless, existing methods neglect the compatibility with 8-bit quantized computation, which leads to severe integer overflow issue during inference. This issue could result a disastrous degradation in QNNs (to random guessing level), completely destroying model utility. Moreover, the model confidentiality and inference integrity also face a substantial threat due to the limited data representation space. To safeguard accurate and efficient inference for QNNs, TEE-Shielded QNN Partition (TSQP) are proposed, which presents three key insights: Firstly, Quantization Manager is designed to convert white-box inference to black-box by shielding critical scales in TEE. Additionally, overflow concerns are effectively addressed using reduced-range approaches. Secondly, by leveraging the Information Bottleneck theory to enhance model training, we introduce Parameter De-Similarity to defend against powerful Model Stealing attacks that existing methods are vulnerable to. Thirdly, the Integrity Monitor is suggested to detect inference integrity breaches in an oblivious manner. In contrast, existing method can be bypassed due to the lack of obliviousness. Experimental results demonstrate that proposed TSQP maintains high accuracy and achieves accurate integrity breaches detection. Our method achieves more than $8\\times$ speedup compared to full TEE inference, while reducing Model Stealing attacks accuracy from $3.99\\times$ to $1.29\\times$. To our best knowledge, proposed method is the first TEE-shielded secure inference solution that achieves model confidentiality, inference integrity and model utility on QNNs.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yu Sun",
        "Gaojian Xiong",
        "Jianhua Liu",
        "Zheng Liu",
        "Jian Cui"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f961eb51aed4d1db5b246ac4cc842e0faa820ca2",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "369d7792462ab184ed6dc53cab70b9b101d9d034",
      "title": "Sim4Rec: Data-Free Model Extraction Attack on Sequential Recommendation",
      "abstract": "Model extraction attack shows promising performance in revealing sequential recommendation (SeqRec) robustness, e.g., as an upstream task of transfer-based attack to provide optimization feedback for downstream attacks. However, existing work either heavily relies on impractical prior knowledge or has impressive attack performance. In this paper, we focus on data-free model extraction attack on SeqRec, which aims to efficiently train a surrogate model that closely imitates the target model in a practical setting. Conducting such an attack is challenging. First, imitating sequential training data for accurate model extraction is hard without prior knowledge. Second, limited queries for the target model require the attack to be efficient. To address these challenges, we propose a novel adversarial framework Sim4Rec which includes two modules, i.e., controllable sequence generation and reinforced adversarial distillation. The former allows a sequential generator to produce synthetic data similar to training data through pre-training with controllable generated samples. The latter efficiently extracts the target model via reinforced adversarial knowledge distillation. Extensive experiments demonstrate the advancement of Sim4Rec.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yihao Wang",
        "Jiajie Su",
        "Chaochao Chen",
        "Meng Han",
        "Chi Zhang",
        "Jun Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/369d7792462ab184ed6dc53cab70b9b101d9d034",
      "pdf_url": "",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1b5caff453174871e8c9b374b72659a7c63fa830",
      "title": "CopyQNN: Quantum Neural Network Extraction Attack under Varying Quantum Noise",
      "abstract": "Quantum Neural Networks (QNNs) have shown significant value across domains, with well-trained QNNs representing critical intellectual property often deployed via cloud-based QNN-as-a-Service (QNNaaS) platforms. Recent work has examined QNN model extraction attacks using classical and emerging quantum strategies. These attacks involve adversaries querying QNNaaS platforms to obtain labeled data for training local substitute QNNs that replicate the functionality of cloud-based models. However, existing approaches have largely over-looked the impact of varying quantum noise inherent in noisy intermediate-scale quantum (NISQ) computers, limiting their effectiveness in real-world settings. To address this limitation, we propose the CopyQNN framework, which employs a three-step data cleaning method to eliminate noisy data based on its noise sensitivity. This is followed by the integration of contrastive and transfer learning within the quantum domain, enabling efficient training of substitute QNNs using a limited but cleaned set of queried data. Experimental results on NISQ computers demonstrate that a practical implementation of CopyQNN significantly outperforms state-of-the-art QNN extraction attacks, achieving an average performance improvement of 8.73% across all tasks while reducing the number of required queries by 90\u00d7, with only a modest increase in hardware overhead.",
      "year": 2025,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhenxiao Fu",
        "Leyi Zhao",
        "Xuhong Zhang",
        "Yilun Xu",
        "Gang Huang",
        "Fan Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1b5caff453174871e8c9b374b72659a7c63fa830",
      "pdf_url": "",
      "publication_date": "2025-04-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "neural network extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e569cd7a6612a86e89c2e09c36f75fdcce6dd453",
      "title": "Delving into Cryptanalytic Extraction of PReLU Neural Networks",
      "abstract": "The machine learning problem of model extraction was first introduced in 1991 and gained prominence as a cryptanalytic challenge starting with Crypto 2020. For over three decades, research in this field has primarily focused on ReLU-based neural networks. In this work, we take the first step towards the cryptanalytic extraction of PReLU neural networks, which employ more complex nonlinear activation functions than their ReLU counterparts. We propose a raw output-based parameter recovery attack for PReLU networks and extend it to more restrictive scenarios where only the top-m probability scores are accessible. Our attacks are rigorously evaluated through end-to-end experiments on diverse PReLU neural networks, including models trained on the MNIST dataset. To the best of our knowledge, this is the first practical demonstration of PReLU neural network extraction across three distinct attack scenarios.",
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Yi Chen",
        "Xiaoyang Dong",
        "Ruijie Ma",
        "Yan Shen",
        "Anyu Wang",
        "Hongbo Yu",
        "Xiaoyun Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/e569cd7a6612a86e89c2e09c36f75fdcce6dd453",
      "pdf_url": "",
      "publication_date": "2025-09-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0daf186b202c94263c68c6ffd19c3539c080a1f2",
      "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives",
      "abstract": "Machine learning (ML) models have significantly grown in complexity and utility, driving advances across multiple domains. However, substantial computational resources and specialized expertise have historically restricted their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have addressed these barriers by providing scalable, convenient, and affordable access to sophisticated ML models through user-friendly APIs. While this accessibility promotes widespread use of advanced ML capabilities, it also introduces vulnerabilities exploited through Model Extraction Attacks (MEAs). Recent studies have demonstrated that adversaries can systematically replicate a target model's functionality by interacting with publicly exposed interfaces, posing threats to intellectual property, privacy, and system security. In this paper, we offer a comprehensive survey of MEAs and corresponding defense strategies. We propose a novel taxonomy that classifies MEAs according to attack mechanisms, defense approaches, and computing environments. Our analysis covers various attack techniques, evaluates their effectiveness, and highlights challenges faced by existing defenses, particularly the critical trade-off between preserving model utility and ensuring security. We further assess MEAs within different computing paradigms and discuss their technical, ethical, legal, and societal implications, along with promising directions for future research. This systematic survey aims to serve as a valuable reference for researchers, practitioners, and policymakers engaged in AI security and privacy. Additionally, we maintain an online repository continuously updated with related literature at https://github.com/kzhao5/ModelExtractionPapers.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/0daf186b202c94263c68c6ffd19c3539c080a1f2",
      "pdf_url": "",
      "publication_date": "2025-08-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c544b7021fc25bcd828d1721da82d25e8a16b407",
      "title": "Information Extraction from Lumbar Spine MRI Radiology Reports Using GPT4: Accuracy and Benchmarking Against Research-Grade Comprehensive Scoring",
      "abstract": "Background/Objectives: This study aimed to create a pipeline for standardized data extraction from lumbar-spine MRI radiology reports using a large language model (LLM) and assess the agreement of the extracted data with research-grade semi-quantitative scoring. Methods: We included a subset of data from a multi-site NIH-funded cohort study of chronic low back pain (cLBP) participants. After initial prompt development, a secure application programming interface (API) deployment of OpenAIs GPT-4 was used to extract different classes of pathology from the clinical radiology report. Unsupervised UMAP and agglomerative clustering of the pathology terms\u2019 embeddings provided insight into model comprehension for optimized prompt design. Model extraction was benchmarked against human extraction (gold standard) with F1 scores and false-positive and false-negative rates (FPR/FNR). Then, an expert MSK radiologist provided comprehensive research-grade scores of the images, and agreement with report-extracted data was calculated using Cohen\u2019s kappa. Results: Data from 230 patients with cLBP were included (mean age 53.2 years, 54% women). The overall model performance for extracting data from clinical reports was excellent, with a mean F1 score of 0.96 across pathologies. The mean FPR was marginally higher than the FNR (5.1% vs. 3.0%). Agreement with comprehensive scoring was moderate (kappa 0.424), and the underreporting of lateral recess stenosis (FNR 63.6%) and overreporting of disc pathology (FPR 42.7%) were noted. Conclusions: LLMs can accurately extract highly detailed information on lumbar spine imaging pathologies from radiology reports. Moderate agreement between the LLM and comprehensive scores underscores the need for less subjective, machine-based data extraction from imaging.",
      "year": 2025,
      "venue": "Diagnostics",
      "authors": [
        "Katharina Ziegeler",
        "Virginie Kreutzinger",
        "Michelle W Tong",
        "C. T. Chin",
        "E. Bahroos",
        "Po-Hung Wu",
        "Noah B. Bonnheim",
        "Aaron J Fields",
        "Jeffrey C. Lotz",
        "Thomas M Link",
        "S. Majumdar"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/c544b7021fc25bcd828d1721da82d25e8a16b407",
      "pdf_url": "",
      "publication_date": "2025-04-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e9534b0f74ff371aa086ecc30d95a633ecad7ddc",
      "title": "Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses",
      "abstract": "The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ganghua Wang",
        "Yuhong Yang",
        "Jie Ding"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e9534b0f74ff371aa086ecc30d95a633ecad7ddc",
      "pdf_url": "",
      "publication_date": "2025-02-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "90360529f01cf996d62369fd0c47af3b1823c7f4",
      "title": "Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning",
      "abstract": "Federated Learning (FL) is a collaborative learning framework designed to protect client data, yet it remains highly vulnerable to Intellectual Property (IP) threats. Model extraction (ME) attack poses a significant risk to Machine-Learning-as-a-Service (MLaaS) platforms, enabling attackers to replicate confidential models by querying Black-Box (without internal insight) APIs. Despite FL\u2019s privacy-preserving goals, its distributed nature makes it particularly susceptible to such attacks. This paper examines the vulnerability of the FL-based victim model to two types of model extraction attacks. For various federated clients built under NVFlare platform, we implemented ME attack across two deep-learning architectures and three image datasets. We evaluate the proposed ME attack performance using various metrics, including accuracy, fidelity, and KL divergence. The experiments show that for various FL clients, the accuracy and fidelity of the extraction model are closely related to the size of the attack query set. Additionally, we explore a transfer learning-based approach where pre-trained models serve as the starting point for the extraction process. The results indicate that the accuracy and fidelity of the fine-tuned pre-trained extraction models are notably higher, particularly with smaller query sets, highlighting potential advantages for attackers.",
      "year": 2025,
      "venue": "International Conference on Wireless Communications and Mobile Computing",
      "authors": [
        "Sayyed Farid Ahamed",
        "Sandip Roy",
        "Soumya Banerjee",
        "Marc Vucovich",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "E. Bowen",
        "Sachin Shetty"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/90360529f01cf996d62369fd0c47af3b1823c7f4",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "268ce1e9492455e825a3894b0f2713d14e376d36",
      "title": "ADAGE: Active Defenses Against GNN Extraction",
      "abstract": "Graph Neural Networks (GNNs) achieve high performance in various real-world applications, such as drug discovery, traffic states prediction, and recommendation systems. The fact that building powerful GNNs requires a large amount of training data, powerful computing resources, and human expertise turns the models into lucrative targets for model stealing attacks. Prior work has revealed that the threat vector of stealing attacks against GNNs is large and diverse, as an attacker can leverage various heterogeneous signals ranging from node labels to high-dimensional node embeddings to create a local copy of the target GNN at a fraction of the original training costs. This diversity in the threat vector renders the design of effective and general defenses challenging and existing defenses usually focus on one particular stealing setup. Additionally, they solely provide means to identify stolen model copies rather than preventing the attack. To close this gap, we propose the first and general Active Defense Against GNN Extraction (ADAGE). ADAGE builds on the observation that stealing a model's full functionality requires highly diverse queries to leak its behavior across the input space. Our defense monitors this query diversity and progressively perturbs outputs as the accumulated leakage grows. In contrast to prior work, ADAGE can prevent stealing across all common attack setups. Our extensive experimental evaluation using six benchmark datasets, four GNN models, and three types of adaptive attackers shows that ADAGE penalizes attackers to the degree of rendering stealing impossible, whilst preserving predictive performance on downstream tasks. ADAGE, thereby, contributes towards securely sharing valuable GNNs in the future.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jing Xu",
        "Franziska Boenisch",
        "Adam Dziedzic"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/268ce1e9492455e825a3894b0f2713d14e376d36",
      "pdf_url": "",
      "publication_date": "2025-02-27",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dd898d4226db4c4dca01e46de96fe3acbc6087ec",
      "title": "A Method for Extracting Black Box Models Based on Interpretable Attention",
      "abstract": "Deep neural networks have achieved remarkable success in face recognition. However, their vulnerability has attracted considerable attention. Researchers can analyse the weaknesses of face recognition models by extracting their functionality, aiming to enhance the security performance of these models. The findings of the study reveal that current model extraction methods are afflicted with notable drawbacks, namely low similarity in capturing model functionality and insufficient availability of samples. These limitations significantly impede the analysis of model security performance. We propose an interpretable attention\u2010based method for black\u2010box model extraction, enhancing the similarity between substitute and victim model functionality. Our main contributions are summarized as follows: (i) This study addresses the issue of limited sample training caused by the restricted number of black\u2010box hard label queries. (ii) By applying input perturbations, we obtain feedback from deep black\u2010box models, enabling us to identify facial local regions and the distribution of feature weights that positively influence predictions. (iii) By normalizing the feature weight distribution matrix and associating it with the attention weight matrix, the construction of an attention mask for the dataset is achieved, enabling differential attention to features in different regions. (iv) Leveraging a pre\u2010trained base model, we extract relevant knowledge and features, facilitating cross\u2010domain knowledge transfer. Experiments on Emore, PubFig and CASIA\u2010WebFace show that our method outperforms traditional methods by 10%\u201320% in model consistency for the same query budget. Also, our method achieves the highest model stealing consistency on the three datasets: 94.51%, 93.27% and 91.74%, respectively.",
      "year": 2025,
      "venue": "Expert Syst. J. Knowl. Eng.",
      "authors": [
        "Lijun Gao",
        "Huibin Tian",
        "Kai Liu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/dd898d4226db4c4dca01e46de96fe3acbc6087ec",
      "pdf_url": "",
      "publication_date": "2025-06-03",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6678e884da10e0d6cd763811069a813700b685a6",
      "title": "Exploring Query Efficient Data Generation Towards Data-Free Model Stealing in Hard Label Setting",
      "abstract": "Data-free model stealing involves replicating the functionality of a target model into a substitute model without accessing the target model's structure, parameters, or training data. Instead, the adversary can only access the target model's predictions for generated samples. Once the substitute model closely approximates the behavior of the target model, attackers can exploit its white-box characteristics for subsequent malicious activities, such as adversarial attacks. Existing methods within cooperative game frameworks often produce samples with high confidence for the prediction of the substitute model, which makes it difficult for the substitute model to replicate the behavior of the target model. This paper presents a new data-free model stealing approach called Query Efficient Data Generation (QEDG). We introduce two distinct loss functions to ensure the generation of sufficient samples that closely and uniformly align with the target model's decision boundary across multiple classes. Building on the limitation of current methods, which typically yield only one piece of supervised information per query, we propose the query-free sample augmentation that enables the acquisition of additional supervised information without increasing the number of queries. Motivated by theoretical analysis, we adopt the consistency rate metric, which more accurately evaluates the similarity between the substitute and target models. We conducted extensive experiments to verify the effectiveness of our proposed method, which achieved better performance with fewer queries compared to the state-of-the-art methods on the real MLaaS scenario and five datasets.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Gaozheng Pei",
        "Shaojie Lyu",
        "Ke Ma",
        "Pinci Yang",
        "Qianqian Xu",
        "Yingfei Sun"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6678e884da10e0d6cd763811069a813700b685a6",
      "pdf_url": "",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6a9d7ffd4856883122048dacf19c149f7ad92a85",
      "title": "Examining the Threat Landscape: Foundation Models and Model Stealing",
      "abstract": "Foundation models (FMs) for computer vision learn rich and robust representations, enabling their adaptation to task/domain-specific deployments with little to no fine-tuning. However, we posit that the very same strength can make applications based on FMs vulnerable to model stealing attacks. Through empirical analysis, we reveal that models fine-tuned from FMs harbor heightened susceptibility to model stealing, compared to conventional vision architectures like ResNets. We hypothesize that this behavior is due to the comprehensive encoding of visual patterns and features learned by FMs during pre-training, which are accessible to both the attacker and the victim. We report that an attacker is able to obtain 94.28% agreement (matched predictions with victim) for a Vision Transformer based victim model (ViT-L/16) trained on CIFAR-10 dataset, compared to only 73.20% agreement for a ResNet-18 victim, when using ViT-L/16 as the thief model. We arguably show, for the first time, that utilizing FMs for downstream tasks may not be the best choice for deployment in commercial APIs due to their susceptibility to model theft. We thereby alert model owners towards the associated security risks, and highlight the need for robust security measures to safeguard such models against theft. Code is available at https://github.com/rajankita/foundation_model_stealing.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ankita Raj",
        "Deepankar Varma",
        "Chetan Arora"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6a9d7ffd4856883122048dacf19c149f7ad92a85",
      "pdf_url": "",
      "publication_date": "2025-02-25",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f99ae2cfcb94dc21f4089be22c3b6daaa65eeeb9",
      "title": "MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models",
      "abstract": "Model extraction attacks aim to replicate the functionality of a black-box model through query access, threatening the intellectual property (IP) of machine-learning-as-a-service (MLaaS) providers. Defending against such attacks is challenging, as it must balance efficiency, robustness, and utility preservation in the real-world scenario. Despite the recent advances, most existing defenses presume that attacker queries have out-of-distribution (OOD) samples, enabling them to detect and disrupt suspicious inputs. However, this assumption is increasingly unreliable, as modern models are trained on diverse datasets and attackers often operate under limited query budgets. As a result, the effectiveness of these defenses is significantly compromised in realistic deployment scenarios. To address this gap, we propose MISLEADER (enseMbles of dIStiLled modEls Against moDel ExtRaction), a novel defense strategy that does not rely on OOD assumptions. MISLEADER formulates model protection as a bilevel optimization problem that simultaneously preserves predictive fidelity on benign inputs and reduces extractability by potential clone models. Our framework combines data augmentation to simulate attacker queries with an ensemble of heterogeneous distilled models to improve robustness and diversity. We further provide a tractable approximation algorithm and derive theoretical error bounds to characterize defense effectiveness. Extensive experiments across various settings validate the utility-preserving and extraction-resistant properties of our proposed defense strategy. Our code is available at https://github.com/LabRAI/MISLEADER.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Xueqi Cheng",
        "Minxing Zheng",
        "Shixiang Zhu",
        "Yushun Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f99ae2cfcb94dc21f4089be22c3b6daaa65eeeb9",
      "pdf_url": "",
      "publication_date": "2025-06-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6971c737329185c98d62432dcbcbd36c03c04c6b",
      "title": "Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?",
      "abstract": "Deep Neural Networks (DNNs) have attracted significant attention, and their internal models are now considered valuable intellectual assets. Extracting these internal models through access to a DNN is conceptually similar to extracting a secret key via oracle access to a block cipher. Consequently, cryptanalytic techniques, particularly differential-like attacks, have been actively explored recently. ReLU-based DNNs are the most commonly and widely deployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024) assume access to exact output logits, which are usually invisible, more recent works (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting, where only the final classification result (e.g.,\"dog\"or\"car\") is available to the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that model extraction is feasible in polynomial time even under this restricted setting. In this paper, we first show that the assumptions underlying their attack become increasingly unrealistic as the attack-target depth grows. In practice, satisfying these assumptions requires an exponential number of queries with respect to the attack depth, implying that the attack does not always run in polynomial time. To address this critical limitation, we propose a novel attack method called CrossLayer Extraction. Instead of directly extracting the secret parameters (e.g., weights and biases) of a specific neuron, which incurs exponential cost, we exploit neuron interactions across layers to extract this information from deeper layers. This technique significantly reduces query complexity and mitigates the limitations of existing model extraction approaches.",
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Akira Ito",
        "Takayuki Miura",
        "Yosuke Todo"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6971c737329185c98d62432dcbcbd36c03c04c6b",
      "pdf_url": "",
      "publication_date": "2025-10-08",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "cb626a77f1e0c634d557ca88af22547f21f16afa",
      "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses",
      "abstract": "Graph-structured data, which captures non-Euclidean relationships and interactions between entities, is growing in scale and complexity. As a result, training state-of-the-art graph machine learning (GML) models have become increasingly resource-intensive, turning these models and data into invaluable Intellectual Property (IP). To address the resource-intensive nature of model training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an efficient solution by leveraging third-party cloud services for model development and management. However, deploying such models in GMLaaS also exposes them to potential threats from attackers. Specifically, while the APIs within a GMLaaS system provide interfaces for users to query the model and receive outputs, they also allow attackers to exploit and steal model functionalities or sensitive training data, posing severe threats to the safety of these GML models and the underlying graph data. To address these challenges, this survey systematically introduces the first taxonomy of threats and defenses at the level of both GML model and graph-structured data. Such a tailored taxonomy facilitates an in-depth understanding of GML IP protection. Furthermore, we present a systematic evaluation framework to assess the effectiveness of IP protection methods, introduce a curated set of benchmark datasets across various domains, and discuss their application scopes and future challenges. Finally, we establish an open-sourced versatile library named PyGIP, which evaluates various attack and defense techniques in GMLaaS scenarios and facilitates the implementation of existing benchmark methods. The library resource can be accessed at: https://labrai.github.io/PyGIP. We believe this survey will play a fundamental role in intellectual property protection for GML and provide practical recipes for the GML community.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Lincan Li",
        "Bolin Shen",
        "Chenxi Zhao",
        "Yuxiang Sun",
        "Kaixiang Zhao",
        "Shirui Pan",
        "Yushun Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/cb626a77f1e0c634d557ca88af22547f21f16afa",
      "pdf_url": "",
      "publication_date": "2025-08-27",
      "keywords_matched": [
        "steal model",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "098c756f393dc1c3fafa8348fabc3a97410f4473",
      "title": "Navigating the Deep: Signature Extraction on Deep Neural Networks",
      "abstract": "Neural network model extraction has emerged in recent years as an important security concern, as adversaries attempt to recover a network's parameters via black-box queries. A key step in this process is signature extraction, which aims to recover the absolute values of the network's weights layer by layer. Prior work, notably by Carlini et al. (2020), introduced a technique inspired by differential cryptanalysis to extract neural network parameters. However, their method suffers from several limitations that restrict its applicability to networks with a few layers only. Later works focused on improving sign extraction, but largely relied on the assumption that signature extraction itself was feasible. In this work, we revisit and refine the signature extraction process by systematically identifying and addressing for the first time critical limitations of Carlini et al.'s signature extraction method. These limitations include rank deficiency and noise propagation from deeper layers. To overcome these challenges, we propose efficient algorithmic solutions for each of the identified issues, greatly improving the efficiency of signature extraction. Our approach permits the extraction of much deeper networks than was previously possible. We validate our method through extensive experiments on ReLU-based neural networks, demonstrating significant improvements in extraction depth and accuracy. For instance, our extracted network matches the target network on at least 95% of the input space for each of the eight layers of a neural network trained on the CIFAR-10 dataset, while previous works could barely extract the first three layers. Our results represent a crucial step toward practical attacks on larger and more complex neural network architectures.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Haolin Liu",
        "Adrien Siproudhis",
        "Samuel Experton",
        "Peter Lorenz",
        "Christina Boura",
        "Thomas Peyrin"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/098c756f393dc1c3fafa8348fabc3a97410f4473",
      "pdf_url": "",
      "publication_date": "2025-06-20",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4a1c5a240e9ca7572f7d1545710f45c3df5f7b54",
      "title": "Unlocking High-Fidelity Learning: Towards Neuron-Grained Model Extraction",
      "abstract": "Model extraction (ME) attacks replicate valuable closed-box machine learning (ML) models via malicious query interactions. Cutting-edge attacks focus on actively designing query samples to enhance model fidelity and imprudently adhere to the standard ML training approach. This causes a deviation from the true objective of learning a model over a task. In this article, we innovatively shift our focus from query selection to training process optimization, aiming to boost the similarity of the copy model with the victim model from neuron to model level. We leverage neuron matching theory to attain this objective and develop a general training booster framework, MEBooster, to fully exploit this theory. MEBooster comprises an initial bootstrapping phase that furnishes initial parameters and an optimal model architecture, followed by a post-processing phase that employs fine-tuning for enhanced neuron matching. Notably, MEBooster can seamlessly integrate with all existing model extraction attacks, enhancing their overall performance. Performance evaluation shows up to 58.10% fidelity gain in image classification. From a defender's perspective, we introduce a novel defensive strategy called Stochastic Norm Enlargement (SNE) to mitigate the risk of such attacks by enlarging the model parameters\u2019 norm property in training. Performance evaluation shows up to 58.81% extractability (i.e., fidelity) reduction.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Yaxin Xiao",
        "Haibo Hu",
        "Qingqing Ye",
        "Li Tang",
        "Zi Liang",
        "Huadi Zheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4a1c5a240e9ca7572f7d1545710f45c3df5f7b54",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "51f750009d83c9112509bed3946077b032d78ccc",
      "title": "Measuring the Vulnerability Disclosure Policies of AI Vendors",
      "abstract": "As AI is increasingly integrated into products and critical systems, researchers are paying greater attention to identifying related vulnerabilities. Effective remediation depends on whether vendors are willing to accept and respond to AI vulnerability reports. In this paper, we examine the disclosure policies of 264 AI vendors. Using a mixed-methods approach, our quantitative analysis finds that 36% of vendors provide no disclosure channel, and only 18% explicitly mention AI-related risks. Vulnerabilities involving data access, authorization, and model extraction are generally considered in-scope, while jailbreaking and hallucination are frequently excluded. Through qualitative analysis, we further identify three vendor postures toward AI vulnerabilities - proactive clarification (n = 46, include active supporters, AI integrationists, and back channels), silence (n = 115, include self-hosted and hosted vendors), and restrictive (n = 103). Finally, by comparing vendor policies against 1,130 AI incidents and 359 academic publications, we show that bug bounty policy evolution has lagged behind both academic research and real-world events.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yangheran Piao",
        "Jingjie Li",
        "Daniel W. Woods"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/51f750009d83c9112509bed3946077b032d78ccc",
      "pdf_url": "",
      "publication_date": "2025-09-07",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "57be40e5b844e37895cadcc5f9b729ecdc59b69d",
      "title": "TensorShield: Safeguarding On-Device Inference by Shielding Critical DNN Tensors with TEE",
      "abstract": "To safeguard user data privacy, on-device inference has emerged as a prominent paradigm on mobile and Internet of Things (IoT) devices. This paradigm involves deploying a model provided by a third party on local devices to perform inference tasks. However, it exposes the private model to two primary security threats: model stealing (MS) and membership inference attacks (MIA). To mitigate these risks, existing wisdom deploys models within Trusted Execution Environments (TEEs), which is a secure isolated execution space. Nonetheless, the constrained secure memory capacity in TEEs makes it challenging to achieve full model security with low inference latency. This paper fills the gap with TensorShield, the first efficient on-device inference work that shields partial tensors of the model while still fully defending against MS and MIA. The key enabling techniques in TensorShield include: (i) a novel eXplainable AI (XAI) technique exploits the model's attention transition to assess critical tensors and shields them in TEE to achieve secure inference, and (ii) two meticulous designs with critical feature identification and latency-aware placement to accelerate inference while maintaining security. Extensive evaluations show that TensorShield delivers almost the same security protection as shielding the entire model inside TEE, while being up to 25.35\u00d7 (avg. 5.85\u00d7) faster than the state-of-the-art work, without accuracy loss.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Tong Sun",
        "Bowen Jiang",
        "Hailong Lin",
        "Borui Li",
        "Yixiao Teng",
        "Yi Gao",
        "Wei Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/57be40e5b844e37895cadcc5f9b729ecdc59b69d",
      "pdf_url": "",
      "publication_date": "2025-05-28",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bbd216e8ca69d2583e1d203a4fdf8e9bf4e07f3d",
      "title": "Inside the Mind of an Attacker: Review Sistematika Tujuan Pencurian Machine Learning Model",
      "abstract": "Abstrak - Pencurian model (model stealing) menjadi salah satu ancaman serius dalam penerapan machine learning modern, terutama pada layanan berbasis API dan cloud. Artikel ini mengulas secara sistematik berbagai tujuan di balik serangan pencurian model untuk memahami motif penyerang dan implikasinya bagi pengembang sistem. Metode penulisan berupa kajian literatur terkini yang mengklasifikasikan tujuan pencurian ke dalam delapan kategori utama: (1) pencurian properti internal seperti arsitektur, bobot, dan hyperparameter; (2) peniruan perilaku model untuk menghasilkan efektivitas setara dan konsistensi prediksi pada data normal maupun adversarial; (3) transfer pengetahuan untuk distillation dan deployment ringan; (4) serangan privasi berupa membership inference dan model inversion; (5) monetisasi dengan menjual model bajakan atau menyediakan layanan API ilegal; (6) pencurian kemampuan pertahanan adversarial untuk meningkatkan efektivitas serangan; (7) spionase industri untuk reverse engineering model pesaing; serta (8) penghindaran regulasi dengan mencuri model yang sudah tersertifikasi. Review ini menegaskan bahwa ancaman pencurian model tidak hanya merugikan secara teknis, tetapi juga membuka peluang eksploitasi ekonomi ilegal, kebocoran data sensitif, dan persaingan usaha tidak sehat. Pemahaman yang detail atas ragam tujuan ini diharapkan mendorong perancang sistem untuk mengembangkan strategi pertahanan yang lebih cermat dan menyeluruh.Kata kunci: Machine Learning; Model Stealing; API; Adversarial; Transfer Pengetahuan;\u00a0Abstract - Model stealing has become one of the most serious threats in modern machine learning applications, especially in API- and cloud-based services. This article systematically reviews the various objectives behind model stealing attacks to understand the attackers\u2019 motivations and their implications for system developers. The writing method is a current literature review that classifies model stealing objectives into eight main categories: (1) theft of internal properties such as architecture, weights, and hyperparameters; (2) imitation of model behavior to achieve comparable effectiveness and prediction consistency on both normal and adversarial data; (3) knowledge transfer for distillation and lightweight deployment; (4) privacy attacks through membership inference and model inversion; (5) monetization by selling stolen models or offering illegal API services; (6) stealing adversarial robustness to improve attack effectiveness; (7) industrial espionage for reverse engineering competitor models; and (8) regulatory evasion by stealing pre-certified models. This review emphasizes that model stealing threats are not merely technical issues but also open opportunities for illegal economic exploitation, leakage of sensitive data, and unfair business competition. A detailed understanding of these diverse objectives is expected to encourage system designers to develop more careful and comprehensive defense strategies.Keywords: Machine Learning; Model Stealing; API; Adversarial; Knowledge Transfer;",
      "year": 2025,
      "venue": "Jurnal Nasional Komputasi dan Teknologi Informasi (JNKTI)",
      "authors": [
        "Mulkan Fadhli"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bbd216e8ca69d2583e1d203a4fdf8e9bf4e07f3d",
      "pdf_url": "",
      "publication_date": "2025-07-27",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2e50f4dc488356303b0f76a1db35d5e672ef3b28",
      "title": "Attackers Can Do Better: Over- and Understated Factors of Model Stealing Attacks",
      "abstract": "Machine learning (ML) models were shown to be vulnerable to model stealing attacks, which lead to intellectual property infringement. Among other attack methods, substitute model training is an all-encompassing attack applicable to any machine learning model whose behaviour can be approximated from input-output queries. Whereas previous works mainly focused on improving the performance of substitute models by, e.g. developing a new substitute training method, there have been only limited ablation studies that try to understand the impact the strength of an attacker has on the substitute model's performance. As a result, different authors came to diverse, sometimes contradicting, conclusions. In this work, we exhaustively examine the ambivalent influence of different factors resulting from varying the attacker's capabilities and knowledge on a substitute training attack. Our findings suggest that some of the factors that have been considered important in the past are, in fact, not that influential; instead, we discover new correlations between attack conditions and success rate. In particular, we demonstrate that better-performing target models enable higher-fidelity attacks and explain the intuition behind this phenomenon. Further, we propose to shift the focus from the complexity of target models toward the complexity of their learning tasks. Therefore, for the substitute model, rather than aiming for a higher architecture complexity, we suggest focusing on getting data of higher complexity and an appropriate architecture. Finally, we demonstrate that even in the most limited data-free scenario, there is no need to overcompensate weak knowledge with unrealistic capabilities in the form of millions of queries. Our results often exceed or match the performance of previous attacks that assume a stronger attacker, suggesting that these stronger attacks are likely endangering a model owner's intellectual property to a significantly higher degree than shown until now.",
      "year": 2025,
      "venue": "2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "Andreas Rauber"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2e50f4dc488356303b0f76a1db35d5e672ef3b28",
      "pdf_url": "",
      "publication_date": "2025-03-08",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b066eaa019bf6e94540dc54e735defb19d910bf8",
      "title": "Stealix: Model Stealing via Prompt Evolution",
      "abstract": "Model stealing poses a significant security risk in machine learning by enabling attackers to replicate a black-box model without access to its training data, thus jeopardizing intellectual property and exposing sensitive information. Recent methods that use pre-trained diffusion models for data synthesis improve efficiency and performance but rely heavily on manually crafted prompts, limiting automation and scalability, especially for attackers with little expertise. To assess the risks posed by open-source pre-trained models, we propose a more realistic threat model that eliminates the need for prompt design skills or knowledge of class names. In this context, we introduce Stealix, the first approach to perform model stealing without predefined prompts. Stealix uses two open-source pre-trained models to infer the victim model's data distribution, and iteratively refines prompts through a genetic algorithm, progressively improving the precision and diversity of synthetic images. Our experimental results demonstrate that Stealix significantly outperforms other methods, even those with access to class names or fine-grained prompts, while operating under the same query budget. These findings highlight the scalability of our approach and suggest that the risks posed by pre-trained generative models in model stealing may be greater than previously recognized.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhixiong Zhuang",
        "Hui-Po Wang",
        "Maria-Irina Nicolae",
        "Mario Fritz"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b066eaa019bf6e94540dc54e735defb19d910bf8",
      "pdf_url": "",
      "publication_date": "2025-06-06",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5ad7f669afa84855e00c743422026c8c5e91e411",
      "title": "I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks",
      "abstract": "Model stealing attacks endanger the confidentiality of machine learning models offered as a service. Although these models are kept secret, a malicious party can query a model to label data samples and train their own substitute model, violating intellectual property. While novel attacks in the field are continually being published, their design and evaluations are not standardised, making it challenging to compare prior works and assess progress in the field. This paper is the first to address this gap by providing recommendations for designing and evaluating model stealing attacks. To this end, we study the largest group of attacks that rely on training a substitute model -- those attacking image classification models. We propose the first comprehensive threat model and develop a framework for attack comparison. Further, we analyse attack setups from related works to understand which tasks and models have been studied the most. Based on our findings, we present best practices for attack development before, during, and beyond experiments and derive an extensive list of open research questions regarding the evaluation of model stealing attacks. Our findings and recommendations also transfer to other problem domains, hence establishing the first generic evaluation methodology for model stealing attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "Kathrin Grosse",
        "Andreas Rauber"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5ad7f669afa84855e00c743422026c8c5e91e411",
      "pdf_url": "",
      "publication_date": "2025-08-29",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1d3483a45263d2f67dedba087ff267194c7cfa1d",
      "title": "Model-Guardian: Protecting against Data-Free Model Stealing Using Gradient Representations and Deceptive Predictions",
      "abstract": "Model stealing attack is increasingly threatening the confidentiality of machine learning models deployed in the cloud. Recent studies reveal that adversaries can exploit data synthesis techniques to steal machine learning models even in scenarios devoid of real data, leading to data-free model stealing attacks. Existing defenses against such attacks suffer from limitations, including poor effectiveness, insufficient generalization ability, and low comprehensiveness. In response, this paper introduces a novel defense framework named Model-Guardian. Comprising two components, Data-Free Model Stealing Detector (DFMS-Detector) and Deceptive Predictions (DPreds), Model-Guardian is designed to address the shortcomings of current defenses with the help of the artifact properties of synthetic samples and gradient representations of samples. Extensive experiments on seven prevalent data-free model stealing attacks showcase the effectiveness and superior generalization ability of Model-Guardian, outperforming eleven defense methods and establishing a new state-of-the-art performance. Notably, this work pioneers the utilization of various GANs and diffusion models for generating highly realistic query samples in attacks, with Model-Guardian demonstrating accurate detection capabilities.",
      "year": 2025,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Yu Xuan",
        "Zhendong Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1d3483a45263d2f67dedba087ff267194c7cfa1d",
      "pdf_url": "",
      "publication_date": "2025-03-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16cfca0d74d329b2b3f2d927047468f09300e33e",
      "title": "CERBEROS: Compression-Based Efficient and Robust Optimized Security for Model Stealing Defense",
      "abstract": "Model stealing attacks pose an increasing threat to the confidentiality and intellectual property of artificial intelligence (AI) models. Existing defenses\u2013such as query monitoring, output perturbation, multi-model output variation, and post hoc verification\u2013fall short in on-device applications where models must run under strict memory and computation budgets. These approaches typically incur high memory or latency overhead due to their reliance on auxiliary models or additional inference-time processing. To address these limitations, we propose CERBEROS, a defense framework designed to achieve security against model stealing with deployability in resource-constrained environments. At its core, CERBEROS introduces a novel neural architecture with multiple classification heads trained jointly for output diversification, while sharing a single feature extraction backbone to minimize unnecessary memory usage. At inference, CERBEROS reveals the prediction of a randomly selected head, thereby misleading adversaries while preserving test accuracy for legitimate users, without requiring separate models or costly output modification. In addition, we integrate structured pruning into training to compress the backbone while retaining the classification heads. This ensures that functional diversity across heads remains achievable even under tight resource constraints. Our experiments show that CERBEROS effectively mitigates model replication attacks while consistently maintaining task performance across widely used convolutional neural networks and benchmark datasets. Furthermore, it achieves significant reductions in memory consumption and inference latency compared to prior defenses, offering a practical and efficient solution for securing on-device AI models.",
      "year": 2025,
      "venue": "IEEE Access",
      "authors": [
        "Sohyun Keum",
        "Jeonghyun Lee",
        "Sangkyun Lee"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/16cfca0d74d329b2b3f2d927047468f09300e33e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16743a0a8daad27538a0bc734ed42abca3a14289",
      "title": "Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy",
      "abstract": "Malicious users attempt to replicate commercial models functionally at low cost by training a clone model with query responses. It is challenging to timely prevent such model-stealing attacks to achieve strong protection and maintain utility. In this paper, we propose a novel non-parametric detector called Account-aware Distribution Discrepancy (ADD) to recognize queries from malicious users by leveraging account-wise local dependency. We formulate each class as a Multivariate Normal distribution (MVN) in the feature space and measure the malicious score as the sum of weighted class-wise distribution discrepancy. The ADD detector is combined with random-based prediction poisoning to yield a plug-and-play defense module named D-ADD for image classification models. Results of extensive experimental studies show that D-ADD achieves strong defense against different types of attacks with little interference in serving benign users for both soft and hard-label settings.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jian-Ping Mei",
        "Weibin Zhang",
        "Jie Chen",
        "Xuyun Zhang",
        "Tiantian Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/16743a0a8daad27538a0bc734ed42abca3a14289",
      "pdf_url": "",
      "publication_date": "2025-03-16",
      "keywords_matched": [
        "model stealing",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e445ba60a5f0dd22cb9188df23c228c5ec7a1f45",
      "title": "Model Rake: A Defense Against Stealing Attacks in Split Learning",
      "abstract": "Split learning is a prominent framework for vertical federated learning, where multiple clients collaborate with a central server for model training by exchanging intermediate embeddings. Recently, it is shown that an adversarial server can exploit the intermediate embeddings to train surrogate models to replace the bottom models on the clients (i.e., model stealing). The surrogate models can also be used to reconstruct private training data of the clients (i.e., data stealing).\n\nTo defend against these stealing attacks, we propose Model Rake (i.e., Rake), which runs two bottom models on each client and differentiates their output spaces to make the two models distinct. Rake hinders the stealing attacks because it is difficult for a surrogate model to approximate two distinct bottom models. We prove that, under some assumptions, the surrogate model converges to the average of the two bottom models and thus will be inaccurate. Extensive experiments show that Rake is much more effective than existing methods in defending against both model and data stealing attacks, and the accuracy of normal model training is not affected.",
      "year": 2025,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Qinbo Zhang",
        "Xiao Yan",
        "Yanfeng Zhao",
        "Fangcheng Fu",
        "Quanqing Xu",
        "Yukai Ding",
        "Xiaokai Zhou",
        "Chuang Hu",
        "Jiawei Jiang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e445ba60a5f0dd22cb9188df23c228c5ec7a1f45",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d7acc4f16a3e3c33a9544bbe80a72ab218b23869",
      "title": "On Stealing Graph Neural Network Models",
      "abstract": "Current graph neural network (GNN) model-stealing methods rely heavily on queries to the victim model, assuming no hard query limits. However, in reality, the number of allowed queries can be severely limited. In this paper, we demonstrate how an adversary can extract a GNN with very limited interactions with the model. Our approach first enables the adversary to obtain the model backbone without making direct queries to the victim model and then to strategically utilize a fixed query limit to extract the most informative data. The experiments on eight real-world datasets demonstrate the effectiveness of the attack, even under a very restricted query limit and under defense against model extraction in place. Our findings underscore the need for robust defenses against GNN model extraction threats.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Marcin Podhajski",
        "Jan Dubi'nski",
        "Franziska Boenisch",
        "Adam Dziedzic",
        "Agnieszka Pregowska",
        "Tomasz P. Michalak"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d7acc4f16a3e3c33a9544bbe80a72ab218b23869",
      "pdf_url": "",
      "publication_date": "2025-11-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5102a7c89adeba6818f90b1964106397a2fb7a37",
      "title": "Dynamic Gradient Compression and Attack Defense Strategy for Privacy Enhancement of Heterogeneous Data in Federated Learning",
      "abstract": "An innovatively constructed dynamic perception mechanism based on temporal and spatial dual dimensions is proposed. In particular, it dynamically adjusts the gradient compression ratio depending on the convergence rate of the model (e.g., using high compression ratio for fast convergence at early stage and fine-tuning later), which is realized by integrating loss variation rate and training cycle into compression ratio equation. In spatial dimension, this method adapts to heterogeneous data distributions by introducing a sample weight factor into the non-IID measurement index so that the heterogeneity of data can be quantified. A dynamic privacy budget allocation strategy based on data sensitivity matrix ensures adaptive noise injection and hierarchical encryption. In contrast to traditional methods, the anomaly detection module introduces high order statistical moments (skewness, kurtosis), combined with machine learning based attack classification methods, to detect gradient poisoning and model stealing attacks in real time.",
      "year": 2025,
      "venue": "2025 10th International Symposium on Advances in Electrical, Electronics and Computer Engineering (ISAEECE)",
      "authors": [
        "Conghui Wei",
        "Yaqian Lu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5102a7c89adeba6818f90b1964106397a2fb7a37",
      "pdf_url": "",
      "publication_date": "2025-06-20",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f8caf3ee31a4d1d4d72f87997c104aaeb56e0a33",
      "title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features",
      "abstract": "Large vision models (LVMs) achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to LVMs. However, this paper reveals that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized LVMs by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by computing the output differences between the shadow and victim models, without altering the victim model or its training process. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously. Our codes are available at https://github.com/zlh-thu/Holmes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Linghui Zhu",
        "Yiming Li",
        "Haiqin Weng",
        "Yan Liu",
        "Tianwei Zhang",
        "Shu-Tao Xia",
        "Zhi Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8caf3ee31a4d1d4d72f87997c104aaeb56e0a33",
      "pdf_url": "",
      "publication_date": "2025-06-24",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4ced0ca56a1f623ebe1860f2551ee5b8011c9b87",
      "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data",
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries. Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods. We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yixu Wang",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4ced0ca56a1f623ebe1860f2551ee5b8011c9b87",
      "pdf_url": "",
      "publication_date": "2025-09-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "056036a3ce3a21daf80167dee622b6b515f9490e",
      "title": "Black-box model functionality stealing for Vietnamese sentiment analysis",
      "abstract": "Black-box deep learning models often keep critical components such as model architecture, hyperparameters, and training data confidential, allowing users to observe only the inputs and outputs without understanding their internal workings. Consequently, there is growing interested in developing \"knockoff\" models that replicate the behavior of these black-box models without direct access to internal details. We have conducted extensive studies on function extraction attacks targeting English text sentiment analysis models. By employing random or adaptive sampling methods, we have successfully reconstructed knockoff models that achieve functionality equivalent to the original models with high similarity. In this study, we extend our investigation to sentiment analysis datasets in Vietnamese. Experimental results demonstrate that for black-box models in Vietnamese text sentiment analysis, our method remains effective, successfully constructing models with equivalent functionality.",
      "year": 2025,
      "venue": "Journal of Military Science and Technology",
      "authors": [
        "Cong Pham",
        "Viet-Binh Do",
        "Trung-Nguyen Hoang",
        "Cao-Truong Tran"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/056036a3ce3a21daf80167dee622b6b515f9490e",
      "pdf_url": "",
      "publication_date": "2025-06-25",
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bb3cf7ad6f2528490b40a8266ca1fa4dc5b929f4",
      "title": "Assessing Risk of Stealing Proprietary Models for Medical Imaging Tasks",
      "abstract": "The success of deep learning in medical imaging applications has led several companies to deploy proprietary models in diagnostic workflows, offering monetized services. Even though model weights are hidden to protect the intellectual property of the service provider, these models are exposed to model stealing (MS) attacks, where adversaries can clone the model's functionality by querying it with a proxy dataset and training a thief model on the acquired predictions. While extensively studied on general vision tasks, the susceptibility of medical imaging models to MS attacks remains inadequately explored. This paper investigates the vulnerability of black-box medical imaging models to MS attacks under realistic conditions where the adversary lacks access to the victim model's training data and operates with limited query budgets. We demonstrate that adversaries can effectively execute MS attacks by using publicly available datasets. To further enhance MS capabilities with limited query budgets, we propose a two-step model stealing approach termed QueryWise. This method capitalizes on unlabeled data obtained from a proxy distribution to train the thief model without incurring additional queries. Evaluation on two medical imaging models for Gallbladder Cancer and COVID-19 classification substantiates the effectiveness of the proposed attack. The source code is available at https://github.com/rajankita/QueryWise.",
      "year": 2025,
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "authors": [
        "Ankita Raj",
        "Harsh Swaika",
        "Deepankar Varma",
        "Chetan Arora"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bb3cf7ad6f2528490b40a8266ca1fa4dc5b929f4",
      "pdf_url": "",
      "publication_date": "2025-06-24",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c100542ca883890adf1fa185e07e13e9f4e5ec46",
      "title": "DeepAW: A Customized DNN Watermarking Scheme Against Unreliable Participants",
      "abstract": "Training DNNs requires large amounts of labeled data, costly computational resources, and tremendous human effort, resulting in such models being a valuable commodity. In collaborative learning scenarios, unreliable participants are widespread due to data collected from a diverse set of end-users that differ in quality and quantity. It is important to note that failure to take into account the contributions of all participants in the collaborative model training process when sharing the model with them could potentially result in a deterioration in collaborative efforts. In this paper, we propose a customized DNN watermarking scheme to safeguard the model ownership, namely DeepAW, achieving robustness to model stealing attacks and collaborative fairness in the presence of unreliable participants. Specifically, DeepAW leverages the tightly binding between the embedded watermarking and the model performance to defend against the model stealing attacks, resulting in the sharp decline of the model performance encountering any attempt at watermarking modification. DeepAW achieves collaborative fairness by detecting unreliable participants and customizing the model performance according to the participants' contributions. Furthermore, we set up three model stealing attacks and four types of unreliable participants. The experimental results demonstrate the effectiveness, robustness, and collaborative fairness of DeepAW.",
      "year": 2025,
      "venue": "IEEE Transactions on Network Science and Engineering",
      "authors": [
        "Shen Lin",
        "Xiaoyu Zhang",
        "Xu Ma",
        "Xiaofeng Chen",
        "Willy Susilo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c100542ca883890adf1fa185e07e13e9f4e5ec46",
      "pdf_url": "",
      "publication_date": "2025-07-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a2c361d27c70f4b61da39732c338c54145a4cb82",
      "title": "Exploiting Power Side-Channel Vulnerabilities in XGBoost Accelerator",
      "abstract": "XGBoost (eXtreme Gradient Boosting), a widelyused decision tree algorithm, plays a crucial role in applications such as ransomware and fraud detection. While its performance is well-established, its security against model extraction on hardware platforms like Field Programmable Gate Arrays (FPGAs) has not been fully explored. In this paper, we demonstrate a significant vulnerability where sensitive model data can be leaked from an XGBoost implementation through side-channel attacks (SCAs). By analyzing variations in power consumption, we show how an attacker can infer node features within the XGBoost model, leading to the extraction of critical data. We conduct an experiment using the XGBoost accelerator FAXID on the Sakura-X platform, demonstrating a method to deduce model decisions by monitoring power consumptions. The results show that on average 367k tests are sufficient to leak sensitive values. Our findings underscore the need for improved hardware and algorithmic protections to safeguard machine learning models from these types of attacks.",
      "year": 2025,
      "venue": "Design Automation Conference",
      "authors": [
        "Yimeng Xiao",
        "Archit Gajjar",
        "Aydin Aysu",
        "Paul Franzon"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a2c361d27c70f4b61da39732c338c54145a4cb82",
      "pdf_url": "",
      "publication_date": "2025-06-22",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "86c0661fdb12bcde0709e91416510fda72f10f13",
      "title": "A Comprehensive Survey of Model Extraction Attacks: Current Trends, Defenses, and Future Directions",
      "abstract": "Model extraction attacks pose a significant threat to Machine Learning (ML) systems, especially in cloud-based services like Machine Learning as a Service (MLaaS). Attacks aim to steal proprietary models by replicating their functionality or extracting their internal parameters. This paper reviews model extraction attack types, examining existing defensive techniques and weaknesses in current defenses. Promising defense mechanisms are discussed, including adaptive privacy budgets, hybrid defense strategies, combining multiple methods, and hardwarebased security solutions. Emerging attack models like collaborative attacks in federated learning environments are explored. Future research focuses on adaptive defenses and Artificial Intelligence (AI)-driven detection methods to improve model robustness and contribute to more resilient machine learning systems.",
      "year": 2025,
      "venue": "2025 1st International Conference on Secure IoT, Assured and Trusted Computing (SATC)",
      "authors": [
        "Quazi Rian Hasnaine",
        "Yaodan Hu",
        "Mohamed I. Ibrahem",
        "Mostafa M. Fouda"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/86c0661fdb12bcde0709e91416510fda72f10f13",
      "pdf_url": "",
      "publication_date": "2025-02-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "68f0633a0be1a7009c4caf765529d898b2450044",
      "title": "SONNI: Secure Oblivious Neural Network Inference",
      "abstract": "In the standard privacy-preserving Machine learning as-a-service (MLaaS) model, the client encrypts data using homomorphic encryption and uploads it to a server for computation. The result is then sent back to the client for decryption. It has become more and more common for the computation to be outsourced to third-party servers. In this paper we identify a weakness in this protocol that enables a completely undetectable novel model-stealing attack that we call the Silver Platter attack. This attack works even under multikey encryption that prevents a simple collusion attack to steal model parameters. We also propose a mitigation that protects privacy even in the presence of a malicious server and malicious client or model provider (majority dishonest). When compared to a state-of-the-art but small encrypted model with 32k parameters, we preserve privacy with a failure chance of 1.51 x 10^-28 while batching capability is reduced by 0.2%. Our approach uses a novel results-checking protocol that ensures the computation was performed correctly without violating honest clients' data privacy. Even with collusion between the client and the server, they are unable to steal model parameters. Additionally, the model provider cannot learn any client data if maliciously working with the server.",
      "year": 2025,
      "venue": "International Conference on Security and Cryptography",
      "authors": [
        "Luke Sperling",
        "Sandeep S. Kulkarni"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/68f0633a0be1a7009c4caf765529d898b2450044",
      "pdf_url": "",
      "publication_date": "2025-04-26",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "64bebdbe7fa9a6b173533e64c35960e0378ea21a",
      "title": "Knockoff Branch: Model Stealing Attack via Adding Neurons in the Pre-Trained Model",
      "abstract": "We introduce Knockoff Branch: adding few neurons as a knockoff container for learning stolen features. Model stealing attacks extract the functionality from the victim model by querying APIs. Prior work substantially enhanced transferability and improved query efficiency between the adversary model and the victim model. However, there is still a limited understanding of the knockoff itself. For knockoff, the model is either compared to the same type but with different structures or different types and capacities. For this reason, we propose a framework to analyze the knockoff quality for a single model, specifically reinvestigating transformer-based extraction. We observed that 1) when the adversary can access the public pretrained model, full fine-tuning is not necessary. This allows a knockoff to require only about 0.5% of trainable parameters and 20 epochs. 2) Although querying by out-of-distribution datasets leads to a sub-optimal knockoff, this issue can be mitigated by scaling branch features, even without using complicated sampling strategies. Our proposed method is lightweight and achieves high accuracy, at most similar to white-box knowledge distillation (higher performance than the victim model). https://github.com/onlyin-hung/knockoff-branch.",
      "year": 2025,
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "authors": [
        "Li-Ying Hung",
        "Cooper Cheng-Yuan Ku"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/64bebdbe7fa9a6b173533e64c35960e0378ea21a",
      "pdf_url": "",
      "publication_date": "2025-02-26",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bc15874a504bc3abf610ce66fc8bc8d5e8d6e7ac",
      "title": "Too Clever by Half: Detecting Sampling-based Model Stealing Attacks by Their Own Cleverness",
      "abstract": "Machine learning as a service (MLaaS) has gained significant popularity and market traction in recent years, driven by advancements in Artificial Intelligence particularly Generative AI (GAI). However, MLaaS faces severe challenges from sampling-based model stealing attacks (MSAs), where attackers strategically query the targeted ML models provided by MLaaS providers to minimize the query burden while closely replicating the model\u2019s functionality. Such MSAs pose severe consequences, including intellectual property (IP) theft and potential leakage of private training data. Unfortunately, existing defenses either sacrifice model utility or fail to generalize across diverse MSAs.In this paper, we propose DIARY, an innovative detection method specifically tailored to sampling-based MSAs by exploiting their inherent sophistication. Our key insight is that \u2018clever\u2019 malicious queries tend to extract more information from the targeted (victim) model than typical benign queries, as these attacks iteratively refine their queries by examining and analyzing prior queries and the corresponding responses. Hence we design DIARY to extract timing dependence within a query sequence and incorporate contrastive learning for properly characterizing such dependency that holds for different sampling-based MSAs. Comprehensive evaluations using five different sampling-based MSAs and two state-of-the-art defense baselines across four popular datasets consistently validate DIARY\u2019s superior performance.",
      "year": 2025,
      "venue": "IEEE International Conference on Distributed Computing Systems",
      "authors": [
        "Xin Yao",
        "Chenyang Wang",
        "Yimin Chen",
        "Kecheng Huang",
        "Jiawei Guo",
        "Ming Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bc15874a504bc3abf610ce66fc8bc8d5e8d6e7ac",
      "pdf_url": "",
      "publication_date": "2025-07-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fdf69d7c5067bd1eddba2cc577b72be20a3546b0",
      "title": "A Method for Stealing Traffic Detection Models Based on Equivalent Feature Sets",
      "abstract": "Machine learning models have shown excellent performance in the field of traffic detection. However, they also face new security challenges. This study focuses on model stealing attacks in the traffic detection field. Existing model stealing methods are difficult to implement in real - world network environments. They either cannot be applied to the traffic detection field or require prior knowledge of the target model's feature set. To achieve the stealing of traffic detection models in a truly black - box scenario and overcome the strong assumption of relying on the known target model feature set in existing methods, this study proposes a method for stealing traffic detection models based on feature inference. This study introduces the concept of \u201cequivalent feature set\u201d, analyzes the prediction logic of the target model and the characteristics of traffic data through two feature inference algorithms, constructs an equivalent feature set, and uses it as the feature set for training a substitute model. The results of stealing experiments on multiple target models show that in most experimental settings, the stealing rate exceeds 85%, with a maximum of 96.64%, demonstrating the high efficiency and stability of the method. At the same time, experimental verification shows that the equivalent feature set can accurately capture key features, significantly improving the stealing effect of the substitute model, with an improvement amplitude of more than 40.48%.",
      "year": 2025,
      "venue": "International Conference Civil Engineering and Architecture",
      "authors": [
        "Long Meng",
        "Bin Lu",
        "Xu Gao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fdf69d7c5067bd1eddba2cc577b72be20a3546b0",
      "pdf_url": "",
      "publication_date": "2025-04-25",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a744498909536ee4a0752f3fe2430c13268a2b82",
      "title": "Siamese: Stealing Fine-Tuned Visual Foundation Models via Diversified Prompting",
      "abstract": "Visual foundation models, characterized by their robust generalization and adaptability, serve as the basis for a wide array of downstream tasks. When fine-tuned for specific tasks, these models encapsulate confidential and valuable task-specific knowledge, making them prime targets for model stealing (MS) attacks. While recent efforts have exposed MS threats in practical scenarios such as data-free and hard-label contexts, these attacks predominantly target traditional victim models trained from scratch. Fine-tuned visual foundation models, pre-trained on vast and diverse datasets and then fine-tuned on downstream tasks, present significant challenges for traditional MS attacks to extract task-specific knowledge. In this paper, we introduce an innovative MS attack, named SIAMESE, to steal fine-tuned visual foundation models under black-box, data-free, and hard-label settings. The core approach of SIAMESE involves constructing a stolen model using a foundation model that is efficiently and concurrently fine-tuned with multiple diversified soft prompts. To integrate the knowledge derived from these prompts, we propose a novel and tractable loss function that analyzes the output distributions while enforcing orthogonality among the prompts to minimize interference. Additionally, a unique alignment module enhances SIAMESE by synchronizing interpretations between the victim and stolen models. Extensive experiments validate that SIAMESE outperforms state-of-the-art baseline attacks over 10% in accuracy, exposing the heightened vulnerability of fine-tuned visual foundation models to MS threats.",
      "year": 2025,
      "venue": "Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing",
      "authors": [
        "Madhureeta Das",
        "Gaurav Bagwe",
        "Miao Pan",
        "Kaichen Yang",
        "X. Yuan",
        "Lan Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a744498909536ee4a0752f3fe2430c13268a2b82",
      "pdf_url": "",
      "publication_date": "2025-12-03",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "557b82a8e54356b25b20fd96bd86dc449d8d10bc",
      "title": "Security Challenges and Mitigation Strategies in Generative AI Systems",
      "abstract": "This article examines the critical security challenges and mitigation strategies in generative AI systems. The article explores how these systems have transformed various sectors, particularly in financial markets and critical infrastructure, while introducing significant security concerns. The article analyzes various types of adversarial attacks, including input perturbation and backdoor attacks, and their impact on AI model performance. Additionally, it investigates model stealing threats and data privacy concerns in AI deployments. The article presents comprehensive mitigation strategies, including advanced defense mechanisms, enhanced protection frameworks, and secure access control implementations. The article findings demonstrate the effectiveness of integrated security approaches in protecting AI systems while maintaining operational efficiency. This article contributes to the growing body of knowledge on AI security by providing evidence-based strategies for protecting generative AI systems across different application domains.",
      "year": 2025,
      "venue": "International Journal of Scientific Research in Computer Science Engineering and Information Technology",
      "authors": [
        "Satya Naga Mallika Pothukuchi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/557b82a8e54356b25b20fd96bd86dc449d8d10bc",
      "pdf_url": "https://doi.org/10.32628/cseit25112377",
      "publication_date": "2025-03-05",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ab3cd21bb78472e4fe7f4568b025abc76e3f59a9",
      "title": "Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses",
      "abstract": "Quantum Machine Learning (QML) integrates quantum computing with classical machine learning, primarily to solve classification, regression and generative tasks. However, its rapid development raises critical security challenges in the Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines adversarial threats unique to QML systems, focusing on vulnerabilities in cloud-based deployments, hybrid architectures, and quantum generative models. Key attack vectors include model stealing via transpilation or output extraction, data poisoning through quantum-specific perturbations, reverse engineering of proprietary variational quantum circuits, and backdoor attacks. Adversaries exploit noise-prone quantum hardware and insufficiently secured QML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership, and functionality. Defense mechanisms leverage quantum properties to counter these threats. Noise signatures from training hardware act as non-invasive watermarks, while hardware-aware obfuscation techniques and ensemble strategies disrupt cloning attempts. Emerging solutions also adapt classical adversarial training and differential privacy to quantum settings, addressing vulnerabilities in quantum neural networks and generative architectures. However, securing QML requires addressing open challenges such as balancing noise levels for reliability and security, mitigating cross-platform attacks, and developing quantum-classical trust frameworks. This chapter summarizes recent advances in attacks and defenses, offering a roadmap for researchers and practitioners to build robust, trustworthy QML systems resilient to evolving adversarial landscapes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Archisman Ghosh",
        "Satwik Kundu",
        "Swaroop Ghosh"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ab3cd21bb78472e4fe7f4568b025abc76e3f59a9",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b7ac762bbb6a90d1ed6b0ede09a2410cff9f1961",
      "title": "DAV: An Adaptive Defense Framework for Model Extraction Attacks",
      "abstract": "Machine learning platforms offer paid APIs to enable personalized inference services. However, model extraction attacks greatly threaten their intellectual property rights. Malicious users can create query samples using proxy datasets or generative models to train a clone model. Existing defense approaches usually focus on models that return soft-labels, and cannot effectively handle extracting attacks against hard-label models. In this paper, we propose an adaptive defense framework named DAV, which consists of a malicious query detector and an adaptive perturbation mechanism. Two perturbation strategies can be selected based on the detection results and the malicious query rate within the buffer queue, including accuracy-preserving perturbation and maximum-minimum probability inverse perturbation. Comprehensive experimental results show that DAV can significantly reduce the accuracy of the clone model with little impact on the performance of the victim model and benign queries, no matter whether the returned probabilities are for soft-label or hard-label.",
      "year": 2025,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Peng Sui",
        "Jiapeng Zhou",
        "Yu Chen",
        "Youhuizi Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b7ac762bbb6a90d1ed6b0ede09a2410cff9f1961",
      "pdf_url": "",
      "publication_date": "2025-06-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ae1157c1fba9603a98566209bc266f6b7a534629",
      "title": "RADEP: A Resilient Adaptive Defense Framework Against Model Extraction Attacks",
      "abstract": "Machine Learning as a Service (MLaaS) enables users to leverage powerful machine learning models through cloud-based APIs, offering scalability and ease of deployment. However, these services are vulnerable to model extraction attacks, where adversaries repeatedly query the application programming interface (API) to reconstruct a functionally similar model, compromising intellectual property and security. Despite various defense strategies being proposed, many suffer from high computational costs, limited adaptability to evolving attack techniques, and a reduction in performance for legitimate users. In this paper, we introduce a Resilient Adaptive Defense Framework for Model Extraction Attack Protection (RADEP), a multifaceted defense framework designed to counteract model extraction attacks through a multi-layered security approach. RADEP employs progressive adversarial training to enhance model resilience against extraction attempts. Malicious query detection is achieved through a combination of uncertainty quantification and behavioral pattern analysis, effectively identifying adversarial queries. Furthermore, we develop an adaptive response mechanism that dynamically modifies query outputs based on their suspicion scores, reducing the utility of stolen models. Finally, ownership verification is enforced through embedded watermarking and backdoor triggers, enabling reliable identification of unauthorized model use. Experimental evaluations demonstrate that RADEP significantly reduces extraction success rates while maintaining high detection accuracy with minimal impact on legitimate queries. Extensive experiments show that RADEP effectively defends against model extraction attacks and remains resilient even against adaptive adversaries, making it a reliable security framework for MLaaS models.",
      "year": 2025,
      "venue": "International Conference on Wireless Communications and Mobile Computing",
      "authors": [
        "Amit Chakraborty",
        "Sayyed Farid Ahamed",
        "Sandip Roy",
        "Soumya Banerjee",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "E. Bowen",
        "Sachin Shetty"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ae1157c1fba9603a98566209bc266f6b7a534629",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f0912c3c7cbcc0ffd85d5334848b17e9128eaf5d",
      "title": "Augmenting Model Extraction Attacks Against Disruption-Based Defenses",
      "abstract": "Existing research has demonstrated that deep neural networks are susceptible to model extraction attacks, where an attacker can construct a substitute model with similar functionality to the victim model by querying the black-box victim model. To counter such attacks, various disruption-based defenses have been proposed. These defenses disrupt the output results of queries before returning them to potential attackers. In this paper, we propose the first defense-penetrating model extraction attack framework, aimed at breaking disruption-based defense methods. Our proposed attack framework comprises two key modules: disruption detection and disruption recovery, which can be integrated into generic model extraction attacks. Specifically, the disruption detection module uses a novel meta-learning-based algorithm to infer the defense strategy employed by the defender, by learning the key differences between the distributions of disrupted and undisrupted query results. Once the defense method is inferred, the disruption recovery module is designed to restore clean query results from the disrupted query results, using a carefully-designed generative model. We conducted extensive experiments on 5 commonly-used datasets to evaluate the effectiveness of our proposed framework. The results demonstrate that the substitute model accuracy of current model extraction attacks can be significantly improved by up to 82.42%, even when faced with four state-of-the-art model extraction defenses. Moreover, our attack approach shows promising results in penetrating unknown defenses in real-world cloud service APIs hosted by Microsoft Azure and Face++.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xueluan Gong",
        "Shuaike Li",
        "Yanjiao Chen",
        "Mingzhe Li",
        "Rubin Wei",
        "Qian Wang",
        "Kwok-Yan Lam"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f0912c3c7cbcc0ffd85d5334848b17e9128eaf5d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "504b2d98d26dfc4bf46b53de5c5c30acfcd0082a",
      "title": "Model Extraction Attack and Its Countermeasure for Denoising Diffusion Implicit Models",
      "abstract": "Recently, the threat of cyber attacks against machine learning models has been increasing. Typical examples include Model Extraction Attack (MEA), which steals the functionality of a victim model by creating its clone model that has almost the same functionality. Thus, the literature has studied MEA and its defense methods, mainly focusing on image recognition models. However, no existing studies evaluate the risk of MEA on diffusion-based image generation models, despite the recent advances and widespread use of image generation AI services powered by diffusion models. In this paper, we first demonstrate the feasibility of MEA on DDIM, one of the most common diffusion-based image generation models. Then, as a countermeasure, we propose a defense method that detects clone models of DDIM. In the proposed method, we add a small number of out-of-distribution images, referred to as \u201cmarking images\u201d, to the training dataset of a victim DDIM model. This technique provides the property of occasionally generating marking images for the victim model. This property works as a watermark and is inherited by the clone models, being used as a clue for detecting them. In the results of our experiments conducted on face, fruit, and church image datasets, the proposed defense method can correctly detect all clone models without seriously degrading the usability of victim DDIM models.",
      "year": 2025,
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "authors": [
        "Hayato Shoji",
        "Kazuaki Nakamura"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/504b2d98d26dfc4bf46b53de5c5c30acfcd0082a",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fb09eaceb17a4813fd8ce3496d01e1c78c6cec06",
      "title": "CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense",
      "abstract": "Large language models have gained widespread attention recently, but their potential security vulnerabilities, especially privacy leakage, are also becoming apparent. To test and evaluate for data extraction risks in LLM, we proposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and Defense. We introduce several innovative components, including Dynamic Loss, Additive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested to enhance the consistency of the soft prompt tuning process. Through extensive experimentation with various combinations, we achieved an extraction rate of 65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other reference works confirm our superior extraction rates. We evaluate CoSPED on more scenarios, achieving Pythia model extraction rate of 51.7% and introducing cross-model comparison. Finally, we explore defense through Rank-One Model Editing and achieve a reduction in the extraction rate to 1.6%, which proves that our analysis of extraction mechanisms can directly inform effective mitigation strategies against soft prompt-based attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhuochen Yang",
        "Fok Kar Wai",
        "V. Thing"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fb09eaceb17a4813fd8ce3496d01e1c78c6cec06",
      "pdf_url": "",
      "publication_date": "2025-10-13",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d4fd59d2fed19b3ff5a112dd1cf535ebf02ac309",
      "title": "GFFx: A Rust-based suite of utilities for ultra-fast genomic feature extraction",
      "abstract": "Genome annotations are becoming increasingly comprehensive due to the discovery of diverse regulatory elements and transcript variants. However, this improvement in annotation resolution poses major challenges for efficient querying, especially across large genomes and pangenomes. Existing tools often exhibit performance bottlenecks when handling large-scale genome annotation files, particularly for region-based queries and hierarchical model extraction. Here, we present GFFx, a Rust-based toolkit for ultra-fast and scalable genome annotation access. GFFx introduces a compact, model-aware indexing system inspired by binning strategies and leverages Rust\u2019s strengths in execution speed, memory safety, and multithreading. It supports both feature- and region-based extraction with significant improvements in runtime and scalability over existing tools. Distributed via Cargo, GFFx provides a cross-platform command-line interface and a reusable library with a clean API, enabling seamless integration into custom pipelines. Benchmark results demonstrate that GFFx offers substantial speedups and makes a practical, extensible solution for genome annotation workflows.",
      "year": 2025,
      "venue": "bioRxiv",
      "authors": [
        "Baohua Chen",
        "Dongya Wu",
        "Guojie Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d4fd59d2fed19b3ff5a112dd1cf535ebf02ac309",
      "pdf_url": "",
      "publication_date": "2025-01-06",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "63fb4344b3b77b8b23b89c732d31274520c8f026",
      "title": "Exploring A Model Type Detection Attack against Machine Learning as A Service",
      "abstract": "Recently, Machine-Learning-as-a-Service (MLaaS) systems are reported to be vulnerable to varying novel attacks, e.g., model extraction attacks and adversarial examples. However, in our investigation, we notice that the majority of MLaas attacks are not as threatening as expected due to model-type-sensitive problem. Literally speaking, many MLaaS attacks are designed for only a specific type of models. Without model type info as default prior knowledge, these attacks suffer from great performance degradation, or even become infeasible! In this paper, we demonstrate a novel attack method, named SNOOPER, to resolve the model-type-sensitive problem of MLaaS attacks. Specifically, SNOOPER is integrated with multiple self-designed model-type-detection modules. Each module can judge whether a given black-box model belongs to a specific type of models by analyzing its query-response pattern. Then, after proceeding with all modules, the attacker can know the type of its target model in the querying stage, and accordingly choose the optimal attack method. Also, to save budget, the queries can be re-used in the latter attack stage. We call such a kind of attack as model-type-detection attack. Finally, we experiment with SNOOPER on some popular model classes, including decision trees, linear models, non-linear models and neural networks. The results show that SNOOPER is capable of detecting the model type with more than 90% accuracy.",
      "year": 2025,
      "venue": "Journal of Intelligent Computing and Networking",
      "authors": [
        "Yilong Yang",
        "Xinjing Liu",
        "Ruidong Han",
        "Yang Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/63fb4344b3b77b8b23b89c732d31274520c8f026",
      "pdf_url": "",
      "publication_date": "2025-11-18",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ff22d38603caf509358f50ef67244b039c63a5b1",
      "title": "Budget and Frequency Controlled Cost-Aware Model Extraction Attack on Sequential Recommenders",
      "abstract": "Sequential recommenders are integral to many applications yet remain vulnerable to model extraction attacks, in which adversaries can recover information about the deployed model by issuing queries to a black-box without internal access. From the attacker's perspective, existing studies impose a fixed and limited query budget but overlook optimal allocation, resulting in redundant or low-value requests. Furthermore, the scarce data obtained through these costly queries is typically handled by crude random sampling, resulting in low diversity and information coverage with actual data. In this paper, we propose a novel approach, named Budget and Frequency Controlled Cost-Aware Model Extraction Attack (BECOME), for extracting black-box sequential recommenders, which extends the standard extraction framework with two cost-aware innovations: Feedback-Driven Dynamic Budgeting periodically evaluates the victim model to refine query allocation and steer sequence generation adaptively. Rank-Aware Frequency Controlling integrates frequency constraints with ranking guidance in the next-item sampler to select high-value items and broaden information coverage. Experiments on public datasets and representative sequential recommender architectures demonstrate that our method achieves superior extraction performance. Our code is released at https://github.com/Loche2/BECOME.",
      "year": 2025,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Lei Zhou",
        "Min Gao",
        "Zongwei Wang",
        "Yibing Bai"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ff22d38603caf509358f50ef67244b039c63a5b1",
      "pdf_url": "",
      "publication_date": "2025-11-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "112442beb4161e251a44ea532893c8a32eb17ba3",
      "title": "Ownership Infringement Detection for Generative Adversarial Networks Against Model Stealing",
      "abstract": "Generative adversarial networks (GANs) have shown remarkable success in image synthesis, making GAN models themselves commercially valuable to legitimate model owners. Therefore, it is critical to technically protect the intellectual property of GANs. Prior works need to tamper with the training set or training process to verify the ownership of a GAN. In this article, we show that these methods are not robust to emerging model extraction attacks. Then, we propose a new method GAN-Guards which utilizes the common characteristics of a target model and its stolen models for ownership infringement detection. Our method can be directly applicable to all well-trained GANs as it does not require retraining target models. Extensive experimental results show that our new method achieves superior detection performance, compared with the watermark-based and fingerprint-based methods. Finally, we demonstrate the effectiveness of our method with respect to the number of generations of model extraction attacks, the number of generated samples, and adaptive attacks.",
      "year": 2025,
      "venue": "IEEE Transactions on Artificial Intelligence",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/112442beb4161e251a44ea532893c8a32eb17ba3",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8cf823eb4c9309221953480a9b98c7700136c546",
      "title": "SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment",
      "abstract": "With the increasing deployment of Large Language Models (LLMs) on mobile and edge platforms, securing them against model extraction attacks has become a pressing concern. However, protecting model privacy without sacrificing the performance benefits of untrusted AI accelerators, such as GPUs, presents a challenging trade-off. In this paper, we initiate the study of high-performance execution on LLMs and present SecureInfer, a hybrid framework that leverages a heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate privacy-critical components while offloading compute-intensive operations to untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts an information-theoretic and threat-informed partitioning strategy: security-sensitive components, including non-linear layers, projection of attention head, FNN transformations, and LoRA adapters, are executed inside an SGX enclave, while other linear operations (matrix multiplication) are performed on the GPU after encryption and are securely restored within the enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and evaluate it across performance and security metrics. Our results show that SecureInfer offers strong security guarantees with reasonable performance, offering a practical solution for secure on-device model inference.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tushar Nayan",
        "Ziqi Zhang",
        "Ruimin Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8cf823eb4c9309221953480a9b98c7700136c546",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a1a88758960b51b8f7b47d8d429df74c28f12b6c",
      "title": "Stealing AI Model Weights Through Covert Communication Channels",
      "abstract": "AI models are often regarded as valuable intellectual property due to the high cost of their development, the competitive advantage they provide, and the proprietary techniques involved in their creation. As a result, AI model stealing attacks pose a serious concern for AI model providers. In this work, we present a novel attack targeting wireless devices equipped with AI hardware accelerators. The attack unfolds in two phases. In the first phase, the victim's device is compromised with a hardware Trojan (HT) designed to covertly leak model weights through a hidden communication channel, without the victim realizing it. In the second phase, the adversary uses a nearby wireless device to intercept the victim's transmission frames during normal operation and incrementally reconstruct the complete weight matrix. The proposed attack is agnostic to both the AI model architecture and the hardware accelerator used. We validate our approach through a hardware-based demonstration involving four diverse AI models of varying types and sizes. We detail the design of the HT and the covert channel, highlighting their stealthy nature. Additionally, we analyze the impact of bit error rates on the reception and propose an error mitigation technique. The effectiveness of the attack is evaluated based on the accuracy of the reconstructed models with stolen weights and the time required to extract them. Finally, we explore potential defense mechanisms.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Valentin Barbaza",
        "Al\u00e1n Rodrigo D\u00edaz Rizo",
        "Hassan Aboushady",
        "Spyridon Raptis",
        "H. Stratigopoulos"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a1a88758960b51b8f7b47d8d429df74c28f12b6c",
      "pdf_url": "",
      "publication_date": "2025-09-30",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6e068deff65d5f69213fd9a6bf3389dc514c1cf5",
      "title": "Stabilizing Data-Free Model Extraction",
      "abstract": "Model extraction is a severe threat to Machine Learning-as-a-Service systems, especially through data-free approaches, where dishonest users can replicate the functionality of a black-box target model without access to realistic data. Despite recent advancements, existing data-free model extraction methods suffer from the oscillating accuracy of the substitute model. This oscillation, which could be attributed to the constant shift in the generated data distribution during the attack, makes the attack impractical since the optimal substitute model cannot be determined without access to the target model's in-distribution data. Hence, we propose MetaDFME, a novel data-free model extraction method that employs meta-learning in the generator training to reduce the distribution shift, aiming to mitigate the substitute model's accuracy oscillation. In detail, we train our generator to iteratively capture the meta-representations of the synthetic data during the attack. These meta-representations can be adapted with a few steps to produce data that facilitates the substitute model to learn from the target model while reducing the effect of distribution shifts. Our experiments on popular baseline image datasets, MNIST, SVHN, CIFAR-10, and CIFAR-100, demonstrate that MetaDFME outperforms the current state-of-the-art data-free model extraction method while exhibiting a more stable substitute model's accuracy during the attack.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Dat-Thinh Nguyen",
        "Kim-Hung Le",
        "Nhien-An Le-Khac"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6e068deff65d5f69213fd9a6bf3389dc514c1cf5",
      "pdf_url": "",
      "publication_date": "2025-09-14",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f6ee2e687a10d8045c1ffe4293535e76b293de70",
      "title": "Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms",
      "abstract": "Online collaborative medical prediction platforms offer convenience and real-time feedback by leveraging massive electronic health records. However, growing concerns about privacy and low prediction quality can deter patient participation and doctor cooperation. In this paper, we first clarify the privacy attacks, namely attribute attacks targeting patients and model extraction attacks targeting doctors, and specify the corresponding privacy principles. We then propose a privacy-preserving mechanism and integrate it into a novel one-shot distributed learning framework, aiming to simultaneously meet both privacy requirements and prediction performance objectives. Within the framework of statistical learning theory, we theoretically demonstrate that the proposed distributed learning framework can achieve the optimal prediction performance under specific privacy requirements. We further validate the developed privacy-preserving collaborative medical prediction platform through both toy simulations and real-world data experiments.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shao-Bo Lin",
        "Xiaotong Liu",
        "Yao Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f6ee2e687a10d8045c1ffe4293535e76b293de70",
      "pdf_url": "",
      "publication_date": "2025-07-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9d0f0fb82a339debb181382f725baa278bb202d7",
      "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors",
      "abstract": "In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access. We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Wenlong Meng",
        "Shuguo Fan",
        "Chengkun Wei",
        "Min Chen",
        "Yuwei Li",
        "Yuanchao Zhang",
        "Zhikun Zhang",
        "Wenzhi Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9d0f0fb82a339debb181382f725baa278bb202d7",
      "pdf_url": "",
      "publication_date": "2025-06-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0b20c29e4fefc4acaa8e2bcf0f684ffa5f1863b0",
      "title": "Hybrid Fingerprinting for Effective Detection of Cloned Neural Networks",
      "abstract": "As artificial intelligence plays an increasingly important role in decision-making within critical infrastructure, ensuring the authenticity and integrity of neural networks is crucial. This paper addresses the problem of detecting cloned neural networks. We present a method for identifying clones that employs a combination of metrics from both the information and physical domains: output predictions, probability score vectors, and power traces measured from the device running the neural network during inference. We compare the effectiveness of each metric individually, as well as in combination. Our results show that the effectiveness of both the information and the physical domain metrics is excellent for a clone that is a near replica of the target neural network. Furthermore, both the physical domain metric individually and the hybrid approach outperform the information domain metrics at detecting clones whose weights were extracted with low accuracy. The presented method offers a practical solution for verifying neural network authenticity and integrity. It is particularly useful in scenarios where neural networks are at risk of model extraction attacks, such as in cloud-based machine learning services.",
      "year": 2025,
      "venue": "IEEE International Symposium on Multiple-Valued Logic",
      "authors": [
        "Can Aknesil",
        "Elena Dubrova",
        "Niklas Lindskog",
        "Jakob Sternby",
        "H\u00e5kan Englund"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0b20c29e4fefc4acaa8e2bcf0f684ffa5f1863b0",
      "pdf_url": "",
      "publication_date": "2025-06-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6033eb92723833c3ca2c478cfde3956e0d1e388a",
      "title": "Activation Functions Considered Harmful: Recovering Neural Network Weights through Controlled Channels",
      "abstract": "With high-stakes machine learning applications increasingly moving to untrusted end-user or cloud environments, safeguarding pre-trained model parameters becomes essential for protecting intellectual property and user privacy. Recent advancements in hardware-isolated enclaves, notably Intel SGX, hold the promise to secure the internal state of machine learning applications even against compromised operating systems. However, we show that privileged software adversaries can exploit input-dependent memory access patterns in common neural network activation functions to extract secret weights and biases from an SGX enclave. Our attack leverages the SGX-Step framework to obtain a noise-free, instruction-granular page-access trace. In a case study of an 11-input regression network using the Tensorflow Microlite library, we demonstrate complete recovery of all first-layer weights and biases, as well as partial recovery of parameters from deeper layers under specific conditions. Our novel attack technique requires only 20 queries per input per weight to obtain all first-layer weights and biases with an average absolute error of less than 1%, improving over prior model stealing attacks. Additionally, a broader ecosystem analysis reveals the widespread use of activation functions with input-dependent memory access patterns in popular machine learning frameworks (either directly or via underlying math libraries). Our findings highlight the limitations of deploying confidential models in SGX enclaves and emphasise the need for stricter side-channel validation of machine learning implementations, akin to the vetting efforts applied to secure cryptographic libraries.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jesse Spielman",
        "David Oswald",
        "Mark Ryan",
        "Jo Van Bulck"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6033eb92723833c3ca2c478cfde3956e0d1e388a",
      "pdf_url": "",
      "publication_date": "2025-03-24",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "818a74f810f9f1b5bf7243279c97d22f57e97cbf",
      "title": "ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained Models Against Extraction",
      "abstract": "Pre-trained models are valuable intellectual property, capturing both domain-specific and domain-invariant features within their weight spaces. However, model extraction attacks threaten these assets by enabling unauthorized source-domain inference and facilitating cross-domain transfer via the exploitation of domain-invariant features. In this work, we introduce **ProDiF**, a novel framework that leverages targeted weight space manipulation to secure pre-trained models against extraction attacks. **ProDiF** quantifies the transferability of filters and perturbs the weights of critical filters in unsecured memory, while preserving actual critical weights in a Trusted Execution Environment (TEE) for authorized users. A bi-level optimization further ensures resilience against adaptive fine-tuning attacks. Experimental results show that **ProDiF** reduces source-domain accuracy to near-random levels and decreases cross-domain transferability by 74.65\\%, providing robust protection for pre-trained models. This work offers comprehensive protection for pre-trained DNN models and highlights the potential of weight space manipulation as a novel approach to model security.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tong Zhou",
        "Shijin Duan",
        "Gaowen Liu",
        "Charles Fleming",
        "R. Kompella",
        "Shaolei Ren",
        "Xiaolin Xu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/818a74f810f9f1b5bf7243279c97d22f57e97cbf",
      "pdf_url": "",
      "publication_date": "2025-03-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6c1f2957365c03221827f076190da814769a79b2",
      "title": "From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks",
      "abstract": "The advent of Machine Learning as a Service (MLaaS) has heightened the trade-off between model explainability and security. In particular, explainability techniques, such as counterfactual explanations, inadvertently increase the risk of model extraction attacks, enabling unauthorized replication of proprietary models. In this paper, we formalize and characterize the risks and inherent complexity of model reconstruction, focusing on the\"oracle''queries required for faithfully inferring the underlying prediction function. We present the first formal analysis of model extraction attacks through the lens of competitive analysis, establishing a foundational framework to evaluate their efficiency. Focusing on models based on additive decision trees (e.g., decision trees, gradient boosting, and random forests), we introduce novel reconstruction algorithms that achieve provably perfect fidelity while demonstrating strong anytime performance. Our framework provides theoretical bounds on the query complexity for extracting tree-based model, offering new insights into the security vulnerabilities of their deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Awa Khouna",
        "Julien Ferry",
        "Thibaut Vidal"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6c1f2957365c03221827f076190da814769a79b2",
      "pdf_url": "",
      "publication_date": "2025-02-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "54ca14c7326e174dfe3627ced320a122e11d6d29",
      "title": "Model Extraction for Image Denoising Networks",
      "abstract": "Model Extraction (ME) replicates the performance of another entity\u2019s pretrained model without authorization. While extensively studied in image classification, object detection, and other tasks, ME for image restoration has been scarcely studied despite its broad applications. This paper presents a novel ME framework for image denoising networks, a fundamental one in image restoration. The framework tackles unique challenges like the black-box nature of the victim model, limiting access to its parameters, gradients, and outputs, and the difficulty of acquiring data that matches the original noise distribution while having adequate diversity. Our solution involves simulating the victim\u2019s noise conditions to transform clean images into noisy ones and introducing loss functions to optimize the generator and substitute model. Experiments show that our method closely approximates the victim model\u2019s performance and improves generalization in some scenarios. To the best of our knowledge, this work is the first to address ME in the field of image restoration, paving the way for future research in this area.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Huan Teng",
        "Yuhui Quan",
        "Yong Xu",
        "Jun Huang",
        "Hui Ji"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/54ca14c7326e174dfe3627ced320a122e11d6d29",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "17440e21a757f14e630d7443c89f018b79cc1754",
      "title": "StegGuard: Secrets Encoder and Decoder Act as Fingerprint of Self-Supervised Pretrained Model",
      "abstract": "In this work, we propose StegGuard, a novel fingerprinting mechanism to verify the ownership of a suspect pretrained model using steganography, where the pretrained model is obtained via self-supervised learning. A critical perspective in StegGuard is that the unique characteristic of the transformation from an image to an embedding, conducted by the pretrained model, can be equivalently captured by how an encoder embeds secrets into images and how a decoder extracts them from the embeddings with tolerable error. While each independently trained pretrained model has a distinct transformation, a piracy model exhibits a transformation similar to that of the victim. Based on these observations, StegGuard learns a pair of secrets encoder and decoder as the fingerprint of the victim model. Additionally, a Frequency domain channel attention Embedding block is introduced into the encoder to adaptively embed secrets into suitable frequency bands. During verification, if the secrets embedded into the query images can be extracted with an acceptable error from the embeddings of the query images, the suspect model is determined to be piracy; otherwise, it is deemed independent. Extensive experiments demonstrate that with as few as 100 query images, StegGuard achieves high piracy detection accuracy and robustness against model stealing attacks, including model extraction, fine-tuning, pruning, embedding noising and shuffle. Compared to existing methods, StegGuard consistently achieves lower p-values for piracy models (as low as 1e-14) and higher p-values for independent models (up to 0.99), confirming its effectiveness and reliability.",
      "year": 2025,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xingdong Ren",
        "Hanzhou Wu",
        "Yinggui Wang",
        "Haojie Liu",
        "Xiaofeng Lu",
        "Guangling Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/17440e21a757f14e630d7443c89f018b79cc1754",
      "pdf_url": "",
      "publication_date": "2025-09-15",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d47ec1770b15e2757e747b23b3789f4c24c91bb7",
      "title": "Explore the vulnerability of black-box models via diffusion models",
      "abstract": "Recent advancements in diffusion models have enabled high-fidelity and photorealistic image generation across diverse applications. However, these models also present security and privacy risks, including copyright violations, sensitive information leakage, and the creation of harmful or offensive content that could be exploited maliciously. In this study, we uncover a novel security threat where an attacker leverages diffusion model APIs to generate synthetic images, which are then used to train a high-performing substitute model. This enables the attacker to execute model extraction and transfer-based adversarial attacks on black-box classification models with minimal queries, without needing access to the original training data. The generated images are sufficiently high-resolution and diverse to train a substitute model whose outputs closely match those of the target model. Across the seven benchmarks, including CIFAR and ImageNet subsets, our method shows an average improvement of 27.37% over state-of-the-art methods while using just 0.01 times of the query budget, achieving a 98.68% success rate in adversarial attacks on the target model.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jiacheng Shi",
        "Yanfu Zhang",
        "Huajie Shao",
        "Ashley Gao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d47ec1770b15e2757e747b23b3789f4c24c91bb7",
      "pdf_url": "",
      "publication_date": "2025-06-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "135ea890e84a64ea2b6786cc795b94e214b70050",
      "title": "FlatD: Protecting Deep Neural Network Program from Reversing Attacks",
      "abstract": "The emergence of Deep Learning compilers provides automated optimization and compilation across Deep Learning frameworks and hardware platforms, which enhances the performance of AI service and benefits the deployment to edge devices and low-power processors. However, deep neural network (DNN) programs generated by Deep Learning compilers introduce a new attack interface. They are targeted by new model extraction attacks that can fully or partially rebuild the DNN model by reversing the DNN programs. Unfortunately, no defense countermeasure is designed to hinder this kind of attack. To address the issue, we investigate all of the state-of-the-art reversing-based model extraction attacks and identify an essential component shared across the frameworks. Based on this observation, we propose FlatD, the first defense framework for DNN programs toward reversing-based model extraction attacks. FlatD manipulates and conceals the original Control Flow Graphs of DNN programs based on Control Flow Flattening. Unlike traditional Control Flow Flattening, FlatD ensures the DNN programs are challenging for attackers to recover their Control Flow Graphs and gain necessary information statically. Our evaluation shows that, compared to the traditional Control Flow Flattening (O-LLVM), FlatD provides more effective and stealthy protection to DNN programs with similar performance and lower scale.",
      "year": 2025,
      "venue": "2025 IEEE/ACM 47th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
      "authors": [
        "Jinquan Zhang",
        "Zihao Wang",
        "Dinghao Wu",
        "Pei Wang",
        "Rui Zhong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/135ea890e84a64ea2b6786cc795b94e214b70050",
      "pdf_url": "",
      "publication_date": "2025-04-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0ed57747e5dbfe69ffa822ac1bb9c38067e97c76",
      "title": "Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs",
      "abstract": "As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored. We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuing 48,000 instruction queries to Meditron-7B and collecting 25,000 benign instruction response pairs, we fine-tune a LLaMA3 8B surrogate via parameter efficient LoRA under a zero-alignment supervision setting, requiring no access to model weights, safety filters, or training data. With a cost of $12, the surrogate achieves strong fidelity on benign inputs while producing unsafe completions for 86% of adversarial prompts, far exceeding both Meditron-7B (66%) and the untuned base model (46%). This reveals a pronounced functional-ethical gap, task utility transfers, while alignment collapses. To analyze this collapse, we develop a dynamic adversarial evaluation framework combining Generative Query (GQ)-based harmful prompt generation, verifier filtering, category-wise failure analysis, and adaptive Random Search (RS) jailbreak attacks. We also propose a layered defense system, as a prototype detector for real-time alignment drift in black-box deployments. Our findings show that benign-only black-box distillation exposes a practical and under-recognized threat: adversaries can cheaply replicate medical LLM capabilities while stripping safety mechanisms, underscoring the need for extraction-aware safety monitoring.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Sohely Jahan",
        "Ruimin Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0ed57747e5dbfe69ffa822ac1bb9c38067e97c76",
      "pdf_url": "",
      "publication_date": "2025-12-10",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-12"
    },
    {
      "paper_id": "ad8a3f00e1b77aa19a71a7a166ee6ae533e7e3bc",
      "title": "Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems",
      "abstract": "Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.",
      "year": 2025,
      "venue": "",
      "authors": [
        "A. Foundjem",
        "L. Tidjon",
        "L. D. Silva",
        "Foutse Khomh"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ad8a3f00e1b77aa19a71a7a166ee6ae533e7e3bc",
      "pdf_url": "",
      "publication_date": "2025-12-29",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-12-31"
    },
    {
      "paper_id": "6a4dab913871b24c71bb3e66db20babe2a68880f",
      "title": "DivQAT: Enhancing Robustness of Quantized Convolutional Neural Networks against Model Extraction Attacks",
      "abstract": "Convolutional Neural Networks (CNNs) and their quantized counterparts are vulnerable to extraction attacks, posing a significant threat of IP theft. Yet, the robustness of quantized models against these attacks is little studied compared to large models. Previous defenses propose to inject calculated noise into the prediction probabilities. However, these defenses are limited since they are not incorporated during the model design and are only added as an afterthought after training. Additionally, most defense techniques are computationally expensive and often have unrealistic assumptions about the victim model that are not feasible in edge device implementations and do not apply to quantized models. In this paper, we propose DivQAT, a novel algorithm to train quantized CNNs based on Quantization Aware Training (QAT) aiming to enhance their robustness against extraction attacks. To the best of our knowledge, our technique is the first to modify the quantization process to integrate a model extraction defense into the training process. Through empirical validation on benchmark vision datasets, we demonstrate the efficacy of our technique in defending against model extraction attacks without compromising model accuracy. Furthermore, combining our quantization technique with other defense mechanisms improves their effectiveness compared to traditional QAT.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Kacem Khaled",
        "F. Magalh\u00e3es",
        "G. Nicolescu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6a4dab913871b24c71bb3e66db20babe2a68880f",
      "pdf_url": "",
      "publication_date": "2025-12-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2026-01-02"
    },
    {
      "paper_id": "9487d29364645c2f086387ff817ad5fd14b33c41",
      "title": "A Comprehensive Defense Framework Against Model Extraction Attacks",
      "abstract": "As a promising service, Machine Learning as a Service (MLaaS) provides personalized inference functions for clients through paid APIs. Nevertheless, it is vulnerable to model extraction attacks, in which an attacker can extract a functionally-equivalent model by repeatedly querying the APIs with crafted samples. While numerous works have been proposed to defend against model extraction attacks, existing efforts are accompanied by limitations and low comprehensiveness. In this article, we propose AMAO, a comprehensive defense framework against model extraction attacks. Specifically, AMAO consists of four interlinked successive phases: adversarial training is first exploited to weaken the effectiveness of model extraction attacks. Then, malicious query detection is used to detect malicious queries and mark malicious users. After that, we develop a label-flipping poisoning attack to instruct the adaptive query responses to malicious users. Besides, the image pHash algorithm is employed to ensure the indistinguishability of the query responses. Finally, the perturbed results are served as a backdoor to verify the ownership of any suspicious model. Extensive experiments demonstrate that AMAO outperforms existing defenses in defending against model extraction attacks and is also robust against the adaptive adversary who is aware of the defense.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Wenbo Jiang",
        "Hongwei Li",
        "Guowen Xu",
        "Tianwei Zhang",
        "Rongxing Lu"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/9487d29364645c2f086387ff817ad5fd14b33c41",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3c3479215bcc775f8e37d7d5055a16b698904c9a",
      "title": "FedDSE: Distribution-aware Sub-model Extraction for Federated Learning over Resource-constrained Devices",
      "abstract": "Sub-model extraction based federated learning has emerged as a popular strategy for training models on resource-constrained devices. However, existing methods treat all clients equally and extract sub-models using predetermined rules, which disregard the statistical heterogeneity across clients and may lead to fierce competition among them. Specifically, this paper identifies that when making predictions, different clients tend to activate different neurons of the entire model related to their respective distributions. If highly activated neurons from some clients with one distribution are incorporated into the sub-model allocated to other clients with different distributions, they will be forced to fit the new distributions, which can hinder their activation over the previous clients and result in a performance reduction. Motivated by this finding, we propose a novel method called FedDSE, which can reduce the conflicts among clients by extracting sub-models based on the data distribution of each client. The core idea of FedDSE is to empower each client to adaptively extract neurons from the entire model based on their activation over the local dataset. We theoretically show that FedDSE can achieve an improved classification score and convergence over general neural networks with the ReLU activation function. Experimental results on various datasets and models show that FedDSE outperforms all state-of-the-art baselines.",
      "year": 2024,
      "venue": "The Web Conference",
      "authors": [
        "Haozhao Wang",
        "Yabo Jia",
        "Meng Zhang",
        "Qi Hu",
        "Hao Ren",
        "Peng Sun",
        "Yonggang Wen",
        "Tianwei Zhang"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/3c3479215bcc775f8e37d7d5055a16b698904c9a",
      "pdf_url": "",
      "publication_date": "2024-05-13",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3b82c1d871b0d5ab043e96cc4a73b77dfd03695e",
      "title": "Unraveling Attacks to Machine-Learning-Based IoT Systems: A Survey and the Open Libraries Behind Them",
      "abstract": "The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This article embarks on a comprehensive exploration of the security threats arising from ML\u2019s integration into various facets of IoT, spanning various attack types, including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria, such as adversary models, attack targets, and key security attributes (confidentiality, integrity, and availability). We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts. Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy. We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape. Through our comprehensive review and analysis, this article seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Chao Liu",
        "Boxi Chen",
        "Wei Shao",
        "Chris Zhang",
        "Kelvin Wong",
        "Yi Zhang"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/3b82c1d871b0d5ab043e96cc4a73b77dfd03695e",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "14b7cdf6349611cdc3d271c1672ef275e0d101ae",
      "title": "MEA-Defender: A Robust Watermark against Model Extraction Attack",
      "abstract": "Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been trained using deep learning algorithms. To protect the Intellectual Property (IP) of the original owners over such DNN models, backdoor-based watermarks have been extensively studied. However, most of such watermarks fail upon model extraction attack, which utilizes input samples to query the target model and obtains the corresponding outputs, thus training a substitute model using such input-output pairs. In this paper, we propose a novel watermark to protect IP of DNN models against model extraction, named MEA-Defender. In particular, we obtain the watermark by combining two samples from two source classes in the input domain and design a watermark loss function that makes the output domain of the watermark within that of the main task samples. Since both the input domain and the output domain of our watermark are indispensable parts of those of the main task samples, the watermark will be extracted into the stolen model along with the main task during model extraction. We conduct extensive experiments on four model extraction attacks, using five datasets and six models trained based on supervised learning and self-supervised learning algorithms. The experimental results demonstrate that MEA-Defender is highly robust against different model extraction attacks, and various watermark removal/detection approaches.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Peizhuo Lv",
        "Hualong Ma",
        "Kai Chen",
        "Jiachen Zhou",
        "Shengzhi Zhang",
        "Ruigang Liang",
        "Shenchen Zhu",
        "Pan Li",
        "Yingjun Zhang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/14b7cdf6349611cdc3d271c1672ef275e0d101ae",
      "pdf_url": "https://arxiv.org/pdf/2401.15239",
      "publication_date": "2024-01-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "52a0ceaf916deff733410f07e05215826bb1c86d",
      "title": "LiteMoE: Customizing On-device LLM Serving via Proxy Submodel Tuning",
      "abstract": "Considering limited on-device resources, current practices are attempting to deploy a system-level mixture-of-experts (MoE)-based foundation LLM shared by multiple mobile apps on a device to support mobile intelligence. However, mobile apps are hard to customize their services that require tuning adapters associated with the LLM using private in-app data. The difficulty arises due to both the limited on-device resources and the restricted control that apps have over the foundation LLM. To address this issue, in this work, we propose LiteMoE, a novel proxy submodel tuning framework that supports mobile apps to efficiently fine-tune customized adapters on devices using proxy submodels. The key technique behind LiteMoE is a post-training submodel extraction method, whereby without additional re-training, we can identify and reserve critical experts, match and merge moderate experts, to extract a lightweight and effective proxy submodel from the foundation LLM for a certain app. We implemented a prototype of LiteMoE and evaluated it over various MoE-based LLMs and mobile computing tasks. The results show that with LiteMoE, mobile apps are able to fine-tune customized adapters on resource-limited devices, achieving 12.7% accuracy improvement and 6.6\u00d7 memory reduction compared with operating the original foundation LLM.",
      "year": 2024,
      "venue": "ACM International Conference on Embedded Networked Sensor Systems",
      "authors": [
        "Zhuang Yan",
        "Zhenzhe Zheng",
        "Fan Wu",
        "Guihai Chen"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/52a0ceaf916deff733410f07e05215826bb1c86d",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3666025.3699355",
      "publication_date": "2024-11-04",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "title": "TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) have been widely applied in various scenarios. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct model stealing (MS) attacks. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead. To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices. The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE, which can freshly authorize each request based on its input. Extensive experiments show that TransLinkGuard achieves the same security as the black-box guarantees with negligible overhead.",
      "year": 2024,
      "venue": "ACM Multimedia",
      "authors": [
        "Qinfeng Li",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Yangfan Xie",
        "Xuhong Zhang",
        "Tianyu Du",
        "Sheng Cheng",
        "Xun Wang",
        "Jianwei Yin"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "pdf_url": "https://arxiv.org/pdf/2404.11121",
      "publication_date": "2024-04-17",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1e8b05c5b703f443b5470bef2b3aca1b064b1709",
      "title": "DeMistify: Identifying On-Device Machine Learning Models Stealing and Reuse Vulnerabilities in Mobile Apps",
      "abstract": "Mobile apps have become popular for providing artificial intelligence (AI) services via on-device machine learning (ML) techniques. Unlike accomplishing these AI services on remote servers traditionally, these on-device techniques process sensitive information required by AI services locally, which can mitigate the severe con-cerns of the sensitive data collection on the remote side. However, these on-device techniques have to push the core of ML expertise (e.g., models) to smartphones locally, which are still subject to similar vulnerabilities on the remote clouds and servers, especially when facing the model stealing attack. To defend against these attacks, developers have taken various protective measures. Unfor-tunately, we have found that these protections are still insufficient, and on-device ML models in mobile apps could be extracted and reused without limitation. To better demonstrate its inadequate protection and the feasibility of this attack, this paper presents DeMistify, which statically locates ML models within an app, slices relevant execution components, and finally generates scripts auto-matically to instrument mobile apps to successfully steal and reuse target ML models freely. To evaluate DeMistify and demonstrate its applicability, we apply it on 1,511 top mobile apps using on-device ML expertise for several ML services based on their install numbers from Google Play and DeMistify can successfully execute 1250 of them (82.73%). In addition, an in-depth study is conducted to understand the on-device ML ecosystem in the mobile application.",
      "year": 2024,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Pengcheng Ren",
        "Chaoshun Zuo",
        "Xiaofeng Liu",
        "Wenrui Diao",
        "Qingchuan Zhao",
        "Shanqing Guo"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/1e8b05c5b703f443b5470bef2b3aca1b064b1709",
      "pdf_url": "",
      "publication_date": "2024-02-06",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0e8594a161f1670e939a16699bd5cc8fd2f8335a",
      "title": "QUEEN: Query Unlearning Against Model Extraction",
      "abstract": "Model extraction attacks currently pose a non-negligible threat to the security and privacy of deep learning models. By querying the model with a small dataset and using the query results as the ground-truth labels, an adversary can steal a piracy model with performance comparable to the original model. Two key issues that cause the threat are, on the one hand, accurate and unlimited queries can be obtained by the adversary; on the other hand, the adversary can aggregate the query results to train the model step by step. The existing defenses usually employ model watermarking or fingerprinting to protect the ownership. However, these methods cannot proactively prevent the violation from happening. To mitigate the threat, we propose QUEEN (QUEry unlEarNing) that proactively launches counterattacks on potential model extraction attacks from the very beginning. To limit the potential threat, QUEEN has sensitivity measurement and outputs perturbation that prevents the adversary from training a piracy model with high performance. In sensitivity measurement, QUEEN measures the single query sensitivity by its distance from the center of its cluster in the feature space. To reduce the learning accuracy of attacks, for the highly sensitive query batch, QUEEN applies query unlearning, which is implemented by gradient reverse to perturb the softmax output such that the piracy model will generate reverse gradients to worsen its performance unconsciously. Experiments show that QUEEN outperforms the state-of-the-art defenses against various model extraction attacks with a relatively low cost to the model accuracy. The artifact is publicly available at https://github.com/MaraPapMann/QUEEN.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Lefeng Zhang",
        "Bo Liu",
        "Derui Wang",
        "Wanlei Zhou",
        "Minhui Xue"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/0e8594a161f1670e939a16699bd5cc8fd2f8335a",
      "pdf_url": "http://arxiv.org/pdf/2407.01251",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8f0679e01cb36c995fc5fbc9d364160bd72ce8cd",
      "title": "Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum Neural Networks",
      "abstract": "Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack. In this study, we assess the efficacy of such attacks in the realm of quantum computing. Our findings revealed that model stealing attacks can produce clone models achieving up to 0.9 \u00d7 and 0.99 \u00d7 clone test accuracy when trained using Top-1 and Top-k labels, respectively (k: num_classes). To defend against these attacks, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP). Despite limited success with our defense techniques, it has led to an important discovery: QML models trained on noisy hardwares are naturally resistant to perturbation or obfuscation-based defenses or attacks.",
      "year": 2024,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Satwik Kundu",
        "Debarshi Kundu",
        "Swaroop Ghosh"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/8f0679e01cb36c995fc5fbc9d364160bd72ce8cd",
      "pdf_url": "https://arxiv.org/pdf/2402.11687",
      "publication_date": "2024-02-18",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f3892c3b89b4b6fca4465308fcc9a99388eb019b",
      "title": "QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines",
      "abstract": "Variational quantum circuits (VQCs) have become a powerful tool for implementing Quantum Neural Networks (QNNs), addressing a wide range of complex problems. Well-trained VQCs serve as valuable intellectual assets hosted on cloud-based Noisy Intermediate Scale Quantum (NISQ) computers, making them susceptible to malicious VQC stealing attacks. However, traditional model extraction techniques designed for classical machine learning models encounter challenges when applied to NISQ computers due to significant noise in current devices. In this paper, we introduce QuantumLeak, an effective and accurate QNN model extraction technique from cloud-based NISQ machines. Compared to existing classical model stealing techniques, QuantumLeak improves local VQC accuracy by 4.99%~7.35% across diverse datasets and VQC architectures.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhenxiao Fu",
        "Min Yang",
        "Cheng Chu",
        "Yilun Xu",
        "Gang Huang",
        "Fan Chen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/f3892c3b89b4b6fca4465308fcc9a99388eb019b",
      "pdf_url": "http://arxiv.org/pdf/2403.10790",
      "publication_date": "2024-03-16",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "title": "Model Stealing for Any Low-Rank Language Model",
      "abstract": "Model stealing, where a learner tries to recover an unknown model via carefully chosen queries, is a critical problem in machine learning, as it threatens the security of proprietary models and the privacy of data they are trained on. In recent years, there has been particular interest in stealing large language models (LLMs). In this paper, we aim to build a theoretical understanding of stealing language models by studying a simple and mathematically tractable setting. We study model stealing for Hidden Markov Models (HMMs), and more generally low-rank language models. We assume that the learner works in the conditional query model, introduced by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient algorithm in the conditional query model, for learning any low-rank distribution. In other words, our algorithm succeeds at stealing any language model whose output distribution is low-rank. This improves upon the previous result which also requires the unknown distribution to have high \u201cfidelity\u201d \u2013 a property that holds only in restricted cases. There are two key insights behind our algorithm: First, we represent the conditional distributions at each timestep by constructing barycentric spanners among a collection of vectors of exponentially large dimension. Second, for sampling from our representation, we iteratively solve a sequence of convex optimization problems that involve projection in relative entropy to prevent compounding of errors over the length of the sequence. This is an interesting example where, at least theoretically, allowing a machine learning model to solve more complex problems at inference time can lead to drastic improvements in its performance.",
      "year": 2024,
      "venue": "Symposium on the Theory of Computing",
      "authors": [
        "Allen Liu",
        "Ankur Moitra"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "pdf_url": "",
      "publication_date": "2024-11-12",
      "keywords_matched": [
        "model stealing",
        "stealing language model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6dc6c055a006b3b8bbbd10a44336877c9b190907",
      "title": "Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them",
      "abstract": "The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine Learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This paper embarks on a comprehensive exploration of the security threats arising from ML's integration into various facets of IoT, spanning various attack types including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria such as adversary models, attack targets, and key security attributes (confidentiality, availability, and integrity). We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts. Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy. We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape. Through our comprehensive review and analysis, this paper seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Chao Liu",
        "Boxi Chen",
        "Wei Shao",
        "Chris Zhang",
        "Kelvin Wong",
        "Yi Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/6dc6c055a006b3b8bbbd10a44336877c9b190907",
      "pdf_url": "",
      "publication_date": "2024-01-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c535f60659724b84d5a2169d434617ba49f005bf",
      "title": "CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) exhibit strong generalization capabilities across diverse tasks and are increasingly deployed on edge devices for efficiency and privacy reasons. However, deploying proprietary LLMs at the edge without adequate protection introduces critical security threats. Attackers can extract model weights and architectures, enabling unauthorized copying and misuse. Even when protective measures prevent full extraction of model weights, attackers may still perform advanced attacks, such as fine-tuning, to further exploit the model. Existing defenses against these threats typically incur significant computational and communication overhead, making them impractical for edge deployment. To safeguard the edge-deployed LLMs, we introduce CoreGuard, a computation- and communication-efficient protection method. CoreGuard employs an efficient protection protocol to reduce computational overhead and minimize communication overhead via a propagation protocol. Extensive experiments show that CoreGuard achieves upper-bound security protection with negligible overhead.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Qinfeng Li",
        "Yangfan Xie",
        "Tianyu Du",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Hao Peng",
        "Xinkui Zhao",
        "Xianwei Zhu",
        "Jianwei Yin",
        "Xuhong Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/c535f60659724b84d5a2169d434617ba49f005bf",
      "pdf_url": "",
      "publication_date": "2024-10-16",
      "keywords_matched": [
        "model stealing",
        "extract model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b892ac05369526432384a4cdf1d4d087f8bc45de",
      "title": "Beyond Slow Signs in High-fidelity Model Extraction",
      "abstract": "Deep neural networks, costly to train and rich in intellectual property value, are increasingly threatened by model extraction attacks that compromise their confidentiality. Previous attacks have succeeded in reverse-engineering model parameters up to a precision of float64 for models trained on random data with at most three hidden layers using cryptanalytical techniques. However, the process was identified to be very time consuming and not feasible for larger and deeper models trained on standard benchmarks. Our study evaluates the feasibility of parameter extraction methods of Carlini et al. [1] further enhanced by Canales-Mart\\'inez et al. [2] for models trained on standard benchmarks. We introduce a unified codebase that integrates previous methods and reveal that computational tools can significantly influence performance. We develop further optimisations to the end-to-end attack and improve the efficiency of extracting weight signs by up to 14.8 times compared to former methods through the identification of easier and harder to extract neurons. Contrary to prior assumptions, we identify extraction of weights, not extraction of weight signs, as the critical bottleneck. With our improvements, a 16,721 parameter model with 2 hidden layers trained on MNIST is extracted within only 98 minutes compared to at least 150 minutes previously. Finally, addressing methodological deficiencies observed in previous studies, we propose new ways of robust benchmarking for future model extraction attacks.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Hanna Foerster",
        "Robert D. Mullins",
        "Ilia Shumailov",
        "Jamie Hayes"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/b892ac05369526432384a4cdf1d4d087f8bc45de",
      "pdf_url": "",
      "publication_date": "2024-06-14",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee51e69d40e027a6fd3fb2dca78d520e83737ae1",
      "title": "Rethinking Adversarial Robustness in the Context of the Right to be Forgotten",
      "abstract": "The past few years have seen an intense research interest in the practical needs of the \u201cright to be forgotten\u201d, which has motivated researchers to develop machine unlearning methods to unlearn a fraction of training data and its lineage. While existing machine unlearning methods prioritize the protection of individuals\u2019 private data, they over-look investigating the unlearned models\u2019 susceptibility to adversarial attacks and security breaches. In this work, we uncover a novel security vulnerability of machine unlearning based on the insight that adversarial vulnerabilities can be bol-stered, especially for adversarially robust models. To exploit this observed vulnerability, we pro-pose a novel attack called Adv ersarial U nlearning A ttack (AdvUA), which aims to generate a small fraction of malicious unlearning requests during the unlearning process. AdvUA causes a significant reduction of adversarial robustness in the unlearned model compared to the original model, providing an entirely new capability for adversaries that is infeasible in conventional machine learning pipelines. Notably, we also show that AdvUA can effectively enhance model stealing attacks by extracting additional decision boundary information, further emphasizing the breadth and significance of our research. We also conduct both theoretical analysis and computational complexity of AdvUA. Extensive numerical studies are performed to demonstrate the effectiveness and efficiency of the proposed attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Chenxu Zhao",
        "Wei Qian",
        "Yangyi Li",
        "Aobo Chen",
        "Mengdi Huai"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/ee51e69d40e027a6fd3fb2dca78d520e83737ae1",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-20"
    },
    {
      "paper_id": "74ed0e78bba8fda5a72d3db69593f4caeca82559",
      "title": "TPUXtract: An Exhaustive Hyperparameter Extraction Framework",
      "abstract": "Model stealing attacks on AI/ML devices undermine intellectual property rights, compromise the competitive advantage of the original model developers, and potentially expose sensitive data embedded in the model\u2019s behavior to unauthorized parties. While previous research works have demonstrated successful side-channelbased model recovery in embedded microcontrollers and FPGA-based accelerators, the exploration of attacks on commercial ML accelerators remains largely unexplored. Moreover, prior side-channel attacks fail when they encounter previously unknown models. This paper demonstrates the first successful model extraction attack on the Google Edge Tensor Processing Unit (TPU), an off-the-shelf ML accelerator. Specifically, we show a hyperparameter stealing attack that can extract all layer configurations including the layer type, number of nodes, kernel/filter sizes, number of filters, strides, padding, and activation function. Most notably, our attack is the first comprehensive attack that can extract previously unseen models. This is achieved through an online template-building approach instead of a pre-trained ML-based approach used in prior works. Our results on a black-box Google Edge TPU evaluation show that, through obtained electromagnetic traces, our proposed framework can achieve 99.91% accuracy, making it the most accurate one to date. Our findings indicate that attackers can successfully extract various types of models on a black-box commercial TPU with utmost detail and call for countermeasures.",
      "year": 2024,
      "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.",
      "authors": [
        "Ashley Kurian",
        "Anuj Dubey",
        "Ferhat Yaman",
        "Aydin Aysu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/74ed0e78bba8fda5a72d3db69593f4caeca82559",
      "pdf_url": "https://doi.org/10.46586/tches.v2025.i1.78-103",
      "publication_date": "2024-12-09",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0beb8f26d688ce182cea25c361e0c3311b1afb69",
      "title": "Inversion-Guided Defense: Detecting Model Stealing Attacks by Output Inverting",
      "abstract": "Model stealing attacks involve creating copies of machine learning models that have similar functionalities to the original model without proper authorization. Such attacks raise significant concerns about the intellectual property of the machine learning models. Nonetheless, current defense mechanisms against such attacks tend to exhibit certain drawbacks, notably in terms of utility, and robustness. For example, watermarking-based defenses require victim models to be retrained for embedding watermarks, which can potentially impact the main task performance. Moreover, other defenses, especially fingerprinting-based methods, often rely on specific samples like adversarial examples to verify ownership of the target model. These approaches might prove less robust against adaptive attacks, such as model stealing with adversarial training. It remains unclear whether normal examples, as opposed to adversarial ones, can effectively reflect the characteristics of stolen models. To tackle these challenges, we propose a novel method that leverages a neural network as a decoder to inverse the suspicious model\u2019s outputs. Inspired by model inversion attacks, we argue that this decoding process will unveil hidden patterns inherent in the original outputs of the suspicious model. Drawing from these decoding outcomes, we calculate specific metrics to determine the legitimacy of the suspicious models. We validate the efficacy of our defense technique against diverse model stealing attacks, specifically within the domain of classification tasks based on deep neural networks.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Shuai Zhou",
        "Tianqing Zhu",
        "Dayong Ye",
        "Wanlei Zhou",
        "Wei Zhao"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/0beb8f26d688ce182cea25c361e0c3311b1afb69",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9fef6937c39de73fdc3531790e938274caa84de0",
      "title": "AugSteal: Advancing Model Steal With Data Augmentation in Active Learning Frameworks",
      "abstract": "With the proliferation of machine learning models in diverse applications, the issue of model security has increasingly become a focal point. Model steal attacks can cause significant financial losses to model owners and potentially threaten the security of their application scenarios. Traditional model steal attacks are primarily directed at soft-label black boxes, but their effectiveness significantly diminishes or even fails in hard-label scenarios. To address this, for hard-label black boxes, this study proposes an active learning-based Fusion Augmentation Model Stealing Framework (AugSteal). This framework initially utilizes large-scale irrelevant public datasets for deep filtering and feature extraction to generate reliable, diverse, and representative high-quality data subsets as the stealing dataset. Subsequently, we developed an adaptive active learning selection strategy that selects data samples with significant information gain for different black-box models, enhancing the attack\u2019s specificity and effectiveness. Finally, to further address the trade-off between query budget and steal precision, this paper designed a Fusion Augmentation training method constituted of two different loss functions, enabling the substitute model to closely approximate the decision distribution of the target black box.The comprehensive experimental results indicate that, compared to the current state-of-the-art attack methods, our approach achieved a maximum performance gain of 8.21% in functional similarity for the substitute models in simulated black-box scenarios CIFAR10, SVHN, CALTECH256, and the real-world application Tencent Cloud API.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Lijun Gao",
        "Wenjun Liu",
        "Kai Liu",
        "Jiehong Wu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9fef6937c39de73fdc3531790e938274caa84de0",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fbf3119f69e29cebac200d1b0e9a2f7440e1845d",
      "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
      "abstract": "Machine learning has revolutionized numerous domains, playing a crucial role in driving advancements and enabling data-centric processes. The significance of data in training models and shaping their performance cannot be overstated. Recent research has highlighted the heterogeneous impact of individual data samples, particularly the presence of valuable data that significantly contributes to the utility and effectiveness of machine learning models. However, a critical question remains unanswered: are these valuable data samples more vulnerable to machine learning attacks? In this work, we investigate the relationship between data importance and machine learning attacks by analyzing five distinct attack types. Our findings reveal notable insights. For example, we observe that high importance data samples exhibit increased vulnerability in certain attacks, such as membership inference and model stealing. By analyzing the linkage between membership inference vulnerability and data importance, we demonstrate that sample characteristics can be integrated into membership metrics by introducing sample-specific criteria, therefore enhancing the membership inference performance. These findings emphasize the urgent need for innovative defense mechanisms that strike a balance between maximizing utility and safeguarding valuable data against potential exploitation.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Rui Wen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/fbf3119f69e29cebac200d1b0e9a2f7440e1845d",
      "pdf_url": "",
      "publication_date": "2024-09-05",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "62c9a0ed00f28ac6aedbee59ba11d26bb486216f",
      "title": "Privacy Implications of Explainable AI in Data-Driven Systems",
      "abstract": "Machine learning (ML) models, demonstrably powerful, suffer from a lack of interpretability. The absence of transparency, often referred to as the black box nature of ML models, undermines trust and urges the need for efforts to enhance their explainability. Explainable AI (XAI) techniques address this challenge by providing frameworks and methods to explain the internal decision-making processes of these complex models. Techniques like Counterfactual Explanations (CF) and Feature Importance play a crucial role in achieving this goal. Furthermore, high-quality and diverse data remains the foundational element for robust and trustworthy ML applications. In many applications, the data used to train ML and XAI explainers contain sensitive information. In this context, numerous privacy-preserving techniques can be employed to safeguard sensitive information in the data, such as differential privacy. Subsequently, a conflict between XAI and privacy solutions emerges due to their opposing goals. Since XAI techniques provide reasoning for the model behavior, they reveal information relative to ML models, such as their decision boundaries, the values of features, or the gradients of deep learning models when explanations are exposed to a third entity. Attackers can initiate privacy breaching attacks using these explanations, to perform model extraction, inference, and membership attacks. This dilemma underscores the challenge of finding the right equilibrium between understanding ML decision-making and safeguarding privacy.",
      "year": 2024,
      "venue": "xAI",
      "authors": [
        "Fatima Ezzeddine"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/62c9a0ed00f28ac6aedbee59ba11d26bb486216f",
      "pdf_url": "",
      "publication_date": "2024-06-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "94621729b6ca9e811d8a052dc3457e98df457676",
      "title": "Fully Exploiting Every Real Sample: SuperPixel Sample Gradient Model Stealing",
      "abstract": "Model stealing (MS) involves querying and observing the output of a machine learning model to steal its capabilities. The quality of queried data is crucial, yet obtaining a large amount of real data for MS is often challenging. Recent works have reduced reliance on real data by using generative models. However, when high-dimensional query data is required, these methods are impractical due to the high costs of querying and the risk of model collapse. In this work, we propose using sample gradients (SG) to enhance the utility of each real sample, as SG provides crucial guidance on the decision boundaries of the victim model. However, utilizing SG in the model stealing scenario faces two challenges: 1. Pixel-level gradient estimation requires ex-tensive query volume and is susceptible to defenses. 2. The estimation of sample gradients has a significant variance. This paper proposes Superpixel Sample Gradient stealing (SPSG) for model stealing under the constraint of limited real samples. With the basic idea of imitating the victim model's low-variance patch-level gradients instead ofpixel-level gradients, SPSG achieves efficient sample gradient es-timation through two steps. First, we perform patch-wise perturbations on query images to estimate the average gradient in different regions of the image. Then, we filter the gradients through a threshold strategy to reduce variance. Exhaustive experiments demonstrate that, with the same number of real samples, SPSG achieves accuracy, agreements, and adversarial success rate significantly surpassing the current state-of-the-art MS methods. Codes are available at https://github.com/zyI123456aBISPSG_attack.",
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Yunlong Zhao",
        "Xiaoheng Deng",
        "Yijing Liu",
        "Xin-jun Pei",
        "Jiazhi Xia",
        "Wei Chen"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/94621729b6ca9e811d8a052dc3457e98df457676",
      "pdf_url": "https://arxiv.org/pdf/2406.18540",
      "publication_date": "2024-05-18",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f63f7d623e1738fbc314143a1ad1812045caffff",
      "title": "Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are recognized as potent tools for processing real-world data organized in graph structures. Especially inductive GNNs, which allow for the processing of graph-structured data without relying on predefined graph structures, are becoming increasingly important in a wide range of applications. As such these networks become attractive targets for model-stealing attacks where an adversary seeks to replicate the functionality of the targeted network. Significant efforts have been devoted to developing model-stealing attacks that extract models trained on images and texts. However, little attention has been given to stealing GNNs trained on graph data. This paper identifies a new method of performing unsupervised model-stealing attacks against inductive GNNs, utilizing graph contrastive learning and spectral graph augmentations to efficiently extract information from the targeted model. The new type of attack is thoroughly evaluated on six datasets and the results show that our approach outperforms the current state-of-the-art by Shen et al. (2021). In particular, our attack surpasses the baseline across all benchmarks, attaining superior fidelity and downstream accuracy of the stolen model while necessitating fewer queries directed toward the target model.",
      "year": 2024,
      "venue": "European Conference on Artificial Intelligence",
      "authors": [
        "Marcin Podhajski",
        "Jan Dubi'nski",
        "Franziska Boenisch",
        "Adam Dziedzic",
        "A. Pregowska",
        "Tomasz P. Michalak"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/f63f7d623e1738fbc314143a1ad1812045caffff",
      "pdf_url": "",
      "publication_date": "2024-05-20",
      "keywords_matched": [
        "extract model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1ab2ad9f96ffe88bb1e8817984ad78a1e9d7bf86",
      "title": "Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation",
      "abstract": "Model extraction attacks (MEAs) enable an attacker to replicate the functionality of a victim deep neural network (DNN) model by only querying its API service remotely, posing a severe threat to the security and integrity of pay-per-query DNN-based services. Although the majority of current research on MEAs has primarily concentrated on neural classifiers, there is a growing prevalence of image-to-image translation (I2IT) tasks in our everyday activities. However, techniques developed for MEA of DNN classifiers cannot be directly transferred to the case of I2IT, rendering the vulnerability of I2IT models to MEA attacks often underestimated. This paper unveils the threat of MEA in I2IT tasks from a new perspective. Diverging from the traditional approach of bridging the distribution gap between attacker queries and victim training samples, we opt to mitigate the effect caused by the different distributions, known as the domain shift. This is achieved by introducing a new regularization term that penalizes high-frequency noise, and seeking a flatter minimum to avoid overfitting to the shifted distribution. Extensive experiments on different image translation tasks, including image super-resolution and style transfer, are performed on different backbone victim models, and the new design consistently outperforms the baseline by a large margin across all metrics. A few real-life I2IT APIs are also verified to be extremely vulnerable to our attack, emphasizing the need for enhanced defenses and potentially revised API publishing policies.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Di Mi",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Shengshan Hu",
        "Qi Zhong",
        "Haizhuan Yuan",
        "Shirui Pan"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/1ab2ad9f96ffe88bb1e8817984ad78a1e9d7bf86",
      "pdf_url": "https://research-repository.griffith.edu.au/bitstreams/4224c508-a124-40eb-94f2-46c6b9821946/download",
      "publication_date": "2024-03-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a35f48cf47cf57ee86246106e7e99d071e31b30e",
      "title": "Revisiting Black-box Ownership Verification for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for processing graph-structured data, enabling applications in various domains. Yet, GNNs are vulnerable to model extraction attacks, imposing risks to intellectual property. To mitigate model extraction attacks, model ownership verification is considered an effective method. However, throughout a series of empirical studies, we found that the existing GNN ownership verification methods either mandate unrealistic conditions or present unsatisfactory accuracy under the most practical settings\u2014the black-box setting where the verifier only requires access to the final output (e.g., posterior probability) of the target model and the suspect model.Inspired by the studies, we propose a new, black-box GNN ownership verification method that involves local independent models and shadow surrogate models to train a classifier for performing ownership verification. Our method boosts the verification accuracy by exploiting two insights: (1) We consider the overall behaviors of the target model for decision-making, better utilizing its holistic fingerprinting; (2) We enrich the fingerprinting of the target model by masking a subset of features of its training data, injecting extra information to facilitate ownership verification.To assess the effectiveness of our proposed method, we perform an intensive series of evaluations with 5 popular datasets, 5 mainstream GNN architectures, and 16 different settings. Our method achieves nearly perfect accuracy with a marginal impact on the target model in all cases, significantly outperforming the existing methods and enlarging their practicality. We also demonstrate that our method maintains robustness against adversarial attempts to evade the verification.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Ruikai Zhou",
        "Kang Yang",
        "Xiuling Wang",
        "Wendy Hui Wang",
        "Jun Xu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a35f48cf47cf57ee86246106e7e99d071e31b30e",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "be44a0e3c11ffe4944873eb415ced25ed36f7534",
      "title": "Protecting Confidential Virtual Machines from Hardware Performance Counter Side Channels",
      "abstract": "In modern cloud platforms, it is becoming more important to preserve the privacy of guest virtual machines (VMs) from the untrusted host. To this end, Secure Encrypted Virtualization (SEV) is developed as a hardware extension to protect VMs by encrypting their memory pages and register states. Unfortunately, such confidential VMs are still vulnerable to micro-architectural side channels, and Hardware Performance Counters (HPCs) are a prominent information leakage source. To make matters worse, currently there is no systematic defense against the HPC side channels. We introduce Aegis, a unified framework for demystifying the inherent relations between the instruction execution and HPC event statistics, and defending VMs against HPC side channels with provable privacy guarantee and minimal performance overhead. Aegis consists of three modules. Application Profiler profiles the application offline and adopts information theory to quantitatively estimate the vulnerability of HPC events. Event Fuzzer leverages the fuzzing technique to automatically generate interesting inputs, i.e., instruction sequences, that can effectively alter the HPC observations. Event Obfuscator injects noisy instructions into the protected VM based on the differential privacy mechanisms for high efficiency and privacy. We present three case studies to demonstrate that Aegis can defeat different types of HPC side-channel attacks (i.e., website fingerprinting, DNN model extraction, keystroke sniffing). Evaluations show that Aegis can effectively decrease the attack accuracy from 90% to 2%, with only 3% overhead on the application execution time and 7% overhead on the CPU usage.",
      "year": 2024,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Xiaoxuan Lou",
        "Kangjie Chen",
        "Guowen Xu",
        "Han Qiu",
        "Shangwei Guo",
        "Tianwei Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/be44a0e3c11ffe4944873eb415ced25ed36f7534",
      "pdf_url": "",
      "publication_date": "2024-06-24",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e1412b3c4eca9421bd6c53432d7b1524ddf17c50",
      "title": "Adversarial Machine Learning In Network Security: A Systematic Review Of Threat Vectors And Defense Mechanisms",
      "abstract": "Adversarial Machine Learning (AML) has emerged as a critical area of research within network security, addressing the evolving challenge of adversaries exploiting machine learning (ML) models. This systematic review adopts the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) methodology to comprehensively examine threat vectors and defense mechanisms in AML. The study identifies, categorizes, and evaluates existing research focused on adversarial attacks targeting ML algorithms in network security applications, including evasion, poisoning, and model extraction attacks. By rigorously following the PRISMA guidelines, a systematic search across multiple scholarly databases yielded a robust dataset of peer-reviewed articles that were screened, reviewed, and analyzed for inclusion. The review outlines key adversarial techniques employed against ML systems, such as gradient-based attack strategies and black-box attacks and explores the underlying vulnerabilities in network security architectures. Additionally, it highlights defense mechanisms, including adversarial training, input preprocessing, and robust model design, discussing their efficacy and limitations in mitigating adversarial threats. The study also identifies critical gaps in current research, such as the lack of standardized benchmarking for adversarial defenses and the need for scalable and real-time AML solutions.",
      "year": 2024,
      "venue": "Innovatech Engineering Journal",
      "authors": [
        "Abdul Awal Mintoo",
        "Ashrafur Rahman Nabil",
        "Md Ashraful Alam",
        "Imran Ahmad"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e1412b3c4eca9421bd6c53432d7b1524ddf17c50",
      "pdf_url": "",
      "publication_date": "2024-11-14",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2b28136746a29ff698f57106c292d0d6e0181629",
      "title": "Efficient Data-Free Model Stealing with Label Diversity",
      "abstract": "Machine learning as a Service (MLaaS) allows users to query the machine learning model in an API manner, which provides an opportunity for users to enjoy the benefits brought by the high-performance model trained on valuable data. This interface boosts the proliferation of machine learning based applications, while on the other hand, it introduces the attack surface for model stealing attacks. Existing model stealing attacks have relaxed their attack assumptions to the data-free setting, while keeping the effectiveness. However, these methods are complex and consist of several components, which obscure the core on which the attack really depends. In this paper, we revisit the model stealing problem from a diversity perspective and demonstrate that keeping the generated data samples more diverse across all the classes is the critical point for improving the attack performance. Based on this conjecture, we provide a simplified attack framework. We empirically signify our conjecture by evaluating the effectiveness of our attack, and experimental results show that our approach is able to achieve comparable or even better performance compared with the state-of-the-art method. Furthermore, benefiting from the absence of redundant components, our method demonstrates its advantages in attack efficiency and query budget.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yiyong Liu",
        "Rui Wen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2b28136746a29ff698f57106c292d0d6e0181629",
      "pdf_url": "",
      "publication_date": "2024-03-29",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4595c8a7e1571c9974b2393ed127f05fabe72164",
      "title": "Model Stealing Detection for IoT Services Based on Multidimensional Features",
      "abstract": "Model stealing (MS) attacks pose a significant security concern for machine learning models on cloud platforms, as they can reconstruct a substitute model with limited effort to evade ownership. While detection-based methods show promise in preventing MS attacks, they often face practical challenges. Specifically, setting an appropriate threshold to distinguish malicious features from benign ones is a difficult task, often leading to a tradeoff between false alarm rates and detection accuracy. To address this challenge, we design a multidimensional feature extraction-and-distinction scheme called MED. It is achieved through a two-layer optimization: 1) the inner layer of extraction to maximize the difference of extracted multidimensional features between attack and benign samples and 2) the outer layer of distinction to maximize the accuracy of distinguishing malicious features automatically. Recognizing that different MS attacks result in varied features, we design a group of feature extraction functions in the inner layer optimization, which addresses the limitations of single-feature-based detection methods. Further, we employ three differently characterized models for distinction, enabling MED to distinguish different types of malicious features. Comprehensive experiments are conducted to evaluate the effectiveness of the proposed scheme: MED can detect all types of MS attacks with no more than 100 samples, with an average detection rate greater than 0.99.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xinjing Liu",
        "Taifeng Liu",
        "Hao Yang",
        "Jiakang Dong",
        "Zuobin Ying",
        "Zhuo Ma"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4595c8a7e1571c9974b2393ed127f05fabe72164",
      "pdf_url": "",
      "publication_date": "2024-12-15",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "87004d053c0c2b0f91c293ef26d817d5a05e017c",
      "title": "Exploring Zero-Day Attacks on Machine Learning and Deep Learning Algorithms",
      "abstract": "In the rapidly evolving field of artificial intelligence, machine learning (ML) and deep learning (DL) algorithms have emerged as powerful tools for solving complex problems in various domains, including cyber security. However, as these algorithms become increasingly prevalent, they also face new security challenges. One of the most significant of these challenges is the threat of zero-day attacks, which exploit unknown and unpredictable vulnerabilities in the algorithms or the data they process. \nThis paper provides a comprehensive overview of zero-day attacks on ML/DL algorithms, exploring their types, causes, effects, and potential countermeasures. The paper begins by introducing the concept and definition of zero-day attacks, providing a clear understanding of this emerging threat. It then reviews the existing research on zero-day attacks on ML/DL algorithms, focusing on three main categories: data poisoning attacks, adversarial input attacks, and model stealing attacks. Each of these attack types poses unique challenges and requires specific countermeasures. \nThe paper also discusses the potential impacts and risks of these attacks on various application domains. For instance, in facial expression recognition, an adversarial input attack could lead to misclassification of emotions, with serious implications for user experience and system integrity. In object classification, a data poisoning attack could cause the algorithm to misidentify critical objects, potentially endangering human lives in applications like autonomous driving. In satellite intersection recognition, a model stealing attack could compromise national security by revealing sensitive information. \nFinally, the paper presents some possible protection methods against zero-day attacks on ML/DL algorithms. These include anomaly detection techniques to identify unusual patterns in the data or the algorithm\u2019s behaviour, model verification and validation methods to ensure the algorithm\u2019s correctness and robustness, federated learning approaches to protect the privacy of the training data, and differential privacy techniques to add noise to the data or the algorithm\u2019s outputs to prevent information leakage. \nThe paper concludes by highlighting some open issues and future directions for research in this area, emphasizing the need for ongoing efforts to secure ML/DL algorithms against zero-day attacks.",
      "year": 2024,
      "venue": "European Conference on Cyber Warfare and Security",
      "authors": [
        "Marie Kov\u00e1\u0159ov\u00e1"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/87004d053c0c2b0f91c293ef26d817d5a05e017c",
      "pdf_url": "https://papers.academic-conferences.org/index.php/eccws/article/download/2310/2134",
      "publication_date": "2024-06-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "be6dc29e9773c5242b0a44877df60af0feae9adb",
      "title": "Sample Correlation for Fingerprinting Deep Face Recognition",
      "abstract": "Face recognition has witnessed remarkable advancements in recent years, thanks to the development of deep learning techniques. However, an off-the-shelf face recognition model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting, as a model stealing detection method, aims to verify whether a suspect model is stolen from the victim model, gaining more and more attention nowadays. Previous methods always utilize transferable adversarial examples as the model fingerprint, but this method is known to be sensitive to adversarial defense and transfer learning techniques. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-JC that selects JPEG compressed samples as model inputs and calculates the correlation matrix among their model outputs. Extensive results validate that SAC successfully defends against various model stealing attacks in deep face recognition, encompassing face verification and face emotion recognition, exhibiting the highest performance in terms of AUC, p-value and F1 score. Furthermore, we extend our evaluation of SAC-JC to object recognition datasets including Tiny-ImageNet and CIFAR10, which also demonstrates the superior performance of SAC-JC to previous methods. The code will be available at https://github.com/guanjiyang/SAC_JC.",
      "year": 2024,
      "venue": "International Journal of Computer Vision",
      "authors": [
        "Jiyang Guan",
        "Jian Liang",
        "Yanbo Wang",
        "R. He"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/be6dc29e9773c5242b0a44877df60af0feae9adb",
      "pdf_url": "",
      "publication_date": "2024-10-25",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b3744d928fcd84b7e2296d5983ba199a300955e0",
      "title": "Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices",
      "abstract": "With growing popularity, deep learning (DL) models are becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large models. Most of those DL models are proprietary to the companies who thus strive to keep their private models safe from the model extraction attack (MEA), whose aim is to steal the model by training surrogate models. Nowadays, companies are inclined to offload the models from central servers to edge/endpoint devices. As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim model and obtain various pieces of the model information, such as the model architecture (MA) and image dimension (ID). Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and defensive sides in that they may learn which pieces of information exposed by SCA are more important than the others. Our analysis additionally reveals that by grasping the victim model information from SCA, MEA can get highly effective and successful even without any prior knowledge of the model. Finally, to evince the practicality of our analysis results, we empirically apply SCA, and subsequently, carry out MEA under realistic threat assumptions. The results show up to 5.8 times better performance than when the adversary has no model information about the victim model.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Younghan Lee",
        "Sohee Jun",
        "Yungi Cho",
        "Woorim Han",
        "Hyungon Moon",
        "Y. Paek"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/b3744d928fcd84b7e2296d5983ba199a300955e0",
      "pdf_url": "",
      "publication_date": "2024-03-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1abdfdd7cd2d325e09b82d4d3a5dee97000d27a0",
      "title": "Layer Sequence Extraction of Optimized DNNs Using Side-Channel Information Leaks",
      "abstract": "Deep neural network (DNN) intellectual property (IP) models must be kept undisclosed to avoid revealing trade secrets. Recent works have devised machine learning techniques that leverage on side-channel information leakage of the target platform to reverse engineer DNN architectures. However, these works fail to perform successful attacks on DNNs that have undergone performance optimizations (i.e., operator fusion) using DNN compilers, e.g., Apache tensor virtual machine (TVM). We propose a two-phase attack framework to infer the layer sequences of optimized DNNs through side-channel information leakage. In the first phase, we use a recurrent network with multihead attention components to learn the intra and interlayer fusion patterns from GPU traces of TVM-optimized DNNs, in order to accurately predict the operation distribution. The second phase uses a model to learn the run-time temporal correlations between operations and layers, which enables the prediction of layer sequence. An encoding strategy is proposed to overcome the convergence issues faced by existing learning-based methods when inferring the layer sequences of optimized DNNs. Extensive experiments show that our learning-based framework outperforms state-of-the-art DNN model extraction techniques. Our framework is also the first to effectively reverse engineer both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) using side-channel leakage.",
      "year": 2024,
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": [
        "Yidan Sun",
        "Guiyuan Jiang",
        "Xinwang Liu",
        "Peilan He",
        "Siew-Kei Lam"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1abdfdd7cd2d325e09b82d4d3a5dee97000d27a0",
      "pdf_url": "",
      "publication_date": "2024-10-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d678d169552a667531d66e09a75b4e4d13e3c044",
      "title": "Exploring the Efficacy of Learning Techniques in Model Extraction Attacks on Image Classifiers: A Comparative Study",
      "abstract": "In the rapidly evolving landscape of cybersecurity, model extraction attacks pose a significant challenge, undermining the integrity of machine learning models by enabling adversaries to replicate proprietary algorithms without direct access. This paper presents a comprehensive study on model extraction attacks towards image classification models, focusing on the efficacy of various Deep Q-network (DQN) extensions for enhancing the performance of surrogate models. The goal is to identify the most efficient approaches for choosing images that optimize adversarial benefits. Additionally, we explore synthetic data generation techniques, including the Jacobian-based method, Linf-projected Gradient Descent (LinfPGD), and Fast Gradient Sign Method (FGSM) aiming to facilitate the training of adversary models with enhanced performance. Our investigation also extends to the realm of data-free model extraction attacks, examining their feasibility and performance under constrained query budgets. Our investigation extends to the comparison of these methods under constrained query budgets, where the Prioritized Experience Replay (PER) technique emerges as the most effective, outperforming other DQN extensions and synthetic data generation methods. Through rigorous experimentation, including multiple trials to ensure statistical significance, this work provides valuable insights into optimizing model extraction attacks.",
      "year": 2024,
      "venue": "Applied Sciences",
      "authors": [
        "Dong Han",
        "Reza Babaei",
        "Shangqing Zhao",
        "Samuel Cheng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d678d169552a667531d66e09a75b4e4d13e3c044",
      "pdf_url": "",
      "publication_date": "2024-04-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "edfc6d47efc3cf83a8f9fb7fbb2dc02135f83846",
      "title": "Secure AI Systems: Emerging Threats and Defense Mechanisms",
      "abstract": "The capability of artificial intelligence (AI), increasingly embedded in critical domains, faces a complex array of security threats. It has motivated researchers to explore the security vulnerability of AI solutions and propose effective countermeasures. This article offers a comprehensive exploration of diverse attacks on AI models, including backdoors (Trojans), adversarial, fault injection, data poisoning, model inversion, model extraction, membership inference attacks, etc. These security vulnerabilities are classified into two broad categories, namely, Supply Chain Attacks and Runtime Attacks. We highlight threat models, attack strategies, and defenses to secure AI systems against these attacks. The work also underscores the significance of developing secure and robust AI models and their implementation to safeguard sensitive data and embedded systems. We present some emerging research directions on secure AI systems.",
      "year": 2024,
      "venue": "Asian Test Symposium",
      "authors": [
        "Habibur Rahaman",
        "Atri Chatterjee",
        "S. Bhunia"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/edfc6d47efc3cf83a8f9fb7fbb2dc02135f83846",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4d5f1a09ddd9b3e984b97195e29c1c61a56e4d84",
      "title": "Bio-Rollup: a new privacy protection solution for biometrics based on two-layer scalability-focused blockchain",
      "abstract": "The increased use of artificial intelligence generated content (AIGC) among vast user populations has heightened the risk of private data leaks. Effective auditing and regulation remain challenging, further compounding the risks associated with the leaks involving model parameters and user data. Blockchain technology, renowned for its decentralized consensus mechanism and tamper-resistant properties, is emerging as an ideal tool for documenting, auditing, and analyzing the behaviors of all stakeholders in machine learning as a service (MLaaS). This study centers on biometric recognition systems, addressing pressing privacy and security concerns through innovative endeavors. We conducted experiments to analyze six distinct deep neural networks, leveraging a dataset quality metric grounded in the query output space to quantify the value of the transfer datasets. This analysis revealed the impact of imbalanced datasets on training accuracy, thereby bolstering the system\u2019s capacity to detect model data thefts. Furthermore, we designed and implemented a novel Bio-Rollup scheme, seamlessly integrating technologies such as certificate authority, blockchain layer two scaling, and zero-knowledge proofs. This innovative scheme facilitates lightweight auditing through Merkle proofs, enhancing efficiency while minimizing blockchain storage requirements. Compared to the baseline approach, Bio-Rollup restores the integrity of the biometric system and simplifies deployment procedures. It effectively prevents unauthorized use through certificate authorization and zero-knowledge proofs, thus safeguarding user privacy and offering a passive defense against model stealing attacks.",
      "year": 2024,
      "venue": "PeerJ Computer Science",
      "authors": [
        "Jian Yun",
        "Yusheng Lu",
        "Xinyang Liu",
        "Jingdan Guan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4d5f1a09ddd9b3e984b97195e29c1c61a56e4d84",
      "pdf_url": "https://doi.org/10.7717/peerj-cs.2268",
      "publication_date": "2024-09-09",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c3df2c0b6c68c4db4e946ed8432e7f7b2269ecf3",
      "title": "Enhancing TinyML Security: Study of Adversarial Attack Transferability",
      "abstract": "The recent strides in artificial intelligence (AI) and machine learning (ML) have propelled the rise of TinyML, a paradigm enabling AI computations at the edge without dependence on cloud connections. While TinyML offers real-time data analysis and swift responses critical for diverse applications, its devices' intrinsic resource limitations expose them to security risks. This research delves into the adversarial vulnerabilities of AI models on resource-constrained embedded hardware, with a focus on Model Extraction and Evasion Attacks. Our findings reveal that adversarial attacks from powerful host machines could be transferred to smaller, less secure devices like ESP32 and Raspberry Pi. This illustrates that adversarial attacks could be extended to tiny devices, underscoring vulnerabilities, and emphasizing the necessity for reinforced security measures in TinyML deployments. This exploration enhances the comprehension of security challenges in TinyML and offers insights for safeguarding sensitive data and ensuring device dependability in AI-powered edge computing settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Parin Shah",
        "Yuvaraj Govindarajulu",
        "Pavan Kulkarni",
        "Manojkumar Parmar"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c3df2c0b6c68c4db4e946ed8432e7f7b2269ecf3",
      "pdf_url": "",
      "publication_date": "2024-07-16",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "467cabe6f85318ef74987895cbf2f1e46f5c1d01",
      "title": "EMGAN: Early-Mix-GAN on Extracting Server-Side Model in Split Federated Learning",
      "abstract": "Split Federated Learning (SFL) is an emerging edge-friendly version of Federated Learning (FL), where clients process a small portion of the entire model. While SFL was considered to be resistant to Model Extraction Attack (MEA) by design, a recent work shows it is not necessarily the case. In general, gradient-based MEAs are not effective on a target model that is changing, as is the case in training-from-scratch applications. In this work, we propose a strong MEA during the SFL training phase. The proposed Early-Mix-GAN (EMGAN) attack effectively exploits gradient queries regardless of data assumptions. EMGAN adopts three key components to address the problem of inconsistent gradients. Specifically, it employs (i) Early-learner approach for better adaptability, (ii) Multi-GAN approach to introduce randomness in generator training to mitigate mode collapse, and (iii) ProperMix to effectively augment the limited amount of synthetic data for a better approximation of the target domain data distribution. EMGAN achieves excellent results in extracting server-side models. With only 50 training samples, EMGAN successfully extracts a 5-layer server-side model of VGG-11 on CIFAR-10, with 7% less accuracy than the target model. With zero training data, the extracted model achieves 81.3% accuracy, which is significantly better than the 45.5% accuracy of the model extracted by the SoTA method. The code is available at \"https://github.com/zlijingtao/SFL-MEA\".",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jingtao Li",
        "Xing Chen",
        "Li Yang",
        "A. S. Rakin",
        "Deliang Fan",
        "Chaitali Chakrabarti"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/467cabe6f85318ef74987895cbf2f1e46f5c1d01",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/29258/30374",
      "publication_date": "2024-03-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d80e74e8ed6789b0901a3365f7185cb1ae9a991e",
      "title": "Defending Against Label-Only Attacks via Meta-Reinforcement Learning",
      "abstract": "Machine learning models are susceptible to a range of adversarial activities. These attacks are designed to either infer private information from the target model or deceive it. For instance, an attacker may attempt to discern if a given data example is from the model\u2019s training set (membership inference attacks) or create adversarial examples to mislead the model to make incorrect predictions (adversarial example attacks). Numerous defense methods have been proposed to counter these attacks. However, these methods typically share two common limitations. Firstly, most are not designed to address label-only attacks, which is a newly emerged kind of attacks that rely solely on the hard labels predicted by the target model. Secondly, they are often developed to mitigate specific attacks rather than universally various attacks. To address these limitations, this paper proposes a novel defense method that focuses on the most challenging attacks, i.e., label-only attacks, and can handle various types of label-only attacks. The key idea is to strategically modify the target model\u2019s predicted labels using a meta-reinforcement learning technique. This ensures that attackers receive incorrect labels while benign users continue to receive correct labels. Notably, the defender, i.e., the owner of the target model, can make effective decisions without knowledge of the attacker\u2019s behavior. The experimental results demonstrate that our proposed method is an effective defense against a range of attacks, including label-only model stealing, label-only membership inference, label-only model inversion, and label-only adversarial example attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Dayong Ye",
        "Tianqing Zhu",
        "Kun Gao",
        "Wanlei Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d80e74e8ed6789b0901a3365f7185cb1ae9a991e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "aee0bc1bc011d8da14aa209d0af984a9cc6b227f",
      "title": "Streamlining DNN Obfuscation to Defend Against Model Stealing Attacks",
      "abstract": "Side-channel-based Deep Neural Network (DNN) model stealing has become a major concern with the advent of learning-based attacks. In respond to this threat, defence mechanisms have been presented to obfuscate the DNN execution, making it difficult to infer the correlation between side-channel information and DNN architecture. However, state-of-the-art (SOTA) DNN obfuscation is time-consuming, requires expert-level changes in existing DNN compilers (e.g., Tensor Virtual Machine (TVM)), and often relies on prior knowledge of the attack models. In this work, we study the impact of various obfuscation levels on the defence effectiveness, and present a streamlined DNN obfuscation process that is extremely fast and is agnostic to any attack models. Our study reveals that by just modifying the scheduling of DNN operations on the GPU, we can achieve comparable defense performance as the SOTA in an attack agnostic manner. We also propose a simple algorithm that determines an effective scheduling configuration for mitigating DNN model stealing at a fraction of a time required by SOTA obfuscation methods. Our method can be easily integrated into existing DNN compilers as a security feature, even by non-experts, to protect their DNN against side-channel attacks.",
      "year": 2024,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Yidan Sun",
        "Siew-Kei Lam",
        "Guiyuan Jiang",
        "Peilan He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/aee0bc1bc011d8da14aa209d0af984a9cc6b227f",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2df09db143feb7aa0fc30ac548f0aaa4c628c3cd",
      "title": "Stealthy Imitation: Reward-guided Environment-free Policy Stealing",
      "abstract": "Deep reinforcement learning policies, which are integral to modern control systems, represent valuable intellectual property. The development of these policies demands considerable resources, such as domain expertise, simulation fidelity, and real-world validation. These policies are potentially vulnerable to model stealing attacks, which aim to replicate their functionality using only black-box access. In this paper, we propose Stealthy Imitation, the first attack designed to steal policies without access to the environment or knowledge of the input range. This setup has not been considered by previous model stealing methods. Lacking access to the victim's input states distribution, Stealthy Imitation fits a reward model that allows to approximate it. We show that the victim policy is harder to imitate when the distribution of the attack queries matches that of the victim. We evaluate our approach across diverse, high-dimensional control tasks and consistently outperform prior data-free approaches adapted for policy stealing. Lastly, we propose a countermeasure that significantly diminishes the effectiveness of the attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhixiong Zhuang",
        "Maria-Irina Nicolae",
        "Mario Fritz"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/2df09db143feb7aa0fc30ac548f0aaa4c628c3cd",
      "pdf_url": "",
      "publication_date": "2024-05-11",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dca9242b227eebb3f052139dcb6a45c4dcbfde83",
      "title": "\"Yes, My LoRD.\" Guiding Language Model Extraction with Locality Reinforced Distillation",
      "abstract": "Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that I) The convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and II) LoRD can reduce query complexity while mitigating watermark protection through our exploration-based stealing. Extensive experiments validate the superiority of our method in extracting various state-of-the-art commercial LLMs. Our code is available at: https://github.com/liangzid/LoRD-MEA .",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zi Liang",
        "Qingqing Ye",
        "Yanyun Wang",
        "Sen Zhang",
        "Yaxin Xiao",
        "Ronghua Li",
        "Jianliang Xu",
        "Haibo Hu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/dca9242b227eebb3f052139dcb6a45c4dcbfde83",
      "pdf_url": "",
      "publication_date": "2024-09-04",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "55414dc21b091006bf868b28008c9fc30fa38dca",
      "title": "Model Extraction Attack against On-device Deep Learning with Power Side Channel",
      "abstract": "The proliferation of on-device deep learning models in resource-constrained environments has led to significant advancements in privacy-preserving machine learning. However, the deployment of these models also introduces new security challenges, one of which is the vulnerability to model extraction attacks. In this paper, we investigate a novel attack with power side channel to extract on-device deep learning model deployed, which poses a substantial threat to on-device deep learning systems. By carefully monitoring power consumption during inference, an adversary can gain insights into the model\u2019s internal behavior, potentially compromising the model\u2019s intellectual property and sensitive data. Through experiments on a real-world embedded device (Jetson Nano) and various types of deep learning models, we demonstrate that the proposed attack can extract models with high fidelity. Based on experiments, we find that the power side channel-assisted model extraction attack can achieve high attacking success rate, up to 96.7% and 87.5% under close world and open world settings. This research sheds light on the evolving landscape of security threats in the context of on-device DL and provides valuable insights into safeguarding these models from potential adversaries.",
      "year": 2024,
      "venue": "IEEE International Symposium on Quality Electronic Design",
      "authors": [
        "Jiali Liu",
        "Han Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/55414dc21b091006bf868b28008c9fc30fa38dca",
      "pdf_url": "",
      "publication_date": "2024-04-03",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dfbbf3fa36cd03aa41d4170ba672e332f8295bd0",
      "title": "Unveiling Intellectual Property Vulnerabilities of GAN-Based Distributed Machine Learning through Model Extraction Attacks",
      "abstract": "Generative Adversarial Networks (GANs), as a cornerstone of artificial intelligence (AI), are widely recognized as the intellectual property (IP) of their owners, given the sensitivity of the training data and the commercial value tied to the models. Model extraction attacks, which aim to steal well-trained proprietary models, pose a significant threat to model IP. Nevertheless, current research predominately focuses on the context of machine learning as a service (MLaaS), where the emphasis lies in understanding the attack knowledge acquired through black-box API queries. This restricted perspective exposes a critical gap in investigating model extraction attacks within realistic distributed settings for generative tasks. In this work, we present the first investigation into model extraction attacks against GANs in distributed settings. We provide a comprehensive attack taxonomy, considering three different levels of knowledge the adversary can obtain in practice. Based on it, we introduce a novel model extraction attack named MoEx, which focuses on the GAN-based distributed learning scenario, i.e., Multi-Discriminator GANs, a typical asymmetric distributed setting. MoEx uses the objective function simulation, leveraging data exchanged during the learning process, to approximate the GAN generator owned by the server. We define two attack goals for MoEx, fidelity extraction and accuracy extraction. Then we comprehensively evaluate the effectiveness of MoEx's two goals with real-world datasets. Our results demonstrate its robust capabilities in extracting generators with high fidelity and accuracy compared with existing methods.",
      "year": 2024,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Mengyao Ma",
        "Shuofeng Liu",
        "M.A.P. Chamikara",
        "Mohan Baruwal Chhetri",
        "Guangdong Bai"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/dfbbf3fa36cd03aa41d4170ba672e332f8295bd0",
      "pdf_url": "",
      "publication_date": "2024-10-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fbfb0ed07c0c8c84a959e0f4de6db90d5d65772f",
      "title": "Efficient and Effective Model Extraction",
      "abstract": "Model extraction aims to steal a functionally similar copy from a machine learning as a service (MLaaS) API with minimal overhead, typically for illicit profit or as a precursor to further attacks, posing a significant threat to the MLaaS ecosystem. However, recent studies have shown that model extraction is highly inefficient, particularly when the target task distribution is unavailable. In such cases, even substantially increasing the attack budget fails to produce a sufficiently similar replica, reducing the adversary\u2019s motivation to pursue extraction attacks. In this paper, we revisit the elementary design choices throughout the extraction lifecycle. We propose an embarrassingly simple yet dramatically effective algorithm, Efficient and Effective Model Extraction (E3), focusing on both query preparation and training routine. E3 achieves superior generalization compared to state-of-the-art methods while minimizing computational costs. For instance, with only 0.005\u00d7 the query budget and less than 0.2\u00d7 the runtime, E3 outperforms classical generative model based data-free model extraction by an absolute accuracy improvement of over 50% on CIFAR-10. Our findings underscore the persistent threat posed by model extraction and suggest that it could serve as a valuable benchmarking algorithm for future security evaluations.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Hongyu Zhu",
        "Wentao Hu",
        "Sichu Liang",
        "Fangqi Li",
        "Wenwen Wang",
        "Shilin Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/fbfb0ed07c0c8c84a959e0f4de6db90d5d65772f",
      "pdf_url": "",
      "publication_date": "2024-09-21",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7c1924e7f6a9c335ebb83c50996fa93e4bb62bcb",
      "title": "Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples",
      "abstract": "We introduce Adversarial Sparse Teacher (AST), a robust defense method against distillation-based model stealing attacks. Our approach trains a teacher model using adversarial examples to produce sparse logit responses and increase the entropy of the output distribution. Typically, a model generates a peak in its output corresponding to its prediction. By leveraging adversarial examples, AST modifies the teacher model\u2019s original response, embedding a few altered logits into the output, while keeping the primary response slightly higher. Concurrently, all remaining logits are elevated to further increase the output distribution\u2019s entropy. All these complex manipulations are performed using an optimization function with our proposed Exponential Predictive Divergence (EPD) loss function. EPD allows us to maintain higher entropy levels compared to traditional KL divergence, effectively confusing attackers. Experiments on the CIFAR-10 and CIFAR-100 datasets demonstrate that AST outperforms state-of-the-art methods, providing effective defense against model stealing, while preserving high accuracy. The source codes are publicly available at https://github.com/codeofanon/AdversarialSparseTeacher",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "E. Y\u0131lmaz",
        "H. Keles"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/7c1924e7f6a9c335ebb83c50996fa93e4bb62bcb",
      "pdf_url": "",
      "publication_date": "2024-03-08",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "99712e009858ba394bd119092bc2ebcc502a8892",
      "title": "DualCOS: Query-Efficient Data-Free Model Stealing with Dual Clone Networks and Optimal Samples",
      "abstract": "Although data-free model stealing attacks are free from reliance on real data, they suffer from limitations, including low accuracy and high query budgets, which restrict their practical feasibility. In this paper, we propose a novel data-free model stealing framework called DualCOS. As a whole, DualCOS is divided into two stages: interactive training and semi-supervised boosting. To optimize the usage of query budgets, we use a dual clone model architecture to address the challenge of querying victim model during generator training. We also introduce active learning-based sampling strategy and sample reuse mechanism to achieve an efficient query process. Furthermore, once query budget is exhausted, the semi-supervised boosting is employed to continue improving the final clone accuracy. Through extensive evaluations, we demonstrate the superiority of our proposed method in terms of accuracy and query efficiency, particularly in scenarios involving hard labels and multiple classes.",
      "year": 2024,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Yu Xuan",
        "Zhendong Zhao"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/99712e009858ba394bd119092bc2ebcc502a8892",
      "pdf_url": "",
      "publication_date": "2024-07-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "04ba64269e3cedd3bf0117218d0c69cba2bd5abb",
      "title": "Securing Machine Learning: Understanding Adversarial Attacks and Bias Mitigation",
      "abstract": "This paper offers a comprehensive examination of adversarial vulnerabilities in machine learning (ML) models and strategies for mitigating fairness and bias issues. It analyses various adversarial attack vectors encompassing evasion, poisoning, model inversion, exploratory probes, and model stealing, elucidating their potential to compromise model integrity and induce misclassification or information leakage. In response, a range of defence mechanisms including adversarial training, certified defences, feature transformations, and ensemble methods are scrutinized, assessing their effectiveness and limitations in fortifying ML models against adversarial threats. Furthermore, the study explores the nuanced landscape of fairness and bias in ML, addressing societal biases, stereotypes reinforcement, and unfair treatment, proposing mitigation strategies like fairness metrics, bias auditing, de-biasing techniques, and human-in-the-loop approaches to foster fairness, transparency, and ethical AI deployment. This synthesis advocates for interdisciplinary collaboration to build resilient, fair, and trustworthy AI systems amidst the evolving technological paradigm.",
      "year": 2024,
      "venue": "International Journal of Innovative Science and Research Technology",
      "authors": [
        "Archit Lakhani",
        "Neyah Rohit"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/04ba64269e3cedd3bf0117218d0c69cba2bd5abb",
      "pdf_url": "https://doi.org/10.38124/ijisrt/ijisrt24jun1671",
      "publication_date": "2024-07-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d6b246a94fc9347ad096c5762f743e45b6ab922c",
      "title": "Genetic Improvement for DNN Security",
      "abstract": "Genetic improvement (GI) in Deep Neural Networks (DNNs) has traditionally enhanced neural architecture and trained DNN parameters. Recently, GI has supported large language models by optimizing DNN operator scheduling on accelerator clusters. However, with the rise of adversarial AI, particularly model extraction attacks, there is an unexplored potential for GI in fortifying Machine Learning as a Service (MLaaS) models. We suggest a novel application of GI \u2014 not to improve model performance, but to diversify operator parallelism for the purpose of a moving target defense against model extraction attacks. We discuss an application of GI to create a DNN model defense strategy that uses probabilistic isolation, offering unique benefits not present in current DNN defense systems.",
      "year": 2024,
      "venue": "International Genetic Improvement Workshop",
      "authors": [
        "Hunter Baxter",
        "Yu Huang",
        "Kevin Leach"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/d6b246a94fc9347ad096c5762f743e45b6ab922c",
      "pdf_url": "",
      "publication_date": "2024-04-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9e3844cfb19402bde4df26cb1e1810faae8d5881",
      "title": "Beowulf: Mitigating Model Extraction Attacks Via Reshaping Decision Regions",
      "abstract": "Machine Learning as a Service (MLaaS) enables resource-constrained users to access well-trained models through a publicly accessible Application Programming Interface (API) on a pay-per-query basis. Nevertheless, model owners may face the potential threats of model extraction attacks where malicious users replicate valuable commercial models based on query results. Existing defenses against model extraction attacks, however, either sacrifice prediction accuracy or fail to thwart more advanced attacks. In this paper, we propose a novel model extraction defense, dubbed Beowulf 1 , which draws inspiration from theoretical findings that models with complex and narrow decision regions are difficult to be reproduced. Rather than arbitrarily altering decision regions, which may jeopardize the predictive capacity of the victim model, we introduce a dummy class, carefully synthesized using both random and adversarial noises. The random noise broadens the coverage of the dummy class, and the adversarial noise impacts decision regions near decision boundaries with normal classes. To further improve the model utility, we propose to employ data augmentation methods to seamlessly integrate the dummy class and the normal classes. Extensive evaluations on CIFAR-10, GTSRB, CIFAR-100, and ImageNette datasets demonstrate that Beowulf can significantly reduce the extraction accuracy of 6 state-of-the-art model extraction attacks by as much as 80%. Moreover, we show that Beowulf is also robust to adaptive model extraction attacks.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xueluan Gong",
        "Rubin Wei",
        "Ziyao Wang",
        "Yuchen Sun",
        "Jiawen Peng",
        "Yanjiao Chen",
        "Qian Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/9e3844cfb19402bde4df26cb1e1810faae8d5881",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670267",
      "publication_date": "2024-12-02",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "72df9bf57845936f81ce918adbc4b95b92aa1f9e",
      "title": "MTL-Leak: Privacy Risk Assessment in Multi-Task Learning",
      "abstract": "Multi-task learning (MTL) supports simultaneous training over multiple related tasks and learns the shared representation. While improving the generalization ability of training on a single task, MTL has higher privacy risk than traditional single-task learning because more sensitive information is extracted and learned in a correlated manner. Unfortunately, very few works have attempted to address the privacy risks posed by MTL. In this article, we first investigate such risk by designing model extraction attack (MEA) and membership inference attack (MIA) in MTL. Then we evaluate the privacy risks on six MTL model architectures and two popular MTL datasets, whose results show that both the number of tasks and the complexity of training data play an important role in the attack performance. Our investigation shows that MTL is more vulnerable than traditional single-task learning under both attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hongyang Yan",
        "Anli Yan",
        "Li Hu",
        "Jiaming Liang",
        "Haibo Hu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/72df9bf57845936f81ce918adbc4b95b92aa1f9e",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "21e6d0743c99e614ccff2533a591affcc3438e14",
      "title": "OML: A Primitive for Reconciling Open Access with Owner Control in AI Model Distribution",
      "abstract": "The current paradigm of AI model distribution presents a fundamental dichotomy: models are either closed and API-gated, sacrificing transparency and local execution, or openly distributed, sacrificing monetization and control. We introduce OML(Open-access, Monetizable, and Loyal AI Model Serving), a primitive that enables a new distribution paradigm where models can be freely distributed for local execution while maintaining cryptographically enforced usage authorization. We are the first to introduce and formalize this problem, introducing rigorous security definitions tailored to the unique challenge of white-box model protection: model extraction resistance and permission forgery resistance. We prove fundamental bounds on the achievability of OML properties and characterize the complete design space of potential constructions, from obfuscation-based approaches to cryptographic solutions. To demonstrate practical feasibility, we present OML 1.0, a novel OML construction leveraging AI-native model fingerprinting coupled with crypto-economic enforcement mechanisms. Through extensive theoretical analysis and empirical evaluation, we establish OML as a foundational primitive necessary for sustainable AI ecosystems. This work opens a new research direction at the intersection of cryptography, machine learning, and mechanism design, with critical implications for the future of AI distribution and governance.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Zerui Cheng",
        "Edoardo Contente",
        "Ben Finch",
        "Oleg Golev",
        "J. Hayase",
        "Andrew Miller",
        "Niusha Moshrefi",
        "Anshul Nasery",
        "Sandeep Nailwal",
        "Sewoong Oh",
        "Himanshu Tyagi",
        "P. Viswanath"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/21e6d0743c99e614ccff2533a591affcc3438e14",
      "pdf_url": "",
      "publication_date": "2024-11-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a57be99734b62dfd2a54a15eb46c7359fdb7db58",
      "title": "SwiftThief: Enhancing Query Efficiency of Model Stealing by Contrastive Learning",
      "abstract": "Model-stealing attacks are emerging as a severe threat to AI-based services because an adversary can create models that duplicate the functionality of the black-box AI models inside the services with regular query-based access. To avoid detection or query costs, the model-stealing adversary must consider minimizing the number of queries to obtain an accurate clone model. To achieve this goal, we propose SwiftThief, a novel model-stealing framework that utilizes both queried and unqueried data to reduce query complexity. In particular, SwiftThief uses contrastive learning, a recent technique for representation learning. We formulate a new objective function for model stealing consisting of self-supervised (for abundant unqueried inputs from public datasets) and soft-supervised (for queried inputs) contrastive losses, jointly optimized with an output matching loss (for queried inputs). In addition, we suggest a new sampling strategy to prioritize rarely queried classes to improve attack performance. Our experiments proved that SwiftThief could significantly enhance the efficiency of model-stealing attacks compared to the existing methods, achieving similar attack performance using only half of the query budgets of the competing approaches. Also, SwiftThief showed high competence even when a defense was activated for the victims.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Jeonghyun Lee",
        "Sungmin Han",
        "Sangkyun Lee"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a57be99734b62dfd2a54a15eb46c7359fdb7db58",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "model stealing",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9c616cb25371fb66f3dacabafe1abc81a5fbaae5",
      "title": "I Can Retrieve More than Images: Contrastive Stealing Attack against Deep Hashing Models",
      "abstract": "Deep hashing models have revolutionized traditional hashing methods by delivering superior performance, and have been applied in real-world applications such as Pinterest and Amazon, which are known as deep hashing-based retrieval systems. Behind their revolutionary representation capability, the requirements for training a deep hashing model expose it to the risks of potential model stealing attacks - a cheap way to mimic the well-trained hashing performance while circumventing the demanding requirements. Since the attacker is able to obtain the outputs of deep hashing models by querying the retrieval systems, the conventional stealing attacks relying on matching exact outputs can not be applied in this problem. In this paper, we propose a contrastive-based and GAN-enhanced stealing framework to leverage the informative knowledge of retrieved data. Our empirical results demonstrate that our stealing framework can train a substitute hashing model with a retrieval accuracy ranging from 80% to 110% of the target hashing model while utilizing significantly fewer training resources. Furthermore, we conduct attacks on the target hashing model using adversarial examples generated by the stolen model, resulting in an attack success rate that can be 3 times higher compared to attacks conducted without the substitute model. Finally, we leverage existing defense strategies to mitigate our attack, resulting in a stealing effectiveness decrease of no more than 4%.",
      "year": 2024,
      "venue": "2024 IEEE International Conference on Web Services (ICWS)",
      "authors": [
        "X. You",
        "Mi Zhang",
        "Jianwei Xu",
        "Min Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9c616cb25371fb66f3dacabafe1abc81a5fbaae5",
      "pdf_url": "",
      "publication_date": "2024-07-07",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "45b2b736cd8c66cb2bbb7efeb0907807d8f6fbe0",
      "title": "GanTextKnockoff: stealing text sentiment analysis model functionality using synthetic data",
      "abstract": "Today, black-box machine learning models are often subject to extraction attacks that aim to retrieve their internal information. Black-box model extraction attacks are typically conducted by providing input data and, based on observing the output results, constructing a new model that functions equivalently to the original. This process is usually carried out by leveraging available data from public repositories or synthetic data generated by generative models. Most model extraction attack methods using synthetic data have been concentrated in the field of computer vision, with minimal research focused on model extraction in natural language processing. In this paper, we propose a method that utilizes synthetic textual data to construct a new model with high accuracy and similarity to the original black-box sentiment analysis model.",
      "year": 2024,
      "venue": "Journal of Military Science and Technology",
      "authors": [
        "Cong Pham",
        "Trung-Nguyen Hoang",
        "Cao-Truong Tran",
        "Viet-Binh Do"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/45b2b736cd8c66cb2bbb7efeb0907807d8f6fbe0",
      "pdf_url": "",
      "publication_date": "2024-12-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9166ef92f94c521f1b5f78c51bac2dac805115a7",
      "title": "Quantum Neural Network Extraction Attack via Split Co-Teaching",
      "abstract": "Quantum Neural Networks (QNNs), now offered as QNN-as-a-Service (QNNaaS), have become key targets for model extraction attacks. Existing methods use ensemble learning to train substitute QNNs, but our analysis reveals significant limitations in real-world environments, where noise and cost constraints undermine their effectiveness. In this work, we introduce a novel attack, split co-teaching, which uses label variations to split queried data by noise sensitivity and employs co-teaching schemes to enhance extraction accuracy. The experimental results show that our approach outperforms classical extraction attacks by 6.5%~9.5% and existing QNN extraction methods by 0.1%~3.7% across various tasks.",
      "year": 2024,
      "venue": "2025 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)",
      "authors": [
        "Zhenxiao Fu",
        "Fan Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9166ef92f94c521f1b5f78c51bac2dac805115a7",
      "pdf_url": "",
      "publication_date": "2024-09-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "neural network extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d011a91d041692e9915c99f4e0d3c9b2136bb3f",
      "title": "STMS: An Out-Of-Distribution Model Stealing Method Based on Causality",
      "abstract": "Machine learning, particularly deep learning, is extensively applied in various real-life scenarios. However, recent research has highlighted the severe infringement of privacy and intellectual property caused by model stealing attacks. Therefore, more researchers are dedicated to studying the principles and methods of such attacks to promote the security development of artificial intelligence. Most of the existing model stealing attacks rely on prior information of the attacked models and consider ideal conditions. In order to better understand and defend against model stealing in real-world scenarios, we propose a novel model stealing method, named STMS, based on causal inference learning. For the first time, we introduce the problem of out-of-distribution generalization into the model stealing domain. The proposed approach operates under more challenging conditions, where the training and testing data of the target model are unknown, black-box, hard-label outputs, and there is a distribution shift during the testing phase. STMS achieves comparable or better stealing accuracy and generalization performance than prior works on multiple datasets and tasks. Moreover, this universal framework can be applied to improve the effectiveness of other model stealing methods and can also be migrated to other areas of machine learning.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Zhendong Zhao",
        "Yu Xuan",
        "Bisheng Tang",
        "Xiaoying Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3d011a91d041692e9915c99f4e0d3c9b2136bb3f",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "931a9beaf77e0019bcfa2412b8210b83cb70aa1a",
      "title": "MCD: Defense Against Query-Based Black-Box Surrogate Attacks",
      "abstract": "Deep neural networks (DNNs) is susceptible to surrogate attacks, where adversaries use surrogate data and corresponding outputs from the target model to build their own stolen model. Model stealing attacks jeopardize model privacy and model owners' commercial benefits. To address this issue, this paper proposes a hybrid protection approach-Maximize the confidence differences between benign samples and adversarial samples (MCD), to protect models from theft. Firstly, the LogitNorm approach is used to overcome the overconfidence problem in adversary query classification. Then, samples are divided into four groups according to ES and RS. Different groups are poisoned by different degrees. In addition to enhancing defensive performance and accounting for model integrity, the MCD uses a trigger to confirm the cloned model's owner. Experimental results show that the MCD defends against a variety of original models and attack techniques well. Against KnockoffNets and DFME attacks, the MCD yields an average defense performance of 54.58 % on five datasets, which is a great improvement over other defenses. Compared to other poisoning techniques, the Strong Poisoning (SP) module reduces the adversary's accuracy by 48.23 % on average. Additionally, the MCD overcomes the issue of OOD overconfidence while safeguarding the model accuracy in OOD detection and reduces the misclassification rate of ID samples for multiple OOD datasets.",
      "year": 2024,
      "venue": "IEEE International Conference on Systems, Man and Cybernetics",
      "authors": [
        "Yiwen Zou",
        "Wing W. Y. Ng",
        "Xueli Zhang",
        "Brick Loo",
        "Xingfu Yan",
        "Ran Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/931a9beaf77e0019bcfa2412b8210b83cb70aa1a",
      "pdf_url": "",
      "publication_date": "2024-10-06",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8d0fa6b361fc6319eac027da608a99f03b541111",
      "title": "Detecting Backdoor Attacks in Black-Box Neural Networks through Hardware Performance Counters",
      "abstract": "Deep Neural Networks (DNNs) have made significant strides, but their susceptibility to backdoor attacks still remains a concern. Most defenses typically assume access to white-box models or poisoned data, requirements that are often not feasible in practice, especially for proprietary DNNs. Existing defenses in a black-box setting usually rely on confidence scores of DNN's predictions. However, this exposes DNNs to the risk of model stealing attacks, a significant concern for proprietary DNNs. In this paper, we introduce a novel strategy for detecting back-doors, focusing on a more realistic black-box scenario where only hard-label (i.e., without any prediction confidence) query access is available. Our strategy utilizes data flow dynamics in a computational environment during DNN inference to identify potential backdoor inputs and is agnostic of trigger types or their locations in the input. We observe that a clean image and its corresponding backdoor counterpart with a trigger induce distinct patterns across various microarchitectural activities during the inference phase. We exploit these variations captured by Hardware Performance Counters (HPCs) and use principles of the Gaussian Mixture Model to detect backdoor inputs. To the best of our knowledge, this is the first work that utilizes HPCs for detecting backdoors in DNNs. Extensive evaluation considering a range of benchmark datasets, DNN architectures, and trigger patterns shows the efficacy of the proposed method in distinguishing between clean and backdoor inputs using HPCs.",
      "year": 2024,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Manaar Alam",
        "Yue Wang",
        "Michail Maniatakos"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8d0fa6b361fc6319eac027da608a99f03b541111",
      "pdf_url": "",
      "publication_date": "2024-03-25",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3f25ac027cb369ed864d3d7ef10a00b1f5877738",
      "title": "Extracting DNN Architectures via Runtime Profiling on Mobile GPUs",
      "abstract": "Deep Neural Networks (DNNs) have become invaluable intellectual property for AI providers due to advancements fueled by a decade of research and development. However, recent studies have demonstrated the effectiveness of model extraction attacks, which threaten this value by stealing DNN models. These attacks can lead to misuse of personal data, safety risks in critical systems, and the spread of misinformation. This paper explores model extraction attacks on DNN models deployed on mobile devices, using runtime profiles as a side-channel. Since mobile devices are resource constrained, DNN deployments require optimization efforts to reduce latency. The main hurdle in extracting DNN architectures in this scenario is that optimization techniques, such as operator-level and graph-level fusion, can obfuscate the association between runtime profile operators and their corresponding DNN layers, posing challenges for adversaries to accurately predict the computation performed. To overcome this, we propose a novel method analyzing GPU call profiles to identify the original DNN architecture. Our approach achieves full accuracy in extracting DNN architectures from a predefined set, even when layer information is obscured. For unseen architectures, a layer-by-layer hyperparameter extraction method guided by sub-layer patterns is introduced, also achieving high accuracy. This research achieves two firsts: 1) targeting mobile GPUs for DNN architecture extraction and 2) successfully extracting architectures from optimized models with fused layers.",
      "year": 2024,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Dong Hyub Kim",
        "Jonah O\u2019Brien Weiss",
        "Sandip Kundu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3f25ac027cb369ed864d3d7ef10a00b1f5877738",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "517a03d4025905d23928bbfdd4b1dfe595873ab3",
      "title": "TrustZoneTunnel: A Cross-World Pattern History Table-Based Microarchitectural Side-Channel Attack",
      "abstract": "ARM's TrustZone is a hardware-based trusted execution environment (TEE), prevalent in mobile devices, IoT edge systems, and autonomous systems. Within TrustZone, security-sensitive applications reside in a hardware-isolated secure world, protected from the normal-world's applications, OS, debugger, peripherals, and memory. However, microarchitectural side-channel vulnerabilities have been discovered on shared on-chip resources, such as caches and branch prediction unit (BPU). In this paper, we propose TrustZoneTunnel, the first Pattern History Table (PHT)-based side-channel attack on TrustZone, which is able to reveal the complete control flow of a trusted application in the secure world. We reverse-engineer the PHT indexing for ARM processors and develop key primitives for cross-world attacks, including well-controlled world-switching, PHT collision construction between two worlds, and precise PHT state-setting and checking functions. Furthermore, we introduce a novel model extraction attack against TrustZone based deep neural network, which can recover model parameters using only the side-channel leakage of vital branch instructions, obviating the need for model output or logits while prior research work requires such knowledge for model extraction.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Tianhong Xu",
        "A. A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/517a03d4025905d23928bbfdd4b1dfe595873ab3",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d853215713815135bd419b5748c99c1ec03192ad",
      "title": "A New Approach in Mitigating Adversarial Attacks on Machine Learning",
      "abstract": "Machine learning is a powerful tool that has the potential to transform many industries, and thus is open to security attacks. Such attacks on machine learning algorithms are known as adversarial attacks. Adversarial attacks are designed to deceive or mislead machine learning models by introducing malicious input data, modifying existing data, or exploiting weaknesses in the algorithms used to train the models. These attacks can be targeted, deliberate, and sophisticated, leading to serious consequences such as incorrect decision-making, data breaches, and loss of intellectual property. Poisoning attacks, evasion attacks, model stealing, and model inversion attacks are some examples of adversarial attacks. At the moment, most researchers are focusing on a defense approach to mitigate these attacks. This approach aims to create a strong defense system that can detect and respond to attacks in real-time, prevent unauthorized access to systems and data, and mitigate the impact of security breaches. Unfortunately, this approach has some disadvantages, one of which is limited effectiveness. Despite the use of multiple defense measures, determined attackers can still find ways to breach systems and access sensitive data. This is due to the nature of the defense approach, which never addresses the root of the problem and thus can lead to the repetition of such attacks. In this paper, a new approach is proposed, namely using the forensic approach. The proposed approach will investigate attacks against machine learning, identify the root cause of the attack, determine the extent of the damage, and gather information that can be used to prevent similar incidents in the future.",
      "year": 2024,
      "venue": "IEEE Symposium on Wireless Technology and Applications",
      "authors": [
        "Abomakhleb Abdulruhman I Ahmad",
        "K. A. Jalil"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d853215713815135bd419b5748c99c1ec03192ad",
      "pdf_url": "",
      "publication_date": "2024-07-20",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "92013fa050869d3deb74b3fc4ac0874ffbf1544e",
      "title": "A PSO-based Method to Test Deep Learning Library at API Level",
      "abstract": "In recent years, deep learning (DL) is widely used in various fields. DL library bugs could result in security issues and even some losses like data loss and model stealing. As a result, testing DL libraries is the focus of an increasing number of studies. However, there are still issues with these works, such as poor test sample selection and overly general test oracle, which result in ineffective and insufficient testing. In this paper, we present a LEAPI-PSO method based on particle swarm optimization (PSO) algorithm for testing DL libraries at API level, which tackles the inadequacies of existing testing techniques. LEAPI-PSO initially chooses high-quality seed samples by using input coverage and the seed progeny tree. Then, by using PSO, LEAPI-PSO generates test samples that are more likely to reveal API bugs. Based on eight mutation strategies, LEAPI-PSO can produce rich and varied test samples and input them into the API to check for bugs using test oracle. The two most popular DL libraries, PyTorch and Tensorflow, have been used in this study to validate and evaluate LEAPI-PSO. The result shows that LEAPI-PSO is capable to successfully find bugs including crashes, logical errors, and documentation errors. We report 115 bugs to the DL library developers, 95 of which are confirmed, and 63 of which are fixed.",
      "year": 2024,
      "venue": "Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering",
      "authors": [
        "Shuyan Liao",
        "Chun Shan"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/92013fa050869d3deb74b3fc4ac0874ffbf1544e",
      "pdf_url": "",
      "publication_date": "2024-01-26",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3e24fcec8b51278cb2234560da6f6933f8fc7426",
      "title": "MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction",
      "abstract": "The rise of Machine Learning as a Service (MLaaS) has led to the widespread deployment of machine learning models trained on diverse datasets. These models are employed for predictive services through APIs, raising concerns about the security and confidentiality of the models due to emerging vulnerabilities in prediction APIs. Of particular concern are model cloning attacks, where individuals with limited data and no knowledge of the training dataset manage to replicate a victim model's functionality through black-box query access. This commonly entails generating adversarial queries to query the victim model, thereby creating a labeled dataset. This paper proposes\"MisGUIDE\", a two-step defense framework for Deep Learning models that disrupts the adversarial sample generation process by providing a probabilistic response when the query is deemed OOD. The first step employs a Vision Transformer-based framework to identify OOD queries, while the second step perturbs the response for such queries, introducing a probabilistic loss function to MisGUIDE the attackers. The aim of the proposed defense method is to reduce the accuracy of the cloned model while maintaining accuracy on authentic queries. Extensive experiments conducted on two benchmark datasets demonstrate that the proposed framework significantly enhances the resistance against state-of-the-art data-free model extraction in black-box settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "M. Gurve",
        "S. Behera",
        "Satyadev Ahlawat",
        "Yamuna Prasad"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3e24fcec8b51278cb2234560da6f6933f8fc7426",
      "pdf_url": "",
      "publication_date": "2024-03-27",
      "keywords_matched": [
        "model extraction",
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "28e8ac6073a4abf2a220bc7951908c40d9d0b1f2",
      "title": "Proteus: Preserving Model Confidentiality during Graph Optimizations",
      "abstract": "Deep learning (DL) models have revolutionized numerous domains, yet optimizing them for computational efficiency remains a challenging endeavor. Development of new DL models typically involves two parties: the model developers and performance optimizers. The collaboration between the parties often necessitates the model developers exposing the model architecture and computational graph to the optimizers. However, this exposure is undesirable since the model architecture is an important intellectual property, and its innovations require significant investments and expertise. During the exchange, the model is also vulnerable to adversarial attacks via model stealing. This paper presents Proteus, a novel mechanism that enables model optimization by an independent party while preserving the confidentiality of the model architecture. Proteus obfuscates the protected model by partitioning its computational graph into subgraphs and concealing each subgraph within a large pool of generated realistic subgraphs that cannot be easily distinguished from the original. We evaluate Proteus on a range of DNNs, demonstrating its efficacy in preserving confidentiality without compromising performance optimization opportunities. Proteus effectively hides the model as one alternative among up to $10^{32}$ possible model architectures, and is resilient against attacks with a learning-based adversary. We also demonstrate that heuristic based and manual approaches are ineffective in identifying the protected model. To our knowledge, Proteus is the first work that tackles the challenge of model confidentiality during performance optimization. Proteus will be open-sourced for direct use and experimentation, with easy integration with compilers such as ONNXRuntime.",
      "year": 2024,
      "venue": "Conference on Machine Learning and Systems",
      "authors": [
        "Yubo Gao",
        "Maryam Haghifam",
        "Christina Giannoula",
        "Renbo Tu",
        "Gennady Pekhimenko",
        "Nandita Vijaykumar"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/28e8ac6073a4abf2a220bc7951908c40d9d0b1f2",
      "pdf_url": "",
      "publication_date": "2024-04-18",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "704d6ace88e1f5cbea2a6465cc627e196a6fe440",
      "title": "VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces",
      "abstract": "In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of our method under various scenarios, including the availability of top-k prediction probabilities, top-k prediction labels, and top-1 labels.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Somnath Sendhil Kumar",
        "Yuvaraj Govindarajulu",
        "Pavan Kulkarni",
        "Manojkumar Parmar"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/704d6ace88e1f5cbea2a6465cc627e196a6fe440",
      "pdf_url": "",
      "publication_date": "2024-08-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a9f0a5405f75e2596ebde4b0339dcc20b3b72db3",
      "title": "Exploring the Validity of Knockoff Nets Model Stealing Attack on Vgg16 Based on Different Models",
      "abstract": "Model stealing attacks represented by the Knockoff Nets method steal the intellectual property of AI models by black-box querying. Model stealing attacks on a wide range of deep learning models have attracted widespread attention in recent years. However, there has not been any research on stealing attacks based on common models such as VGG16, ResNet18, AlexNet, etc., especially since the research on the validity of the attack on the VGG16 model is still insufficient. Therefore, in this paper, three types of models, VGG16, ResNet18, and AlexNet, are used as the models for stealing, and the Knockoff Nets method is used to carry out stealing attacks on the pre-trained model of VGG16, which is capable of cat and dog image recognition. This paper analyzes the stealing similarity, stealing model accuracy and stealing training time so as to reflect the validity of stealing. The paper shows that Knockoff Nets based on three types of models, VGG16, ResNet18, and AlexNet, are all effective against the VGG16 model stealing attack, and the more similar the architectures of the stealing model and the victim's model are, the better the stealing effect is. In addition, to a certain extent, the stealing training time and the stealing model accuracy are affected by the architecture of model used to steal. This paper reveals the validity of the Knockoff Nets model stealing attack against VGG16 based on three types of models, namely VGG16, ResNet18, and AlexNet, to provide a reference for model security protection.",
      "year": 2024,
      "venue": "Applied and Computational Engineering",
      "authors": [
        "Yunxi Hei"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a9f0a5405f75e2596ebde4b0339dcc20b3b72db3",
      "pdf_url": "https://www.ewadirect.com/proceedings/ace/article/view/17285/pdf",
      "publication_date": "2024-11-26",
      "keywords_matched": [
        "model stealing",
        "stealing model",
        "model stealing attack",
        "knockoff nets",
        "knockoff net"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0c713d0c069bfe03c945bfe7f67e9176ea8bcaff",
      "title": "Enhancing Data-Free Model Stealing Attack on Robust Models",
      "abstract": "Machine Learning Model Deployment as a Service (MLaaS) has surged in popularity, offering substantial business value. However, the significant resources and costs required to train models have raised concerns about Model Stealing Attacks (MSAs), where attackers create a clone model to replicate the knowledge of a victim model without access to its parameters. In data-free MSA, attackers also lack access to the training data for the victim model. In this setting, existing MSA methods rely on Generative Adversarial Networks (GANs) to generate images to query the victim model. However, GANs are known to suffer from model collapse, resulting in limited diversity in generated images. The lack of diversity in generated images will significantly impact the accuracy of the clone model, especially in stealing robust models trained with adversarial training. Recent studies have demonstrated that Denoising Diffusion Probabilistic Models (DDPMs) outperform GANs in generating images with greater diversity. In our data-free MSA framework, using DDPM as the generator to steal robust models significantly increases the effectiveness, improving the accuracy of the clone model from 21.34% to 60.23% compared to the GANs-based approach DFME, and requires fewer queries. We further use denoise diffusion GANs to address the problem of low sampling speed of DDPM, while retaining the advantage of its high sample diversity and obtaining better results.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Jianping He",
        "Haichang Gao",
        "Yunyi Zhou"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0c713d0c069bfe03c945bfe7f67e9176ea8bcaff",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9bedd67004ef344b801365ae28c09dce28410517",
      "title": "Stolen Subwords: Importance of Vocabularies for Machine Translation Model Stealing",
      "abstract": "In learning-based functionality stealing, the attacker is trying to build a local model based on the victim's outputs. The attacker has to make choices regarding the local model's architecture, optimization method and, specifically for NLP models, subword vocabulary, such as BPE. On the machine translation task, we explore (1) whether the choice of the vocabulary plays a role in model stealing scenarios and (2) if it is possible to extract the victim's vocabulary. We find that the vocabulary itself does not have a large effect on the local model's performance. Given gray-box model access, it is possible to collect the victim's vocabulary by collecting the outputs (detokenized subwords on the output). The results of the minimum effect of vocabulary choice are important more broadly for black-box knowledge distillation.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Vil\u00e9m Zouhar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9bedd67004ef344b801365ae28c09dce28410517",
      "pdf_url": "",
      "publication_date": "2024-01-29",
      "keywords_matched": [
        "model stealing",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "08e1f102344bc341e8109a5c23f78093a4c53323",
      "title": "Protecting Object Detection Models from Model Extraction Attack via Feature Space Coverage",
      "abstract": "The model extraction attack is an attack pattern aimed at stealing well-trained machine learning models' functionality or privacy information. With the gradual popularization of AI-related technologies in daily life, various well-trained models are being deployed. As a result, these models are considered valuable assets and attractive to model extraction attackers. Currently, the academic community primarily focuses on defense for model extraction attacks in the context of classification, with little attention to the more commonly used task scenario of object detection. Therefore, we propose a detection framework targeting model extraction attacks against object detection models in this paper. The framework first locates suspicious users based on feature coverage in query traffic and uses an active verification module to confirm whether the identified suspicious users are attackers. Through experiments conducted in multiple task scenarios, we validate the effectiveness and detection efficiency of the proposed method.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Zeyu Li",
        "Yuwen Pu",
        "Xuhong Zhang",
        "Yu Li",
        "Jinbao Li",
        "Shouling Ji"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/08e1f102344bc341e8109a5c23f78093a4c53323",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "deef9baed03e2eb53aac92a38b5cfa6317dc1019",
      "title": "A Hard-Label Cryptanalytic Extraction of Non-Fully Connected Deep Neural Networks using Side-Channel Attacks",
      "abstract": "During the past decade, Deep Neural Networks (DNNs) proved their value on a large variety of subjects. However despite their high value and public accessibility, the protection of the intellectual property of DNNs is still an issue and an emerging research field. Recent works have successfully extracted fully-connected DNNs using cryptanalytic methods in hard-label settings, proving that it was possible to copy a DNN with high fidelity, i.e., high similitude in the output predictions. However, the current cryptanalytic attacks cannot target complex, i.e., not fully connected, DNNs and are limited to special cases of neurons present in deep networks. In this work, we introduce a new end-to-end attack framework designed for model extraction of embedded DNNs with high fidelity. We describe a new black-box side-channel attack which splits the DNN in several linear parts for which we can perform cryptanalytic extraction and retrieve the weights in hard-label settings. With this method, we are able to adapt cryptanalytic extraction, for the first time, to non-fully connected DNNs, while maintaining a high fidelity. We validate our contributions by targeting several architectures implemented on a microcontroller unit, including a Multi-Layer Perceptron (MLP) of 1.7 million parameters and a shortened MobileNetv1. Our framework successfully extracts all of these DNNs with high fidelity (88.4% for the MobileNetv1 and 93.2% for the MLP). Furthermore, we use the stolen model to generate adversarial examples and achieve close to white-box performance on the victim's model (95.8% and 96.7% transfer rate).",
      "year": 2024,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Beno\u00eet Coqueret",
        "Mathieu Carbone",
        "Olivier Sentieys",
        "Gabriel Zaid"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/deef9baed03e2eb53aac92a38b5cfa6317dc1019",
      "pdf_url": "",
      "publication_date": "2024-11-15",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e57dea53d0d1f28d330a0f635232d07c37d9403f",
      "title": "2.5D Interposer HBM Signal Integrity Analysis Based on Fast MoM",
      "abstract": "High-bandwidth memory (HBM) placed side-by-side with ASIC on silicon interposer is capable of delivering the TB/s bandwidth. To maintain such high bandwidth, it is crucial to perform extensive signal integrity analysis and optimization. In this paper, a novel methodology for fast HBM signal integrity analysis and optimization is proposed, which includes accurate HBM channel model extraction and fast HBM channel simulation in both frequency domain and time domain with IBIS-AMI models to ultimately enable full signal coverage with automation.",
      "year": 2024,
      "venue": "International Conference on Information Communication and Management",
      "authors": [
        "Wenliang Dai",
        "Ping Liu",
        "Liguo Jiang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e57dea53d0d1f28d330a0f635232d07c37d9403f",
      "pdf_url": "",
      "publication_date": "2024-10-25",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b9a4b5e07b9973e968ea746159bfe88c0801f854",
      "title": "Security Concerns of Machine Learning Hardware",
      "abstract": "AI-as-a-Service (AIaaS) has been emerging with model providers deploying their models on cloud and model consumers using the model. Recently, ML models are being deployed on edge devices to improve cost and response time. The widespread usage of machine learning has made the study of security in the context of Machine Learning (ML) very critical. Model extraction attacks focuses on extracting model parameters such as weights and biases which can be used to clone a ML target model deployed on the cloud or on an edge device hardware. This paper explores different types of attacks on ML models primarily focusing on model extraction attacks on ML hardware such as scan-chain and side-channel attacks. The paper present an analysis of various such attacks and their countermeasures. Possible future directions of work are also discussed.",
      "year": 2024,
      "venue": "Asian Test Symposium",
      "authors": [
        "Nilotpola Sarma",
        "E. Bhawani",
        "E. Reddy",
        "C. Karfa"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b9a4b5e07b9973e968ea746159bfe88c0801f854",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e01dfe4abe3e952a9425a4fac38e673c008ae3e1",
      "title": "CLUES: Collusive Theft of Conditional Generative Adversarial Networks",
      "abstract": "Conditional Generative Adversarial Networks (cGANs) are increasingly popular web-based synthesis services accessed through a query API, e.g., cGANs generate a cat image based on a \u201ccat\u201d query. However, cGAN-based synthesizers can be stolen via adversaries' queries, i.e., model thieves. The prevailing adversarial assumption is that thieves act independently: they query the deployed cGAN (i.e., the victim), and train a stolen cGAN using the images obtained from the victim. A popular anti-theft defense consists in throttling down the number of queries from any given user. We consider a more realistic adversarial scenario: model thieves collude to query the victim, and then train the stolen cGAN. Clues is a new collusive model stealing framework, enabling thieves to bypass throttle-based defenses and steal cGANs more efficiently than through individual efforts. Thieves collect queried images and train a stolen cGAN in a federated manner. We evaluate Clues on three image datasets, e.g., MNIST, FashionMNIST and CelebA. We experimentally show the scalability of the proposed attack strategies against the number of thieves and the queried images, the impact of a classical noise-based defense, a passive watermarking defense and a JPEG-based countermeasure. Our evaluation shows that such a collusive stealing strategy gets close to 4 units of Frechet Inception Distance from a victim model. Our code is readily available to the research community: https://zenodo.org/records/10224340.",
      "year": 2024,
      "venue": "IEEE International Symposium on Reliable Distributed Systems",
      "authors": [
        "Simon Queyrut",
        "V. Schiavoni",
        "Lydia Chen",
        "Pascal Felber",
        "Robert Birke"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e01dfe4abe3e952a9425a4fac38e673c008ae3e1",
      "pdf_url": "",
      "publication_date": "2024-09-30",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1f98f66ade820ce31d5aa5099f60e856f52db700",
      "title": "AuthNet: Neural Network with Integrated Authentication Logic",
      "abstract": "Model stealing, i.e., unauthorized access and exfiltration of deep learning models, has become one of the major threats. Proprietary models may be protected by access controls and encryption. However, in reality, these measures can be compromised due to system breaches, query-based model extraction or a disgruntled insider. Security hardening of neural networks is also suffering from limits, for example, model watermarking is passive, cannot prevent the occurrence of piracy and not robust against transformations. To this end, we propose a native authentication mechanism, called AuthNet, which integrates authentication logic as part of the model without any additional structures. Our key insight is to reuse redundant neurons with low activation and embed authentication bits in an intermediate layer, called a gate layer. Then, AuthNet fine-tunes the layers after the gate layer to embed authentication logic so that only inputs with special secret key can trigger the correct logic of AuthNet. It exhibits two intuitive advantages. It provides the last line of defense, i.e., even being exfiltrated, the model is not usable as the adversary cannot generate valid inputs without the key. Moreover, the authentication logic is difficult to inspect and identify given millions or billions of neurons in the model. We theoretically demonstrate the high sensitivity of AuthNet to the secret key and its high confusion for unauthorized samples. AuthNet is compatible with any convolutional neural network, where our extensive evaluations show that AuthNet successfully achieves the goal in rejecting unauthenticated users (whose average accuracy drops to 22.03%) with a trivial accuracy decrease (1.18% on average) for legitimate users, and is robust against model transformation and adaptive attacks.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yuling Cai",
        "Fan Xiang",
        "Guozhu Meng",
        "Yinzhi Cao",
        "Kai Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1f98f66ade820ce31d5aa5099f60e856f52db700",
      "pdf_url": "",
      "publication_date": "2024-05-24",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b0376b413ffaa728518fc0e6102ff31d0cb35e65",
      "title": "Attack Data is Not Solely Paramount: A Universal Model Extraction Enhancement Method",
      "abstract": "Model extraction (ME) attacks, aiming to steal the functionality or parameters of the victim model, have become a widespread research topic. Most functional ME attack methodologies follow a uniform framework, which we summarize in three steps: initially choosing appropriate attack data, then querying the victim model with this data, and finally, training an incipient clone model based on the victim model\u2019s outputs. Despite much focus on data selection, the latter two steps have been somewhat neglected. Noticing this, we explore a method for the information of attack data labels to enhance the accuracy of the clone model. Specifically, we utilized the incipient clone model to identify similarities between the leaked private data and the attack data, subsequently appending the labels from the leaked data to those of the attack data. Then, we employed these modified attack data labels to fine-tune the incipient clone model, obtaining an enhanced clone model with higher accuracy. The enhancement was applied to three representative ME attack methodologies that primarily focus on the first step. Results show that the enhanced model reveals a higher accuracy than the three basic attacks. In summary, our approach suggests that future research should extend beyond data selection.",
      "year": 2024,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Chuang Liang",
        "Jie Huang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b0376b413ffaa728518fc0e6102ff31d0cb35e65",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction",
        "clone model",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b6b0c38b86d8b8c430f901aed448eb512fb757e0",
      "title": "On the Security Vulnerabilities of MRAM-Based in-Memory Computing Architectures Against Model Extraction Attacks",
      "abstract": "This paper studies the security vulnerabilities of embedded nonvolatile memory (eNVM)-based in-memory computing (IMC) architectures to model extraction attacks (MEAs). These attacks allow the reconstruction of private training data from trained model parameters thereby leaking sensitive user information. The presence of analog noise in eNVM-based IMC computation suggests that they may be intrinsically robust to MEA. However, we show that this conjecture is false. Specifically, we consider the scenario where an attacker aims to retrieve model parameters via input-output query access, and propose three attacks that exploit the statistics of the IMC computation. We demonstrate the efficacy of these attacks in extracting the model parameters of the last layer of a ResNet-20 network from the bitcell array of an MRAM-based IMC prototype in 22 nm process. Employing the proposed MEAs, the attacker obtains a CIFAR-10 accuracy within 0.1 % of that of a $N=64$ dimensional, $7 \\mathrm{b} \\times 4 \\mathrm{b}$ fixed-point digital baseline. To the best of our knowledge, this is the first work to demonstrate MEAs for eNVM-based IMC on a real-life IC prototype. Our results indicate the critical importance of investigating the security vulnerabilities of IMCs in general, and eNVM-based IMCs, in particular.",
      "year": 2024,
      "venue": "International Conference on Computer Aided Design",
      "authors": [
        "Saion K. Roy",
        "Naresh R. Shanbhag"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b6b0c38b86d8b8c430f901aed448eb512fb757e0",
      "pdf_url": "",
      "publication_date": "2024-10-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f5a16eb238e96accbe067c68886fc29d1202817f",
      "title": "Efficient Model Extraction via Boundary Sampling",
      "abstract": "This paper introduces a novel data-free model extraction attack that significantly advances the current state-of-the-art in terms of efficiency, accuracy, and effectiveness. Traditional black-box methods rely on using the victim's model as an oracle to label a vast number of samples within high-confidence areas. This approach not only requires an extensive number of queries but also results in a less accurate and less transferable model. In contrast, our method innovates by focusing on sampling low-confidence areas (along the decision boundaries) and employing an evolutionary algorithm to optimize the sampling process. These novel contributions allow for a dramatic reduction in the number of queries needed by the attacker by a factor of 10x to 600x while simultaneously improving the accuracy of the stolen model. Moreover, our approach improves boundary alignment, resulting in better transferability of adversarial examples from the stolen model to the victim's model (increasing the attack success rate from 60% to 82% on average). Finally, we accomplish all of this with a strict black-box assumption on the victim, with no knowledge of the target's architecture or dataset. We demonstrate our attack on three datasets with increasingly larger resolutions and compare our performance to four state-of-the-art model extraction attacks.",
      "year": 2024,
      "venue": "AISec@CCS",
      "authors": [
        "Maor Biton Dor",
        "Yisroel Mirsky"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f5a16eb238e96accbe067c68886fc29d1202817f",
      "pdf_url": "http://arxiv.org/pdf/2410.15429",
      "publication_date": "2024-10-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "30b87afb2801b52c8ee6ed381ed6e7a56ee68b70",
      "title": "CaBaGe: Data-Free Model Extraction using ClAss BAlanced Generator Ensemble",
      "abstract": "Machine Learning as a Service (MLaaS) is often provided as a pay-per-query, black-box system to clients. Such a black-box approach not only hinders open replication, validation, and interpretation of model results, but also makes it harder for white-hat researchers to identify vulnerabilities in the MLaaS systems. Model extraction is a promising technique to address these challenges by reverse-engineering black-box models. Since training data is typically unavailable for MLaaS models, this paper focuses on the realistic version of it: data-free model extraction. We propose a data-free model extraction approach, CaBaGe, to achieve higher model extraction accuracy with a small number of queries. Our innovations include (1) a novel experience replay for focusing on difficult training samples; (2) an ensemble of generators for steadily producing diverse synthetic data; and (3) a selective filtering process for querying the victim model with harder, more balanced samples. In addition, we create a more realistic setting, for the first time, where the attacker has no knowledge of the number of classes in the victim training data, and create a solution to learn the number of classes on the fly. Our evaluation shows that CaBaGe outperforms existing techniques on seven datasets -- MNIST, FMNIST, SVHN, CIFAR-10, CIFAR-100, ImageNet-subset, and Tiny ImageNet -- with an accuracy improvement of the extracted models by up to 43.13%. Furthermore, the number of queries required to extract a clone model matching the final accuracy of prior work is reduced by up to 75.7%.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jonathan Rosenthal",
        "Shanchao Liang",
        "Kevin Zhang",
        "Lin Tan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/30b87afb2801b52c8ee6ed381ed6e7a56ee68b70",
      "pdf_url": "",
      "publication_date": "2024-09-16",
      "keywords_matched": [
        "model extraction",
        "clone model",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "18c9536bbc914ab9d33fd4687c4518bda0e82a0f",
      "title": "Enhancing Data-Free Robustness Stealing Attack via Boundary Data Generation",
      "abstract": "With the continuous development of Machine Learning as a Service (MLaaS), model stealing has become an emerging problem in machine learning security in recent years. In model stealing, one typically obtains the soft labels of model queries and a proxy dataset as prior knowledge, but this scenario is highly idealised. How to steal models without data and hard labels is a pressing problem that needs to be solved. The current mainstream of model stealing attack methods mainly focus on stealing the accuracy of the model and overlook the robustness of the model. However, robustness is essential in security applications such as facial recognition and secure payment scenarios. Moreover, building robust models usually requires costly adversarial training and fine-tuning, making these models the primary targets for theft. To address these issues, in this paper, we propose a new data-free robustness stealing method under data-free conditions from the perspective of data generation, thereby better shaping the classification boundary data to optimise the accuracy and robustness of the models. Through testing, our method achieved clean accuracy and robust accuracy of 53.69% and 21.0%, respectively, under the more complex CIFAR-100 dataset classification. These results are only 3.06% and 3.94% different from the target model, respectively, showing a significant improvement over recent research.",
      "year": 2024,
      "venue": "2024 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics",
      "authors": [
        "Xiaoji Ma",
        "Weihao Guo",
        "Pingyuan Ge",
        "Ying Chen",
        "Qiuling Yue",
        "Yuqing Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/18c9536bbc914ab9d33fd4687c4518bda0e82a0f",
      "pdf_url": "",
      "publication_date": "2024-08-19",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "69ccce44213a79c679ccde2615c5dfb1fa4f6406",
      "title": "Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine Learning Services",
      "abstract": "Pre-trained encoders available online have been widely adopted to build downstream machine learning (ML) services, but various attacks against these encoders also post security and privacy threats toward such a downstream ML service paradigm. We unveil a new vulnerability: the Pre-trained Encoder Inference (PEI) attack, which can extract sensitive encoder information from a targeted downstream ML service that can then be used to promote other ML attacks against the targeted service. By only providing API accesses to a targeted downstream service and a set of candidate encoders, the PEI attack can successfully infer which encoder is secretly used by the targeted service based on candidate ones. Compared with existing encoder attacks, which mainly target encoders on the upstream side, the PEI attack can compromise encoders even after they have been deployed and hidden in downstream ML services, which makes it a more realistic threat. We empirically verify the effectiveness of the PEI attack on vision encoders. we first conduct PEI attacks against two downstream services (i.e., image classification and multimodal generation), and then show how PEI attacks can facilitate other ML attacks (i.e., model stealing attacks vs. image classification models and adversarial attacks vs. multimodal generative models). Our results call for new security and privacy considerations when deploying encoders in downstream services. The code is available at https://github.com/fshp971/encoder-inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Shaopeng Fu",
        "Xuexue Sun",
        "Ke Qing",
        "Tianhang Zheng",
        "Di Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/69ccce44213a79c679ccde2615c5dfb1fa4f6406",
      "pdf_url": "",
      "publication_date": "2024-08-05",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a2d3025cee2b0e66d6e1516fc9955fd279f48eb5",
      "title": "Late Breaking Results: Extracting QNNs from NISQ Computers via Ensemble Learning",
      "abstract": "The recent success of Quantum Neural Networks (QNNs) prompts model extraction attacks on cloud platforms, even under black-box constraints. These attacks repeatedly query the victim QNN with malicious inputs. However, existing extraction attacks tailored for classical models yield local substitute QNNs with limited performance due to NISQ computer noise. Drawing from bagging-based ensemble learning, which uses independent weak learners to learn from noisy data, we introduce a novel QNN extraction approach. Our experimental results show this quantum ensemble learning approach improves local QNN accuracy by up to 15.09% compared to previous techniques.",
      "year": 2024,
      "venue": "Design Automation Conference",
      "authors": [
        "Zhenxiao Fu",
        "Fan Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a2d3025cee2b0e66d6e1516fc9955fd279f48eb5",
      "pdf_url": "",
      "publication_date": "2024-06-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "208f2c852d48f8c04e2d4cacc8c803691ac0d983",
      "title": "Watermarking Counterfactual Explanations",
      "abstract": "Counterfactual (CF) explanations for ML model predictions provide actionable recourse recommendations to individuals adversely impacted by predicted outcomes. However, despite being preferred by end-users, CF explanations have been shown to pose significant security risks in real-world applications; in particular, malicious adversaries can exploit CF explanations to perform query-efficient model extraction attacks on the underlying proprietary ML model. To address this security challenge, we propose CFMark, a novel model-agnostic watermarking framework for detecting unauthorized model extraction attacks relying on CF explanations. CFMark involves a novel bi-level optimization problem to embed an indistinguishable watermark into the generated CF explanation such that any future model extraction attacks using these watermarked CF explanations can be detected using a null hypothesis significance testing (NHST) scheme. At the same time, the embedded watermark does not compromise the quality of the CF explanations. We evaluate CFMark across diverse real-world datasets, CF explanation methods, and model extraction techniques. Our empirical results demonstrate CFMark's effectiveness, achieving an F-1 score of ~0.89 in identifying unauthorized model extraction attacks using watermarked CF explanations. Importantly, this watermarking incurs only a negligible degradation in the quality of generated CF explanations (i.e., ~1.3% degradation in validity and ~1.6% in proximity). Our work establishes a critical foundation for the secure deployment of CF explanations in real-world applications.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hangzhi Guo",
        "Amulya Yadav"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/208f2c852d48f8c04e2d4cacc8c803691ac0d983",
      "pdf_url": "",
      "publication_date": "2024-05-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "696ddfebeea945fd0a0144e06135a6b151dc47cb",
      "title": "Model extraction via active learning by fusing prior and posterior knowledge from unlabeled data",
      "abstract": "As machine learning models become increasingly integrated into practical applications and are made accessible via public APIs, the risk of model extraction attacks has gained prominence. This study presents an innovative and efficient approach to model extraction attacks, aimed at reducing query costs and enhancing attack effectiveness. The method begins by leveraging a pre-trained model to identify high-confidence samples from unlabeled datasets. It then employs unsupervised contrastive learning to thoroughly dissect the structural nuances of these samples, constructing a dataset of high quality that precisely mirrors a variety of features. A mixed information confidence strategy is employed to refine the query set, effectively probing the decision boundaries of the target model. By integrating consistency regularization and pseudo-labeling techniques, reliance on authentic labels is minimized, thus improving the feature extraction capabilities and predictive precision of the surrogate models. Evaluation on four major datasets reveals that the models crafted through this method bear a close functional resemblance to the original models, with a real-world API test success rate of 62.35%, which vouches for the method\u2019s validity.",
      "year": 2024,
      "venue": "Journal of Intelligent &amp; Fuzzy Systems",
      "authors": [
        "Lijun Gao",
        "Kai Liu",
        "Wenjun Liu",
        "Jiehong Wu",
        "Xiao Jin"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/696ddfebeea945fd0a0144e06135a6b151dc47cb",
      "pdf_url": "",
      "publication_date": "2024-03-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7727e2a4116a63759d0a942b31732c2014007029",
      "title": "LDPKiT: Superimposing Remote Queries for Privacy-Preserving Local Model Training",
      "abstract": "Users of modern Machine Learning (ML) cloud services face a privacy conundrum -- on one hand, they may have concerns about sending private data to the service for inference, but on the other hand, for specialized models, there may be no alternative but to use the proprietary model of the ML service. In this work, we present LDPKiT, a framework for non-adversarial, privacy-preserving model extraction that leverages a user's private in-distribution data while bounding privacy leakage. LDPKiT introduces a novel superimposition technique that generates approximately in-distribution samples, enabling effective knowledge transfer under local differential privacy (LDP). Experiments on Fashion-MNIST, SVHN, and PathMNIST demonstrate that LDPKiT consistently improves utility while maintaining privacy, with benefits that become more pronounced at stronger noise levels. For example, on SVHN, LDPKiT achieves nearly the same inference accuracy at $\\epsilon=1.25$ as at $\\epsilon=2.0$, yielding stronger privacy guarantees with less than a 2% accuracy reduction. We further conduct sensitivity analyses to examine the effect of dataset size on performance and provide a systematic analysis of latent space representations, offering theoretical insights into the accuracy gains of LDPKiT.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Kexin Li",
        "Aastha Mehta",
        "David Lie"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7727e2a4116a63759d0a942b31732c2014007029",
      "pdf_url": "",
      "publication_date": "2024-05-25",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "eb9378e6c7613404e347327caf130bdcdff1d66f",
      "title": "Model Extraction Attacks on Text-to-Image Generative Adversarial Networks",
      "abstract": "Model extraction attack refers to attackers ille-gally obtaining the functionality of a victim model by querying it. Currently, attacks primarily focus on discriminative models in computer vision. However, model extraction attacks on generative models, especially tasks like generating images from text, remain underexplored. The task of generating corresponding images for text is not only captivating but also highly challenging. In this study, we are the first to comprehensively investigate the feasibility of executing model extraction attacks on Text-to-Image Generative Adversarial Networks (T2I-GANs). To provide a more nuanced understanding, we introduce the concepts of fidelity and accuracy in model extraction attacks targeting T2I-GANs. Extensive experimental validation in black-box attack scenarios demonstrates that we achieve high-fidelity and high-accuracy extraction of T2I-GAN models. We employ the CLIP model to filter queried data, resulting in a fidelity of 81 % for the substitute model. Furthermore, through subsampling techniques, we effectively filter high-quality samples that closely resemble the distribution of real datasets, thereby increasing accuracy to 87 %.",
      "year": 2024,
      "venue": "2024 IEEE Cyber Science and Technology Congress (CyberSciTech)",
      "authors": [
        "Ying Chen",
        "Weihao Guo",
        "Pingyuan Ge",
        "Xiaoji Ma",
        "Yuqing Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/eb9378e6c7613404e347327caf130bdcdff1d66f",
      "pdf_url": "",
      "publication_date": "2024-11-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c25d2a27f1abe169d7b68078071b6698f0980469",
      "title": "Protecting Language Generation Models via Invisible Watermarking",
      "abstract": "Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as\"synonym randomization\". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the generation quality of protected APIs. Our method demonstrates an absolute improvement of 19 to 29 points on mean average precision (mAP) in detecting suspects compared to previous methods against watermark removal attacks.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Xuandong Zhao",
        "Yu-Xiang Wang",
        "Lei Li"
      ],
      "citation_count": 107,
      "url": "https://www.semanticscholar.org/paper/c25d2a27f1abe169d7b68078071b6698f0980469",
      "pdf_url": "https://arxiv.org/pdf/2302.03162",
      "publication_date": "2023-02-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "title": "APMSA: Adversarial Perturbation Against Model Stealing Attacks",
      "abstract": "Training a Deep Learning (DL) model requires proprietary data and computing-intensive resources. To recoup their training costs, a model provider can monetize DL models through Machine Learning as a Service (MLaaS). Generally, the model is deployed at the cloud, while providing a publicly accessible Application Programming Interface (API) for paid queries to obtain benefits. However, model stealing attacks have posed security threats to this model monetizing scheme as they steal the model without paying for future extensive queries. Specifically, an adversary queries a targeted model to obtain input-output pairs and thus infer the model\u2019s internal working mechanism by reverse-engineering a substitute model, which has deprived model owner\u2019s business advantage and leaked the privacy of the model. In this work, we observe that the confidence vector or the top-1 confidence returned from the model under attack (MUA) varies in a relative large degree given different queried inputs. Therefore, rich internal information of the MUA is leaked to the attacker that facilities her reconstruction of a substitute model. We thus propose to leverage adversarial confidence perturbation to hide such varied confidence distribution given different queries, consequentially against model stealing attacks (dubbed as APMSA). In other words, the confidence vectors returned now is similar for queries from a specific category, considerably reducing information leakage of the MUA. To achieve this objective, through automated optimization, we constructively add delicate noise into per input query to make its confidence close to the decision boundary of the MUA. Generally, this process is achieved in a similar means of crafting adversarial examples but with a distinction that the hard label is preserved to be the same as the queried input. This retains the inference utility (i.e., without sacrificing the inference accuracy) for normal users but bounded the leaked confidence information to the attacker in a small constrained area (i.e., close to decision boundary). The later renders greatly deteriorated accuracy of the attacker\u2019s substitute model. As the APMSA serves as a plug-in front-end and requires no change to the MUA, it is thus generic and easy to deploy. The high efficacy of APMSA is validated through experiments on datasets of CIFAR10 and GTSRB. Given a MUA model of ResNet-18 on the CIFAR10, our defense can degrade the accuracy of the stolen model by up to 15% (rendering the stolen model useless to a large extent) with 0% accuracy drop for normal user\u2019s hard-label inference request.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Jiliang Zhang",
        "Shuang Peng",
        "Yansong Gao",
        "Zhi Zhang",
        "Q. Hong"
      ],
      "citation_count": 76,
      "url": "https://www.semanticscholar.org/paper/796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e169ce8cc1627ff18f8fc4361f622bb31d33326b",
      "title": "No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML",
      "abstract": "On-device ML introduces new security challenges: DNN models become white-box accessible to device users. Based on white-box information, adversaries can conduct effective model stealing (MS) against model weights and membership inference attack (MIA) against training data privacy. Using Trusted Execution Environments (TEEs) to shield on-device DNN models aims to downgrade (easy) white-box attacks to (harder) black-box attacks. However, one major shortcoming of TEEs is the sharply increased latency (up to 50\u00d7). To accelerate TEE-shield DNN computation with GPUs, researchers proposed several model partition techniques. These solutions, referred to as TEE-Shielded DNN Partition (TSDP), partition a DNN model into two parts, offloading1 the privacy-insensitive part to the GPU while shielding the privacy-sensitive part within the TEE. However, the community lacks an in-depth understanding of the seemingly encouraging privacy guarantees offered by existing TSDP solutions during DNN inference. This paper benchmarks existing TSDP solutions using both MS and MIA across a variety of DNN models, datasets, and metrics. We show important findings that existing TSDP solutions are vulnerable to privacy-stealing attacks and are not as safe as commonly believed. We also unveil the inherent difficulty in deciding the optimal DNN partition configurations, which vary across datasets and models. Based on lessons harvested from the experiments, we present TEESlice, a novel TSDP method that defends against MS and MIA during DNN inference. Unlike existing approaches, TEESlice follows a partition-before-training strategy, which allows for accurate separation between privacy-related weights from public weights. TEESlice delivers the same security protection as shielding the entire DNN model inside TEE (the \"upper-bound\" security guarantees) with over 10\u00d7less overhead (in both experimental and real-world environments) than prior TSDP solutions and no accuracy loss. We make the code and artifacts publicly available on the Internet.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Ziqi Zhang",
        "Chen Gong",
        "Yifeng Cai",
        "Yuanyuan Yuan",
        "Bingyan Liu",
        "Ding Li",
        "Yao Guo",
        "Xiangqun Chen"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/e169ce8cc1627ff18f8fc4361f622bb31d33326b",
      "pdf_url": "https://arxiv.org/pdf/2310.07152",
      "publication_date": "2023-10-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "81cd9575100643a3463465ec19e90ee78e122f93",
      "title": "SoK: Model Inversion Attack Landscape: Taxonomy, Challenges, and Future Roadmap",
      "abstract": "A crucial module of the widely applied machine learning (ML) model is the model training phase, which involves large-scale training data, often including sensitive private data. ML models trained on these sensitive data suffer from significant privacy concerns since ML models can intentionally or unintendedly leak information about training data. Adversaries can exploit this information to perform privacy attacks, including model extraction, membership inference, and model inversion. While a model extraction attack steals and replicates a trained model functionality, and membership inference infers the data sample's inclusiveness to the training set, a model inversion attack has the goal of inferring the training data sample's sensitive attribute value or reconstructing the training sample (i.e., image/audio/text). Distinct and inconsistent characteristics of model inversion attack make this attack even more challenging and consequential, opening up model inversion attack as a more prominent and increasingly expanding research paradigm. Thereby, to flourish research in this relatively underexplored model inversion domain, we conduct the first-ever systematic literature review of the model inversion attack landscape. We characterize model inversion attacks and provide a comprehensive taxonomy based on different dimensions. We illustrate foundational perspectives emphasizing methodologies and key principles of the existing attacks and defense techniques. Finally, we discuss challenges and open issues in the existing model inversion attacks, focusing on the roadmap for future research directions.",
      "year": 2023,
      "venue": "IEEE Computer Security Foundations Symposium",
      "authors": [
        "S. Dibbo"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/81cd9575100643a3463465ec19e90ee78e122f93",
      "pdf_url": "",
      "publication_date": "2023-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "40b40e942db469663609ad6c1911cca235079434",
      "title": "D-DAE: Defense-Penetrating Model Extraction Attacks",
      "abstract": "Recent studies show that machine learning models are vulnerable to model extraction attacks, where the adversary builds a substitute model that achieves almost the same performance of a black-box victim model simply via querying the victim model. To defend against such attacks, a series of methods have been proposed to disrupt the query results before returning them to potential attackers, greatly degrading the performance of existing model extraction attacks.In this paper, we make the first attempt to develop a defense-penetrating model extraction attack framework, named D-DAE, which aims to break disruption-based defenses. The linchpins of D-DAE are the design of two modules, i.e., disruption detection and disruption recovery, which can be integrated with generic model extraction attacks. More specifically, after obtaining query results from the victim model, the disruption detection module infers the defense mechanism adopted by the defender. We design a meta-learning-based disruption detection algorithm for learning the fundamental differences between the distributions of disrupted and undisrupted query results. The algorithm features a good generalization property even if we have no access to the original training dataset of the victim model. Given the detected defense mechanism, the disruption recovery module tries to restore a clean query result from the disrupted query result with well-designed generative models. Our extensive evaluations on MNIST, FashionMNIST, CIFAR-10, GTSRB, and ImageNette datasets demonstrate that D-DAE can enhance the substitute model accuracy of the existing model extraction attacks by as much as 82.24% in the face of 4 state-of-the-art defenses and combinations of multiple defenses. We also verify the effectiveness of D-DAE in penetrating unknown defenses in real-world APIs hosted by Microsoft Azure and Face++.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yanjiao Chen",
        "Rui Guan",
        "Xueluan Gong",
        "Jianshuo Dong",
        "Meng Xue"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/40b40e942db469663609ad6c1911cca235079434",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e562c8ee825ed4e1b3ec4037f6e9e1194d355f07",
      "title": "Dual Student Networks for Data-Free Model Stealing",
      "abstract": "Existing data-free model stealing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent to optimizing a lower bound on the generator's loss if we had access to the target model gradients. We show that our new optimization framework provides more accurate gradient estimation of the target model and better accuracies on benchmark classification datasets. Additionally, our approach balances improved query efficiency with training computation cost. Finally, we demonstrate that our method serves as a better proxy model for transfer-based adversarial attacks than existing data-free model stealing methods.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "James Beetham",
        "Navid Kardan",
        "A. Mian",
        "M. Shah"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/e562c8ee825ed4e1b3ec4037f6e9e1194d355f07",
      "pdf_url": "https://arxiv.org/pdf/2309.10058",
      "publication_date": "2023-09-18",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1b0e8f3727f452d8ef13950ae61c631be8956306",
      "title": "Model Extraction Attacks Revisited",
      "abstract": "Model extraction (ME) attacks represent one major threat to Machine-Learning-as-a-Service (MLaaS) platforms by \"stealing\" the functionality of confidential machine-learning models through querying black-box APIs. Over seven years have passed since ME attacks were first conceptualized in the seminal work [75]. During this period, substantial advances have been made in both ME attacks and MLaaS platforms, raising the intriguing question: How has the vulnerability of MLaaS platforms to ME attacks been evolving? In this work, we conduct an in-depth study to answer this critical question. Specifically, we characterize the vulnerability of current, mainstream MLaaS platforms to ME attacks from multiple perspectives including attack strategies, learning techniques, surrogatemodel design, and benchmark tasks. Many of our findings challenge previously reported results, suggesting emerging patterns of ME vulnerability. Further, by analyzing the vulnerability of the same MLaaS platforms using historical datasets from the past four years, we retrospectively characterize the evolution of ME vulnerability over time, leading to a set of interesting findings. Finally, we make suggestions about improving the current practice of MLaaS in terms of attack robustness. Our study sheds light on the current state of ME vulnerability in the wild and points to several promising directions for future research.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Jiacheng Liang",
        "Ren Pang",
        "Changjiang Li",
        "Ting Wang"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/1b0e8f3727f452d8ef13950ae61c631be8956306",
      "pdf_url": "https://arxiv.org/pdf/2312.05386",
      "publication_date": "2023-12-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "afbc7f9d4a4dcf0cf02ab3efd043904b361fa886",
      "title": "DeepTheft: Stealing DNN Model Architectures through Power Side Channel",
      "abstract": "Deep Neural Network (DNN) models are often deployed in resource-sharing clouds as Machine Learning as a Service (MLaaS) to provide inference services. To steal model architectures that are of valuable intellectual properties, a class of attacks has been proposed via different side-channel leakage, posing a serious security challenge to MLaaS.Also targeting MLaaS, we propose a new end-to-end attack, DeepTheft, to accurately recover complex DNN model architectures on general processors via the RAPL (Running Average Power Limit)-based power side channel. While unprivileged access to the RAPL has been disabled in bare-metal OSes, we observe that the RAPL is still legitimately accessible in a platform as a service, e.g., the latest docker environment of version 20.10.18 used in this work. However, an attacker can acquire only a low sampling rate (1 KHz) of the time-series energy traces from the RAPL interface, rendering existing techniques ineffective in stealing large and deep DNN models. To this end, we design a novel and generic learning-based framework consisting of a set of meta-models, based on which DeepTheft is demonstrated to have high accuracy in recovering a large number (thousands) of models architectures from different model families including the deepest ResNet152. Particularly, DeepTheft has achieved a Levenshtein Distance Accuracy of 99.75% in recovering network structures, and a weighted average F1 score of 99.60% in recovering diverse layer-wise hyperparameters. Besides, our proposed learning framework is general to other time-series side-channel signals. To validate its generalization, another existing side channel is exploited, i.e., CPU frequency. Different from RAPL, CPU frequency is accessible to unprivileged users in bare-metal OSes. By using our generic learning framework trained against CPU frequency traces, DeepTheft has shown similarly high attack performance in stealing model architectures.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yansong Gao",
        "Huming Qiu",
        "Zhi Zhang",
        "Binghui Wang",
        "Hua Ma",
        "A. Abuadbba",
        "Minhui Xue",
        "Anmin Fu",
        "Surya Nepal"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/afbc7f9d4a4dcf0cf02ab3efd043904b361fa886",
      "pdf_url": "https://arxiv.org/pdf/2309.11894",
      "publication_date": "2023-09-21",
      "keywords_matched": [
        "steal model",
        "stealing model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b4149005980a11919731e6b4c1833d8b0af59424",
      "title": "Deep Neural Network Watermarking against Model Extraction Attack",
      "abstract": "Deep neural network (DNN) watermarking is an emerging technique to protect the intellectual property of deep learning models. At present, many DNN watermarking algorithms have been proposed to achieve provenance verification by embedding identify information into the internals or prediction behaviors of the host model. However, most methods are vulnerable to model extraction attacks, where attackers collect output labels from the model to train a surrogate or a replica. To address this issue, we present a novel DNN watermarking approach, named SSW, which constructs an adaptive trigger set progressively by optimizing over a pair of symmetric shadow models to enhance the robustness to model extraction. Precisely, we train a positive shadow model supervised by the prediction of the host model to mimic the behaviors of potential surrogate models. Additionally, a negative shadow model is normally trained to imitate irrelevant independent models. Using this pair of shadow models as a reference, we design a strategy to update the trigger samples appropriately such that they tend to persist in the host model and its stolen copies. Moreover, our method could well support two specific embedding schemes: embedding the watermark via fine-tuning or from scratch. Our extensive experimental results on popular datasets demonstrate that our SSW approach outperforms state-of-the-art methods against various model extraction attacks in whether trigger set classification accuracy based or hypothesis test based verification. The results also show that our method is robust to common model modification schemes including fine-tuning and model compression.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Jingxuan Tan",
        "Nan Zhong",
        "Zhenxing Qian",
        "Xinpeng Zhang",
        "Sheng Li"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/b4149005980a11919731e6b4c1833d8b0af59424",
      "pdf_url": "",
      "publication_date": "2023-10-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "06718e68bea215f2155bca2e08b70ad5d2aff62f",
      "title": "QUDA: Query-Limited Data-Free Model Extraction",
      "abstract": "Model extraction attack typically refers to extracting non-public information from a black-box machine learning model. Its unauthorized nature poses significant threat to intellectual property rights of the model owners. By using the well-designed queries and the predictions returned from the victim model, the adversary is able to train a clone model from scratch to obtain similar functionality as victim model. Recently, some methods have been proposed to perform model extraction attacks without using any in-distribution data (Data-free setting). Although these methods have been shown to achieve high clone accuracy, their query budgets are typically around 10 million or even exceed 20 million in some datasets, which lead to a high cost of model stealing and can be easily defended by limiting the number of queries. To illustrate the severe threats induced by model extraction attacks with limited query budget in realistic scenarios, we propose QUDA \u2013 a novel QUey-limited DAta-free model extraction attack that incorporates GAN pre-trained by public unrelated dataset to provide weak image prior and the technique of deep reinforcement learning to make query generation strategy more efficient. Compared with the state-of-the-art data-free model extraction method, QUDA achieves better results under query-limited condition (0.1M query budget) in FMNIST and CIFAR-10 datasets, and even outperforms the baseline method in most cases when QUDA uses only 10% query budget of its. QUDA issued a warning that solely relying on the limited numbers of queries or the confidentiality of training data is not reliable to protect model\u2019s security and privacy. Potential countermeasures, such as detection-based defense approach, are also provided.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Zijun Lin",
        "Ke Xu",
        "Chengfang Fang",
        "Huadi Zheng",
        "Aneez Ahmed Jaheezuddin",
        "Jie Shi"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/06718e68bea215f2155bca2e08b70ad5d2aff62f",
      "pdf_url": "",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fefdabd7bd1c0007c0ef7db5faa6486f28166c32",
      "title": "Membership Inference Attacks Against Sequential Recommender Systems",
      "abstract": "Recent studies have demonstrated the vulnerability of recommender systems to membership inference attacks, which determine whether a user\u2019s historical data was utilized for model training, posing serious privacy leakage issues. Existing works assumed that member and non-member users follow different recommendation modes, and then infer membership based on the difference vector between the user\u2019s historical behaviors and the recommendation list. The previous frameworks are invalid against inductive recommendations, such as sequential recommendations, since the disparities of difference vectors constructed by the recommendations between members and non-members become imperceptible. This motivates us to dig deeper into the target model. In addition, most MIA frameworks assume that they can obtain some in-distribution data from the same distribution of the target data, which is hard to gain in recommender system. To address these difficulties, we propose a Membership Inference Attack framework against sequential recommenders based on Model Extraction(ME-MIA). Specifically, we train a surrogate model to simulate the target model based on two universal loss functions. For a given behavior sequence, the loss functions ensure the recommended items and corresponding rank of the surrogate model are consistent with the target model\u2019s recommendation. Due to the special training mode of the surrogate model, it is hard to judge which user is its member(non-member). Therefore, we establish a shadow model and use shadow model\u2019s members(non-members) to train the attack model later. Next, we build a user feature generator to construct representative feature vectors from the shadow(surrogate) model. The crafting feature vectors are finally input into the attack model to identify users\u2019 membership. Furthermore, to tackle the high cost of obtaining in-distribution data, we develop two variants of ME-MIA, realizing data-efficient and even data-free MIA by fabricating authentic in-distribution data. Notably, the latter is impossible in the previous works. Finally, we evaluate ME-MIA against multiple sequential recommendation models on three real-world datasets. Experimental results show that ME-MIA and its variants can achieve efficient extraction and outperform state-of-the-art algorithms in terms of attack performance.",
      "year": 2023,
      "venue": "The Web Conference",
      "authors": [
        "Zhihao Zhu",
        "Chenwang Wu",
        "Rui Fan",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/fefdabd7bd1c0007c0ef7db5faa6486f28166c32",
      "pdf_url": "",
      "publication_date": "2023-04-30",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "cdd4b86f3038a4e4f813a0cc9d10d6b1928ddf8a",
      "title": "YOLO-Class: Detection and Classification of Aircraft Targets in Satellite Remote Sensing Images Based on YOLO-Extract",
      "abstract": "With the continuous advancement of remote sensing technology, satellite remote sensing images have become one of the important means of obtaining information on the earth surface. But in the current research on aircraft target detection and classification in remote sensing images, the imbalanced data samples, large variations in target scales and backgrounds, and target occlusion have led to low average precision and slow detection speed in detection and classification tasks. Therefore, this paper proposes the YOLO-class model. Firstly, the YOLO-Extract model is transferred to optimize the detection of small targets, dense targets, and occluded targets. Secondly, Representative Batch Normalization and Mish activation function are used to optimize the Conv module, and VariFocal loss is used to optimize the classification loss function to improve the accuracy caused by imbalanced data samples. Finally, RepVGG modules are designed in the Backbone to further improve the detection accuracy of the model. Simulation results show that compared with the YOLO-Extract model, YOLO-class improves the detection accuracy from 0.608 to 0.704 and FPS from 36.16 to 39.598.",
      "year": 2023,
      "venue": "IEEE Access",
      "authors": [
        "Zhiguo Liu",
        "Yuan Gao",
        "Qianqian Du"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/cdd4b86f3038a4e4f813a0cc9d10d6b1928ddf8a",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10271344.pdf",
      "publication_date": null,
      "keywords_matched": [
        "extract model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "406d4e8d2df6f6b58e65016fea31004f781d93e7",
      "title": "DisGUIDE: Disagreement-Guided Data-Free Model Extraction",
      "abstract": "Recent model-extraction attacks on Machine Learning as a Service (MLaaS) systems have moved towards data-free approaches, showing the feasibility of stealing models trained with difficult-to-access data. However, these attacks are ineffective or limited due to the low accuracy of extracted models and the high number of queries to the models under attack. The high query cost makes such techniques infeasible for online MLaaS systems that charge per query.\nWe create a novel approach to get higher accuracy and query efficiency than prior data-free model extraction techniques. Specifically, we introduce a novel generator training scheme that maximizes the disagreement loss between two clone models that attempt to copy the model under attack. This loss, combined with diversity loss and experience replay, enables the generator to produce better instances to train the clone models. Our evaluation on popular datasets CIFAR-10 and CIFAR-100 shows that our approach improves the final model accuracy by up to 3.42% and 18.48% respectively. The average number of queries required to achieve the accuracy of the prior state of the art is reduced by up to 64.95%. We hope this will promote future work on feasible data-free model extraction and defenses against such attacks.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jonathan Rosenthal",
        "Eric Enouen",
        "H. Pham",
        "Lin Tan"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/406d4e8d2df6f6b58e65016fea31004f781d93e7",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/26150/25922",
      "publication_date": "2023-06-26",
      "keywords_matched": [
        "model extraction",
        "stealing model",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d2efd249eaffbf3675957444271ca97330156a1",
      "title": "Protecting Regression Models With Personalized Local Differential Privacy",
      "abstract": "The equation-solving model extraction attack is an intuitively simple but devastating attack to steal confidential information of regression models through a sufficient number of queries. Complete mitigation is difficult. Thus, the development of countermeasures is focused on degrading the attack effectiveness as much as possible without losing the model utilities. We investigate a novel personalized local differential privacy mechanism to defend against the attack. We obfuscate the model by adding high-dimensional Gaussian noise on model coefficients. Our solution can adaptively produce the noise to protect the model on the fly. We thoroughly evaluate the performance of our mechanisms using real-world datasets. The experiment shows that the proposed scheme outperforms the existing differential-privacy-enabled solution, i.e., 4 times more queries are required to achieve the same attack result. We also plan to publish the relevant codes to the community for further research.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Xiaoguang Li",
        "Haonan Yan",
        "Zelei Cheng",
        "Wenhai Sun",
        "Hui Li"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/3d2efd249eaffbf3675957444271ca97330156a1",
      "pdf_url": "",
      "publication_date": "2023-03-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
      "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",
      "abstract": "Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences. However, to achieve optimal performance, LLMs often require adaptation with private data, which poses privacy and security challenges. Several techniques have been proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA), Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative privacy and security properties have not been systematically investigated. In this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security). Our results show that there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Rui Wen",
        "Tianhao Wang",
        "Michael Backes",
        "Yang Zhang",
        "Ahmed Salem"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
      "pdf_url": "",
      "publication_date": "2023-10-17",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f84fb561bf1b253e0997d864c3c2ff374190c86d",
      "title": "MirrorNet: A TEE-Friendly Framework for Secure On-Device DNN Inference",
      "abstract": "Deep neural network (DNN) models have become prevalent in edge devices for real-time inference. However, they are vulnerable to model extraction attacks and require protection. Existing defense approaches either fail to fully safeguard model confidentiality or result in significant latency issues. To overcome these challenges, this paper presents MirrorNet, which leverages Trusted Execution Environment (TEE) to enable secure on-device DNN inference. It generates a TEE-friendly implementation for any given DNN model to protect the model confidentiality, while meeting the stringent computation and storage constraints of TEE. The framework consists of two key components: the backbone model (BackboneNet), which is stored in the normal world but achieves lower inference accuracy, and the Companion Partial Monitor (CPM), a lightweight mirrored branch stored in the secure world, preserving model confidentiality. During inference, the CPM monitors the intermediate results from the BackboneNet and rectifies the classification output to achieve higher accuracy. To enhance flexibility, MirrorNet incorporates two modules: the CPM Strategy Generator, which generates various protection strategies, and the Performance Emulator, which estimates the performance of each strategy and selects the most optimal one. Extensive experiments demonstrate the effectiveness of MirrorNet in providing security guarantees while maintaining low computation latency, making MirrorNet a practical and promising solution for secure on-device DNN inference. For the evaluation, MirrorNet can achieve a 18.6% accuracy gap between authenticated and illegal use, while only introducing 0.99% hardware overhead.",
      "year": 2023,
      "venue": "2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)",
      "authors": [
        "Ziyu Liu",
        "Yukui Luo",
        "Shijin Duan",
        "Tong Zhou",
        "Xiaolin Xu"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/f84fb561bf1b253e0997d864c3c2ff374190c86d",
      "pdf_url": "https://arxiv.org/pdf/2311.09489",
      "publication_date": "2023-10-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "45310a683fa761bbaa03ea9969fcf5bc7021624d",
      "title": "SCMA: A Scattering Center Model Attack on CNN-SAR Target Recognition",
      "abstract": "Convolutional neural networks (CNNs) have been widely used in synthetic aperture radar (SAR) target recognition, which can extract feature automatically. However, due to its own structural flaws, CNNs are easy to be fooled by adversarial examples, even if they have excellent performance. In this letter, a novel attack named scattering center model attack (SCMA) is designed, and its generation process does not rely on the prior knowledge of any neural network. Therefore, we can get a stable way which can be applied to any neural network. In addition, an improved scattering center model extraction method, which is the pre-part of SCMA, can filter out the useless noise to optimize the stability of attack. In the experiment, SCMA is compared with advanced attack algorithms. From the experimental results, it is clear to find that SCMA has excellent performance in terms of transfer attack success rate. Furthermore, visualization and interpretability analysis underpin the theoretical feasibility of SCMA.",
      "year": 2023,
      "venue": "IEEE Geoscience and Remote Sensing Letters",
      "authors": [
        "Weibo Qin",
        "Bo Long",
        "Feng Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/45310a683fa761bbaa03ea9969fcf5bc7021624d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "title": "Categorical Inference Poisoning: Verifiable Defense Against Black-Box DNN Model Stealing Without Constraining Surrogate Data and Query Times",
      "abstract": "Deep Neural Network (DNN) models have offered powerful solutions for a wide range of tasks, but the cost to develop such models is nontrivial, which calls for effective model protection. Although black-box distribution can mitigate some threats, model functionality can still be stolen via black-box surrogate attacks. Recent studies have shown that surrogate attacks can be launched in several ways, while the existing defense methods commonly assume attackers with insufficient in-distribution (ID) data and restricted attacking strategies. In this paper, we relax these constraints and assume a practical threat model in which the adversary not only has sufficient ID data and query times but also can adjust the surrogate training data labeled by the victim model. Then, we propose a two-step categorical inference poisoning (CIP) framework, featuring both poisoning for performance degradation (PPD) and poisoning for backdooring (PBD). In the first poisoning step, incoming queries are classified into ID and (out-of-distribution) OOD ones using an energy score (ES) based OOD detector, and the latter are further classified into high ES and low ES ones, which are subsequently passed to a strong and a weak PPD process, respectively. In the second poisoning step, difficult ID queries are detected by a proposed reliability score (RS) measurement and are passed to PBD. In doing so, the first step OOD poisoning leads to substantial performance degradation in surrogate models, the second step ID poisoning further embeds backdoors in them, while both can preserve model fidelity. Extensive experiments confirm that CIP can not only achieve promising performance against state-of-the-art black-box surrogate attacks like KnockoffNets and data-free model extraction (DFME) but also work well against stronger attacks with sufficient ID and deceptive data, better than the existing dynamic adversarial watermarking (DAWN) and deceptive perturbation defense methods. PyTorch code is available at https://github.com/Hatins/CIP_master.git.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Haitian Zhang",
        "Guang Hua",
        "Xinya Wang",
        "Hao Jiang",
        "Wen Yang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c5de53ba42dd0826dcc0844b8b61ef6a4ed944bd",
      "title": "GrOVe: Ownership Verification of Graph Neural Networks using Embeddings",
      "abstract": "Graph neural networks (GNNs) have emerged as a state-of-the-art approach to model and draw inferences from large scale graph-structured data in various application settings such as social networking. The primary goal of a GNN is to learn an embedding for each graph node in a dataset that encodes both the node features and the local graph structure around the node.Prior work has shown that GNNs are prone to model extraction attacks. Model extraction attacks and defenses have been explored extensively in other non-graph settings. While detecting or preventing model extraction appears to be difficult, deterring them via effective ownership verification techniques offer a potential defense. In non-graph settings, fingerprinting models, or the data used to build them, have shown to be a promising approach toward ownership verification.We present GrOVe, a state-of-the-art GNN model fingerprinting scheme that, given a target model and a suspect model, can reliably determine if the suspect model was trained independently of the target model or if it is a surrogate of the target model obtained via model extraction. We show that GrOVe can distinguish between surrogate and independent models even when the independent model uses the same training dataset and architecture as the original target model.Using six benchmark datasets and three model architectures, we show that GrOVe consistently achieves low falsepositive and false-negative rates. We demonstrate that GrOVe is robust against known fingerprint evasion techniques while remaining computationally efficient.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Asim Waheed",
        "Vasisht Duddu",
        "N. Asokan"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/c5de53ba42dd0826dcc0844b8b61ef6a4ed944bd",
      "pdf_url": "https://arxiv.org/pdf/2304.08566",
      "publication_date": "2023-04-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
      "title": "Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks",
      "abstract": "Despite the broad application of Machine Learning models as a Service (MLaaS), they are vulnerable to model stealing attacks. These attacks can replicate the model functionality by using the black-box query process without any prior knowledge of the target victim model. Existing stealing defenses add deceptive perturbations to the victim's posterior probabilities to mislead the attackers. However, these defenses are now suffering problems of high inference computational overheads and unfavorable trade-offs between benign accuracy and stealing robustness, which challenges the feasibility of deployed models in practice. To address the problems, this paper proposes Isolation and Induction (InI), a novel and effective training framework for model stealing defenses. Instead of deploying auxiliary defense modules that introduce redundant inference time, InI directly trains a defensive model by isolating the adversary's training gradient from the expected gradient, which can effectively reduce the inference computational cost. In contrast to adding perturbations over model predictions that harm the benign accuracy, we train models to produce uninformative outputs against stealing queries, which can induce the adversary to extract little useful knowledge from victim models with minimal impact on the benign performance. Extensive experiments on several visual classification datasets (e.g., MNIST and CIFAR10) demonstrate the superior robustness (up to 48% reduction on stealing accuracy) and speed (up to 25.4\u00d7 faster) of our InI over other state-of-the-art methods. Our codes can be found in https://github.com/DIG-Beihang/InI-Model-Stealing-Defense.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Jun Guo",
        "Xingyu Zheng",
        "Aishan Liu",
        "Siyuan Liang",
        "Yisong Xiao",
        "Yichao Wu",
        "Xianglong Liu"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
      "pdf_url": "http://arxiv.org/pdf/2308.00958",
      "publication_date": "2023-08-02",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "28eaa9c2377116327199cb1eb2c9d7b93b948bb4",
      "title": "Detection of Crucial Power Side Channel Data Leakage in Neural Networks",
      "abstract": "Neural network (NN) accelerators are now extensively utilized in a range of applications that need a high degree of security, such as driverless cars, NLP, and image recognition. Due to privacy issues and the high cost, hardware implementations contained within NN Propagators were often not accessible for general populace. Additionally with power and time data, accelerators also disclose critical data by electro-magnetic (EM) sided channels. Within this study, we demonstrate a side-channel information-based attack that can successfully steal models from large-scale NN accelerators deployed on real-world hardware. The use of these accelerators is widespread. The proposed method of attack consists of two distinct phases: 1) Using EM side-channel data to estimate networking's underlying architecture; 2) Using margin-dependent, attackers learning actively in estimating parameters, notably weights. Deducing the underlying network structure from EM sidechannel data. Inferring the underlying network structure from EM sidechannel data. Experimental findings demonstrate that the disclosed attack technique can be used to precisely retrieve the large-scale NN via the use of EM side-channel information leaking. Overall, our attack shows how critical it is to conceal electromagnetic (EM) traces for massive NN accelerators in practical settings.",
      "year": 2023,
      "venue": "International Telecommunication Networks and Applications Conference",
      "authors": [
        "A. A. Ahmed",
        "Mohammad Kamrul Hasan",
        "Nurhizam Safie Mohd Satar",
        "N. Nafi",
        "A. Aman",
        "S. Islam",
        "Saif Aamer Fadhil"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/28eaa9c2377116327199cb1eb2c9d7b93b948bb4",
      "pdf_url": "",
      "publication_date": "2023-11-29",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "14b4aff027ccf8fde0b19ac60b8e653c621aff30",
      "title": "Practical and Efficient Model Extraction of Sentiment Analysis APIs",
      "abstract": "Despite their stunning performance, developing deep learning models from scratch is a formidable task. Therefore, it popularizes Machine-Learning-as-a-Service (MLaaS), where general users can access the trained models of MLaaS providers via Application Programming Interfaces (APIs) on a pay-per-query basis. Unfortunately, the success of MLaaS is under threat from model extraction attacks, where attackers intend to extract a local model of equivalent functionality to the target MLaaS model. However, existing studies on model extraction of text analytics APIs frequently assume adversaries have strong knowledge about the victim model, like its architecture and parameters, which hardly holds in practice. Besides, since the attacker's and the victim's training data can be considerably discrepant, it is non-trivial to perform efficient model extraction. In this paper, to advance the understanding of such attacks, we propose a framework, PEEP, for practical and efficient model extraction of sentiment analysis APIs with only query access. Specifically, PEEP features a learning-based scheme, which employs out-of-domain public corpora and a novel query strategy to construct proxy training data for model extraction. Besides, PEEP introduces a greedy search algorithm to settle an appropriate architecture for the extracted model. We conducted extensive experiments with two victim models across three datasets and two real-life commercial sentiment analysis APIs. Experimental results corroborate that PEEP can consistently outperform the state-of-the-art baselines in terms of effectiveness and efficiency.",
      "year": 2023,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Weibin Wu",
        "Jianping Zhang",
        "Victor Junqiu Wei",
        "Xixian Chen",
        "Zibin Zheng",
        "Irwin King",
        "M. Lyu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/14b4aff027ccf8fde0b19ac60b8e653c621aff30",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4ae98576016b691dfda5a78d0a88d19e8ce15103",
      "title": "Holistic Implicit Factor Evaluation of Model Extraction Attacks",
      "abstract": "Model extraction attacks (MEAs) allow adversaries to replicate a surrogate model analogous to the target model's decision pattern. While several attacks and defenses have been studied in-depth, the underlying reasons behind our susceptibility to them often remain unclear. Analyzing these implication influence factors helps to promote secure deep learning (DL) systems, it requires studying extraction attacks in various scenarios to determine the success of different attacks and the hallmarks of DLs. However, understanding, implementing, and evaluating even a single attack requires extremely high technical effort, making it impractical to study the vast number of unique extraction attack scenarios. To this end, we present a first-of-its-kind holistic evaluation of implication factors for MEAs which relies on the attack process abstracted from state-of-the-art MEAs. Specifically, we concentrate on four perspectives. we consider the impact of the task accuracy, model architecture, and robustness of the target model on MEAs, as well as the impact of the model architecture of the surrogate model on MEAs. Our empirical evaluation includes an ablation study over sixteen model architectures and four image datasets. Surprisingly, our study shows that improving the robustness of the target model via adversarial training is more vulnerable to model extraction attacks.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Anli Yan",
        "Hongyang Yan",
        "Li Hu",
        "Xiaozhang Liu",
        "Teng Huang"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/4ae98576016b691dfda5a78d0a88d19e8ce15103",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c1a2f4520dfd66119a07fd3e0754e4a9aecbc78f",
      "title": "MERCURY: An Automated Remote Side-channel Attack to Nvidia Deep Learning Accelerator",
      "abstract": "DNN accelerators have been widely deployed in many scenarios to speed up the inference process and reduce the energy consumption. One big concern about the usage of the accelerators is the confidentiality of the deployed models: model inference execution on the accelerators could leak side-channel information, which enables an adversary to preciously recover the model details. Such model extraction attacks can not only compromise the intellectual property of DNN models, but also facilitate some adversarial attacks. Although previous works have demonstrated a number of side-channel techniques to extract models from DNN accelerators, they are not practical for two reasons. (1) They only target simplified accelerator implementations, which have limited practicality in the real world. (2) They require heavy human analysis and domain knowledge. To overcome these limitations, this paper presents MERCURY, the first automated remote side-channel attack against the off-the-shelf Nvidia DNN accelerator. The key insight of MERCURY is to model the side-channel extraction process as a sequence-to-sequence problem. The adversary can leverage a time-to-digital converter (TDC) to remotely collect the power trace of the target model\u2019s inference. Then he uses a learning model to automatically recover the architecture details of the victim model from the power trace without any prior knowledge. The adversary can further use the attention mechanism to localize the leakage points that contribute most to the attack. Evaluation results indicate that MERCURY can keep the error rate of model extraction below 1%.",
      "year": 2023,
      "venue": "International Conference on Field-Programmable Technology",
      "authors": [
        "Xi-ai Yan",
        "Xiaoxuan Lou",
        "Guowen Xu",
        "Han Qiu",
        "Shangwei Guo",
        "Chip-Hong Chang",
        "Tianwei Zhang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/c1a2f4520dfd66119a07fd3e0754e4a9aecbc78f",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/171839/2/_DR_NTU_An_Automated_Remote_Side_channel_Attack_to_FPGA_based_DNN_Accelerators.pdf",
      "publication_date": "2023-08-02",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "148adb6df70218017aba770047cacb3c9e745411",
      "title": "A Desynchronization-Based Countermeasure Against Side-Channel Analysis of Neural Networks",
      "abstract": "Model extraction attacks have been widely applied, which can normally be used to recover confidential parameters of neural networks for multiple layers. Recently, side-channel analysis of neural networks allows parameter extraction even for networks with several multiple deep layers with high effectiveness. It is therefore of interest to implement a certain level of protection against these attacks. In this paper, we propose a desynchronization-based countermeasure that makes the timing analysis of activation functions harder. We analyze the timing properties of several activation functions and design the desynchronization in a way that the dependency on the input and the activation type is hidden. We experimentally verify the effectiveness of the countermeasure on a 32-bit ARM Cortex-M4 microcontroller and employ a t-test to show the side-channel information leakage. The overhead ultimately depends on the number of neurons in the fully-connected layer, for example, in the case of 4096 neurons in VGG-19, the overheads are between 2.8% and 11%.",
      "year": 2023,
      "venue": "International Conference on Cyber Security Cryptography and Machine Learning",
      "authors": [
        "J. Breier",
        "Dirmanto Jap",
        "Xiaolu Hou",
        "S. Bhasin"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/148adb6df70218017aba770047cacb3c9e745411",
      "pdf_url": "http://arxiv.org/pdf/2303.18132",
      "publication_date": "2023-03-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "163514e0787a540b8c7983424c5ca8ae8e3f6f9d",
      "title": "MEDIC: Remove Model Backdoors via Importance Driven Cloning",
      "abstract": "We develop a novel method to remove injected backdoors in deep learning models. It works by cloning the benign behaviors of a trojaned model to a new model of the same structure. It trains the clone model from scratch on a very small subset of samples and aims to minimize a cloning loss that denotes the differences between the activations of important neurons across the two models. The set of important neurons varies for each input, depending on their magnitude of activations and their impact on the classification result. We theoretically show our method can better recover benign functions of the backdoor model. Meanwhile, we prove our method can be more effective in removing back-doors compared with fine-tuning. Our experiments show that our technique can effectively remove nine different types of backdoors with minor benign accuracy degradation, outper-forming the state-of-the-art backdoor removal techniques that are based on fine-tuning, knowledge distillation, and neuron pruning.1",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Qiuling Xu",
        "Guanhong Tao",
        "J. Honorio",
        "Yingqi Liu",
        "Guangyu Shen",
        "Siyuan Cheng",
        "Xiangyu Zhang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/163514e0787a540b8c7983424c5ca8ae8e3f6f9d",
      "pdf_url": "",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8ffd3bb7b0b26b33fd6f317052214e6e84dec291",
      "title": "Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders",
      "abstract": "Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose Bucks for Buckets (B4B), the first active defense that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task.vB4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space. To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations. This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy. Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jan Dubi'nski",
        "S. Pawlak",
        "Franziska Boenisch",
        "Tomasz Trzci'nski",
        "Adam Dziedzic"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/8ffd3bb7b0b26b33fd6f317052214e6e84dec291",
      "pdf_url": "https://arxiv.org/pdf/2310.08571",
      "publication_date": "2023-10-12",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b885e291f660df34d9f22777dc0678bdf8e0860d",
      "title": "Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data",
      "abstract": "We study design of black-box model extraction attacks that can send minimal number of queries from a publicly available dataset to a target ML model through a predictive API with an aim to create an informative and distributionally equivalent replica of the target. First, we define distributionally equivalent and Max-Information model extraction attacks, and reduce them into a variational optimisation problem. The attacker sequentially solves this optimisation problem to select the most informative queries that simultaneously maximise the entropy and reduce the mismatch between the target and the stolen models. This leads to an active sampling-based query selection algorithm, Marich, which is model-oblivious. Then, we evaluate Marich on different text and image data sets, and different models, including CNNs and BERT. Marich extracts models that achieve $\\sim 60-95\\%$ of true model's accuracy and uses $\\sim 1,000 - 8,500$ queries from the publicly available datasets, which are different from the private training datasets. Models extracted by Marich yield prediction distributions, which are $\\sim 2-4\\times$ closer to the target's distribution in comparison to the existing active sampling-based attacks. The extracted models also lead to $84-96\\%$ accuracy under membership inference attacks. Experimental results validate that Marich is query-efficient, and capable of performing task-accurate, high-fidelity, and informative model extraction.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Pratik Karmakar",
        "D. Basu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/b885e291f660df34d9f22777dc0678bdf8e0860d",
      "pdf_url": "http://arxiv.org/pdf/2302.08466",
      "publication_date": "2023-02-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3c103408ff825aad19d715edc01025a8c3fccdb4",
      "title": "EZClone: Improving DNN Model Extraction Attack via Shape Distillation from GPU Execution Profiles",
      "abstract": "Deep Neural Networks (DNNs) have become ubiquitous due to their performance on prediction and classification problems. However, they face a variety of threats as their usage spreads. Model extraction attacks, which steal DNNs, endanger intellectual property, data privacy, and security. Previous research has shown that system-level side-channels can be used to leak the architecture of a victim DNN, exacerbating these risks. We propose two DNN architecture extraction techniques catering to various threat models. The first technique uses a malicious, dynamically linked version of PyTorch to expose a victim DNN architecture through the PyTorch profiler. The second, called EZClone, exploits aggregate (rather than time-series) GPU profiles as a side-channel to predict DNN architecture, employing a simple approach and assuming little adversary capability as compared to previous work. We investigate the effectiveness of EZClone when minimizing the complexity of the attack, when applied to pruned models, and when applied across GPUs. We find that EZClone correctly predicts DNN architectures for the entire set of PyTorch vision architectures with 100% accuracy. No other work has shown this degree of architecture prediction accuracy with the same adversarial constraints or using aggregate side-channel information. Prior work has shown that, once a DNN has been successfully cloned, further attacks such as model evasion or model inversion can be accelerated significantly.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jonah O'Brien Weiss",
        "Tiago A. O. Alves",
        "S. Kundu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/3c103408ff825aad19d715edc01025a8c3fccdb4",
      "pdf_url": "http://arxiv.org/pdf/2304.03388",
      "publication_date": "2023-04-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bfe8cd89a70c1780b431457be9b5dfee3dacee14",
      "title": "Cryogenic Characterization and Model Extraction of 5nm Technology Node FinFETs",
      "abstract": "We present cryogenic characterization and compact model extraction of commercially fabricated 5nm technology FinFETs. A modified industry-standard BSIM-CMG model is used to accurately model band-tail, mobility, and velocity saturation effects up to 10K. At 10K, n-FinFET and p-FinFET show 87mV and 92mV threshold voltage shift and sub-threshold slopes of 12.7 and 16.7mV/decade (83% and 78% improvement), respectively. The simulated inverter and ring oscillator at 10K in iso IOFF condition show 38% and 36.53% delay improvement for VDD = 0.75V, respectively. At VDD = 0.35V, inverter simulations show \u223d 70% improvement in delay and Power-Delay-Product. Static leakage and power dissipation are major challenges in FinFETs; the above-mentioned performance enhancements highlight the potential of characterized technology in quantum computers.",
      "year": 2023,
      "venue": "IEEE Electron Devices Technology and Manufacturing Conference",
      "authors": [
        "S. S. Parihar",
        "G. Pahwa",
        "Jun Z. Huang",
        "Weike Wang",
        "K. Imura",
        "Chenming Hu",
        "Y. Chauhan"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/bfe8cd89a70c1780b431457be9b5dfee3dacee14",
      "pdf_url": "",
      "publication_date": "2023-03-07",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "13f08d0ed26a48bb4fa16951e2dbbd87d0ba4797",
      "title": "Defense Against Model Extraction Attacks on Recommender Systems",
      "abstract": "The robustness of recommender systems has become a prominent topic within the research community. Numerous adversarial attacks have been proposed, but most of them rely on extensive prior knowledge, such as all the white-box attacks or most of the black-box attacks which assume that certain external knowledge is available. Among these attacks, the model extraction attack stands out as a promising and practical method, involving training a surrogate model by repeatedly querying the target model. However, there is a significant gap in the existing literature when it comes to defending against model extraction attacks on recommender systems. In this paper, we introduce Gradient-based Ranking Optimization (GRO), which is the first defense strategy designed to counter such attacks. We formalize the defense as an optimization problem, aiming to minimize the loss of the protected target model while maximizing the loss of the attacker's surrogate model. Since top-k ranking lists are non-differentiable, we transform them into swap matrices which are instead differentiable. These swap matrices serve as input to a student model that emulates the surrogate model's behavior. By back-propagating the loss of the student model, we obtain gradients for the swap matrices. These gradients are used to compute a swap loss, which maximizes the loss of the student model. We conducted experiments on three benchmark datasets to evaluate the performance of GRO, and the results demonstrate its superior effectiveness in defending against model extraction attacks.",
      "year": 2023,
      "venue": "Web Search and Data Mining",
      "authors": [
        "Sixiao Zhang",
        "Hongzhi Yin",
        "Hongxu Chen",
        "Cheng Long"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/13f08d0ed26a48bb4fa16951e2dbbd87d0ba4797",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3616855.3635751",
      "publication_date": "2023-10-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "468a78d431be0d6290bb3007e6920e15761b751e",
      "title": "SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models",
      "abstract": "While advanced machine learning (ML) models are deployed in numerous real-world applications, previous works demonstrate these models have security and privacy vulnerabilities. Various empirical research has been done in this field. However, most of the experiments are performed on target ML models trained by the security researchers themselves. Due to the high computational resource requirement for training advanced models with complex architectures, researchers generally choose to train a few target models using relatively simple architectures on typical experiment datasets. We argue that to understand ML models' vulnerabilities comprehensively, experiments should be performed on a large set of models trained with various purposes (not just the purpose of evaluating ML attacks and defenses). To this end, we propose using publicly available models with weights from the Internet (public models) for evaluating attacks and defenses on ML models. We establish a database, namely SecurityNet, containing 910 annotated image classification models. We then analyze the effectiveness of several representative attacks/defenses, including model stealing attacks, membership inference attacks, and backdoor detection on these public models. Our evaluation empirically shows the performance of these attacks/defenses can vary significantly on public models compared to self-trained models. We share SecurityNet with the research community. and advocate researchers to perform experiments on public models to better demonstrate their proposed methods' effectiveness in the future.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Boyang Zhang",
        "Zheng Li",
        "Ziqing Yang",
        "Xinlei He",
        "Michael Backes",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/468a78d431be0d6290bb3007e6920e15761b751e",
      "pdf_url": "",
      "publication_date": "2023-10-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7b6db013d28e72374f301f758f432545a92b22fb",
      "title": "DivTheft: An Ensemble Model Stealing Attack by Divide-and-Conquer",
      "abstract": "Recently, model stealing attacks are widely studied but most of them are focused on stealing a single non-discrete model, e.g., neural networks. For ensemble models, these attacks are either non-executable or suffer from intolerant performance degradation due to the complex model structure (multiple sub-models) and the discreteness possessed by the sub-model (e.g., decision trees). To overcome the bottleneck, this paper proposes a divide-and-conquer strategy called DivTheft to formulate the model stealing attack to common ensemble models by combining active learning (AL). Specifically, based on the boosting learning concept, we divide a hard ensemble model stealing task into multiple simpler ones about single sub-model stealing. Then, we adopt AL to conquer the data-free sub-model stealing task. During the process, the current AL algorithm easily causes the stolen model to be biased because of ignoring the past useful memories. Thus, DivTheft involves a newly designed uncertainty sampling scheme to filter reusable samples from the previously used ones. Experiments show that compared with the prior work, DivTheft can save almost 50% queries while ensuring a competitive agreement rate to the victim model.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Zhuo Ma",
        "Xinjing Liu",
        "Yang Liu",
        "Ximeng Liu",
        "Zhan Qin",
        "Kui Ren"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/7b6db013d28e72374f301f758f432545a92b22fb",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "aa4acbad4d6a3a8195d70c174564df752a5e1ba5",
      "title": "MeaeQ: Mount Model Extraction Attacks with Efficient Queries",
      "abstract": "We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based sampling strategies on publicly available, unannotated data sources. However, these methods often result in selected queries that lack task relevance and data diversity, leading to limited success in achieving satisfactory results with low query costs. In this paper, we propose MeaeQ (Model extraction attack with efficient Queries), a straightforward yet effective method to address these issues. Specifically, we initially utilize a zero-shot sequence inference classifier, combined with API service information, to filter task-relevant data from a public text corpus instead of a problem domain-specific dataset. Furthermore, we employ a clustering-based data reduction technique to obtain representative data as queries for the attack. Extensive experiments conducted on four benchmark datasets demonstrate that MeaeQ achieves higher functional similarity to the victim model than baselines while requiring fewer queries. Our code is available at https://github.com/C-W-D/MeaeQ.",
      "year": 2023,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Chengwei Dai",
        "Minxuan Lv",
        "Kun Li",
        "Wei Zhou"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/aa4acbad4d6a3a8195d70c174564df752a5e1ba5",
      "pdf_url": "",
      "publication_date": "2023-10-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "552d90f3ccc2879a17eb6e8f9c13f9937f6a6734",
      "title": "Model Extraction Attacks on Split Federated Learning",
      "abstract": "Federated Learning (FL) is a popular collaborative learning scheme involving multiple clients and a server. FL focuses on protecting clients' data but turns out to be highly vulnerable to Intellectual Property (IP) threats. Since FL periodically collects and distributes the model parameters, a free-rider can download the latest model and thus steal model IP. Split Federated Learning (SFL), a recent variant of FL that supports training with resource-constrained clients, splits the model into two, giving one part of the model to clients (client-side model), and the remaining part to the server (server-side model). Thus SFL prevents model leakage by design. Moreover, by blocking prediction queries, it can be made resistant to advanced IP threats such as traditional Model Extraction (ME) attacks. While SFL is better than FL in terms of providing IP protection, it is still vulnerable. In this paper, we expose the vulnerability of SFL and show how malicious clients can launch ME attacks by querying the gradient information from the server side. We propose five variants of ME attack which differs in the gradient usage as well as in the data assumptions. We show that under practical cases, the proposed ME attacks work exceptionally well for SFL. For instance, when the server-side model has five layers, our proposed ME attack can achieve over 90% accuracy with less than 2% accuracy degradation with VGG-11 on CIFAR-10.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jingtao Li",
        "A. S. Rakin",
        "Xing Chen",
        "Li Yang",
        "Zhezhi He",
        "Deliang Fan",
        "C. Chakrabarti"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/552d90f3ccc2879a17eb6e8f9c13f9937f6a6734",
      "pdf_url": "http://arxiv.org/pdf/2303.08581",
      "publication_date": "2023-03-13",
      "keywords_matched": [
        "model extraction",
        "steal model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "18918a72fda197ea02671a13c49a95d6b95fc0f3",
      "title": "AUTOLYCUS: Exploiting Explainable Artificial Intelligence (XAI) for Model Extraction Attacks against Interpretable Models",
      "abstract": "Explainable Artificial Intelligence (XAI) aims to uncover the decision-making processes of AI models. However, the data used for such explanations can pose security and privacy risks. Existing literature identifies attacks on machine learning models, including membership inference, model inversion, and model extraction attacks. These attacks target either the model or the training data, depending on the settings and parties involved. XAI tools can increase the vulnerability of model extraction attacks, which is a concern when model owners prefer black-box access, thereby keeping model parameters and architecture private. To exploit this risk, we propose AUTOLYCUS, a novel retraining (learning) based model extraction attack framework against interpretable models under black-box settings. As XAI tools, we exploit Local Interpretable Model-Agnostic Explanations (LIME) and Shapley values (SHAP) to infer decision boundaries and create surrogate models that replicate the functionality of the target model. LIME and SHAP are mainly chosen for their realistic yet information-rich explanations, coupled with their extensive adoption, simplicity, and usability. We evaluate AUTOLYCUS on six machine learning datasets, measuring the accuracy and similarity of the surrogate model to the target model. The results show that AUTOLYCUS is highly effective, requiring significantly fewer queries compared to state-of-the-art attacks, while maintaining comparable accuracy and similarity. We validate its performance and transferability on multiple interpretable ML models, including decision trees, logistic regression, naive bayes, and k-nearest neighbor. Additionally, we show the resilience of AUTOLYCUS against proposed countermeasures.",
      "year": 2023,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Abdullah \u00c7aglar \u00d6ks\u00fcz",
        "Anisa Halimi",
        "Erman Ayday"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/18918a72fda197ea02671a13c49a95d6b95fc0f3",
      "pdf_url": "",
      "publication_date": "2023-02-04",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2b805ba343b54c03ffc637cf9d80a65d68d7ddbd",
      "title": "Deep-Learning Model Extraction Through Software-Based Power Side-Channel",
      "abstract": "Deep learning (DL) techniques have been increasingly applied across various applications, facing a growing number of security threats. One such threat is model extraction, an attack that steals the Intellectual Property of DL models, either by recovering the same functionality or retrieving high-fidelity models. Current model extraction methods can be categorized as learning-based or cryptanalytic, with the latter relying on model queries and computational methods to recover parameters. However, these are limited to shallow neural networks and are computationally prohibitive for deeper DL models. In this paper, we propose leveraging software-based power analysis, specifically the Intel Running Average Power Limit (RAPL) technique, for DL model extraction. RAPL allows us to measure power leakage of the most popular activation function, ReLU, through a software interface. Consequently, the ReLU branch direction can be leaked in the software power side-channel, a vulnerability common in many state-of-the-art DL frameworks. We introduce a novel methodology for model extraction Algorithm from input gradient assisted by side channel information. We implement our attack on the oneDNN framework, the most popular library on Intel processors. Compared to prior work, our model extraction, assisted by the software power side-channel, only requires 0.8% of the queries to retrieve as-layer MLP. We also successfully apply our method to a common Convolutional Neural Network (CNN) - Lenet-5. To the best of our knowledge, this is the first work that extracts CNN models with more than 5 layers based solely on queries and software.",
      "year": 2023,
      "venue": "2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)",
      "authors": [
        "Xiang Zhang",
        "A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/2b805ba343b54c03ffc637cf9d80a65d68d7ddbd",
      "pdf_url": "",
      "publication_date": "2023-10-28",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "23faead2015ba1e36a2c0c535f987c0b36ba8534",
      "title": "Data-Free Hard-Label Robustness Stealing Attack",
      "abstract": "The popularity of Machine Learning as a Service (MLaaS) has led to increased concerns about Model Stealing Attacks (MSA), which aim to craft a clone model by querying MLaaS. Currently, most research on MSA assumes that MLaaS can provide soft labels and that the attacker has a proxy dataset with a similar distribution. However, this fails to encapsulate the more practical scenario where only hard labels are returned by MLaaS and the data distribution remains elusive. Furthermore, most existing work focuses solely on stealing the model accuracy, neglecting the model robustness, while robustness is essential in security-sensitive scenarios, e.g, face-scan payment. Notably, improving model robustness often necessitates the use of expensive techniques such as adversarial training, thereby further making stealing robustness a more lucrative prospect. In response to these identified gaps, we introduce a novel Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which enables the stealing of both model accuracy and robustness by simply querying hard labels of the target model without the help of any natural data. Comprehensive experiments demonstrate the effectiveness of our method. The clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51% against AutoAttack, which are only 4.71% and 8.40% lower than the target model on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is available at: https://github.com/LetheSec/DFHL-RS-Attack.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Xiaojian \\ Yuan",
        "Kejiang Chen",
        "Wen Huang",
        "Jie Zhang",
        "Weiming Zhang",
        "Neng H. Yu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/23faead2015ba1e36a2c0c535f987c0b36ba8534",
      "pdf_url": "",
      "publication_date": "2023-12-10",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "eca3d9bca53842a53b65594762397e583901c437",
      "title": "Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models",
      "abstract": "Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of the most significant bits with about 1500 crafted inputs. These information enable to efficiently train a substitute model, with only 8% of the training dataset, that reaches high fidelity and near identical accuracy level than the victim model.",
      "year": 2023,
      "venue": "ESORICS Workshops",
      "authors": [
        "Kevin Hector",
        "Pierre-Alain Mo\u00ebllic",
        "Mathieu Dumont",
        "J. Dutertre"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eca3d9bca53842a53b65594762397e583901c437",
      "pdf_url": "https://arxiv.org/pdf/2308.16703",
      "publication_date": "2023-08-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "eacb66ac30b489f704dedae7abc6e98429f95c88",
      "title": "Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems",
      "abstract": "As Artificial Intelligence (AI) systems increasingly underpin critical applications, from autonomous vehicles to biometric authentication, their vulnerability to transferable attacks presents a growing concern. These attacks, designed to generalize across instances, domains, models, tasks, modalities, or even hardware platforms, pose severe risks to security, privacy, and system integrity. This survey delivers the first comprehensive review of transferable attacks across seven major categories, including evasion, backdoor, data poisoning, model stealing, model inversion, membership inference, and side-channel attacks. We introduce a unified six-dimensional taxonomy: cross-instance, cross-domain, cross-modality, cross-model, cross-task, and cross-hardware, which systematically captures the diverse transfer pathways of adversarial strategies. Through this framework, we examine both the underlying mechanics and practical implications of transferable attacks on AI systems. Furthermore, we review cutting-edge methods for enhancing attack transferability, organized around data augmentation and optimization strategies. By consolidating fragmented research and identifying critical future directions, this work provides a foundational roadmap for understanding, evaluating, and defending against transferable threats in real-world AI systems.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Guangjing Wang",
        "Ce Zhou",
        "Yuanda Wang",
        "Bocheng Chen",
        "Hanqing Guo",
        "Qiben Yan"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eacb66ac30b489f704dedae7abc6e98429f95c88",
      "pdf_url": "",
      "publication_date": "2023-11-20",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3e4f89f6698ed4b6e5691b896c180315ab7d1d41",
      "title": "Extracting Cloud-based Model with Prior Knowledge",
      "abstract": "Machine Learning-as-a-Service, a pay-as-you-go business pattern, is widely accepted by third-party users and developers. However, the open inference APIs may be utilized by malicious customers to conduct model extraction attacks, i.e., attackers can replicate a cloud-based black-box model merely via querying malicious examples. Existing model extraction attacks mainly depend on the posterior knowledge (i.e., predictions of query samples) from Oracle. Thus, they either require high query overhead to simulate the decision boundary, or suffer from generalization errors and overfitting problems due to query budget limitations. To mitigate it, this work proposes an efficient model extraction attack based on prior knowledge for the first time. The insight is that prior knowledge of unlabeled proxy datasets is conducive to the search for the decision boundary (e.g., informative samples). Specifically, we leverage self-supervised learning including autoencoder and contrastive learning to pre-compile the prior knowledge of the proxy dataset into the feature extractor of the substitute model. Then we adopt entropy to measure and sample the most informative examples to query the target model. Our design leverages both prior and posterior knowledge to extract the model and thus eliminates generalizability errors and overfitting problems. We conduct extensive experiments on open APIs like Traffic Recognition, Flower Recognition, Moderation Recognition, and NSFW Recognition from real-world platforms, Azure and Clarifai. The experimental results demonstrate the effectiveness and efficiency of our attack. For example, our attack achieves 95.1% fidelity with merely 1.8K queries (cost 2.16$) on the NSFW Recognition API. Also, the adversarial examples generated with our substitute model have better transferability than others, which reveals that our scheme is more conducive to downstream attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "S. Zhao",
        "Kangjie Chen",
        "Meng Hao",
        "Jian Zhang",
        "Guowen Xu",
        "Hongwei Li",
        "Tianwei Zhang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/3e4f89f6698ed4b6e5691b896c180315ab7d1d41",
      "pdf_url": "http://arxiv.org/pdf/2306.04192",
      "publication_date": "2023-06-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a85c53617666fe896e1c5834d21dcb30c5de1b8b",
      "title": "The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning",
      "abstract": "Due to the proliferation of malware, defenders are increasingly turning to automation and machine learning as part of the malware detection tool-chain. However, machine learning models are susceptible to adversarial attacks, requiring the testing of model and product robustness. Meanwhile, attackers also seek to automate malware generation and evasion of antivirus systems, and defenders try to gain insight into their methods. This work proposes a new algorithm that combines Malware Evasion and Model Extraction (MEME) attacks. MEME uses model-based reinforcement learning to adversarially modify Windows executable binary samples while simultaneously training a surrogate model with a high agreement with the target model to evade. To evaluate this method, we compare it with two state-of-the-art attacks in adversarial malware creation, using three well-known published models and one antivirus product as targets. Results show that MEME outperforms the state-of-the-art methods in terms of evasion capabilities in almost all cases, producing evasive malware with an evasion rate in the range of 32-73%. It also produces surrogate models with a prediction label agreement with the respective target models between 97-99%. The surrogate could be used to fine-tune and improve the evasion rate in the future.",
      "year": 2023,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "M. Rigaki",
        "S. Garc\u00eda"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/a85c53617666fe896e1c5834d21dcb30c5de1b8b",
      "pdf_url": "",
      "publication_date": "2023-08-31",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a0a7b1aabe2f14696b15bac592b0ad5743ef0b85",
      "title": "Data-Free Model Extraction Attacks in the Context of Object Detection",
      "abstract": "A significant number of machine learning models are vulnerable to model extraction attacks, which focus on stealing the models by using specially curated queries against the target model. This task is well accomplished by using part of the training data or a surrogate dataset to train a new model that mimics a target model in a white-box environment. In pragmatic situations, however, the target models are trained on private datasets that are inaccessible to the adversary. The data-free model extraction technique replaces this problem when it comes to using queries artificially curated by a generator similar to that used in Generative Adversarial Nets. We propose for the first time, to the best of our knowledge, an adversary black box attack extending to a regression problem for predicting bounding box coordinates in object detection. As part of our study, we found that defining a loss function and using a novel generator setup is one of the key aspects in extracting the target model. We find that the proposed model extraction method achieves significant results by using reasonable queries. The discovery of this object detection vulnerability will support future prospects for securing such models.",
      "year": 2023,
      "venue": "International Conference on Virtual Storytelling",
      "authors": [
        "Harshit Shah",
        "G. Aravindhan",
        "Pavan Kulkarni",
        "Yuvaraj Govidarajulu",
        "Manojkumar Somabhai Parmar"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a0a7b1aabe2f14696b15bac592b0ad5743ef0b85",
      "pdf_url": "https://arxiv.org/pdf/2308.05127",
      "publication_date": "2023-08-09",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8504a0c28bd68d820ba6e4e2102ee3e7ebf57df0",
      "title": "Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers",
      "abstract": "Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction, the complexity of the attack is relatively low and we highlight the urgent need for practicable protections that could fit the strong memory and latency requirements of such platforms.",
      "year": 2023,
      "venue": "Smart Card Research and Advanced Application Conference",
      "authors": [
        "Raphael Joud",
        "Pierre-Alain Mo\u00ebllic",
        "S. Ponti\u00e9",
        "J. Rigaud"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/8504a0c28bd68d820ba6e4e2102ee3e7ebf57df0",
      "pdf_url": "",
      "publication_date": "2023-11-02",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bbd454a77f507fb292b607c6bacc941f52474009",
      "title": "Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps",
      "abstract": "The increasing popularity of deep learning (DL) models and the advantages of computing, including low latency and bandwidth savings on smartphones, have led to the emergence of intelligent mobile applications, also known as DL apps, in recent years. However, this technological development has also given rise to several security concerns, including adversarial examples, model stealing, and data poisoning issues. Existing works on attacks and countermeasures for on-device DL models have primarily focused on the models themselves. However, scant attention has been paid to the impact of data processing disturbance on the model inference. This knowledge disparity highlights the need for additional research to fully comprehend and address security issues related to data processing for on-device models. In this paper, we introduce a data processing-based attacks against real-world DL apps. In particular, our attack could influence the performance and latency of the model without affecting the operation of a DL app. To demonstrate the effectiveness of our attack, we carry out an empirical study on 517 real-world DL apps collected from Google Play. Among 320 apps utilizing MLkit, we find that 81.56% of them can be successfully attacked. The results emphasize the importance of DL app developers being aware of and taking actions to secure on-device models from the perspective of data processing.",
      "year": 2023,
      "venue": "SecTL@AsiaCCS",
      "authors": [
        "Ye Sang",
        "Yujin Huang",
        "Shuo Huang",
        "Helei Cui"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/bbd454a77f507fb292b607c6bacc941f52474009",
      "pdf_url": "https://arxiv.org/pdf/2305.03963",
      "publication_date": "2023-05-06",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2b8d3211b4b5f636a5e2719d404b278cea80d8e1",
      "title": "FDINet: Protecting Against DNN Model Extraction Using Feature Distortion Index",
      "abstract": "Machine Learning as a Service (MLaaS) platforms have gained popularity due to their accessibility, cost-efficiency, scalability, and rapid development capabilities. However, recent research has highlighted the vulnerability of cloud-based models in MLaaS to model extraction attacks. In this paper, we introduce FDINet, a novel defense mechanism that leverages the feature distribution of deep neural network (DNN) models. Concretely, by analyzing the feature distribution from the adversary\u2019s queries, we reveal that the feature distribution of these queries deviates from that of the model\u2019s problem domain. Based on this key observation, we propose Feature Distortion Index (FDI), a metric designed to quantitatively measure the feature distribution deviation of received queries. The proposed FDINet utilizes FDI to train a binary detector and exploits FDI similarity to identify colluding adversaries from distributed extraction attacks. We conduct extensive experiments to evaluate FDINet against six state-of-the-art extraction attacks on four benchmark datasets and four popular model architectures. Empirical results demonstrate the following findings: 1) FDINet proves to be highly effective in detecting model extraction, achieving a 100% detection accuracy on DFME and DaST. 2) FDINet is highly efficient, using just 50 queries to raise an extraction alarm with an average confidence of 96.08% for GTSRB. 3) FDINet exhibits the capability to identify colluding adversaries with an accuracy exceeding 91%. Additionally, it demonstrates the ability to detect two types of adaptive attacks.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hongwei Yao",
        "Zheng Li",
        "Haiqin Weng",
        "Feng Xue",
        "Kui Ren",
        "Zhan Qin"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2b8d3211b4b5f636a5e2719d404b278cea80d8e1",
      "pdf_url": "",
      "publication_date": "2023-06-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "761cb683cf1bf94b878f6d7527600c2c62aee796",
      "title": "SAME: Sample Reconstruction against Model Extraction Attacks",
      "abstract": "While deep learning models have shown significant performance across various domains, their deployment needs extensive resources and advanced computing infrastructure. As a solution, Machine Learning as a Service (MLaaS) has emerged, lowering the barriers for users to release or productize their deep learning models. However, previous studies have highlighted potential privacy and security concerns associated with MLaaS, and one primary threat is model extraction attacks. To address this, there are many defense solutions but they suffer from unrealistic assumptions and generalization issues, making them less practical for reliable protection. Driven by these limitations, we introduce a novel defense mechanism, SAME, based on the concept of sample reconstruction. This strategy imposes minimal prerequisites on the defender's capabilities, eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user query history, white-box model access, and additional intervention during model training. It is compatible with existing active defense methods. Our extensive experiments corroborate the superior efficacy of SAME over state-of-the-art solutions. Our code is available at https://github.com/xythink/SAME.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yi Xie",
        "Jie Zhang",
        "Shiqian Zhao",
        "Tianwei Zhang",
        "Xiaofeng Chen"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/761cb683cf1bf94b878f6d7527600c2c62aee796",
      "pdf_url": "",
      "publication_date": "2023-12-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2c3f82769341bec4cfbbda760e42dbe9b16780bc",
      "title": "ShrewdAttack: Low Cost High Accuracy Model Extraction",
      "abstract": "Machine learning as a service (MLaaS) plays an essential role in the current ecosystem. Enterprises do not need to train models by themselves separately. Instead, they can use well-trained models provided by MLaaS to support business activities. However, such an ecosystem could be threatened by model extraction attacks\u2014an attacker steals the functionality of a trained model provided by MLaaS and builds a substitute model locally. In this paper, we proposed a model extraction method with low query costs and high accuracy. In particular, we use pre-trained models and task-relevant data to decrease the size of query data. We use instance selection to reduce query samples. In addition, we divided query data into two categories, namely low-confidence data and high-confidence data, to reduce the budget and improve accuracy. We then conducted attacks on two models provided by Microsoft Azure as our experiments. The results show that our scheme achieves high accuracy at low cost, with the substitution models achieving 96.10% and 95.24% substitution while querying only 7.32% and 5.30% of their training data on the two models, respectively. This new attack approach creates additional security challenges for models deployed on cloud platforms. It raises the need for novel mitigation strategies to secure the models. In future work, generative adversarial networks and model inversion attacks can be used to generate more diverse data to be applied to the attacks.",
      "year": 2023,
      "venue": "Entropy",
      "authors": [
        "Yang Liu",
        "Ji Luo",
        "Yi Yang",
        "Xuan Wang",
        "M. Gheisari",
        "Feng Luo"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2c3f82769341bec4cfbbda760e42dbe9b16780bc",
      "pdf_url": "https://www.mdpi.com/1099-4300/25/2/282/pdf?version=1675337976",
      "publication_date": "2023-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c2e1a7c575e0a10c5493b5390b4c6ce321c6cf8d",
      "title": "Defense against ML-based Power Side-channel Attacks on DNN Accelerators with Adversarial Attacks",
      "abstract": "Artificial Intelligence (AI) hardware accelerators have been widely adopted to enhance the efficiency of deep learning applications. However, they also raise security concerns regarding their vulnerability to power side-channel attacks (SCA). In these attacks, the adversary exploits unintended communication channels to infer sensitive information processed by the accelerator, posing significant privacy and copyright risks to the models. Advanced machine learning algorithms are further employed to facilitate the side-channel analysis and exacerbate the privacy issue of AI accelerators. Traditional defense strategies naively inject execution noise to the runtime of AI models, which inevitably introduce large overheads. In this paper, we present AIAShield, a novel defense methodology to safeguard FPGA-based AI accelerators and mitigate model extraction threats via power-based SCAs. The key insight of AIAShield is to leverage the prominent adversarial attack technique from the machine learning community to craft delicate noise, which can significantly obfuscate the adversary's side-channel observation while incurring minimal overhead to the execution of the protected model. At the hardware level, we design a new module based on ring oscillators to achieve fine-grained noise generation. At the algorithm level, we repurpose Neural Architecture Search to worsen the adversary's extraction results. Extensive experiments on the Nvidia Deep Learning Accelerator (NVDLA) demonstrate that AIAShield outperforms existing solutions with excellent transferability.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Xiaobei Yan",
        "Chip Hong Chang",
        "Tianwei Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/c2e1a7c575e0a10c5493b5390b4c6ce321c6cf8d",
      "pdf_url": "",
      "publication_date": "2023-12-07",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2fd52a3544dc0e3a480d12af01bb978c4d1a59fc",
      "title": "A Taxonomic Survey of Model Extraction Attacks",
      "abstract": "A model extraction attack aims to clone a machine learning target model deployed in the cloud solely by querying the target in a black-box manner. Once a clone is obtained it is possible to launch further attacks with the aid of the local model. In this survey, we analyze existing approaches and present a taxonomic overview of this field based on several important aspects that affect attack efficiency and performance. We present both early works and recently explored directions. We conclude with an analysis of future directions based on recent developments in machine learning methodology.",
      "year": 2023,
      "venue": "Computer Science Symposium in Russia",
      "authors": [
        "Didem Gen\u00e7",
        "Mustafa \u00d6zuysal",
        "E. Tomur"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2fd52a3544dc0e3a480d12af01bb978c4d1a59fc",
      "pdf_url": "",
      "publication_date": "2023-07-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "98ba77eb485e9d10c3f56baac5ff862f59fadbec",
      "title": "Scalable Scan-Chain-Based Extraction of Neural Network Models",
      "abstract": "Scan chains have greatly improved hardware testability while introducing security breaches for confidential data. Scan-chain attacks have extended their scope from cryptoprocessors to AI edge devices. The recently proposed scan-chain-based neural network (NN) model extraction attack (lCCAD 2021) made it possible to achieve fine-grained extraction and is multiple orders of magnitude more efficient both in queries and accuracy than its coarse-grained mathematical counterparts. However, both query formulation complexity and constraint solver failures increase drastically with network depth/size. We demonstrate a more powerful adversary, who is capable of improving scalability while maintaining accuracy, by relaxing high-fidelity constraints to formulate an approximate-fidelity-based layer-constrained least-squares extraction using random queries. We conduct our extraction attack on neural network inference topologies of different depths and sizes, targeting the MNIST digit recognition task. The results show that our method outperforms the scan-chain attack proposed in ICCAD 2021 by an average increase in the extracted neural network's functional accuracy of \u2248 32% and 2\u20133 orders of reduction in queries. Furthermore, we demonstrated that our attack is highly effective even in the presence of countermeasures against adversarial samples.",
      "year": 2023,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Shui Jiang",
        "S. Potluri",
        "Tsungyi Ho"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/98ba77eb485e9d10c3f56baac5ff862f59fadbec",
      "pdf_url": "",
      "publication_date": "2023-04-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3f7a77e8437be83b4cb2810f1de0a6e6b45fe6d7",
      "title": "Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection",
      "abstract": "Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-Supervised learning to minimize costs. However, in the black-box setting, these approaches may select sub-optimal samples as they train only one thief model. Depending on the thief model\u2019s capacity and the data it was pretrained on, the model might even select noisy samples that harm the learning process. In this work, we explore the usage of an ensemble of deep learning models as our thief model. We call our attack Army of Thieves(AOT) as we train multiple models with varying complexities to leverage the crowd\u2019s wisdom. Based on the ensemble\u2019s collective decision, uncertain samples are selected for querying, while the most confident samples are directly included in the training data. Our approach is the first one to utilize an ensemble of thief models to perform model extraction. We outperform the base approaches of existing state-of-the-art methods by at least 3% and achieve a 21% higher adversarial sample transferability than previous work for models trained on the CIFAR-10 dataset. Code is available at: https://github.com/akshitjindal1/AOT_WACV.",
      "year": 2023,
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "authors": [
        "Akshit Jindal",
        "Vikram Goyal",
        "Saket Anand",
        "Chetan Arora"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3f7a77e8437be83b4cb2810f1de0a6e6b45fe6d7",
      "pdf_url": "https://arxiv.org/pdf/2311.04588",
      "publication_date": "2023-11-08",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1a2fd1461e9160246ed51190a455916bb9d48fb3",
      "title": "Bits to BNNs: Reconstructing FPGA ML-IP with Joint Bitstream and Side-Channel Analysis",
      "abstract": "Energy-efficient hardware acceleration platforms for edge deployment of artificial intelligence (AI) and machine learning (ML) applications has been an ongoing research endeavor. Many efforts have focused on optimizing the algorithms and compute structures for use in resource-constrained hardware such as field-programmable gate arrays (FPGAs). Indeed, the difficult nature of crafting the best model makes the ML model itself a valuable intellectual property (IP) asset. This can be problematic, as the IP can now be exposed to an attacker through physical interfaces, enabling threats from side-channel analysis (SCA) attacks. One of the more devastating attacks is the model extraction attack, which threatens piracy and cloning of the valuable IP. While the problem of SCA-based model extraction on FPGA-deployed neural networks has been well-studied, it does not capture the full picture of what vulnerabilities may be present in those platforms. In this paper, we demonstrate how bitstream analysis can be used to obtain neural network parameters and connectivity information from block RAMs (BRAMs). We leverage the knowledge gleaned from the bitstream to mount a power SCA attack to further refine the network reconstruction effort. This is the first method that has approached the problem of ML-IP theft from the angle of FPGA bitstream analysis and suggests that further work is needed to improve security assurance for edge intelligence.",
      "year": 2023,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Brooks Olney",
        "Robert Karam"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/1a2fd1461e9160246ed51190a455916bb9d48fb3",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1365039961e03ac532b63cf4dbd99bbe5352ec0a",
      "title": "A Model Stealing Attack Against Multi-Exit Networks",
      "abstract": "Compared to traditional neural networks with a single output channel, a multi-exit network has multiple exits that allow for early outputs from the model's intermediate layers, thus significantly improving computational efficiency while maintaining similar main task accuracy. Existing model stealing attacks can only steal the model's utility while failing to capture its output strategy, i.e., a set of thresholds used to determine from which exit to output. This leads to a significant decrease in computational efficiency for the extracted model, thereby losing the advantage of multi-exit networks. In this paper, we propose the first model stealing attack against multi-exit networks to extract both the model utility and the output strategy. We employ Kernel Density Estimation to analyze the target model's output strategy and use performance loss and strategy loss to guide the training of the extracted model. Furthermore, we design a novel output strategy search algorithm to maximize the consistency between the victim model and the extracted model's output behaviors. In experiments across multiple multi-exit networks and benchmark datasets, our method always achieves accuracy and efficiency closest to the victim models.",
      "year": 2023,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Pan Li",
        "Peizhuo Lv",
        "Kai Chen",
        "Yuling Cai",
        "Fan Xiang",
        "Shengzhi Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1365039961e03ac532b63cf4dbd99bbe5352ec0a",
      "pdf_url": "",
      "publication_date": "2023-05-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c2a10426d91b95197f8489699026326bddb0eabc",
      "title": "MEAOD: Model Extraction Attack against Object Detectors",
      "abstract": "The widespread use of deep learning technology across various industries has made deep neural network models highly valuable and, as a result, attractive targets for potential attackers. Model extraction attacks, particularly query-based model extraction attacks, allow attackers to replicate a substitute model with comparable functionality to the victim model and present a significant threat to the confidentiality and security of MLaaS platforms. While many studies have explored threats of model extraction attacks against classification models in recent years, object detection models, which are more frequently used in real-world scenarios, have received less attention. In this paper, we investigate the challenges and feasibility of query-based model extraction attacks against object detection models and propose an effective attack method called MEAOD. It selects samples from the attacker-possessed dataset to construct an efficient query dataset using active learning and enhances the categories with insufficient objects. We additionally improve the extraction effectiveness by updating the annotations of the query dataset. According to our gray-box and black-box scenarios experiments, we achieve an extraction performance of over 70% under the given condition of a 10k query budget.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zeyu Li",
        "Chenghui Shi",
        "Yuwen Pu",
        "Xuhong Zhang",
        "Yu Li",
        "Jinbao Li",
        "Shouling Ji"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c2a10426d91b95197f8489699026326bddb0eabc",
      "pdf_url": "",
      "publication_date": "2023-12-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7d0593c25bcd11f26e8deaa97d18da3314f7af48",
      "title": "Rethink before Releasing your Model: ML Model Extraction Attack in EDA",
      "abstract": "Machine learning (ML)-based techniques for electronic design automation (EDA) have boosted the performance of modern integrated circuits (ICs). Such achievement makes ML model to be of importance for the EDA industry. In addition, ML models for EDA are widely considered having high development cost because of the time-consuming and complicated training data generation process. Thus, confidentiality protection for EDA models is a critical issue. However, an adversary could apply model extraction attacks to steal the model in the sense of achieving the comparable performance to the victim's model. As model extraction attacks have posed great threats to other application domains, e.g., computer vision and natural language process, in this paper, we study model extraction attacks for EDA models under two real-world scenarios. It is the first work that (1) introduces model extraction attacks on EDA models and (2) proposes two attack methods against the unlimited and limited query budget scenarios. Our results show that our approach can achieve competitive performance with the well-trained victim model without any performance degradation. Based on the results, we demonstrate that model extraction attacks truly threaten the EDA model privacy and hope to raise concerns about ML security issues in EDA.",
      "year": 2023,
      "venue": "Asia and South Pacific Design Automation Conference",
      "authors": [
        "Chen-Chia Chang",
        "Jingyu Pan",
        "Zhiyao Xie",
        "Jiangkun Hu",
        "Yiran Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7d0593c25bcd11f26e8deaa97d18da3314f7af48",
      "pdf_url": "https://doi.org/10.1145/3566097.3567896",
      "publication_date": "2023-01-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "title": "Model Stealing Attacks and Defenses: Where Are We Now?",
      "abstract": "The success of deep learning in many application domains has been nothing short of dramatic. This has brought the spotlight onto security and privacy concerns with machine learning (ML). One such concern is the threat of model theft. I will discuss work on exploring the threat of model theft, especially in the form of \u201cmodel extraction attacks\u201d \u2014 when a model is made available to customers via an inference interface, a malicious customer can use repeated queries to this interface and use the information gained to construct a surrogate model. I will also discuss possible countermeasures, focusing on deterrence mechanisms that allow for model ownership resolution (MOR) based on watermarking or fingerprinting. In particular, I will discuss the robustness of MOR schemes. I will touch on the issue of conflicts that arise when protection mechanisms for multiple different threats need to be applied simultaneously to a given ML model, using MOR techniques as a case study. This talk is based on work done with my students and collaborators, including Buse Atli Tekgul, Jian Liu, Mika Juuti, Rui Zhang, Samuel Marchal, and Sebastian Szyller. The work was funded in part by Intel Labs in the context of the Private AI consortium.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "N. Asokan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "pdf_url": "",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model theft",
        "model stealing attack",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0d7139b54040b34c6a64187622f8b8038775ff45",
      "title": "On the Limitations of Model Stealing with Uncertainty Quantification Models",
      "abstract": "Model stealing aims at inferring a victim model's functionality at a fraction of the original training cost. While the goal is clear, in practice the model's architecture, weight dimension, and original training data can not be determined exactly, leading to mutual uncertainty during stealing. In this work, we explicitly tackle this uncertainty by generating multiple possible networks and combining their predictions to improve the quality of the stolen model. For this, we compare five popular uncertainty quantification models in a model stealing task. Surprisingly, our results indicate that the considered models only lead to marginal improvements in terms of label agreement (i.e., fidelity) to the stolen model. To find the cause of this, we inspect the diversity of the model's prediction by looking at the prediction variance as a function of training iterations. We realize that during training, the models tend to have similar predictions, indicating that the network diversity we wanted to leverage using uncertainty quantification models is not (high) enough for improvements on the model stealing task.",
      "year": 2023,
      "venue": "The European Symposium on Artificial Neural Networks",
      "authors": [
        "David Pape",
        "Sina D\u00e4ubener",
        "Thorsten Eisenhofer",
        "A. E. Cin\u00e0",
        "Lea Sch\u00f6nherr"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0d7139b54040b34c6a64187622f8b8038775ff45",
      "pdf_url": "https://arxiv.org/pdf/2305.05293",
      "publication_date": "2023-05-09",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "title": "Model Stealing Attack against Recommender System",
      "abstract": "Recent studies have demonstrated the vulnerability of recommender systems to data privacy attacks. However, research on the threat to model privacy in recommender systems, such as model stealing attacks, is still in its infancy. Some adversarial attacks have achieved model stealing attacks against recommender systems, to some extent, by collecting abundant training data of the target model (target data) or making a mass of queries. In this paper, we constrain the volume of available target data and queries and utilize auxiliary data, which shares the item set with the target data, to promote model stealing attacks. Although the target model treats target and auxiliary data differently, their similar behavior patterns allow them to be fused using an attention mechanism to assist attacks. Besides, we design stealing functions to effectively extract the recommendation list obtained by querying the target model. Experimental results show that the proposed methods are applicable to most recommender systems and various scenarios and exhibit excellent attack performance on multiple datasets.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhihao Zhu",
        "Rui Fan",
        "Chenwang Wu",
        "Yi Yang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "32b28fee4daecef301dfe0e37482d17ab6f0e042",
      "title": "Towards few-call model stealing via active self-paced knowledge distillation and diffusion-based image generation",
      "abstract": "Diffusion models showcase strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, i.e. the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of samples through the black-box model to collect labels. Finally, we distill the knowledge of the black-box teacher (attacked model) into a student model (copy of the attacked model), harnessing both labeled and unlabeled data generated by the diffusion model. We employ a novel active self-paced learning framework to make the most of the proxy data during distillation. Our empirical results on three data sets confirm the superiority of our framework over four state-of-the-art methods in the few-call model extraction scenario. We release our code for free non-commercial use at https://github.com/vladhondru25/model-stealing.",
      "year": 2023,
      "venue": "Artificial Intelligence Review",
      "authors": [
        "Vlad Hondru",
        "R. Ionescu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/32b28fee4daecef301dfe0e37482d17ab6f0e042",
      "pdf_url": "",
      "publication_date": "2023-09-29",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8f03f195bacd67b350450aa2a793e7b9861afb17",
      "title": "SparseLock: Securing Neural Network Models in Deep Learning Accelerators",
      "abstract": "Securing neural networks (NNs) against model extraction and parameter exfiltration attacks is an important problem primarily because modern NNs take a lot of time and resources to build and train. We observe that there are no countermeasures (CMs) against recently proposed attacks on sparse NNs and there is no single CM that effectively protects against all types of known attacks for both sparse as well as dense NNs. In this paper, we propose SparseLock, a comprehensive CM that protects against all types of attacks including some of the very recently proposed ones for which no CM exists as of today. We rely on a novel compression algorithm and binning strategy. Our security guarantees are based on the inherent hardness of bin packing and inverse bin packing problems. We also perform a battery of statistical and information theory based tests to successfully show that we leak very little information and side channels in our architecture are akin to random sources. In addition, we show a performance benefit of 47.13% over the nearest competing secure architecture.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Nivedita Shrivastava",
        "S. Sarangi"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8f03f195bacd67b350450aa2a793e7b9861afb17",
      "pdf_url": "",
      "publication_date": "2023-11-05",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3b1cdd062387bccb4b8bfdd46dd61937caab5201",
      "title": "Theoretical Limits of Provable Security Against Model Extraction by Efficient Observational Defenses",
      "abstract": "Can we hope to provide provable security against model extraction attacks? As a step towards a theoretical study of this question, we unify and abstract a wide range of \u201cobservational\u201d model extraction defenses (OMEDs) - roughly, those that attempt to detect model extraction by analyzing the distribution over the adversary's queries. To accompany the abstract OMED, we define the notion of complete OMEDs - when benign clients can freely interact with the model - and sound OMEDs - when adversarial clients are caught and prevented from reverse engineering the model. Our formalism facilitates a simple argument for obtaining provable security against model extraction by complete and sound OMEDs, using (average-case) hardness assumptions for PAC-learning, in a way that abstracts current techniques in the prior literature. The main result of this work establishes a partial computational incompleteness theorem for the OMED: any efficient OMED for a machine learning model computable by a polynomial size decision tree that satisfies a basic form of completeness cannot satisfy soundness, unless the subexponential Learning Parity with Noise (LPN) assumption does not hold. To prove the incompleteness theorem, we introduce a class of model extraction attacks called natural Covert Learning attacks based on a connection to the Covert Learning model of Canetti and Karchmer (TCC '21), and show that such attacks circumvent any defense within our abstract mechanism in a black-box, nonadaptive way. As a further technical contribution, we extend the Covert Learning algorithm of Canetti and Karchmer to work over any \u201cconcise\u201d product distribution (albeit for juntas of a logarithmic number of variables rather than polynomial size decision trees), by showing that the technique of learning with a distributional inverter of Binnendyk et al. (ALT '22) remains viable in the Covert Learning setting.",
      "year": 2023,
      "venue": "2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Ari Karchmer"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/3b1cdd062387bccb4b8bfdd46dd61937caab5201",
      "pdf_url": "",
      "publication_date": "2023-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "title": "Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks",
      "abstract": "Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a \u00d737 faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.",
      "year": 2023,
      "venue": "International Conference on Machine Learning and Applications",
      "authors": [
        "Kacem Khaled",
        "Mouna Dhaouadi",
        "F. Magalh\u00e3es",
        "G. Nicolescu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "pdf_url": "",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ad26add46cae7797de1d638b84a0fa4ad1743a60",
      "title": "FMSA: a meta-learning framework-based fast model stealing attack technique against intelligent network intrusion detection systems",
      "abstract": "Intrusion detection systems are increasingly using machine learning. While machine learning has shown excellent performance in identifying malicious traffic, it may increase the risk of privacy leakage. This paper focuses on implementing a model stealing attack on intrusion detection systems. Existing model stealing attacks are hard to implement in practical network environments, as they either need private data of the victim dataset or frequent access to the victim model. In this paper, we propose a novel solution called Fast Model Stealing Attack (FMSA) to address the problem in the field of model stealing attacks. We also highlight the risks of using ML-NIDS in network security. First, meta-learning frameworks are introduced into the model stealing algorithm to clone the victim model in a black-box state. Then, the number of accesses to the target model is used as an optimization term, resulting in minimal queries to achieve model stealing. Finally, adversarial training is used to simulate the data distribution of the target model and achieve the recovery of privacy data. Through experiments on multiple public datasets, compared to existing state-of-the-art algorithms, FMSA reduces the number of accesses to the target model and improves the accuracy of the clone model on the test dataset to 88.9% and the similarity with the target model to 90.1%. We can demonstrate the successful execution of model stealing attacks on the ML-NIDS system even with protective measures in place to limit the number of anomalous queries.",
      "year": 2023,
      "venue": "Cybersecurity",
      "authors": [
        "Kaisheng Fan",
        "Weizhe Zhang",
        "Guangrui Liu",
        "Hui He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/ad26add46cae7797de1d638b84a0fa4ad1743a60",
      "pdf_url": "https://cybersecurity.springeropen.com/counter/pdf/10.1186/s42400-023-00171-y",
      "publication_date": "2023-08-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2cd2ba46e7b6195521d02606aac5e2cda751b271",
      "title": "Efficient Model Extraction by Data Set Stealing, Balancing, and Filtering",
      "abstract": "Model extraction replicates the functionality of machine learning models deployed as a service. Recently, generative adversarial networks (GANs)-based methods have achieved remarkable performance in data-free model extraction. However, previous methods generate random data in every training batch, resulting in slow convergence and redundant queries. We propose to tackle the task with a much simpler paradigm. Specifically, we steal a data set with GAN before training the clone model rather than during every training batch. Benefiting from full use of the generated data, the proposed paradigm needs less training time and query cost. To improve the class distribution of data, a balancing strategy is applied. Furthermore, the balanced data set is filtered based on adversarial robustness for better quality. Combining the above strategies, we propose an efficient model extraction by data set stealing, balancing, and filtering (DSBF). Experiments on three widely used data sets show that DSBF outperforms previous methods while converging faster and costing fewer queries.",
      "year": 2023,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Panpan Yang",
        "Qinglong Wu",
        "Xinming Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/2cd2ba46e7b6195521d02606aac5e2cda751b271",
      "pdf_url": "",
      "publication_date": "2023-12-15",
      "keywords_matched": [
        "model extraction",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8e5b27857ea5ede4927d6c673ef795cb23b59071",
      "title": "Stolen Risks of Models with Security Properties",
      "abstract": "Verifiable robust machine learning, as a new trend of ML security defense, enforces security properties (e.g., Lipschitzness, Monotonicity) on machine learning models and achieves satisfying accuracy-security trade-off. Such security properties identify a series of evasion strategies of ML security attackers and specify logical constraints on their effects on a classifier (e.g., the classifier is monotonically increasing along some feature dimensions). However, little has been done so far to understand the side effect of those security properties on the model privacy. In this paper, we aim at better understanding the privacy impacts on security properties of robust ML models. Particularly, we report the first measurement study to investigate the model stolen risks of robust models satisfying four security properties (i.e., LocalInvariance, Lipschitzness, SmallNeighborhood, and Monotonicity). Our findings bring to light the factors that influence model stealing attacks and defense performance on models trained with security properties. In addition, to train an ML model satisfying goals in accuracy, security, and privacy, we propose a novel technique, called BoundaryFuzz, which introduces a privacy property into verifiable robust training frameworks to defend against model stealing attacks on robust models. Experimental results demonstrate the defense effectiveness of BoundaryFuzz.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yue Qin",
        "Zhuoqun Fu",
        "Chuyun Deng",
        "Xiaojing Liao",
        "Jia Zhang",
        "Haixin Duan"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8e5b27857ea5ede4927d6c673ef795cb23b59071",
      "pdf_url": "",
      "publication_date": "2023-11-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b7a846254b166447bfbf28852bfde7495332e14d",
      "title": "Making Watermark Survive Model Extraction Attacks in Graph Neural Networks",
      "abstract": "Collecting graph data is costly and well-trained graph neural networks (GNNs) are viewed as intellectual property. To make better use of GNNs, they are used to provide cloud-based services. However, models on cloud-based services may be leaked under model extraction attacks. Adversaries can extract an imitation model by simply querying the GNNs on the cloud-based services. To protect GNNs, watermarks are embedded in the models. However, the watermarks can be removed by the model extraction attacks. To address this issue, we propose adding a watermark that cannot be ignored by queries from the model extraction attacks. Concretely, we add the soft nearest neighbor loss to the loss function of the watermark embedding process to merge the distributions for the normal tasks and watermarks. We also observe that the watermark brings a performance loss to GNNs and propose an optimization method to maintain the model performance. We evaluate our method on multiple real-world datasets to demonstrate the superiority of the method.",
      "year": 2023,
      "venue": "ICC 2023 - IEEE International Conference on Communications",
      "authors": [
        "Haiming Wang",
        "Zhikun Zhang",
        "Min Chen",
        "Shibo He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b7a846254b166447bfbf28852bfde7495332e14d",
      "pdf_url": "",
      "publication_date": "2023-05-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3166f8c8e00f2cd9dc5a903009a4f50d176d9f8a",
      "title": "Data-Free Model Stealing Attack Based on Denoising Diffusion Probabilistic Model",
      "abstract": "Data-free model stealing (MS) attacks use synthetic samples to query a target model and train a substitute model to fit the target model\u2019s predictions, avoiding strong dependence on real datasets used by model developers. However, the existing data-free MS attack methods still have a big gap in generating high-quality query samples for high-precision MS attacks. In this paper, we construct the DDPM-optimized generator to generate data, in which a residual network-like structure is designed to fuse data to synthesize query samples. Our method further improves the quantity and quality of synthetic query samples, and effectively reduces the number of queries to the target model. The results show that the proposed method achieves superior performance compared to state-of-the-art methods.",
      "year": 2023,
      "venue": "2023 IEEE Smart World Congress (SWC)",
      "authors": [
        "Guofeng Gao",
        "Xiaodong Wang",
        "Zhiqiang Wei",
        "Jinghai Ai"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3166f8c8e00f2cd9dc5a903009a4f50d176d9f8a",
      "pdf_url": "",
      "publication_date": "2023-08-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ecb4dff5160be86c97f27d9be1c0b74472af127b",
      "title": "Sniffer: A Novel Model Type Detection System against Machine-Learning-as-a-Service Platforms",
      "abstract": "\n Recent works explore several attacks against Machine-Learning-as-a-Service (MLaaS) platforms (e.g., the model stealing attack), allegedly posing potential real-world threats beyond viability in laboratories. However, hampered by\n model-type-sensitive\n , most of the attacks can hardly break mainstream real-world MLaaS platforms. That is, many MLaaS attacks are designed against only one certain type of model, such as tree models or neural networks. As the black-box MLaaS interface hides model type info, the attacker cannot choose a proper attack method with confidence, limiting the attack performance. In this paper, we demonstrate a system, named Sniffer, that is capable of making model-type-sensitive attacks \"great again\" in real-world applications. Specifically, Sniffer consists of four components: Generator, Querier, Probe, and Arsenal. The first two components work for preparing attack samples. Probe, as the most characteristic component in Sniffer, implements a series of self-designed algorithms to determine the type of models hidden behind the black-box MLaaS interfaces. With model type info unraveled, an optimum method can be selected from Arsenal (containing multiple attack methods) to accomplish its attack. Our demonstration shows how the audience can interact with Sniffer in a web-based interface against five mainstream MLaaS platforms.\n",
      "year": 2023,
      "venue": "Proceedings of the VLDB Endowment",
      "authors": [
        "Zhuo Ma",
        "Yilong Yang",
        "Bin Xiao",
        "Yang Liu",
        "Xinjing Liu",
        "Tong Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ecb4dff5160be86c97f27d9be1c0b74472af127b",
      "pdf_url": "",
      "publication_date": "2023-08-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "db8f3ab8139fdbe1a073f7b6e0d15395ab2e536f",
      "title": "Interesting Near-boundary Data: Inferring Model Ownership for DNNs",
      "abstract": "Deep neural networks (DNNs) require expensive training, which is why the protection of model intellectual property (IP) is becoming more critical. Recently, model stealing has emerged frequently, and many researchers design model watermarking and fingerprinting for verifying model ownership. However, attacks such as ambiguity statements have been used to break the current defense, which poses a challenge to model ownership verification. Therefore, this paper proposes an interesting near-boundary data as evidence for obtaining model ownership and innovatively proposes to infer model ownership instead of verifying model ownership. In this paper, we propose to generate the initial near-boundary data using an algorithm that adds slight noise to generate adversarial examples. We design a generator to privatize the near-boundary data. Our main observation is that the near-boundary data exhibit results close to the classification boundary in both the source model and its derived stolen model. At the end of this work, we design many experiments to verify the effectiveness of the proposed method. The experimental results demonstrate that model ownership can be inferred with high confidence. Noting that our method does not require the training data to be private, and it is extremely costly for model stealers to reuse our method.",
      "year": 2023,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhe Sun",
        "Zongwen Yang",
        "Zhongyu Huang",
        "Yu Zhang",
        "Jianzhong Zhang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/db8f3ab8139fdbe1a073f7b6e0d15395ab2e536f",
      "pdf_url": "",
      "publication_date": "2023-06-18",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "47feadf7b6d9fdc2a9f2339173e967c21554c746",
      "title": "Adversarial machine learning in cybersecurity: Mitigating evolving threats in AI-powered defense systems",
      "abstract": "The increasing integration of artificial intelligence (AI) in cybersecurity has enhanced the ability to detect and mitigate cyber threats in real-time. However, adversarial machine learning (AML) has emerged as a significant challenge, enabling attackers to manipulate AI models and bypass security measures. This study explores the evolving landscape of AML threats and the vulnerabilities they introduce to AI-powered defense systems. The research identifies key adversarial attack techniques, including evasion, poisoning, model inversion, and model extraction, which threaten the integrity and effectiveness of AI-driven cybersecurity mechanisms. This study evaluates various mitigation strategies to address these threats, such as adversarial Training, model hardening, defensive Distillation, and hybrid AI approaches. Through experimental analysis, we assess the robustness of AI defense systems under adversarial attack and measure their effectiveness using key performance metrics, including model accuracy, false positive rates, and computational efficiency. The findings indicate that while adversarial Training improves model resilience, adaptive attack techniques continue to challenge existing defenses, necessitating continuous advancements in cybersecurity frameworks. This research highlights the need for a multi-layered security approach that integrates AI-based anomaly detection, human-AI hybrid security models, and adaptive learning techniques to counter adversarial threats effectively. Additionally, it discusses the broader implications of AML in cybersecurity, including policy considerations, ethical concerns, and future research directions. The study recommends strategies for enhancing AI-powered cyber defense systems to maintain security, reliability, and resilience against evolving adversarial threats.",
      "year": 2023,
      "venue": "World Journal of Advanced Engineering Technology and Sciences",
      "authors": [
        "Ebuka Mmaduekwe Paul",
        "Ugochukwu Mmaduekwe Stanley",
        "Joseph Darko Kessie",
        "Mukhtar Dolapo Salawudeen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/47feadf7b6d9fdc2a9f2339173e967c21554c746",
      "pdf_url": "",
      "publication_date": "2023-12-30",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "43d2aeaf2b8fbb522f50eb05de32758b3678e5ee",
      "title": "SCME: A Self-Contrastive Method for Data-free and Query-Limited Model Extraction Attack",
      "abstract": "Previous studies have revealed that artificial intelligence (AI) systems are vulnerable to adversarial attacks. Among them, model extraction attacks fool the target model by generating adversarial examples on a substitute model. The core of such an attack is training a substitute model as similar to the target model as possible, where the simulation process can be categorized in a data-dependent and data-free manner. Compared with the data-dependent method, the data-free one has been proven to be more practical in the real world since it trains the substitute model with synthesized data. However, the distribution of these fake data lacks diversity and cannot detect the decision boundary of the target model well, resulting in the dissatisfactory simulation effect. Besides, these data-free techniques need a vast number of queries to train the substitute model, increasing the time and computing consumption and the risk of exposure. To solve the aforementioned problems, in this paper, we propose a novel data-free model extraction method named SCME (Self-Contrastive Model Extraction), which considers both the inter- and intra-class diversity in synthesizing fake data. In addition, SCME introduces the Mixup operation to augment the fake data, which can explore the target model's decision boundary effectively and improve the simulating capacity. Extensive experiments show that the proposed method can yield diversified fake data. Moreover, our method has shown superiority in many different attack settings under the query-limited scenario, especially for untargeted attacks, the SCME outperforms SOTA methods by 11.43\\% on average for five baseline datasets.",
      "year": 2023,
      "venue": "International Conference on Neural Information Processing",
      "authors": [
        "Renyang Liu",
        "Jinhong Zhang",
        "Kwok-Yan Lam",
        "Jun Zhao",
        "Wei Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/43d2aeaf2b8fbb522f50eb05de32758b3678e5ee",
      "pdf_url": "",
      "publication_date": "2023-10-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e1ec9a011a049115ce3dc45aeb738ad34d726f87",
      "title": "Decepticon: Attacking Secrets of Transformers",
      "abstract": "With the growing burden of training deep learning models with huge datasets, transfer learning has been widely adopted (e.g., Transformers like BERT, GPT). Transfer learning significantly reduces the time and effort of model training. However, the security impact of using shared pre-trained models has not been evaluated. In this paper, we provide in-depth characterizations of the fine-tuning process and reveal the security vulnerabilities of transfer-learned models. Then, we show a novel two-level model extraction attack; 1) identifying the pre-trained model of a victim transfer-learned model through model fingerprint collected from off-the-shelf GPUs and 2) extracting the entire weights of the victim black-box model based on the hints in the pre-trained model. The extracted model shows almost alike prediction accuracy with over 94% matching prediction outputs with the victim model. The two-level model extraction enables large model weight extraction that is considered as challenging if not impossible through significantly reduced extraction effort.",
      "year": 2023,
      "venue": "IEEE International Symposium on Workload Characterization",
      "authors": [
        "Mujahid Al Rafi",
        "Yuan Feng",
        "Fan Yao",
        "Meng Tang",
        "Hyeran Jeon"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e1ec9a011a049115ce3dc45aeb738ad34d726f87",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "49b98e01423c7b857c931a9d10bec3dc5951dbe2",
      "title": "Ownership Protection of Generative Adversarial Networks",
      "abstract": "Generative adversarial networks (GANs) have shown remarkable success in image synthesis, making GAN models themselves commercially valuable to legitimate model owners. Therefore, it is critical to technically protect the intellectual property of GANs. Prior works need to tamper with the training set or training process, and they are not robust to emerging model extraction attacks. In this paper, we propose a new ownership protection method based on the common characteristics of a target model and its stolen models. Our method can be directly applicable to all well-trained GANs as it does not require retraining target models. Extensive experimental results show that our new method can achieve the best protection performance, compared to the state-of-the-art methods. Finally, we demonstrate the effectiveness of our method with respect to the number of generations of model extraction attacks, the number of generated samples, different datasets, as well as adaptive attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/49b98e01423c7b857c931a9d10bec3dc5951dbe2",
      "pdf_url": "http://arxiv.org/pdf/2306.05233",
      "publication_date": "2023-06-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "349062d82b56fb516b0c7061940470f8d9c256a6",
      "title": "Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems",
      "abstract": "Model-serving systems have become increasingly popular, especially in real-time web applications. In such systems, users send queries to the server and specify the desired performance metrics (e.g., desired accuracy, latency). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks assume a single model can be repeatedly selected for serving inference requests. Modern inference serving systems break this assumption. Thus, they cannot be directly applied to extract a victim model, as models are hidden behind a layer of abstraction exposed by the serving system. An attacker can no longer identify which model she is interacting with. To this end, we first propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accuracy scores within $1\\%$ of the scores obtained when attacking a single, explicitly specified model, as well as up to $14.6\\%$ gain in accuracy and up to $7.7\\%$ gain in fidelity compared to the naive attack. Second, we counter the proposed attack with a noise-based defense mechanism that thwarts fingerprinting by adding noise to the specified performance metrics. The proposed defense strategy reduces the attack's accuracy and fidelity by up to $9.8\\%$ and $4.8\\%$, respectively (on medium-sized model extraction). Third, we show that the proposed defense induces a fundamental trade-off between the level of protection and system goodput, achieving configurable and significant victim model extraction protection while maintaining acceptable goodput ($>80\\%$). We implement the proposed defense in a real system with plans to open source.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Debopam Sanyal",
        "Jui-Tse Hung",
        "Manavi Agrawal",
        "Prahlad Jasti",
        "Shahab Nikkhoo",
        "S. Jha",
        "Tianhao Wang",
        "Sibin Mohan",
        "Alexey Tumanov"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/349062d82b56fb516b0c7061940470f8d9c256a6",
      "pdf_url": "https://arxiv.org/pdf/2307.01292",
      "publication_date": "2023-07-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "82bf68c3e3aa8b2f8ccea0842e43cbe39a5b2dff",
      "title": "Model Stealing Attack against Graph Classification with Authenticity, Uncertainty and Diversity",
      "abstract": "Recent research demonstrates that GNNs are vulnerable to the model stealing attack, a nefarious endeavor geared towards duplicating the target model via query permissions. However, they mainly focus on node classification tasks, neglecting the potential threats entailed within the domain of graph classification tasks. Furthermore, their practicality is questionable due to unreasonable assumptions, specifically concerning the large data requirements and extensive model knowledge. To this end, we advocate following strict settings with limited real data and hard-label awareness to generate synthetic data, thereby facilitating the stealing of the target model. Specifically, following important data generation principles, we introduce three model stealing attacks to adapt to different actual scenarios: MSA-AU is inspired by active learning and emphasizes the uncertainty to enhance query value of generated samples; MSA-AD introduces diversity based on Mixup augmentation strategy to alleviate the query inefficiency issue caused by over-similar samples generated by MSA-AU; MSA-AUD combines the above two strategies to seamlessly integrate the authenticity, uncertainty, and diversity of the generated samples. Finally, extensive experiments consistently demonstrate the superiority of the proposed methods in terms of concealment, query efficiency, and stealing performance.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhihao Zhu",
        "Chenwang Wu",
        "Rui Fan",
        "Yi Yang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/82bf68c3e3aa8b2f8ccea0842e43cbe39a5b2dff",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "78c6861c798e06b5651f45594eb6c8da43708e08",
      "title": "GNMS: A novel method for model stealing based on GAN",
      "abstract": "Many well-performing models are currently deployed on the cloud to provide machine Learning as a service (MLaaS). However, these models are susceptible to Model Stealing Attacks, where attackers can access the model\u2019s functionality, parameters, and internal structure in a black-box. As a result, data-free model stealing methods have gained popularity due to their higher accuracy and not requiring real data. Previous data-free model stealing methods have mainly focused on single scenarios and limited model and dataset variations. In this paper, we introduce a novel generalized network model Stealing method (GNMS), which is suitable for both benchmark and transfer models, achieving high model stealing accuracy across various scenarios. We pre-train generative adversarial network (GAN) using publicly available datasets and efficiently steal model functionality by training a student model with the pre-trained generator and the discriminator. Adversarial samples and the generated image dataset are also used to explore the model\u2019s decision boundaries. During the training of the clone model, we train two clone models to minimize the differences with the target model further. We employ a contrastive learning approach to encourage the models to learn meaningful feature representations by distinguishing between similar and dissimilar data points, thereby enhancing the model\u2019s accuracy. We achieve a model stealing accuracy of 73.02% and 72.93% on more complex datasets CIFAR100 and Caltech101. Surpass the latest DisGUIDE by 3.55% and 2.61%.",
      "year": 2023,
      "venue": "International Conference on Advanced Cloud and Big Data",
      "authors": [
        "Moxuan Zeng",
        "Yangzhong Wang",
        "Yangming Zhang",
        "Jun Niu",
        "Yuqing Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/78c6861c798e06b5651f45594eb6c8da43708e08",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f95c674b837576be6698eda1f6ef89cf2d8c37c4",
      "title": "An attack framework for stealing black-box based on active semi-supervised learning",
      "abstract": "Neural network models are commonly used as black-box services, but they are vulnerable to model stealing attacks, where an attacker can train a substitute model with similar performance to the original model by exploiting limited information related to the target model. This can cause significant losses to the owner of the target model and pose a serious security risk. To advance our understanding of neural networks and promote the evolution of model protection mechanisms, we conducted in-depth research on neural network model stealing attacks. In this paper, we propose a black-box stealing attack framework that combines active and semi-supervised learning, even if the target black-box only provides hard-label output, an effective attack can be achieved, generating a substitute model with the same functionality as the black-box. The framework involves selectively querying the most informative samples for black-box labeling using active learning, which significantly reduces the workload of querying the black-box and enables to achieve better performance with fewer training samples. We also apply semi-supervised learning to leverage the abundance of unlabeled data and further improve model performance. We evaluated our method on various data sets and proved that the stealing ability of our method was significantly higher than 3.86%~26.64% other methods when faced with hardlabel black-box with the same number of queries, which can achieve effective black-box function stealing.",
      "year": 2023,
      "venue": "6th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE 2023)",
      "authors": [
        "Lijun Gao",
        "Yuting Wang",
        "Wenjun Liu",
        "Kai Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f95c674b837576be6698eda1f6ef89cf2d8c37c4",
      "pdf_url": "",
      "publication_date": "2023-08-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "71ba257fa70f6d8cc5485c5cd8026b8935211814",
      "title": "Semantic Awareness Model For Binary Operator Code",
      "abstract": "With the promotion of artificial intelligence in various industries, there have been some organizations and individuals using various means to attack it. Common types of attacks against models include adversarial sample attacks, data poisoning attacks, and model stealing attacks. The above attack methods require a certain understanding of the structure of the model, so it becomes a challenge to restore the category of binary operators from the model.Binary code similarity detection (BCSD) has important applications in code checking, vulnerability detection, and malicious co de analysis. Due to the lack of syntax structure information in binary operator code, it is difficult to determine the type of operator. Recent research has focused on using deep learning models to understand the semantics and structural information of binary code to achieve better results. Recent research has shown that deep learning models, especially natural language processing models, can comprehend the semantics of binary code. In this paper, we propose a method for identifying binary operators which uses a sequence-aware model and a structure-aware model to model binary operators. It combines CNN semantics and Transformer semantics for classification. The evaluation shows that our method can achieve good performance in binary operator classification.",
      "year": 2023,
      "venue": "2023 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)",
      "authors": [
        "Haichao Gao",
        "Liming Fang",
        "Yang Li",
        "Minghui Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/71ba257fa70f6d8cc5485c5cd8026b8935211814",
      "pdf_url": "",
      "publication_date": "2023-11-03",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "46ba62fc7b8166cc559e53d52992f2b1fde706eb",
      "title": "Image Translation-Based Deniable Encryption against Model Extraction Attack",
      "abstract": "In cloud storage applications, data owners\u2019 original images are usually encrypted before being outsourced to the cloud for preserving data owners\u2019 privacy. However, in deep learning model-based image encryption methods, an adversary can conduct the model extraction attack to reveal the model parameters and thus restore the privacy information by obtaining numerous encrypted images. In this paper, we propose an image translation-based deniable encryption (ITDE) scheme to achieve encryption deniability and defend against model extraction attacks. Differing from traditional encryption methods in which encrypted images are visually meaningless, ITDE applies image translation to generate encrypted images in the form of human faces. Moreover, ITDE provides deniability for data owners to keep the encryption parameters private. To defend against model extraction attacks, the defense mechanism is introduced in our proposed ITDE to preserve deep learning models. Experimental results demonstrate the superiority of our proposed methods in terms of encryption deniability and privacy preservation.",
      "year": 2023,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "Yiling Chen",
        "Yuanzhi Yao",
        "Nenghai Yu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/46ba62fc7b8166cc559e53d52992f2b1fde706eb",
      "pdf_url": "",
      "publication_date": "2023-10-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e6c21c16e7b739ff19d90d3432caf01e701c1bf9",
      "title": "Proof of Spacetime as a Defensive Technique Against Model Extraction Attacks",
      "abstract": "\u2014When providing a service that utilizes a machine learning model, the countermeasures against cyber-attacks are required. The model extraction attack is one of the attacks, in which an attacker attempts to replicate the model by obtaining a large number of input-output pairs. While a defense using Proof of Work has already been proposed, an attacker can still conduct model extraction attacks by increasing their computational power. Moreover, this approach leads to unnecessary energy consumption and might not be environmentally friendly. In this paper, the defense method using Proof of Spacetime instead of Proof of Work is proposed to reduce the energy consumption. The Proof of Spacetime is a method to impose spatial and temporal costs on the users of the service. While the Proof of Work makes a user to calculate until permission is granted, the Proof of Spacetime makes a user to keep a result of calculation, so the energy consumption is reduced. Through computer simulations, it was found that systems with Proof of Spacetime, compared to those with Proof of Work, impose 0.79 times the power consumption and 1.07 times the temporal cost on the attackers, while 0.73 times and 0.64 times on the non-attackers. Therefore, the system with Proof of Spacetime can prevent model extraction attacks with lower energy consumption.",
      "year": 2023,
      "venue": "International Journal of Advanced Computer Science and Applications",
      "authors": [
        "Tatsuki Fukuda"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e6c21c16e7b739ff19d90d3432caf01e701c1bf9",
      "pdf_url": "http://thesai.org/Downloads/Volume14No6/Paper_11-Proof_of_Spacetime_as_a_Defensive_Technique_Against_Model.pdf",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "044758b8410c8bec7a542febbb8fb613d666cd1d",
      "title": "Exploring AI Attacks on Hardware Accelerated Targets",
      "abstract": "Artificial Intelligence have become integral to various applications, ranging from image recognition to natural language processing. To meet the increasing demand for real-time and low-power AI inference Hardware accelerated Embedded Systems such as Field Programmable Gate Arrays have emerged as a popular hardware platform. However, the deployment of AI models on Embedded AI Systems introduces new security concerns. AI attacks on such Embedded AI based systems pose significant risks to the integrity, confidentiality, and availability of AI applications. In our experiment, we conducted attacks such as the Model Extraction Attack and Evasion Attack for AI on Embedded Systems. These experimental results highlights the critical importance of implementing strong security measures to protect AI models running on Hardware accelerated Embedded AI Systems, ensuring their ability to withstand potential threats.",
      "year": 2023,
      "venue": "2023 IEEE 2nd International Conference on Data, Decision and Systems (ICDDS)",
      "authors": [
        "Parin Shah",
        "Yuvaraj Govindarajulu",
        "Pavan Kulkarni",
        "Manojkumar Parmar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/044758b8410c8bec7a542febbb8fb613d666cd1d",
      "pdf_url": "",
      "publication_date": "2023-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f8e2413f8f206b00ee6758debdf7f886d5d6fc69",
      "title": "Safety or Not? A Comparative Study for Deep Learning Apps on Smartphones",
      "abstract": "Recent years have witnessed an astonishing explosion in the evolution of mobile applications powered by deep learning (DL) technologies. Considering that inference of DL models in the cloud requires transferring user data to server, which is prone to the risk of user privacy leakage, many developers choose to deploy the models on local devices for executing the inference process. However, this also raises a number of other security issues, such as adversarial attacks, model stealing attacks, etc. To explore the security issues that exist in deep learning applications (DL apps), we conducted the first comprehensive comparative study of the top 200 apps in each category on Google Play. We built DLApplnspector, a vulnerability detection tool that combines dynamic and static analysis methods for dissecting apps, which helped us automate our empirical study on real-world mobile DL apps. First, we identify DL apps and extract their models by using DL Checker. Subsequently, Static Scoper is provided to detect encryption and reusability of DL models. Finally, within Dynamic Scoper, we use reverse engineering techniques on the network traffic to parse out the packets and collect side-channel information during application runtime. Our research shows that the majority of developers prefer to use open-source models, with almost 92% of models successfully parsed. This suggests that most models are unprotected. DL apps are more likely to upload user behaviour and collect private data than Non-DL apps. We provide security recommendations for developers and users to address the issues discovered.",
      "year": 2023,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Jin Au-Yeung",
        "Shanshan Wang",
        "Yuchen Liu",
        "Zhenxiang Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8e2413f8f206b00ee6758debdf7f886d5d6fc69",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4fc5f31a38a19e3acc9501aeb3006810eec0e767",
      "title": "FedRolex: Model-Heterogeneous Federated Learning with Rolling Sub-Model Extraction",
      "abstract": "Most cross-device federated learning (FL) studies focus on the model-homogeneous setting where the global server model and local client models are identical. However, such constraint not only excludes low-end clients who would otherwise make unique contributions to model training but also restrains clients from training large models due to on-device resource bottlenecks. In this work, we propose FedRolex, a partial training (PT)-based approach that enables model-heterogeneous FL and can train a global server model larger than the largest client model. At its core, FedRolex employs a rolling sub-model extraction scheme that allows different parts of the global server model to be evenly trained, which mitigates the client drift induced by the inconsistency between individual client models and server model architectures. We show that FedRolex outperforms state-of-the-art PT-based model-heterogeneous FL methods (e.g. Federated Dropout) and reduces the gap between model-heterogeneous and model-homogeneous FL, especially under the large-model large-dataset regime. In addition, we provide theoretical statistical analysis on its advantage over Federated Dropout and evaluate FedRolex on an emulated real-world device distribution to show that FedRolex can enhance the inclusiveness of FL and boost the performance of low-end devices that would otherwise not benefit from FL. Our code is available at: https://github.com/AIoT-MLSys-Lab/FedRolex",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Samiul Alam",
        "Luyang Liu",
        "Ming Yan",
        "Mi Zhang"
      ],
      "citation_count": 197,
      "url": "https://www.semanticscholar.org/paper/4fc5f31a38a19e3acc9501aeb3006810eec0e767",
      "pdf_url": "http://arxiv.org/pdf/2212.01548",
      "publication_date": "2022-12-03",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d405b58a8f465d5ba2e91f9541e09760904c11a8",
      "title": "I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences",
      "abstract": "Machine-Learning-as-a-Service (MLaaS) has become a widespread paradigm, making even the most complex Machine Learning models available for clients via, e.g., a pay-per-query principle. This allows users to avoid time-consuming processes of data collection, hyperparameter tuning, and model training. However, by giving their customers access to the (predictions of their) models, MLaaS providers endanger their intellectual property such as sensitive training data, optimised hyperparameters, or learned model parameters. In some cases, adversaries can create a copy of the model with (almost) identical behaviour using the the prediction labels only. While many variants of this attack have been described, only scattered defence strategies that address isolated threats have been proposed. To arrive at a comprehensive understanding why these attacks are successful and how they could be holistically defended against, a thorough systematisation of the field of model stealing is necessary. We address this by categorising and comparing model stealing attacks, assessing their performance, and exploring corresponding defence techniques in different settings. We propose a taxonomy for attack and defence approaches and provide guidelines on how to select the right attack or defence strategy based on the goal and available resources. Finally, we analyse which defences are rendered less effective by current attack strategies.",
      "year": 2022,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "A. Rauber"
      ],
      "citation_count": 143,
      "url": "https://www.semanticscholar.org/paper/d405b58a8f465d5ba2e91f9541e09760904c11a8",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3595292",
      "publication_date": "2022-06-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "stealing machine learning"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "35ade8553de7259a5e8105bd20a160f045f9d112",
      "title": "Towards Data-Free Model Stealing in a Hard Label Setting",
      "abstract": "Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adversary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels. In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query budget. We propose a novel GAN-based framework11Project Page: https://sites.google.com/view/dfms-hl that trains the student and generator in tandem to steal the model effectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim's gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-of-the-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scalability of Model Stealing in a restricted access setting on a 100 class dataset as well.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Sunandini Sanyal",
        "Sravanti Addepalli",
        "R. Venkatesh Babu"
      ],
      "citation_count": 104,
      "url": "https://www.semanticscholar.org/paper/35ade8553de7259a5e8105bd20a160f045f9d112",
      "pdf_url": "https://arxiv.org/pdf/2204.11022",
      "publication_date": "2022-04-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a6cb46a2d7549d82abe893602b9a22b406859ebb",
      "title": "SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders",
      "abstract": "Self-supervised learning is an emerging machine learning (ML) paradigm. Compared to supervised learning which leverages high-quality labeled datasets, self-supervised learning relies on unlabeled datasets to pre-train powerful encoders which can then be treated as feature extractors for various downstream tasks. The huge amount of data and computational resources consumption makes the encoders themselves become the valuable intellectual property of the model owner. Recent research has shown that the ML model's copyright is threatened by model stealing attacks, which aim to train a surrogate model to mimic the behavior of a given model. We empirically show that pre-trained encoders are highly vulnerable to model stealing attacks. However, most of the current efforts of copyright protection algorithms such as watermarking concentrate on classifiers. Meanwhile, the intrinsic challenges of pre-trained encoder's copyright protection remain largely unstudied. We fill the gap by proposing SSLGuard, the first watermarking scheme for pre-trained encoders. Given a clean pre-trained encoder, SSLGuard injects a watermark into it and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Our extensive evaluation shows that SSLGuard is effective in watermark injection and verification, and it is robust against model stealing and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yang Zhang"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/a6cb46a2d7549d82abe893602b9a22b406859ebb",
      "pdf_url": "",
      "publication_date": "2022-01-27",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7da6c9273a14eb8681824d0c3ee84e05366c5627",
      "title": "Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders",
      "abstract": "Self-supervised representation learning techniques have been developing rapidly to make full use of unlabeled images. They encode images into rich features that are oblivious to downstream tasks. Behind their revolutionary representation power, the requirements for dedicated model designs and a massive amount of computation resources expose image encoders to the risks of potential model stealing attacks - a cheap way to mimic the well-trained encoder performance while circumventing the demanding requirements. Yet conventional attacks only target supervised classifiers given their predicted labels and/or posteriors, which leaves the vulnerability of unsupervised encoders unexplored. In this paper, we first instantiate the conventional stealing attacks against encoders and demonstrate their severer vulnerability compared with downstream classifiers. To better leverage the rich representation of encoders, we further propose Cont-Steal, a contrastive-learning-based attack, and validate its improved stealing effectiveness in various experiment settings. As a takeaway, we appeal to our community's attention to the intellectual property protection of representation learning techniques, especially to the defenses against encoder stealing attacks like ours.11See our code in https://github.com/zeyangsha/Cont-Steal.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zeyang Sha",
        "Xinlei He",
        "Ning Yu",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/7da6c9273a14eb8681824d0c3ee84e05366c5627",
      "pdf_url": "https://arxiv.org/pdf/2201.07513",
      "publication_date": "2022-01-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0a58e101142efdd9dd653b41d504152e940096be",
      "title": "Are You Stealing My Model? Sample Correlation for Fingerprinting Deep Neural Networks",
      "abstract": "An off-the-shelf model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting aims to verify whether a suspect model is stolen from the victim model, which gains more and more attention nowadays. Previous methods always leverage the transferable adversarial examples as the model fingerprint, which is sensitive to adversarial defense or transfer learning scenarios. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-w that selects wrongly classified normal samples as model inputs and calculates the mean correlation among their model outputs. To reduce the training time, we further develop SAC-m that selects CutMix Augmented samples as model inputs, without the need for training the surrogate models or generating adversarial examples. Extensive results validate that SAC successfully defends against various model stealing attacks, even including adversarial training or transfer learning, and detects the stolen models with the best performance in terms of AUC across different datasets and model architectures. The codes are available at https://github.com/guanjiyang/SAC.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jiyang Guan",
        "Jian Liang",
        "R. He"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/0a58e101142efdd9dd653b41d504152e940096be",
      "pdf_url": "https://arxiv.org/pdf/2210.15427",
      "publication_date": "2022-10-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "274dbb98c63cdd282eb86b0338bdc3c5dfd9b904",
      "title": "Dataset Inference for Self-Supervised Models",
      "abstract": "Self-supervised models are increasingly prevalent in machine learning (ML) since they reduce the need for expensively labeled data. Because of their versatility in downstream applications, they are increasingly used as a service exposed via public APIs. At the same time, these encoder models are particularly vulnerable to model stealing attacks due to the high dimensionality of vector representations they output. Yet, encoders remain undefended: existing mitigation strategies for stealing attacks focus on supervised learning. We introduce a new dataset inference defense, which uses the private training set of the victim encoder model to attribute its ownership in the event of stealing. The intuition is that the log-likelihood of an encoder's output representations is higher on the victim's training data than on test data if it is stolen from the victim, but not if it is independently trained. We compute this log-likelihood using density estimation models. As part of our evaluation, we also propose measuring the fidelity of stolen encoders and quantifying the effectiveness of the theft detection without involving downstream tasks; instead, we leverage mutual information and distance measurements. Our extensive empirical results in the vision domain demonstrate that dataset inference is a promising direction for defending self-supervised models against model stealing.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Adam Dziedzic",
        "Haonan Duan",
        "Muhammad Ahmad Kaleem",
        "Nikita Dhawan",
        "Jonas Guan",
        "Yannis Cattan",
        "Franziska Boenisch",
        "Nicolas Papernot"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/274dbb98c63cdd282eb86b0338bdc3c5dfd9b904",
      "pdf_url": "http://arxiv.org/pdf/2209.09024",
      "publication_date": "2022-09-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2ee4127a2a6aab51a03305d8a564693181bc6424",
      "title": "Increasing the Cost of Model Extraction with Calibrated Proof of Work",
      "abstract": "In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.",
      "year": 2022,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Adam Dziedzic",
        "Muhammad Ahmad Kaleem",
        "Y. Lu",
        "Nicolas Papernot"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/2ee4127a2a6aab51a03305d8a564693181bc6424",
      "pdf_url": "",
      "publication_date": "2022-01-23",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack",
        "prevent model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "af72f901d7a0f2edca55ee008c8893ae93b09971",
      "title": "How to Steer Your Adversary: Targeted and Efficient Model Stealing Defenses with Gradient Redirection",
      "abstract": "Model stealing attacks present a dilemma for public machine learning APIs. To protect financial investments, companies may be forced to withhold important information about their models that could facilitate theft, including uncertainty estimates and prediction explanations. This compromise is harmful not only to users but also to external transparency. Model stealing defenses seek to resolve this dilemma by making models harder to steal while preserving utility for benign users. However, existing defenses have poor performance in practice, either requiring enormous computational overheads or severe utility trade-offs. To meet these challenges, we present a new approach to model stealing defenses called gradient redirection. At the core of our approach is a provably optimal, efficient algorithm for steering an adversary's training updates in a targeted manner. Combined with improvements to surrogate networks and a novel coordinated defense strategy, our gradient redirection defense, called GRAD${}^2$, achieves small utility trade-offs and low computational overhead, outperforming the best prior defenses. Moreover, we demonstrate how gradient redirection enables reprogramming the adversary with arbitrary behavior, which we hope will foster work on new avenues of defense.",
      "year": 2022,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Mantas Mazeika",
        "B. Li",
        "David A. Forsyth"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/af72f901d7a0f2edca55ee008c8893ae93b09971",
      "pdf_url": "https://arxiv.org/pdf/2206.14157",
      "publication_date": "2022-06-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ecfeeca2b8ed651efe27dc45bb8a0e4901e9f756",
      "title": "Side-Channel Attack Analysis on In-Memory Computing Architectures",
      "abstract": "In-memory computing (IMC) systems have great potential for accelerating data-intensive tasks such as deep neural networks (DNNs). As DNN models are generally highly proprietary, the neural network architectures become valuable targets for attacks. In IMC systems, since the whole model is mapped on chip and weight memory read can be restricted, the pre-mapped DNN model acts as a \u201cblack box\u201d for users. However, the localized and stationary weight and data patterns may subject IMC systems to other attacks. In this article, we propose a side-channel attack methodology on IMC architectures. We show that it is possible to extract model architectural information from power trace measurements without any prior knowledge of the neural network. We first developed a simulation framework that can emulate the dynamic power traces of the IMC macros. We then performed side-channel leakage analysis to reverse engineer model information such as the stored layer type, layer sequence, output channel/feature size and convolution kernel size from power traces of the IMC macros. Based on the extracted information, full networks can potentially be reconstructed without any knowledge of the neural network. Finally, we discuss potential countermeasures for building IMC systems that offer resistance to these model extraction attack.",
      "year": 2022,
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": [
        "Ziyu Wang",
        "Fanruo Meng",
        "Yongmo Park",
        "J. Eshraghian",
        "Wei D. Lu"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/ecfeeca2b8ed651efe27dc45bb8a0e4901e9f756",
      "pdf_url": "https://arxiv.org/pdf/2209.02792",
      "publication_date": "2022-09-06",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8bdb27ba98f457549bb7e03d6aa2d5c54a4de79e",
      "title": "On the Difficulty of Defending Self-Supervised Learning against Model Extraction",
      "abstract": "Self-Supervised Learning (SSL) is an increasingly popular ML paradigm that trains models to transform complex inputs into representations without relying on explicit labels. These representations encode similarity structures that enable efficient learning of multiple downstream tasks. Recently, ML-as-a-Service providers have commenced offering trained SSL models over inference APIs, which transform user inputs into useful representations for a fee. However, the high cost involved to train these models and their exposure over APIs both make black-box extraction a realistic security threat. We thus explore model stealing attacks against SSL. Unlike traditional model extraction on classifiers that output labels, the victim models here output representations; these representations are of significantly higher dimensionality compared to the low-dimensional prediction scores output by classifiers. We construct several novel attacks and find that approaches that train directly on a victim's stolen representations are query efficient and enable high accuracy for downstream models. We then show that existing defenses against model extraction are inadequate and not easily retrofitted to the specificities of SSL.",
      "year": 2022,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Adam Dziedzic",
        "Nikita Dhawan",
        "Muhammad Ahmad Kaleem",
        "Jonas Guan",
        "Nicolas Papernot"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/8bdb27ba98f457549bb7e03d6aa2d5c54a4de79e",
      "pdf_url": "http://arxiv.org/pdf/2205.07890",
      "publication_date": "2022-05-16",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a471ef4fae2c4748eac0c13c4b434e7f701c3252",
      "title": "Efficient Query-based Black-box Attack against Cross-modal Hashing Retrieval",
      "abstract": "Deep cross-modal hashing retrieval models inherit the vulnerability of deep neural networks. They are vulnerable to adversarial attacks, especially for the form of subtle perturbations to the inputs. Although many adversarial attack methods have been proposed to handle the robustness of hashing retrieval models, they still suffer from two problems: (1) Most of them are based on the white-box settings, which is usually unrealistic in practical application. (2) Iterative optimization for the generation of adversarial examples in them results in heavy computation. To address these problems, we propose an Efficient Query-based Black-Box Attack (EQB2A) against deep cross-modal hashing retrieval, which can efficiently generate adversarial examples for the black-box attack. Specifically, by sending a few query requests to the attacked retrieval system, the cross-modal retrieval model stealing is performed based on the neighbor relationship between the retrieved results and the query, thus obtaining the knockoffs to substitute the attacked system. A multi-modal knockoffs-driven adversarial generation is proposed to achieve efficient adversarial example generation. While the entire network training converges, EQB2A can efficiently generate adversarial examples by forward-propagation with only given benign images. Experiments show that EQB2A achieves superior attacking performance under the black-box setting.",
      "year": 2022,
      "venue": "ACM Trans. Inf. Syst.",
      "authors": [
        "Lei Zhu",
        "Tianshi Wang",
        "Jingjing Li",
        "Zheng Zhang",
        "Jialie Shen",
        "Xinhua Wang"
      ],
      "citation_count": 31,
      "url": "https://www.semanticscholar.org/paper/a471ef4fae2c4748eac0c13c4b434e7f701c3252",
      "pdf_url": "",
      "publication_date": "2022-09-03",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a4c748225360d89a6b03e1f277daddb64f429bcc",
      "title": "Imitated Detectors: Stealing Knowledge of Black-box Object Detectors",
      "abstract": "Deep neural networks have shown great potential in many practical applications, yet their knowledge is at the risk of being stolen via exposed services (\\eg APIs). In contrast to the commonly-studied classification model extraction, there exist no studies on the more challenging object detection task due to the sufficiency and efficiency of problem domain data collection. In this paper, we for the first time reveal that black-box victim object detectors can be easily replicated without knowing the model structure and training data. In particular, we treat it as black-box knowledge distillation and propose a teacher-student framework named Imitated Detector to transfer the knowledge of the victim model to the imitated model. To accelerate the problem domain data construction, we extend the problem domain dataset by generating synthetic images, where we apply the text-image generation process and provide short text inputs consisting of object categories and natural scenes; to promote the feedback information, we aim to fully mine the latent knowledge of the victim model by introducing an iterative adversarial attack strategy, where we feed victim models with transferable adversarial examples making victim provide diversified predictions with more information. Extensive experiments on multiple datasets in different settings demonstrate that our approach achieves the highest model extraction accuracy and outperforms other model stealing methods by large margins in the problem domain dataset. Our codes can be found at \\urlhttps://github.com/LiangSiyuan21/Imitated-Detectors.",
      "year": 2022,
      "venue": "ACM Multimedia",
      "authors": [
        "Siyuan Liang",
        "Aishan Liu",
        "Jiawei Liang",
        "Longkang Li",
        "Yang Bai",
        "Xiaochun Cao"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/a4c748225360d89a6b03e1f277daddb64f429bcc",
      "pdf_url": "",
      "publication_date": "2022-10-10",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fa8e84e2091105083f3b697158073f8896c7fb4c",
      "title": "Hybrid Annealing Method Based on subQUBO Model Extraction With Multiple Solution Instances",
      "abstract": "Ising machines are expected to solve combinatorial optimization problems efficiently by representing them as Ising models or equivalent quadratic unconstrained binary optimization (QUBO) models . However, upper bound exists on the computable problem size due to the hardware limitations of Ising machines. This paper propose a new hybrid annealing method based on partial QUBO extraction, called subQUBO model extraction, with multiple solution instances. For a given QUBO model, the proposed method obtains <inline-formula><tex-math notation=\"LaTeX\">$N_I$</tex-math><alternatives><mml:math><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq1-3138629.gif\"/></alternatives></inline-formula> quasi-optimal solutions (quasi-ground-state solutions) in some way using a classical computer. The solutions giving these quasi-optimal solutions are called <italic>solution instances</italic>. We extract a size-limited subQUBO model as follows based on a strong theoretical background: we randomly select <inline-formula><tex-math notation=\"LaTeX\">$N_S$</tex-math><alternatives><mml:math><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq2-3138629.gif\"/></alternatives></inline-formula> <inline-formula><tex-math notation=\"LaTeX\">$(N_S<N_I)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo><</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"atobe-ieq3-3138629.gif\"/></alternatives></inline-formula> solution instances among them and focus on a particular binary variable <inline-formula><tex-math notation=\"LaTeX\">$x_i$</tex-math><alternatives><mml:math><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq4-3138629.gif\"/></alternatives></inline-formula> in the <inline-formula><tex-math notation=\"LaTeX\">$N_S$</tex-math><alternatives><mml:math><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq5-3138629.gif\"/></alternatives></inline-formula> solution instances. If <inline-formula><tex-math notation=\"LaTeX\">$x_i$</tex-math><alternatives><mml:math><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq6-3138629.gif\"/></alternatives></inline-formula> value is much <italic>varied</italic> over <inline-formula><tex-math notation=\"LaTeX\">$N_S$</tex-math><alternatives><mml:math><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"atobe-ieq7-3138629.gif\"/></alternatives></inline-formula> solution instances, it is included in the subQUBO model; otherwise, it is not. We find a (quasi-)ground-state solution of the extracted subQUBO model using an Ising machine and add it as a new solution instance. By repeating this process, we can finally obtain a (quasi-)ground-state solution of the original QUBO model. Experimental evaluations confirm that the proposed method can obtain better quasi-ground-state solution than existing methods for large-sized QUBO models.",
      "year": 2022,
      "venue": "IEEE transactions on computers",
      "authors": [
        "Yuta Atobe",
        "Masashi Tawada",
        "N. Togawa"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fa8e84e2091105083f3b697158073f8896c7fb4c",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/12/9880482/09664360.pdf",
      "publication_date": "2022-10-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bf87c3c380802df24628cab8f6dff90b42304f77",
      "title": "Split HE: Fast Secure Inference Combining Split Learning and Homomorphic Encryption",
      "abstract": "This work presents a novel protocol for fast secure inference of neural networks applied to computer vision applications. It focuses on improving the overall performance of the online execution by deploying a subset of the model weights in plaintext on the client's machine, in the fashion of SplitNNs. We evaluate our protocol on benchmark neural networks trained on the CIFAR-10 dataset using SEAL via TenSEAL and discuss runtime and security performances. Empirical security evaluation using Membership Inference and Model Extraction attacks showed that the protocol was more resilient under the same attacks than a similar approach also based on SplitNN. When compared to related work, we demonstrate improvements of 2.5x-10x for the inference time and 14x-290x in communication costs.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "George-Liviu Pereteanu",
        "A. Alansary",
        "Jonathan Passerat-Palmbach"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/bf87c3c380802df24628cab8f6dff90b42304f77",
      "pdf_url": "",
      "publication_date": "2022-02-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "51b8619bcac38d6f1880a313a4af212aac92f71c",
      "title": "MOVE: Effective and Harmless Ownership Verification via Embedded External Features",
      "abstract": "Currently, deep neural networks (DNNs) are widely adopted in different applications. Despite its commercial values, training a well-performing DNN is resource-consuming. Accordingly, the well-trained model is valuable intellectual property for its owner. However, recent studies revealed the threats of model stealing, where the adversaries can obtain a function-similar copy of the victim model, even when they can only query the model. In this paper, we propose an effective and harmless model ownership verification (MOVE) to defend against different types of model stealing simultaneously, without introducing new security risks. In general, we conduct the ownership verification by verifying whether a suspicious model contains the knowledge of defender-specified external features. Specifically, we embed the external features by modifying a few training samples with style transfer. We then train a meta-classifier to determine whether a model is stolen from the victim. This approach is inspired by the understanding that the stolen models should contain the knowledge of features learned by the victim model. In particular, we develop our MOVE method under both glass-boxand closed-box settings and analyze its theoretical foundation to provide comprehensive model protection. Extensive experiments on benchmark datasets verify the effectiveness of our method and its resistance to potential adaptive attacks.",
      "year": 2022,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Yiming Li",
        "Linghui Zhu",
        "Xiaojun Jia",
        "Yang Bai",
        "Yong Jiang",
        "Shutao Xia",
        "Xiaochun Cao"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/51b8619bcac38d6f1880a313a4af212aac92f71c",
      "pdf_url": "",
      "publication_date": "2022-08-04",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e8572d9722e992a770b7c00a8419dda17297a9da",
      "title": "Side-Channel Fuzzy Analysis-Based AI Model Extraction Attack With Information-Theoretic Perspective in Intelligent IoT",
      "abstract": "Accessibility to smart devices provides opportunities for side-channel attacks (SCAs) on artificial intelligent (AI) models in the intelligent Internet of Things (IoT). However, the existing literature exposes some shortcomings: 1) incapability of quantifying and analyzing the leaked information through side channels of the intelligent IoT and 2) inability to devise efficient and accurate SCA algorithms. To address these challenges, we propose a side-channel fuzzy analysis-empowered AI model extraction attack in the intelligent IoT. First, the integrated AI model extraction framework is proposed, including power trace-based structure, execution time-based metaparameters, and hierarchical weight extractions. Then, we develop the information theory-based analysis for the AI model extraction via SCA. We derive a mutual information-enabled quantification method, theoretical lower/upper bounds of information leakage, and the minimum number of attack queries to obtain accurate weights. Furthermore, a fuzzy gray correlation-based multiple-microspace parallel SCA algorithm is proposed to extract model weights in the intelligent IoT. Based on the established information-theoretic analysis model, the proposed fuzzy gray correlation-based SCA algorithm obtains high-precision AI weights. Experimental results, consisting of simulation and real-world experiments, verify that the developed analysis method with the information-theoretic perspective is feasible and demonstrate that the designed fuzzy gray correlation-based SCA algorithm is effective for AI model extraction.",
      "year": 2022,
      "venue": "IEEE transactions on fuzzy systems",
      "authors": [
        "Qianqian Pan",
        "Jun Wu",
        "A. Bashir",
        "Jianhua Li",
        "Jie Wu"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/e8572d9722e992a770b7c00a8419dda17297a9da",
      "pdf_url": "",
      "publication_date": "2022-11-01",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bb6cf8210bb8035e71557dfb45d0170d909ced1f",
      "title": "Towards explainable model extraction attacks",
      "abstract": "One key factor able to boost the applications of artificial intelligence (AI) in security\u2010sensitive domains is to leverage them responsibly, which is engaged in providing explanations for AI. To date, a plethora of explainable artificial intelligence (XAI) has been proposed to help users interpret model decisions. However, given its data\u2010driven nature, the explanation itself is potentially susceptible to a high risk of exposing privacy. In this paper, we first show that the existing XAI is vulnerable to model extraction attacks and then present an XAI\u2010aware dual\u2010task model extraction attack (DTMEA). DTMEA can attack a target model with explanation services, that is, it can extract both the classification and explanation tasks of the target model. More specifically, the substitution model extracted by DTMEA is a multitask learning architecture, consisting of a sharing layer and two task\u2010specific layers for classification and explanation. To reveal which explanation technologies are more vulnerable to expose privacy information, we conduct an empirical evaluation of four major explanation types in the benchmark data set. Experimental results show that the attack accuracy of DTMEA outperforms the predicted\u2010only method with up to 1.25%, 1.53%, 9.25%, and 7.45% in MNIST, Fashion\u2010MNIST, CIFAR\u201010, and CIFAR\u2010100, respectively. By exposing the potential threats on explanation technologies, our research offers the insights to develop effective tools that are able to trade off security\u2010sensitive relationships.",
      "year": 2022,
      "venue": "International Journal of Intelligent Systems",
      "authors": [
        "Anli Yan",
        "Ruitao Hou",
        "Xiaozhang Liu",
        "Hongyang Yan",
        "Teng Huang",
        "Xianmin Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/bb6cf8210bb8035e71557dfb45d0170d909ced1f",
      "pdf_url": "",
      "publication_date": "2022-09-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "52c28b417cf9e0b11d7d49a2600459a7fa0041e4",
      "title": "On the Effectiveness of Dataset Watermarking",
      "abstract": "In a data-driven world, datasets constitute a significant economic value. Dataset owners who spend time and money to collect and curate the data are incentivized to ensure that their datasets are not used in ways that they did not authorize. When such misuse occurs, dataset owners need technical mechanisms for demonstrating their ownership of the dataset in question. Dataset watermarking provides one approach for ownership demonstration which can, in turn, deter unauthorized use. In this paper, we investigate a recently proposed data provenance method, radioactive data, to assess if it can be used to demonstrate ownership of (image) datasets used to train machine learning (ML) models. The original paper radioactive reported that radioactive data is effective in white-box settings. We show that while this is true for large datasets with many classes, it is not as effective for datasets where the number of classes is low (\u0142eq 30) or the number of samples per class is low (\u0142eq 500). We also show that, counter-intuitively, the black-box verification technique described in radioactive is effective for all datasets used in this paper, even when white-box verification in radioactive is not. Given this observation, we show that the confidence in white-box verification can be improved by using watermarked samples directly during the verification process. We also highlight the need to assess the robustness of radioactive data if it were to be used for ownership demonstration since it is an adversarial setting unlike provenance identification. Compared to dataset watermarking, ML model watermarking has been explored more extensively in recent literature. However, most of the state-of-the-art model watermarking techniques can be defeated via model extraction robustness. We show that radioactive data can effectively survive model extraction attacks, which raises the possibility that it can be used for ML model ownership verification robust against model extraction.",
      "year": 2022,
      "venue": "IWSPA@CODASPY",
      "authors": [
        "Buse Gul",
        "Atli Tekgul",
        "N. Asokan"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/52c28b417cf9e0b11d7d49a2600459a7fa0041e4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3510548.3519376",
      "publication_date": "2022-02-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0bdf3349040e5b803e8ca3a9c2cacbd9c840b747",
      "title": "Clairvoyance: Exploiting Far-field EM Emanations of GPU to \"See\" Your DNN Models through Obstacles at a Distance",
      "abstract": "Deep neural networks (DNNs) are becoming increasingly popular in real-world applications, and they are considered valuable assets of enterprises. In recent years, a number of model extraction attacks have been formulated that can be mounted to successfully steal proprietary DNN models. Nevertheless, previous model extraction attacks require either logical access to the target models or physical access to the victim machines, and thus are not suitable for performing model stealing in scenarios where an outside attacker is in the proximity but at a distance.In this paper, we propose a new model extraction attack named Clairvoyance that exploits certain far-field electromagnetic signals emanated from a GPU to steal DNN models at a distance of several meters away from the victim machine even with some obstacles in-between. Using Clairvoyance, an attacker can effectively deduce DNN architectures (e.g., the number of layers and their types) and layer configurations (e.g., the number of kernels, sizes of layers, and sizes of strides). We use several case studies (e.g., VGG and ResNet) to demonstrate its effectiveness.",
      "year": 2022,
      "venue": "2022 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Sisheng Liang",
        "Zihao Zhan",
        "Fan Yao",
        "Long Cheng",
        "Zhenkai Zhang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/0bdf3349040e5b803e8ca3a9c2cacbd9c840b747",
      "pdf_url": "",
      "publication_date": "2022-05-01",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4c57688fde64650fe767a2ba57341520595b5b13",
      "title": "A Practical Introduction to Side-Channel Extraction of Deep Neural Network Parameters",
      "abstract": "Model extraction is a major threat for embedded deep neural network models that leverages an extended attack surface. Indeed, by physically accessing a device, an adversary may exploit side-channel leakages to extract critical information of a model (i.e., its architecture or internal parameters). Different adversarial objectives are possible including a fidelity-based scenario where the architecture and parameters are precisely extracted (model cloning). We focus this work on software implementation of deep neural networks embedded in a high-end 32-bit microcontroller (Cortex-M7) and expose several challenges related to fidelity-based parameters extraction through side-channel analysis, from the basic multiplication operation to the feed-forward connection through the layers. To precisely extract the value of parameters represented in the single-precision floating point IEEE-754 standard, we propose an iterative process that is evaluated with both simulations and traces from a Cortex-M7 target. To our knowledge, this work is the first to target such an high-end 32-bit platform. Importantly, we raise and discuss the remaining challenges for the complete extraction of a deep neural network model, more particularly the critical case of biases.",
      "year": 2022,
      "venue": "Smart Card Research and Advanced Application Conference",
      "authors": [
        "Raphael Joud",
        "Pierre-Alain Mo\u00ebllic",
        "S. Ponti\u00e9",
        "J. Rigaud"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/4c57688fde64650fe767a2ba57341520595b5b13",
      "pdf_url": "https://arxiv.org/pdf/2211.05590",
      "publication_date": "2022-11-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "44b068abddec3e0162670ca15ae1eeb2b247ee72",
      "title": "Model Stealing Defense against Exploiting Information Leak through the Interpretation of Deep Neural Nets",
      "abstract": "Model stealing techniques allow adversaries to create attack models that mimic the functionality of black-box machine learning models, querying only class membership or probability outcomes. Recently, interpretable AI is getting increasing attention, to enhance our understanding of AI models, provide additional information for diagnoses, or satisfy legal requirements. However, it has been recently reported that providing such additional information can make AI models more vulnerable to model stealing attacks. In this paper, we propose DeepDefense, the first defense mechanism that protects an AI model against model stealing attackers exploiting both class probabilities and interpretations. DeepDefense uses a misdirection model to hide the critical information of the original model against model stealing attacks, with minimal degradation on both the class probability and the interpretability of prediction output. DeepDefense is highly applicable for any model stealing scenario since it makes minimal assumptions about the model stealing adversary. In our experiments, DeepDefense shows significantly higher defense performance than the existing state-of-the-art defenses on various datasets and interpreters.",
      "year": 2022,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Jeonghyun Lee",
        "Sungmin Han",
        "Sangkyun Lee"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/44b068abddec3e0162670ca15ae1eeb2b247ee72",
      "pdf_url": "https://www.ijcai.org/proceedings/2022/0100.pdf",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee67b5e85769018e09d76345e648b29ec0cfa8b3",
      "title": "A Tutorial on Adversarial Learning Attacks and Countermeasures",
      "abstract": "Machine learning algorithms are used to construct a mathematical model for a system based on training data. Such a model is capable of making highly accurate predictions without being explicitly programmed to do so. These techniques have a great many applications in all areas of the modern digital economy and artificial intelligence. More importantly, these methods are essential for a rapidly increasing number of safety-critical applications such as autonomous vehicles and intelligent defense systems. However, emerging adversarial learning attacks pose a serious security threat that greatly undermines further such systems. The latter are classified into four types, evasion (manipulating data to avoid detection), poisoning (injection malicious training samples to disrupt retraining), model stealing (extraction), and inference (leveraging over-generalization on training data). Understanding this type of attacks is a crucial first step for the development of effective countermeasures. The paper provides a detailed tutorial on the principles of adversarial machining learning, explains the different attack scenarios, and gives an in-depth insight into the state-of-art defense mechanisms against this rising threat .",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Cato Pauling",
        "Michael Gimson",
        "Muhammed Qaid",
        "Ahmad Kida",
        "Basel Halak"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/ee67b5e85769018e09d76345e648b29ec0cfa8b3",
      "pdf_url": "",
      "publication_date": "2022-02-21",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5639133170aa4f4a598994518d34a9b375494876",
      "title": "A Systematic View of Model Leakage Risks in Deep Neural Network Systems",
      "abstract": "As deep neural networks (DNNs) continue to find applications in ever more domains, the exact nature of the neural network architecture becomes an increasingly sensitive subject, due to either intellectual property protection or risks of adversarial attacks. While prior work has explored aspects of the risk associated with model leakage, exactly which parts of the model are most sensitive and how one infers the full architecture of the DNN when nothing is known about the structure a priori are problems that have been left unexplored. In this paper we address this gap, first by presenting a schema for reasoning about model leakage holistically, and then by proposing and quantitatively evaluating DeepSniffer, a novel learning-based model extraction framework that uses no prior knowledge of the victim model. DeepSniffer is robust to architectural and system noises introduced by the complex memory hierarchy and diverse run-time system optimizations. Taking GPU platforms as a showcase, DeepSniffer performs model extraction by learning both the architecture-level execution features of kernels and the inter-layer temporal association information introduced by the common practice of DNN design. We demonstrate that DeepSniffer works experimentally in the context of an off-the-shelf Nvidia GPU platform running a variety of DNN models and that the extracted models significantly improve attempts at crafting adversarial inputs. The DeepSniffer project has been released in https://github.com/xinghu7788/DeepSniffer.",
      "year": 2022,
      "venue": "IEEE transactions on computers",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Xiaobing Chen",
        "Lei Deng",
        "Yu Ji",
        "Yufei Ding",
        "Zidong Du",
        "Qi Guo",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/5639133170aa4f4a598994518d34a9b375494876",
      "pdf_url": "https://doi.org/10.1109/tc.2022.3148235",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ec461790ee249f9df979ac014243291e3a40794f",
      "title": "On the amplification of security and privacy risks by post-hoc explanations in machine learning models",
      "abstract": "A variety of explanation methods have been proposed in recent years to help users gain insights into the results returned by neural networks, which are otherwise complex and opaque black-boxes. However, explanations give rise to potential side-channels that can be leveraged by an adversary for mounting attacks on the system. In particular, post-hoc explanation methods that highlight input dimensions according to their importance or relevance to the result also leak information that weakens security and privacy. In this work, we perform the first systematic characterization of the privacy and security risks arising from various popular explanation techniques. First, we propose novel explanation-guided black-box evasion attacks that lead to 10 times reduction in query count for the same success rate. We show that the adversarial advantage from explanations can be quantified as a reduction in the total variance of the estimated gradient. Second, we revisit the membership information leaked by common explanations. Contrary to observations in prior studies, via our modified attacks we show significant leakage of membership information (above 100% improvement over prior results), even in a much stricter black-box setting. Finally, we study explanation-guided model extraction attacks and demonstrate adversarial gains through a large reduction in query count.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Pengrui Quan",
        "Supriyo Chakraborty",
        "J. Jeyakumar",
        "Mani Srivastava"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/ec461790ee249f9df979ac014243291e3a40794f",
      "pdf_url": "https://arxiv.org/pdf/2206.14004",
      "publication_date": "2022-06-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4908fdc1feff153269670f0f8aac837346042553",
      "title": "Demystifying Arch-hints for Model Extraction: An Attack in Unified Memory System",
      "abstract": "The deep neural network (DNN) models are deemed confidential due to their unique value in expensive training efforts, privacy-sensitive training data, and proprietary network characteristics. Consequently, the model value raises incentive for adversary to steal the model for profits, such as the representative model extraction attack. Emerging attack can leverage timing-sensitive architecture-level events (i.e., Arch-hints) disclosed in hardware platforms to extract DNN model layer information accurately. In this paper, we take the first step to uncover the root cause of such Arch-hints and summarize the principles to identify them. We then apply these principles to emerging Unified Memory (UM) management system and identify three new Arch-hints caused by UM's unique data movement patterns. We then develop a new extraction attack, UMProbe. We also create the first DNN benchmark suite in UM and utilize the benchmark suite to evaluate UMProbe. Our evaluation shows that UMProbe can extract the layer sequence with an accuracy of 95% for almost all victim test models, which thus calls for more attention to the DNN security in UM system.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Zhendong Wang",
        "Xiaoming Zeng",
        "Xulong Tang",
        "Danfeng Zhang",
        "Xingbo Hu",
        "Yang Hu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/4908fdc1feff153269670f0f8aac837346042553",
      "pdf_url": "http://arxiv.org/pdf/2208.13720",
      "publication_date": "2022-08-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8a435e15c39efebf6cad521c15fff50fcf7e0bfb",
      "title": "Model Extraction Attack and Defense on Deep Generative Models",
      "abstract": "The security issues of machine learning have aroused much attention and model extraction attack is one of them. The definition of model extraction attack is that an adversary can collect data through query access to a victim model and train a substitute model with it in order to steal the functionality of the target model. At present, most of the related work has focused on the research of model extraction attack against discriminative models while this paper pays attention to deep generative models. First, considering the difference of an adversary` goals, the attacks are taxonomized into two different types: accuracy extraction attack and fidelity extraction attack and the effect is evaluated by 1-NN accuracy. Attacks among three main types of deep generative models and the influence of the number of queries are also researched. Finally, this paper studies different defensive techniques to safeguard the models according to their architectures.",
      "year": 2022,
      "venue": "Journal of Physics: Conference Series",
      "authors": [
        "Sheng Liu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/8a435e15c39efebf6cad521c15fff50fcf7e0bfb",
      "pdf_url": "https://doi.org/10.1088/1742-6596/2189/1/012024",
      "publication_date": "2022-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0e9b66bd253a7fd0934a72e350aeed54e0a92df4",
      "title": "Protecting Deep Neural Network Intellectual Property with Architecture-Agnostic Input Obfuscation",
      "abstract": "Deep Convolutional Neural Networks (DCNNs) have revolutionized and improved many aspects of modern life. However, these models are increasingly more complex, and training them to perform at desirable levels is difficult undertaking; hence, the trained parameters represent a valuable intellectual property (IP) asset which a motivated attacker may wish to steal. To better protect the IP, we propose a method of lightweight input obfuscation that is undone prior to inference, where input data is obfuscated in order to use the model to specification. Without using the correct key and unlocking sequence, the accuracy of the classifier is reduced to a random guess, thus protecting the input/output interface and mitigating model extraction attacks which rely on such access. We evaluate the system using a VGG-16 network trained on CIFAR-10, and demonstrate that with an incorrect deobfuscation key or sequence, the classification accuracy drops to a random guess, with an inference timing overhead of 4.4% on an Nvidia-based evaluation platform. The system avoids the costs associated with retraining and has no impact on model accuracy for authorized users.",
      "year": 2022,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Brooks Olney",
        "Robert Karam"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/0e9b66bd253a7fd0934a72e350aeed54e0a92df4",
      "pdf_url": "",
      "publication_date": "2022-06-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "eb39dda2df56270599f2a28bc6433c84c1704949",
      "title": "Extracted BERT Model Leaks More Information than You Think!",
      "abstract": "The collection and availability of big data, combined with advances in pre-trained models (e.g. BERT), have revolutionized the predictive performance of natural language processing tasks. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. Due to significant commercial interest, there has been a surge of attempts to steal remote services via model extraction. Although previous works have made progress in defending against model extraction attacks, there has been little discussion on their performance in preventing privacy leakage. This work bridges this gap by launching an attribute inference attack against the extracted BERT model. Our extensive experiments reveal that model extraction can cause severe privacy leakage even when victim models are facilitated with state-of-the-art defensive strategies.",
      "year": 2022,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Xuanli He",
        "Chen Chen",
        "L. Lyu",
        "Qiongkai Xu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eb39dda2df56270599f2a28bc6433c84c1704949",
      "pdf_url": "https://arxiv.org/pdf/2210.11735",
      "publication_date": "2022-10-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "54d3e2764c3445c89fbaa9c684c5f5d03cb44254",
      "title": "Play the Imitation Game: Model Extraction Attack against Autonomous Driving Localization",
      "abstract": "The security of the Autonomous Driving (AD) system has been gaining researchers\u2019 and public\u2019s attention recently. Given that AD companies have invested a huge amount of resources in developing their AD models, e.g., localization models, these models, especially their parameters, are important intellectual property and deserve strong protection. In this work, we examine whether the confidentiality of production-grade Multi-Sensor Fusion (MSF) models, in particular, Error-State Kalman Filter (ESKF), can be stolen from an outside adversary. We propose a new model extraction attack called TaskMaster that can infer the secret ESKF parameters under black-box assumption. In essence, TaskMaster trains a substitutional ESKF model to recover the parameters, by observing the input and output to the targeted AD system. To precisely recover the parameters, we combine a set of techniques, like gradient-based optimization, search-space reduction and multi-stage optimization. The evaluation result on real-world vehicle sensor dataset shows that TaskMaster is practical. For example, with 25 seconds AD sensor data for training, the substitutional ESKF model reaches centimeter-level accuracy, comparing with the ground-truth model.",
      "year": 2022,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Qifan Zhang",
        "Junjie Shen",
        "Mingtian Tan",
        "Zhe Zhou",
        "Zhou Li",
        "Qi Alfred Chen",
        "Haipeng Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/54d3e2764c3445c89fbaa9c684c5f5d03cb44254",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3564625.3567977",
      "publication_date": "2022-12-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f74980d18e194246618eca55faad7858fe57d08c",
      "title": "Leveraging Ferroelectric Stochasticity and In-Memory Computing for DNN IP Obfuscation",
      "abstract": "With the emergence of the Internet of Things (IoT), deep neural networks (DNNs) are widely used in different domains, such as computer vision, healthcare, social media, and defense. The hardware-level architecture of a DNN can be built using an in-memory computing-based design, which is loaded with the weights of a well-trained DNN model. However, such hardware-based DNN systems are vulnerable to model stealing attacks where an attacker reverse-engineers (REs) and extracts the weights of the DNN model. In this work, we propose an energy-efficient defense technique that combines a ferroelectric field effect transistor (FeFET)-based reconfigurable physically unclonable function (PUF) with an in-memory FeFET XNOR to thwart model stealing attacks. We leverage the inherent stochasticity in the FE domains to build a PUF that helps to corrupt the neural network\u2019s (NN) weights when an adversarial attack is detected. We showcase the efficacy of the proposed defense scheme by performing experiments on graph-NNs (GNNs), a particular type of DNN. The proposed defense scheme is a first of its kind that evaluates the security of GNNs. We investigate the effect of corrupting the weights on different layers of the GNN on the accuracy degradation of the graph classification application for two specific error models of corrupting the FeFET-based PUFs and five different bioinformatics datasets. We demonstrate that our approach successfully degrades the inference accuracy of the graph classification by corrupting any layer of the GNN after a small rewrite pulse.",
      "year": 2022,
      "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits",
      "authors": [
        "Likhitha Mankali",
        "N. Rangarajan",
        "Swetaki Chatterjee",
        "Shubham Kumar",
        "Y. Chauhan",
        "O. Sinanoglu",
        "Hussam Amrouch"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/f74980d18e194246618eca55faad7858fe57d08c",
      "pdf_url": "https://doi.org/10.1109/jxcdc.2022.3217043",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "46234fb6fcc8bb42dd46bf091e3cfad8da86993c",
      "title": "Automatic Nonlinear Nonquasi-Static Diode Model Extraction from Large-Signal Measurements",
      "abstract": "An automatic nonlinear nonquasi-static (NQS) model extraction method is proposed. The procedure is based on a NQS charge that depends on two quasi-static (QS) functions: electric charge and delay. The use of Nonlinear Function Sampling (NFS) operator leads to a crucial link between real and imaginary parts of the spectral coefficients of the QS state functions. This approach is used to build-in a CAD (Computer-Aided Design) model that takes into account these NQS effects for a diode. The excellent numerical results allow to be optimistic in the integration of extracted models into CAD tools looking for emulating diode experimental behaviour.",
      "year": 2022,
      "venue": "European Microwave Integrated Circuits Conference",
      "authors": [
        "A. Garc\u00eda-Luque",
        "T. Mart\u00edn-Guerrero",
        "A. Santarelli",
        "C. Camacho-Pe\u00f1alosa"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/46234fb6fcc8bb42dd46bf091e3cfad8da86993c",
      "pdf_url": "",
      "publication_date": "2022-04-03",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-28"
    },
    {
      "paper_id": "1761a6879ea8d5224aabe0e63424042dc3d901d8",
      "title": "Enhance Model Stealing Attack via Label Refining",
      "abstract": "With machine learning models being increasingly deployed, model stealing attacks have raised an increasing interest. Extracting decision-based models is a more challenging task with the information of class similarity missing. In this paper, we propose a novel and effective model stealing method as Label Refining via Feature Distance (LRFD), to re-dig the class similarity. Specifically, since the information of class similarity can be represented by the distance between samples from different classes in the feature space, we design a soft label construction module inspired by the prototype learning, and transfer the knowledge in the soft label to the substitution model. Extensive experiments conducted on four widely-used datasets consistently demonstrate that our method yields a model with significantly greater functional similarity to the victim model.",
      "year": 2022,
      "venue": "International Conference on the Software Process",
      "authors": [
        "Yixu Wang",
        "Xianming Lin"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1761a6879ea8d5224aabe0e63424042dc3d901d8",
      "pdf_url": "",
      "publication_date": "2022-04-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7cfae03e2dc7b411644acbe5d686b1a6384f4b5e",
      "title": "From copycat to copyright: intellectual property amendments and the development of Chinese online video industries",
      "abstract": "ABSTRACT Many commentators have in the past hailed the production in China of lower cost versions of famous Chinese and international cultural and media products, better known as a shanzhai (\u5c71\u5be8) form of production. Against that, this paper argues that there has been a significant move away from a copycat model in the Chinese creative industries, a trend which should be viewed within the context of China\u2019s obligations as a full member of the WTO. This paper argues that the way in which online video industries have developed and innovated over the last 14 years in China has changed in that online video industries are constantly mutating their business models in response to lawsuits for IP violations instead of simply aligning with existing regulations. By doing that, they are indirectly adapting their business models to local legislation relating to the protection of IP for domestic and international content.",
      "year": 2022,
      "venue": "The International Journal of Cultural Policy",
      "authors": [
        "Filippo Gilardi",
        "A. White",
        "Z. Chen",
        "Shuxin Cheng",
        "Wei Song",
        "Yifan Zhao"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7cfae03e2dc7b411644acbe5d686b1a6384f4b5e",
      "pdf_url": "https://www.tandfonline.com/doi/pdf/10.1080/10286632.2022.2040494?needAccess=true",
      "publication_date": "2022-02-21",
      "keywords_matched": [
        "copycat model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9463a4fc601ccdaa81a034f8d443718ec40330d6",
      "title": "Transformer-based Extraction of Deep Image Models",
      "abstract": "Model extraction attacks pose a threat to the security of ML models and to the privacy of the data used for training. Previous research has shown that such attacks can be either monetarily motivated to gain an edge over competitors or maliciously in order to mount subsequent attacks on the extracted model. In this paper, recent advances in the field of transformers are exploited to propose an attack tailored to the task of image classification that allows stealing complex convolutional neural network models without any knowledge of their architecture. The attack was performed on a range of datasets and target architectures to evaluate the robustness of the proposed attack. With only 100k queries, we were able to recover up to 99.2% of the black-box target network's accuracy on the test set. We conclude that it is possible to effectively steal complex neural networks with relatively little expertise and conventional means \u2013 even without knowledge of the target's architecture. Recently proposed defences have also been examined for their effectiveness in preventing the attack proposed in this paper.",
      "year": 2022,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Verena Battis",
        "A. Penner"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/9463a4fc601ccdaa81a034f8d443718ec40330d6",
      "pdf_url": "",
      "publication_date": "2022-06-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
      "title": "MEGA: Model Stealing via Collaborative Generator-Substitute Networks",
      "abstract": "Deep machine learning models are increasingly deployedin the wild for providing services to users. Adversaries maysteal the knowledge of these valuable models by trainingsubstitute models according to the inference results of thetargeted deployed models. Recent data-free model stealingmethods are shown effective to extract the knowledge of thetarget model without using real query examples, but they as-sume rich inference information, e.g., class probabilities andlogits. However, they are all based on competing generator-substitute networks and hence encounter training instability.In this paper we propose a data-free model stealing frame-work,MEGA, which is based on collaborative generator-substitute networks and only requires the target model toprovide label prediction for synthetic query examples. Thecore of our method is a model stealing optimization con-sisting of two collaborative models (i) the substitute modelwhich imitates the target model through the synthetic queryexamples and their inferred labels and (ii) the generatorwhich synthesizes images such that the confidence of thesubstitute model over each query example is maximized. Wepropose a novel coordinate descent training procedure andanalyze its convergence. We also empirically evaluate thetrained substitute model on three datasets and its applicationon black-box adversarial attacks. Our results show that theaccuracy of our trained substitute model and the adversarialattack success rate over it can be up to 33% and 40% higherthan state-of-the-art data-free black-box attacks.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Chi Hong",
        "Jiyue Huang",
        "L. Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
      "pdf_url": "",
      "publication_date": "2022-01-31",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b18ab575ec113d117bb2482243c412cceb544756",
      "title": "Data-free Defense of Black Box Models Against Adversarial Attacks",
      "abstract": "Several companies often safeguard their trained deep models (i.e. details of architecture, learnt weights, training details etc.) from third-party users by exposing them only as \u2018black boxes' through APIs. Moreover, they may not even provide access to the training data due to proprietary reasons or sensitivity concerns. In this work, we propose a novel defense mechanism for black box models against adversarial attacks in a data-free set up. We construct synthetic data via a generative model and train surrogate network using model stealing techniques. To minimize adversarial contamination on perturbed samples, we propose \u2018wavelet noise remover' (WNR) that performs discrete wavelet decomposition on input images and carefully select only a few important coefficients determined by our \u2018wavelet coefficient selection module' (WCSM). To recover the high-frequency content of the image after noise removal via WNR, we further train a \u2018regenerator' network with an objective to retrieve the coefficients such that the reconstructed image yields similar to original predictions on the surrogate model. At test time, WNR combined with trained regenerator network is prepended to the black box network, resulting in a high boost in adversarial accuracy. Our method improves the adversarial accuracy on CIFAR-10 by 38.98% and 32.01% against the state-of-the-art Auto Attack compared to baseline, even when the attacker uses surrogate architecture (Alexnet-half and Alexnet) similar to the black box architecture (Alexnet) with same model stealing strategy as defender.",
      "year": 2022,
      "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "Gaurav Kumar Nayak",
        "Inder Khatri",
        "Shubham Randive",
        "Ruchit Rawal",
        "Anirban Chakraborty"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b18ab575ec113d117bb2482243c412cceb544756",
      "pdf_url": "https://arxiv.org/pdf/2211.01579",
      "publication_date": "2022-11-03",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b778d465e9223b3a54601033000f61c985b00849",
      "title": "SEEK: model extraction attack against hybrid secure inference protocols",
      "abstract": "Security concerns about a machine learning model used in a prediction-as-a-service include the privacy of the model, the query and the result. Secure inference solutions based on homomorphic encryption (HE) and/or multiparty computation (MPC) have been developed to protect all the sensitive information. One of the most efficient type of solution utilizes HE for linear layers, and MPC for non-linear layers. However, for such hybrid protocols with semi-honest security, an adversary can malleate the intermediate features in the inference process, and extract model information more effectively than methods against inference service in plaintext. In this paper, we propose SEEK, a general extraction method for hybrid secure inference services outputing only class labels. This method can extract each layer of the target model independently, and is not affected by the depth of the model. For ResNet-18, SEEK can extract a parameter with less than 50 queries on average, with average error less than $0.03\\%$.",
      "year": 2022,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Si-Quan Chen",
        "Junfeng Fan"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b778d465e9223b3a54601033000f61c985b00849",
      "pdf_url": "http://arxiv.org/pdf/2209.06373",
      "publication_date": "2022-09-14",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16af85e1e119a3d5d0e44299cd1a34d27a49568a",
      "title": "Holistic risk assessment of inference attacks in machine learning",
      "abstract": "As machine learning expanding application, there are more and more unignorable privacy and safety issues. Especially inference attacks against Machine Learning models allow adversaries to infer sensitive information about the target model, such as training data, model parameters, etc. Inference attacks can lead to serious consequences, including violating individuals privacy, compromising the intellectual property of the owner of the machine learning model. As far as concerned, researchers have studied and analyzed in depth several types of inference attacks, albeit in isolation, but there is still a lack of a holistic rick assessment of inference attacks against machine learning models, such as their application in different scenarios, the common factors affecting the performance of these attacks and the relationship among the attacks. As a result, this paper performs a holistic risk assessment of different inference attacks against Machine Learning models. This paper focuses on three kinds of representative attacks: membership inference attack, attribute inference attack and model stealing attack. And a threat model taxonomy is established. A total of 12 target models using three model architectures, including AlexNet, ResNet18 and Simple CNN, are trained on four datasets, namely CelebA, UTKFace, STL10 and FMNIST.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Yang Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/16af85e1e119a3d5d0e44299cd1a34d27a49568a",
      "pdf_url": "http://arxiv.org/pdf/2212.10628",
      "publication_date": "2022-12-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "87333028a8169e957d1d74b4054ceac9b3d2116a",
      "title": "An Adversarial Learning-based Tor Malware Traffic Detection Model",
      "abstract": "Attackers often use Tor to launch cyberattacks and conduct illegal transactions, threatening cyberspace's security and people's daily lives. Existing methods for malware traffic detection on Tor can be classified as rule-based and network-based, both of which apply machine learning extensively. Tor malware traffic detection systems are often deployed in open network environments. Their machine learning systems are the first to be attacked by adversarial samples. To ensure that Tor is not abused, this paper proposes an Adversarial Learning-based Tor Malware Traffic Detection model, AL-TMTD. We generate realistic attack samples that can evade detection and use these samples to produce an augmented training set for producing hardened detectors. In such a way, we obtain a more resilient Tor malware traffic detection model that achieves adversarial robustness. We validate our proposal through an extensive experimental campaign that considers multiple machine learning algorithms and shadow models. We simulate the adversary to construct functionally approximate shadow models through black-box model extraction and generate adversarial samples to validate the adversarial robustness of our proposed AL-TMTD model. Our experimental results demonstrate that the average accuracy of AL-TMTD after the adversarial retraining is as high as 0.995 in detecting adversarial samples, which is 0.314 without the adversarial retraining, a significant improvement.",
      "year": 2022,
      "venue": "Global Communications Conference",
      "authors": [
        "Xiaoyan Hu",
        "Yishu Gao",
        "Guang Cheng",
        "Hua Wu",
        "Ruidong Li"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/87333028a8169e957d1d74b4054ceac9b3d2116a",
      "pdf_url": "",
      "publication_date": "2022-12-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f1f33c9654649947d2dffb02c12fc36c5fd5116e",
      "title": "Model Extraction Attack against Self-supervised Speech Models",
      "abstract": "Self-supervised learning (SSL) speech models generate meaningful representations of given clips and achieve incredible performance across various downstream tasks. Model extraction attack (MEA) often refers to an adversary stealing the functionality of the victim model with only query access. In this work, we study the MEA problem against SSL speech model with a small number of queries. We propose a two-stage framework to extract the model. In the first stage, SSL is conducted on the large-scale unlabeled corpus to pre-train a small speech model. Secondly, we actively sample a small portion of clips from the unlabeled corpus and query the target model with these clips to acquire their representations as labels for the small model's second-stage training. Experiment results show that our sampling methods can effectively extract the target model without knowing any information about its model architecture.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Tsung-Yuan Hsu",
        "Chen-An Li",
        "Tung-Yu Wu",
        "Hung-yi Lee"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f1f33c9654649947d2dffb02c12fc36c5fd5116e",
      "pdf_url": "https://arxiv.org/pdf/2211.16044",
      "publication_date": "2022-11-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5e7c67c4e963ed3db69401d34ea5dd5a78f8da45",
      "title": "Adversarial Training of Anti-Distilled Neural Network with Semantic Regulation of Class Confidence",
      "abstract": "Knowledge distillation (KD) has been identified as an effective knowledge transfer approach. By learning from the outputs of a pre-trained, over-parameterized teacher network, a compact student network can be trained efficiently to achieve superior performance. Although KD has gained substantial successes, exposure to pre-trained models usually causes potential risks of intellectual property leaks. From a model stealing attacker\u2019s perspective, one can easily mimic the model functionality via KD, resulting in huge financial loss. In this paper, we propose a novel adversarial training framework called semantic nasty teacher, which prevents the teacher model from being copied by the attacker. In specific, we disentangle the semantic relationship in the output logits when training the teacher model, which is the key to success in KD. Experiment results show that neural networks trained with our approach only sacrifices little performance while canceling out the probability of KD-based model stealing.",
      "year": 2022,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "Z. Wang",
        "Chengcheng Li",
        "Husheng Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/5e7c67c4e963ed3db69401d34ea5dd5a78f8da45",
      "pdf_url": "",
      "publication_date": "2022-10-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fcd73d7d8fce02e55b95a5903310667367d8e64f",
      "title": "Generative Extraction of Audio Classifiers for Speaker Identification",
      "abstract": "It is perhaps no longer surprising that machine learning models, especially deep neural networks, are particularly vulnerable to attacks. One such vulnerability that has been well studied is model extraction: a phenomenon in which the attacker attempts to steal a victim's model by training a surrogate model to mimic the decision boundaries of the victim model. Previous works have demonstrated the effectiveness of such an attack and its devastating consequences, but much of this work has been done primarily for image and text processing tasks. Our work is the first attempt to perform model extraction on {\\em audio classification models}. We are motivated by an attacker whose goal is to mimic the behavior of the victim's model trained to identify a speaker. This is particularly problematic in security-sensitive domains such as biometric authentication. We find that prior model extraction techniques, where the attacker \\textit{naively} uses a proxy dataset to attack a potential victim's model, fail. We therefore propose the use of a generative model to create a sufficiently large and diverse pool of synthetic attack queries. We find that our approach is able to extract a victim's model trained on \\texttt{LibriSpeech} using queries synthesized with a proxy dataset based off of \\texttt{VoxCeleb}; we achieve a test accuracy of 84.41\\% with a budget of 3 million queries.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Tejumade Afonja",
        "Lucas Bourtoule",
        "Varun Chandrasekaran",
        "Sageev Oore",
        "Nicolas Papernot"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/fcd73d7d8fce02e55b95a5903310667367d8e64f",
      "pdf_url": "http://arxiv.org/pdf/2207.12816",
      "publication_date": "2022-07-26",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
      "title": "Verify Deep Learning Models Ownership via Preset Embedding",
      "abstract": "A well-trained deep neural network (DNNs) requires massive computing resources and data, therefore it belongs to the model owners\u2019 Intellectual Property (IP). Recent works have shown that the model can be stolen by the adversary without any training data or internal parameters of the model. Currently, there were some defense methods to resist it, by increasing the cost of model stealing attack or detecting the theft afterwards.In this paper, We propose a method to determine theft by detecting whether the victim\u2019s preset embedding exists in the adversary model. Firstly, we convert some training images into grayscale images as embedding and inject them to the training set. Then, we train a binary classifier to determine whether the model is stolen from the victim. The main intuition behind our approach is that the stolen model should contain embedded knowledge learned by the victim model. Our results demonstrate that our method is effective in defending against different types of model theft methods.",
      "year": 2022,
      "venue": "2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)",
      "authors": [
        "Wenxuan Yin",
        "Hai-feng Qian"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5018c07b5581cf9398173b7311c02ad085b6349e",
      "title": "An Enhanced Adversarial Attacks Method on Power System Based on Model Extraction Algorithm",
      "abstract": "Artificial intelligence algorithms fit connections between features and problems from a data-driven perspective. Artificial intelligence algorithms perform iterative training of data through neural network, which provides a new thinking dimension for researchers. The researches on the power grid field are no longer limited to modeling and analysis through traditional physical mechanism methods. However, there are security risks on the neural network model established by artificial intelligence algorithms. Attackers apply model extraction attacks to structure a substitute model of target power system model, which supports other attack algorithms to attack the power system and ultimately affects the normal operation of power system. This paper proposes an enhancement method for adversarial attacks on power system based on model extraction algorithm and tests the promoting effect of adversarial sample attack in various scenarios.",
      "year": 2022,
      "venue": "2022 IEEE 6th Conference on Energy Internet and Energy System Integration (EI2)",
      "authors": [
        "Yucheng Ma",
        "Qi Wang",
        "Zengji Liu",
        "Chao Hong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5018c07b5581cf9398173b7311c02ad085b6349e",
      "pdf_url": "",
      "publication_date": "2022-11-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fb6802bbd6f4f67fbaafac41ba31697712b8e525",
      "title": "Revealing Secrets From Pre-trained Models",
      "abstract": "\u2014With the growing burden of training deep learning models with large data sets, transfer-learning has been widely adopted in many emerging deep learning algorithms. Trans- former models such as BERT are the main player in natural language processing and use transfer-learning as a de facto standard training method. A few big data companies release pre-trained models that are trained with a few popular datasets with which end users and researchers \ufb01ne-tune the model with their own datasets. Transfer-learning signi\ufb01cantly reduces the time and effort of training models. However, it comes at the cost of security concerns. In this paper, we show a new observation that pre-trained models and \ufb01ne-tuned models have signi\ufb01cantly high similarities in weight values. Also, we demonstrate that there exist vendor-speci\ufb01c computing patterns even for the same models. With these new \ufb01ndings, we propose a new model extraction attack that reveals the model architecture and the pre-trained model used by the black-box victim model with vendor-speci\ufb01c computing patterns and then estimates the entire model weights based on the weight value similarities between the \ufb01ne-tuned model and pre-trained model. We also show that the weight similarity can be leveraged for increasing the model extraction feasibility through a novel weight extraction pruning. ,",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Mujahid Al Rafi",
        "Yuan Feng",
        "Hyeran Jeon"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fb6802bbd6f4f67fbaafac41ba31697712b8e525",
      "pdf_url": "http://arxiv.org/pdf/2207.09539",
      "publication_date": "2022-07-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4df5d33673ee1f05d8bca5be3cfc1ed6c18a0745",
      "title": "Adversarial Attacks Against Network Intrusion Detection in IoT Systems",
      "abstract": "Deep learning (DL) has gained popularity in network intrusion detection, due to its strong capability of recognizing subtle differences between normal and malicious network activities. Although a variety of methods have been designed to leverage DL models for security protection, whether these systems are vulnerable to adversarial examples (AEs) is unknown. In this article, we design a novel adversarial attack against DL-based network intrusion detection systems (NIDSs) in the Internet-of-Things environment, with only black-box accesses to the DL model in such NIDS. We introduce two techniques: 1) model extraction is adopted to replicate the black-box model with a small amount of training data and 2) a saliency map is then used to disclose the impact of each packet attribute on the detection results, and the most critical features. This enables us to efficiently generate AEs using conventional methods. With these tehniques, we successfully compromise one state-of-the-art NIDS, Kitsune: the adversary only needs to modify less than 0.005% of bytes in the malicious packets to achieve an average 94.31% attack success rate.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Han Qiu",
        "Tian Dong",
        "Tianwei Zhang",
        "Jialiang Lu",
        "G. Memmi",
        "Meikang Qiu"
      ],
      "citation_count": 213,
      "url": "https://www.semanticscholar.org/paper/4df5d33673ee1f05d8bca5be3cfc1ed6c18a0745",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9bb14ee43eb7607e0bf95bde3fa62882a676eb5d",
      "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
      "abstract": "Inference attacks against Machine Learning (ML) models allow adversaries to learn sensitive information about training data, model parameters, etc. While researchers have studied, in depth, several kinds of attacks, they have done so in isolation. As a result, we lack a comprehensive picture of the risks caused by the attacks, e.g., the different scenarios they can be applied to, the common factors that influence their performance, the relationship among them, or the effectiveness of possible defenses. In this paper, we fill this gap by presenting a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models. We concentrate on four attacks -- namely, membership inference, model inversion, attribute inference, and model stealing -- and establish a threat model taxonomy. Our extensive experimental evaluation, run on five model architectures and four image datasets, shows that the complexity of the training dataset plays an important role with respect to the attack's performance, while the effectiveness of model stealing and membership inference attacks are negatively correlated. We also show that defenses like DP-SGD and Knowledge Distillation can only mitigate some of the inference attacks. Our analysis relies on a modular re-usable software, ML-Doctor, which enables ML model owners to assess the risks of deploying their models, and equally serves as a benchmark tool for researchers and practitioners.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yugeng Liu",
        "Rui Wen",
        "Xinlei He",
        "A. Salem",
        "Zhikun Zhang",
        "M. Backes",
        "Emiliano De Cristofaro",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "citation_count": 152,
      "url": "https://www.semanticscholar.org/paper/9bb14ee43eb7607e0bf95bde3fa62882a676eb5d",
      "pdf_url": "",
      "publication_date": "2021-02-04",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b3086fbbc678a7616ac390a41945f45e0d0ab001",
      "title": "Dataset Inference: Ownership Resolution in Machine Learning",
      "abstract": "With increasingly more data and computation involved in their training, machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model's decision surface, but this is insufficient: the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model's training set is what is common to all stolen copies. The adversary's goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model's owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce $dataset$ $inference$, the process of identifying whether a suspected model copy has private knowledge from the original model's dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model's training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.",
      "year": 2021,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Pratyush Maini"
      ],
      "citation_count": 142,
      "url": "https://www.semanticscholar.org/paper/b3086fbbc678a7616ac390a41945f45e0d0ab001",
      "pdf_url": "",
      "publication_date": "2021-04-21",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "74f6e70fd9a945b517e6495920a1267c01842bd4",
      "title": "DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories",
      "abstract": "Recent advancements in Deep Neural Networks (DNNs) have enabled widespread deployment in multiple security-sensitive domains. The need for resource-intensive training and the use of valuable domain-specific training data have made these models the top intellectual property (IP) for model owners. One of the major threats to DNN privacy is model extraction attacks where adversaries attempt to steal sensitive information in DNN models. In this work, we propose an advanced model extraction framework DeepSteal that steals DNN weights remotely for the first time with the aid of a memory side-channel attack. Our proposed DeepSteal comprises two key stages. Firstly, we develop a new weight bit information extraction method, called HammerLeak, through adopting the rowhammer-based fault technique as the information leakage vector. HammerLeak leverages several novel system-level techniques tailored for DNN applications to enable fast and efficient weight stealing. Secondly, we propose a novel substitute model training algorithm with Mean Clustering weight penalty, which leverages the partial leaked bit information effectively and generates a substitute prototype of the target victim model. We evaluate the proposed model extraction framework on three popular image datasets (e.g., CIFAR-10/100/GTSRB) and four DNN architectures (e.g., ResNet-18/34/Wide-ResNetNGG-11). The extracted substitute model has successfully achieved more than 90% test accuracy on deep residual networks for the CIFAR-10 dataset. Moreover, our extracted substitute model could also generate effective adversarial input samples to fool the victim model. Notably, it achieves similar performance (i.e., ~1-2% test accuracy under attack) as white-box adversarial input attack (e.g., PGD/Trades).",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "A. S. Rakin",
        "Md Hafizul Islam Chowdhuryy",
        "Fan Yao",
        "Deliang Fan"
      ],
      "citation_count": 142,
      "url": "https://www.semanticscholar.org/paper/74f6e70fd9a945b517e6495920a1267c01842bd4",
      "pdf_url": "http://arxiv.org/pdf/2111.04625",
      "publication_date": "2021-11-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "title": "Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!",
      "abstract": "Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.",
      "year": 2021,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Xuanli He",
        "L. Lyu",
        "Qiongkai Xu",
        "Lichao Sun"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "pdf_url": "https://aclanthology.org/2021.naacl-main.161.pdf",
      "publication_date": "2021-03-18",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "864cf501b5c04bfcdc836a9cf1909a51ac1d2a99",
      "title": "Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction",
      "abstract": "We investigate whether model extraction can be used to \u2018steal\u2019 the weights of sequential recommender systems, and the potential threats posed to victims of such attacks. This type of risk has attracted attention in image and text classification, but to our knowledge not in recommender systems. We argue that sequential recommender systems are subject to unique vulnerabilities due to the specific autoregressive regimes used to train them. Unlike many existing recommender attackers, which assume the dataset used to train the victim model is exposed to attackers, we consider a data-free setting, where training data are not accessible. Under this setting, we propose an API-based model extraction method via limited-budget synthetic data generation and knowledge distillation. We investigate state-of-the-art models for sequential recommendation and show their vulnerability under model extraction and downstream attacks. We perform attacks in two stages. (1) Model extraction: given different types of synthetic data and their labels retrieved from a black-box recommender, we extract the black-box model to a white-box model via distillation. (2) Downstream attacks: we attack the black-box model with adversarial samples generated by the white-box recommender. Experiments show the effectiveness of our data-free model extraction and downstream attacks on sequential recommenders in both profile pollution and data poisoning settings.",
      "year": 2021,
      "venue": "ACM Conference on Recommender Systems",
      "authors": [
        "Zhenrui Yue",
        "Zhankui He",
        "Huimin Zeng",
        "Julian McAuley"
      ],
      "citation_count": 84,
      "url": "https://www.semanticscholar.org/paper/864cf501b5c04bfcdc836a9cf1909a51ac1d2a99",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3460231.3474275",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7026ec6c12f325aec884ff802812c3c80319f668",
      "title": "Model Stealing Attacks Against Inductive Graph Neural Networks",
      "abstract": "Many real-world data come in the form of graphs. Graph neural networks (GNNs), a new family of machine learning (ML) models, have been proposed to fully leverage graph data to build powerful applications. In particular, the inductive GNNs, which can generalize to unseen data, become mainstream in this direction. Machine learning models have shown great potential in various tasks and have been deployed in many real-world scenarios. To train a good model, a large amount of data as well as computational resources are needed, leading to valuable intellectual property. Previous research has shown that ML models are prone to model stealing attacks, which aim to steal the functionality of the target models. However, most of them focus on the models trained with images and texts. On the other hand, little attention has been paid to models trained with graph data, i.e., GNNs. In this paper, we fill the gap by proposing the first model stealing attacks against inductive GNNs. We systematically define the threat model and propose six attacks based on the adversary\u2019s background knowledge and the responses of the target models. Our evaluation on six benchmark datasets shows that the proposed model stealing attacks against GNNs achieve promising performance.1",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yun Shen",
        "Xinlei He",
        "Yufei Han",
        "Yang Zhang"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/7026ec6c12f325aec884ff802812c3c80319f668",
      "pdf_url": "https://arxiv.org/pdf/2112.08331",
      "publication_date": "2021-12-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bfb6e56602b658fdabaaa66987ebf685a8139892",
      "title": "Defending against Model Stealing via Verifying Embedded External Features",
      "abstract": "Obtaining a well-trained model involves expensive data collection and training procedures, therefore the model is a valuable intellectual property. Recent studies revealed that adversaries can `steal' deployed models even when they have no training samples and can not get access to the model parameters or structures. Currently, there were some defense methods to alleviate this threat, mostly by increasing the cost of model stealing. In this paper, we explore the defense from another angle by verifying whether a suspicious model contains the knowledge of defender-specified external features. Specifically, we embed the external features by tempering a few training samples with style transfer. We then train a meta-classifier to determine whether a model is stolen from the victim. This approach is inspired by the understanding that the stolen models should contain the knowledge of features learned by the victim model. We examine our method on both CIFAR-10 and ImageNet datasets. Experimental results demonstrate that our method is effective in detecting different types of model stealing simultaneously, even if the stolen model is obtained via a multi-stage stealing process. The codes for reproducing main results are available at Github (https://github.com/zlh-thu/StealingVerification).",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Linghui Zhu",
        "Yiming Li",
        "Xiaojun Jia",
        "Yong Jiang",
        "Shutao Xia",
        "Xiaochun Cao"
      ],
      "citation_count": 78,
      "url": "https://www.semanticscholar.org/paper/bfb6e56602b658fdabaaa66987ebf685a8139892",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/20036/19795",
      "publication_date": "2021-12-07",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9fd00a4044aa2eab1cc5f8bfafee59604fb01989",
      "title": "ModelDiff: testing-based DNN similarity comparison for model reuse detection",
      "abstract": "The knowledge of a deep learning model may be transferred to a student model, leading to intellectual property infringement or vulnerability propagation. Detecting such knowledge reuse is nontrivial because the suspect models may not be white-box accessible and/or may serve different tasks. In this paper, we propose ModelDiff, a testing-based approach to deep learning model similarity comparison. Instead of directly comparing the weights, activations, or outputs of two models, we compare their behavioral patterns on the same set of test inputs. Specifically, the behavioral pattern of a model is represented as a decision distance vector (DDV), in which each element is the distance between the model's reactions to a pair of inputs. The knowledge similarity between two models is measured with the cosine similarity between their DDVs. To evaluate ModelDiff, we created a benchmark that contains 144 pairs of models that cover most popular model reuse methods, including transfer learning, model compression, and model stealing. Our method achieved 91.7% correctness on the benchmark, which demonstrates the effectiveness of using ModelDiff for model reuse detection. A study on mobile deep learning apps has shown the feasibility of ModelDiff on real-world models.",
      "year": 2021,
      "venue": "International Symposium on Software Testing and Analysis",
      "authors": [
        "Yuanchun Li",
        "Ziqi Zhang",
        "Bingyan Liu",
        "Ziyue Yang",
        "Yunxin Liu"
      ],
      "citation_count": 62,
      "url": "https://www.semanticscholar.org/paper/9fd00a4044aa2eab1cc5f8bfafee59604fb01989",
      "pdf_url": "https://arxiv.org/pdf/2106.08890",
      "publication_date": "2021-06-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5beafcd6c0222a2f45863058280873c1ef088ec4",
      "title": "Simulating Unknown Target Models for Query-Efficient Black-box Attacks",
      "abstract": "Many adversarial attacks have been proposed to investigate the security issues of deep neural networks. In the black-box setting, current model stealing attacks train a substitute model to counterfeit the functionality of the target model. However, the training requires querying the target model. Consequently, the query complexity remains high, and such attacks can be defended easily. This study aims to train a generalized substitute model called \"Simulator\", which can mimic the functionality of any unknown target model. To this end, we build the training data with the form of multiple tasks by collecting query sequences generated during the attacks of various existing networks. The learning process uses a mean square error-based knowledge-distillation loss in the meta-learning to minimize the difference between the Simulator and the sampled networks. The meta-gradients of this loss are then computed and accumulated from multiple tasks to update the Simulator and subsequently improve generalization. When attacking a target model that is unseen in training, the trained Simulator can accurately simulate its functionality using its limited feedback. As a result, a large fraction of queries can be transferred to the Simulator, thereby reducing query complexity. Results of the comprehensive experiments conducted using the CIFAR-10, CIFAR-100, and TinyImageNet datasets demonstrate that the proposed approach reduces query complexity by several orders of magnitude compared to the baseline method. The implementation source code is released online 1.",
      "year": 2021,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Chen Ma",
        "Li Chen",
        "Junhai Yong"
      ],
      "citation_count": 59,
      "url": "https://www.semanticscholar.org/paper/5beafcd6c0222a2f45863058280873c1ef088ec4",
      "pdf_url": "https://arxiv.org/pdf/2009.00960",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f4847ab95c43b9d3c214d494ce3852473bf297e4",
      "title": "MEGEX: Data-Free Model Extraction Attack Against Gradient-Based Explainable AI",
      "abstract": "Explainable AI encourages machine learning applications in the real world, whereas data-free model extraction attacks (DFME), in which an adversary steals a trained machine learning model by creating input queries with generative models instead of collecting training data, have attracted attention as a serious threat. In this paper, we propose MEGEX, a data-free model extraction attack against explainable AI that provides gradient-based explanations for inference results, and investigate whether the gradient-based explanations increase the vulnerability to the data-free model extraction attacks. In MEGEX, an adversary leverages explanations by Vanilla Gradient as derivative values for training a generative model. We prove that MEGEX is identical to white-box data-free knowledge distillation, whereby the adversary can train the generative model with the exact gradients. Our experiments show that the adversary in MEGEX can steal highly accurate models - 0.98\u00d7, 0.91\u00d7, and 0.96\u00d7 the victim model accuracy on SVHN, Fashion-MNIST, and CIFAR-10 datasets given 1.5M, 5M, 20M queries, respectively. In addition, we also apply sophisticated gradient-based explanations, i.e., SmoothGrad and Integrated Gradients, to MEGEX. The experimental results indicate that these explanations are potential countermeasures to MEGEX. We also found that the accuracy of the model stolen by the adversary depends on the diversity of query inputs by the generative model.",
      "year": 2021,
      "venue": "SecTL@AsiaCCS",
      "authors": [
        "T. Miura",
        "Toshiki Shibahara",
        "Naoto Yanai"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/f4847ab95c43b9d3c214d494ce3852473bf297e4",
      "pdf_url": "https://arxiv.org/pdf/2107.08909",
      "publication_date": "2021-07-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e32ab49974b032faeca3804685ba739f5fb09790",
      "title": "Trustworthy and Intelligent COVID-19 Diagnostic IoMT Through XR and Deep-Learning-Based Clinic Data Access",
      "abstract": "This article presents a novel extended reality (XR) and deep-learning-based Internet-of-Medical-Things (IoMT) solution for the COVID-19 telemedicine diagnostic, which systematically combines virtual reality/augmented reality (AR) remote surgical plan/rehearse hardware, customized 5G cloud computing and deep learning algorithms to provide real-time COVID-19 treatment scheme clues. Compared to existing perception therapy techniques, our new technique can significantly improve performance and security. The system collected 25 clinic data from the 347 positive and 2270 negative COVID-19 patients in the Red Zone by 5G transmission. After that, a novel auxiliary classifier generative adversarial network-based intelligent prediction algorithm is conducted to train the new COVID-19 prediction model. Furthermore, The Copycat network is employed for the model stealing and attack for the IoMT to improve the security performance. To simplify the user interface and achieve an excellent user experience, we combined the Red Zone\u2019s guiding images with the Green Zone\u2019s view through the AR navigate clue by using 5G. The XR surgical plan/rehearse framework is designed, including all COVID-19 surgical requisite details that were developed with a real-time response guaranteed. The accuracy, recall, F1-score, and area under the ROC curve (AUC) area of our new IoMT were 0.92, 0.98, 0.95, and 0.98, respectively, which outperforms the existing perception techniques with significantly higher accuracy performance. The model stealing also has excellent performance, with the AUC area of 0.90 in Copycat slightly lower than the original model. This study suggests a new framework in the COVID-19 diagnostic integration and opens the new research about the integration of XR and deep learning for IoMT implementation.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Yonghang Tai",
        "Bixuan Gao",
        "Qiong Li",
        "Zhengtao Yu",
        "Chunsheng Zhu",
        "Victor I. Chang"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/e32ab49974b032faeca3804685ba739f5fb09790",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6488907/9585129/09343340.pdf",
      "publication_date": "2021-02-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "446b07b8aaaaa75f0352be89bd97498ee5f69e36",
      "title": "QAIR: Practical Query-efficient Black-Box Attacks for Image Retrieval",
      "abstract": "We study the query-based attack against image retrieval to evaluate its robustness against adversarial examples under the black-box setting, where the adversary only has query access to the top-k ranked unlabeled images from the database. Compared with query attacks in image classification, which produce adversaries according to the returned labels or confidence score, the challenge becomes even more prominent due to the difficulty in quantifying the attack effectiveness on the partial retrieved list. In this paper, we make the first attempt in Query-based Attack against Image Retrieval (QAIR), to completely subvert the top-k retrieval results. Specifically, a new relevance-based loss is designed to quantify the attack effects by measuring the set similarity on the top-k retrieval results before and after attacks and guide the gradient optimization. To further boost the attack efficiency, a recursive model stealing method is proposed to acquire transferable priors on the target model and generate the prior-guided gradients. Comprehensive experiments show that the proposed attack achieves a high attack success rate with few queries against the image retrieval systems under the black-box setting. The attack evaluations on the real-world visual search engine show that it successfully deceives a commercial system such as Bing Visual Search with 98% attack success rate by only 33 queries on average.",
      "year": 2021,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Xiaodan Li",
        "Jinfeng Li",
        "Yuefeng Chen",
        "Shaokai Ye",
        "Yuan He",
        "Shuhui Wang",
        "Hang Su",
        "Hui Xue"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/446b07b8aaaaa75f0352be89bd97498ee5f69e36",
      "pdf_url": "https://arxiv.org/pdf/2103.02927",
      "publication_date": "2021-03-04",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "18da8b08b89646dc18d6734cac6d0222c9239cbf",
      "title": "InverseNet: Augmenting Model Extraction Attacks with Training Data Inversion",
      "abstract": "Cloud service providers, including Google, Amazon, and Alibaba, have now launched machine-learning-as-a-service (MLaaS) platforms, allowing clients to access sophisticated cloud-based machine learning models via APIs. Unfortunately, however, the commercial value of these models makes them alluring targets for theft, and their strategic position as part of the IT infrastructure of many companies makes them an enticing springboard for conducting further adversarial attacks. In this paper, we put forth a novel and effective attack strategy, dubbed InverseNet, that steals the functionality of black-box cloud-based models with only a small number of queries. The crux of the innovation is that, unlike existing model extraction attacks that rely on public datasets or adversarial samples, InverseNet constructs inversed training samples to increase the similarity between the extracted substitute model and the victim model. Further, only a small number of data samples with high confidence scores (rather than an entire dataset) are used to reconstruct the inversed dataset, which substantially reduces the attack cost. Extensive experiments conducted on three simulated victim models and Alibaba Cloud's commercially-available API demonstrate that InverseNet yields a model with significantly greater functional similarity to the victim model than the current state-of-the-art attacks at a substantially lower query budget.",
      "year": 2021,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Wenbin Yang",
        "Guanghao Mei",
        "Qian Wang"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/18da8b08b89646dc18d6734cac6d0222c9239cbf",
      "pdf_url": "https://www.ijcai.org/proceedings/2021/0336.pdf",
      "publication_date": "2021-08-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "57d5abaa047a0401874383628c646918527d4df2",
      "title": "Monitoring-Based Differential Privacy Mechanism Against Query Flooding-Based Model Extraction Attack",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this article, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.",
      "year": 2021,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Haonan Yan",
        "Xiaoguang Li",
        "Hui Li",
        "Jiamin Li",
        "Wenhai Sun",
        "Fenghua Li"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/57d5abaa047a0401874383628c646918527d4df2",
      "pdf_url": "",
      "publication_date": "2021-03-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5179302656cf46cfbcc9cce2b70d4b0c758f7d7c",
      "title": "Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack",
      "abstract": "Previous studies have verified that the functionality of black-box models can be stolen with full probability outputs. However, under the more practical hard-label setting, we observe that existing methods suffer from catastrophic performance degradation. We argue this is due to the lack of rich information in the probability prediction and the overfitting caused by hard labels. To this end, we propose a novel hard-label model stealing method termed \\emph{black-box dissector}, which consists of two erasing-based modules. One is a CAM-driven erasing strategy that is designed to increase the information capacity hidden in hard labels from the victim model. The other is a random-erasing-based self-knowledge distillation module that utilizes soft labels from the substitute model to mitigate overfitting. Extensive experiments on four widely-used datasets consistently demonstrate that our method outperforms state-of-the-art methods, with an improvement of at most $8.27\\%$. We also validate the effectiveness and practical potential of our method on real-world APIs and defense methods. Furthermore, our method promotes other downstream tasks, \\emph{i.e.}, transfer adversarial attacks.",
      "year": 2021,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yixu Wang",
        "Jie Li",
        "Hong Liu",
        "Yongjian Wu",
        "Rongrong Ji"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/5179302656cf46cfbcc9cce2b70d4b0c758f7d7c",
      "pdf_url": "http://arxiv.org/pdf/2105.00623",
      "publication_date": "2021-05-03",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a7f5460b2f2b1a215064e9a0669dd3d43a382af9",
      "title": "Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks",
      "abstract": "Model extraction attacks aim to duplicate a machine learning model through query access to a target model. Early studies mainly focus on discriminative models. Despite the success, model extraction attacks against generative models are less well explored. In this paper, we systematically study the feasibility of model extraction attacks against generative adversarial networks (GANs). Specifically, we first define fidelity and accuracy on model extraction attacks against GANs. Then we study model extraction attacks against GANs from the perspective of fidelity extraction and accuracy extraction, according to the adversary\u2019s goals and background knowledge. We further conduct a case study where the adversary can transfer knowledge of the extracted model which steals a state-of-the-art GAN trained with more than 3 million images to new domains to broaden the scope of applications of model extraction attacks. Finally, we propose effective defense techniques to safeguard GANs, considering a trade-off between the utility and security of GAN models.",
      "year": 2021,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/a7f5460b2f2b1a215064e9a0669dd3d43a382af9",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3485832.3485838",
      "publication_date": "2021-12-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "stealing machine learning"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d167f82f1f1bec0dd173fe3173053eebddae05b8",
      "title": "SEAT: Similarity Encoder by Adversarial Training for Detecting Model Extraction Attack Queries",
      "abstract": "Given black-box access to the prediction API, model extraction attacks can steal the functionality of models deployed in the cloud. In this paper, we introduce the SEAT detector, which detects black-box model extraction attacks so that the defender can terminate malicious accounts. SEAT has a similarity encoder trained by adversarial training. Using the similarity encoder, SEAT detects accounts that make queries that indicate a model extraction attack in progress and cancels these accounts. We evaluate our defense against existing model extraction attacks and against new adaptive attacks introduced in this paper. Our results show that even against adaptive attackers, SEAT increases the cost of model extraction attacks by 3.8 times to 16 times.",
      "year": 2021,
      "venue": "AISec@CCS",
      "authors": [
        "Zhanyuan Zhang",
        "Yizheng Chen",
        "David A. Wagner"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/d167f82f1f1bec0dd173fe3173053eebddae05b8",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3474369.3486863",
      "publication_date": "2021-11-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c6d4602482ca710f01d4a07e8d6638b8af59f856",
      "title": "Preventing DNN Model IP Theft via Hardware Obfuscation",
      "abstract": "Training accurate deep learning (DL) models require large amounts of training data, significant work in labeling the data, considerable computing resources, and substantial domain expertise. In short, they are expensive to develop. Hence, protecting these models, which are valuable storehouses of intellectual properties (IP), against model stealing/cloning attacks is of paramount importance. Today\u2019s mobile processors feature Neural Processing Units (NPUs) to accelerate the execution of DL models. DL models executing on NPUs are vulnerable to hyperparameter extraction via side-channel attacks and model parameter theft via bus monitoring attacks. This paper presents a novel solution to defend against DL IP theft in NPUs during model distribution and deployment/execution via lightweight, keyed model obfuscation scheme. Unauthorized use of such models results in inaccurate classification. In addition, we present an ideal end-to-end deep learning trusted system composed of: 1) model distribution via hardware root-of-trust and public-key cryptography infrastructure (PKI) and 2) model execution via low-latency memory encryption. We demonstrate that our proposed obfuscation solution achieves IP protection objectives without requiring specialized training or sacrificing the model\u2019s accuracy. In addition, the proposed obfuscation mechanism preserves the output class distribution while degrading the model\u2019s accuracy for unauthorized parties, covering any evidence of a hacked model.",
      "year": 2021,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Brunno F. Goldstein",
        "Vinay C. Patil",
        "V. C. Ferreira",
        "A. S. Nery",
        "F. Fran\u00e7a",
        "S. Kundu"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/c6d4602482ca710f01d4a07e8d6638b8af59f856",
      "pdf_url": "",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model stealing",
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "645e890034126dfc08484d3509b3493e85fbd603",
      "title": "Stateful Detection of Model Extraction Attacks",
      "abstract": "Machine-Learning-as-a-Service providers expose machine learning (ML) models through application programming interfaces (APIs) to developers. Recent work has shown that attackers can exploit these APIs to extract good approximations of such ML models, by querying them with samples of their choosing. We propose VarDetect, a stateful monitor that tracks the distribution of queries made by users of such a service, to detect model extraction attacks. Harnessing the latent distributions learned by a modified variational autoencoder, VarDetect robustly separates three types of attacker samples from benign samples, and successfully raises an alarm for each. Further, with VarDetect deployed as an automated defense mechanism, the extracted substitute models are found to exhibit poor performance and transferability, as intended. Finally, we demonstrate that even adaptive attackers with prior knowledge of the deployment of VarDetect, are detected by it.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Kanade",
        "S. Shevade"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/645e890034126dfc08484d3509b3493e85fbd603",
      "pdf_url": "",
      "publication_date": "2021-07-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "975e8d7065161d3dc0020ef343aa1db2a3db5a7b",
      "title": "Student Surpasses Teacher: Imitation Attack for Black-Box NLP APIs",
      "abstract": "Machine-learning-as-a-service (MLaaS) has attracted millions of users to their splendid large-scale models. Although published as black-box APIs, the valuable models behind these services are still vulnerable to imitation attacks. Recently, a series of works have demonstrated that attackers manage to steal or extract the victim models. Nonetheless, none of the previous stolen models can outperform the original black-box APIs. In this work, we conduct unsupervised domain adaptation and multi-victim ensemble to showing that attackers could potentially surpass victims, which is beyond previous understanding of model extraction. Extensive experiments on both benchmark datasets and real-world APIs validate that the imitators can succeed in outperforming the original black-box models on transferred domains. We consider our work as a milestone in the research of imitation attack, especially on NLP APIs, as the superior performance could influence the defense or even publishing strategy of API providers.",
      "year": 2021,
      "venue": "International Conference on Computational Linguistics",
      "authors": [
        "Qiongkai Xu",
        "Xuanli He",
        "L. Lyu",
        "Lizhen Qu",
        "Gholamreza Haffari"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/975e8d7065161d3dc0020ef343aa1db2a3db5a7b",
      "pdf_url": "",
      "publication_date": "2021-08-29",
      "keywords_matched": [
        "model extraction",
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "aa6389843232b205bf6d494f00f2cfdeaa633cd8",
      "title": "Thief, Beware of What Get You There: Towards Understanding Model Extraction Attack",
      "abstract": "Model extraction increasingly attracts research attentions as keeping commercial AI models private can retain a competitive advantage. In some scenarios, AI models are trained proprietarily, where neither pre-trained models nor sufficient in-distribution data is publicly available. Model extraction attacks against these models are typically more devastating. Therefore, in this paper, we empirically investigate the behaviors of model extraction under such scenarios. We find the effectiveness of existing techniques significantly affected by the absence of pre-trained models. In addition, the impacts of the attacker's hyperparameters, e.g. model architecture and optimizer, as well as the utilities of information retrieved from queries, are counterintuitive. We provide some insights on explaining the possible causes of these phenomena. With these observations, we formulate model extraction attacks into an adaptive framework that captures these factors with deep reinforcement learning. Experiments show that the proposed framework can be used to improve existing techniques, and show that model extraction is still possible in such strict scenarios. Our research can help system designers to construct better defense strategies based on their scenarios.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Xinyi Zhang",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/aa6389843232b205bf6d494f00f2cfdeaa633cd8",
      "pdf_url": "",
      "publication_date": "2021-04-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "13d626a08050930fbf1cb072938d9de87e742fee",
      "title": "Extraction of Binarized Neural Network Architecture and Secret Parameters Using Side-Channel Information",
      "abstract": "In recent years, neural networks have been applied to various applications. To speed up the evaluation, a method using binarized network weights has been introduced, facilitating extremely efficient hardware implementation. Using electromagnetic (EM) side-channel analysis techniques, this study presents a framework of model extraction from practical binarized neural network (BNN) hardware. The target BNN hardware is generated and synthesized using open-source and commercial high-level synthesis tools GUINNESS and Xilinx SDSoC, respectively. With the hardware implemented on an up-to-date FPGA chip, we demonstrate how the layers can be identified from a single EM trace measured during the network evaluation, and we also demonstrate how an attacker may use side-channel attacks to recover secret weights used in the network.",
      "year": 2021,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Ville Yli-M\u00e4yry",
        "Akira Ito",
        "N. Homma",
        "S. Bhasin",
        "Dirmanto Jap"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/13d626a08050930fbf1cb072938d9de87e742fee",
      "pdf_url": "",
      "publication_date": "2021-05-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b408a5e1a60c3afd5954613516126b6c512a7804",
      "title": "Model Extraction and Defenses on Generative Adversarial Networks",
      "abstract": "Model extraction attacks aim to duplicate a machine learning model through query access to a target model. Early studies mainly focus on discriminative models. Despite the success, model extraction attacks against generative models are less well explored. In this paper, we systematically study the feasibility of model extraction attacks against generative adversarial networks (GANs). Specifically, we first define accuracy and fidelity on model extraction attacks against GANs. Then we study model extraction attacks against GANs from the perspective of accuracy extraction and fidelity extraction, according to the adversary's goals and background knowledge. We further conduct a case study where an adversary can transfer knowledge of the extracted model which steals a state-of-the-art GAN trained with more than 3 million images to new domains to broaden the scope of applications of model extraction attacks. Finally, we propose effective defense techniques to safeguard GANs, considering a trade-off between the utility and security of GAN models.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/b408a5e1a60c3afd5954613516126b6c512a7804",
      "pdf_url": "",
      "publication_date": "2021-01-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0b498f9f76310da0ccb0cce7ae26ee1039769b89",
      "title": "Stealing Neural Network Models through the Scan Chain: A New Threat for ML Hardware",
      "abstract": "Stealing trained machine learning (ML) models is a new and growing concern due to the model's development cost. Existing work on ML model extraction either applies a mathematical attack or exploits hardware vulnerabilities such as side-channel leakage. This paper shows a new style of attack, for the first time, on ML models running on embedded devices by abusing the scan-chain infrastructure. We illustrate that having course-grained scan-chain access to non-linear layer outputs is sufficient to steal ML models. To that end, we propose a novel small-signal analysis inspired attack that applies small perturbations into the input signals, identifies the quiescent operating points and, selectively activates certain neurons. We then couple this with a Linear Constraint Satisfaction based approach to efficiently extract model parameters such as weights and biases. We conduct our attack on neural network inference topologies defined in earlier works, and we automate our attack. The results show that our attack outperforms mathematical model extraction proposed in CRYPTO 2020, USENIX 2020, and ICML 2020 by an increase in accuracy of $2^{20.7}\\times, 2^{50.7}\\times$, and $2^{33.9}\\times$, respectively, and a reduction in queries by $2^{6.5}\\times, 2^{4.6}\\times$, and $2^{14.2}\\times$, respectively.",
      "year": 2021,
      "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "authors": [
        "S. Potluri",
        "Aydin Aysu"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/0b498f9f76310da0ccb0cce7ae26ee1039769b89",
      "pdf_url": "",
      "publication_date": "2021-11-01",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "steal ML model",
        "steal ML models"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "43c6ffef36a8acda9da688a0fca324abdb64498b",
      "title": "Model Reverse-Engineering Attack against Systolic-Array-Based DNN Accelerator Using Correlation Power Analysis",
      "abstract": "SUMMARY A model extraction attack is a security issue in deep neural networks (DNNs). Information on a trained DNN model is an attractive target for an adversary not only in terms of intellectual property but also of security. Thus, an adversary tries to reveal the sensitive information contained in the trained DNN model from machine-learning services. Previous studies on model extraction attacks assumed that the victim provides a machine-learning cloud service and the adversary accesses the service through formal queries. However, when a DNN model is implemented on an edge device, adversaries can physically access the device and try to reveal the sensitive information contained in the implemented DNN model. We call these physical model extraction attacks model reverse-engineering (MRE) attacks to distinguish them from attacks on cloud services. Power side-channel analyses are often used in MRE attacks to reveal the internal operation from power consumption or electromagnetic leakage. Previous studies, including ours, evaluated MRE attacks against several types of DNN processors with power side-channel analyses. In this paper, information leakage from a systolic array which is used for the matrix multiplication unit in the DNN processors is evaluated. We utilized correlation power analysis (CPA) for the MRE attack and reveal weight parameters of a DNN model from the systolic array. Two types of the systolic array were implemented on \ufb01eld-programmable gate array (FPGA) to demonstrate that CPA reveals weight parameters from those systolic arrays. In addition, we applied an extended analysis approach called \u201cchain CPA\u201d for robust CPA analysis against the systolic arrays. Our experimental results indicate that an adversary can reveal trained model parameters from a DNN accelerator even if the DNN model parameters in the o \ufb00 -chip bus are protected with data encryption. Countermeasures against side-channel leaks will be important for implementing a DNN accelerator on a FPGA or application-speci\ufb01c integrated circuit (ASIC).",
      "year": 2021,
      "venue": "IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences",
      "authors": [
        "Kota Yoshida",
        "M. Shiozaki",
        "S. Okura",
        "Takaya Kubota",
        "T. Fujino"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/43c6ffef36a8acda9da688a0fca324abdb64498b",
      "pdf_url": "https://doi.org/10.1587/transfun.2020cip0024",
      "publication_date": "2021-01-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5b6a6a46caac1316f268a1e6643c10b16fdaef0f",
      "title": "An Exact Poly-Time Membership-Queries Algorithm for Extraction a three-Layer ReLU Network",
      "abstract": "We consider the natural problem of learning a ReLU network from queries, which was recently remotivated by model extraction attacks. In this work, we present a polynomial-time algorithm that can learn a depth-two ReLU network from queries under mild general position assumptions. We also present a polynomial-time algorithm that, under mild general position assumptions, can learn a rich class of depth-three ReLU networks from queries. For instance, it can learn most networks where the number of first layer neurons is smaller than the dimension and the number of second layer neurons. These two results substantially improve state-of-the-art: Until our work, polynomial-time algorithms were only shown to learn from queries depth-two networks under the assumption that either the underlying distribution is Gaussian (Chen et al. (2021)) or that the weights matrix rows are linearly independent (Milli et al. (2019)). For depth three or more, there were no known poly-time results.",
      "year": 2021,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Amit Daniely",
        "Elad Granot"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/5b6a6a46caac1316f268a1e6643c10b16fdaef0f",
      "pdf_url": "",
      "publication_date": "2021-05-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "373936d00c4a357579c4d375de0ce439e4e54d5f",
      "title": "Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs",
      "abstract": "The collection and availability of big data, combined with advances in pre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive performance of modern natural language processing tasks, ranging from text classification to text generation. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. However, BERT-based APIs have exhibited a series of security and privacy vulnerabilities. For example, prior work has exploited the security issues of the BERT-based APIs through the adversarial examples crafted by the extracted model. However, the privacy leakage problems of the BERT-based APIs through the extracted model have not been well studied. On the other hand, due to the high capacity of BERT-based APIs, the fine-tuned model is easy to be overlearned, but what kind of information can be leaked from the extracted model remains unknown. In this work, we bridge this gap by first presenting an effective model extraction attack, where the adversary can practically steal a BERT-based API (the target/victim model) by only querying a limited number of queries. We further develop an effective attribute inference attack which can infer the sensitive attribute of the training data used by the BERT-based APIs. Our extensive experiments on benchmark datasets under various realistic settings validate the potential vulnerabilities of BERT-based APIs. Moreover, we demonstrate that two promising defense methods become ineffective against our attacks, which calls for more effective defense methods.",
      "year": 2021,
      "venue": "",
      "authors": [
        "Chen Chen",
        "Xuanli He",
        "Lingjuan Lyu",
        "Fangzhao Wu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/373936d00c4a357579c4d375de0ce439e4e54d5f",
      "pdf_url": "",
      "publication_date": "2021-05-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "596aee932d0dd5ac1f634ea0a83c9e8f18bda65a",
      "title": "Power-based Attacks on Spatial DNN Accelerators",
      "abstract": "With proliferation of DNN-based applications, the confidentiality of DNN model is an important commercial goal. Spatial accelerators, which parallelize matrix/vector operations, are utilized for enhancing energy efficiency of DNN computation. Recently, model extraction attacks on simple accelerators, either with a single processing element or running a binarized network, were demonstrated using the methodology derived from differential power analysis (DPA) attack on cryptographic devices. This article investigates the vulnerability of realistic spatial accelerators using general, 8-bit, number representation. We investigate two systolic array architectures with weight-stationary dataflow: (1) a 3 \u00d7 1 array for a dot-product operation and (2) a 3 \u00d7 3 array for matrix-vector multiplication. Both are implemented on the SAKURA-G FPGA board. We show that both architectures are ultimately vulnerable. A conventional DPA succeeds fully on the 1D array, requiring 20K power measurements. However, the 2D array exhibits higher security even with 460K traces. We show that this is because the 2D array intrinsically entails multiple MACs simultaneously dependent on the same input. However, we find that a novel template-based DPA with multiple profiling phases is able to fully break the 2D array with only 40K traces. Corresponding countermeasures need to be investigated for spatial DNN accelerators.",
      "year": 2021,
      "venue": "ACM Journal on Emerging Technologies in Computing Systems",
      "authors": [
        "Ge Li",
        "Mohit Tiwari",
        "M. Orshansky"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/596aee932d0dd5ac1f634ea0a83c9e8f18bda65a",
      "pdf_url": "https://arxiv.org/pdf/2108.12579",
      "publication_date": "2021-08-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c092ed52816d6060929bbb40021dbfdc88ba28b7",
      "title": "SCANet: Securing the Weights With Superparamagnetic-MTJ Crossbar Array Networks",
      "abstract": "Deep neural networks (DNNs) form a critical infrastructure supporting various systems, spanning from the iPhone neural engine to imaging satellites and drones. The design of these neural cores is often proprietary or a military secret. Nevertheless, they remain vulnerable to model replication attacks that seek to reverse engineer the network\u2019s synaptic weights. In this article, we propose SCANet (Superparamagnetic-MTJ Crossbar Array Networks), a novel defense mechanism against such model stealing attacks by utilizing the innate stochasticity in superparamagnets. When used as the synapse in DNNs, superparamagnetic magnetic tunnel junctions (s-MTJs) are shown to be significantly more secure than prior memristor-based solutions. The thermally induced telegraphic switching in the s-MTJs is robust and uncontrollable, thus thwarting the attackers from obtaining sensitive data from the network. Using a mixture of both superparamagnetic and conventional MTJs in the neural network (NN), the designer can optimize the time period between the weight updation and the power consumed by the system. Furthermore, we propose a modified NN architecture that can prevent replication attacks while minimizing power consumption. We investigate the effect of the number of layers in the deep network and the number of neurons in each layer on the sharpness of accuracy degradation when the network is under attack. We also explore the efficacy of SCANet in real-time scenarios, using a case study on object detection.",
      "year": 2021,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Dinesh Rajasekharan",
        "N. Rangarajan",
        "Satwik Patnaik",
        "O. Sinanoglu",
        "Y. Chauhan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/c092ed52816d6060929bbb40021dbfdc88ba28b7",
      "pdf_url": "",
      "publication_date": "2021-12-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16d98de120fc1e55000e65a285684c4dc7a04807",
      "title": "Efficiently Learning Any One Hidden Layer ReLU Network From Queries",
      "abstract": "Model extraction attacks have renewed interest in the classic problem of learning neural networks from queries. In this work we give the first polynomial-time algorithm for learning arbitrary one hidden layer neural networks activations provided black-box access to the network. Formally, we show that if $F$ is an arbitrary one hidden layer neural network with ReLU activations, there is an algorithm with query complexity and running time that is polynomial in all parameters that outputs a network $F'$ achieving low square loss relative to $F$ with respect to the Gaussian measure. While a number of works in the security literature have proposed and empirically demonstrated the effectiveness of certain algorithms for this problem, ours is the first with fully polynomial-time guarantees of efficiency even for worst-case networks (in particular our algorithm succeeds in the overparameterized setting).",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Sitan Chen",
        "Adam R. Klivans",
        "Raghu Meka"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/16d98de120fc1e55000e65a285684c4dc7a04807",
      "pdf_url": "",
      "publication_date": "2021-11-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "25800c4599b2e834899a180d77e093fb9282d97e",
      "title": "HODA: Hardness-Oriented Detection of Model Extraction Attacks",
      "abstract": "Model extraction attacks exploit the target model\u2019s prediction API to create a surrogate model, allowing the adversary to steal or reconnoiter the functionality of the target model in the black-box setting. Several recent studies have shown that a data-limited adversaries with no or limited access to the samples from the target model\u2019s training data distribution, can employ synthesized or semantically similar samples to conduct model extraction attacks. In this paper, we introduce the concept of hardness degree to characterize sample difficulty based on the concept of learning speed. The hardness degree of a sample depends on the epoch number at which the predicted label for that sample converges. We investigate the hardness degree of samples and demonstrate that the hardness degree histogram of a data-limited adversary\u2019s sample sequence is differs significantly from that of benign users\u2019 sample sequences. We propose Hardness-Oriented Detection Approach (HODA) to detect the sample sequences of model extraction attacks. Our results indicate that HODA can effectively detect model extraction attack sequences with a high success rate, using only 100 monitored samples. It outperforms all previously proposed methods for model extraction detection.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "A. M. Sadeghzadeh",
        "Amir Mohammad Sobhanian",
        "F. Dehghan",
        "R. Jalili"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/25800c4599b2e834899a180d77e093fb9282d97e",
      "pdf_url": "https://arxiv.org/pdf/2106.11424",
      "publication_date": "2021-06-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0ce9109dd491603364f6e6059d28757b52fafd5d",
      "title": "A Review of Confidentiality Threats Against Embedded Neural Network Models",
      "abstract": "Utilization of Machine Learning (ML) algorithms, especially Deep Neural Network (DNN) models, becomes a widely accepted standard in many domains more particularly IoT-based systems. DNN models reach impressive performances in several sensitive fields such as medical diagnosis, smart transport or security threat detection, and represent a valuable piece of Intellectual Property. Over the last few years, a major trend is the large-scale deployment of models in a wide variety of devices. However, this migration to embedded systems is slowed down because of the broad spectrum of attacks threatening the integrity, confidentiality and availability of embedded models. In this review, we cover the landscape of attacks targeting the confidentiality of embedded DNN models that may have a major impact on critical IoT systems, with a particular focus on model extraction and data leakage. We highlight the fact that Side-Channel Analysis (SCA) is a relatively unexplored bias by which model\u2019s confidentiality can be compromised. Input data, architecture or parameters of a model can be extracted from power or electromagnetic observations, testifying a real need from a security point of view.",
      "year": 2021,
      "venue": "World Forum on Internet of Things",
      "authors": [
        "Raphael Joud",
        "P. Moellic",
        "R\u00e9mi Bernhard",
        "J. Rigaud"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/0ce9109dd491603364f6e6059d28757b52fafd5d",
      "pdf_url": "https://cea.hal.science/cea-04176698/file/A_Review_of_Confidentiality_Threats_Against_Embedded_Neural_Network_Models.pdf",
      "publication_date": "2021-05-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "title": "Model Extraction and Adversarial Attacks on Neural Networks using Switching Power Information",
      "abstract": "Artificial neural networks (ANNs) have gained significant popularity in the last decade for solving narrow AI problems in domains such as healthcare, transportation, and defense. As ANNs become more ubiquitous, it is imperative to understand their associated safety, security, and privacy vulnerabilities. Recently, it has been shown that ANNs are susceptible to a number of adversarial evasion attacks--inputs that cause the ANN to make high-confidence misclassifications despite being almost indistinguishable from the data used to train and test the network. This work explores to what degree finding these examples maybe aided by using side-channel information, specifically switching power consumption, of hardware implementations of ANNs. A black-box threat scenario is assumed, where an attacker has access to the ANN hardware's input, outputs, and topology, but the trained model parameters are unknown. Then, a surrogate model is trained to have similar functional (i.e. input-output mapping) and switching power characteristics as the oracle (black-box) model. Our results indicate that the inclusion of power consumption data increases the fidelity of the model extraction by up to 30 percent based on a mean square error comparison of the oracle and surrogate weights. However, transferability of adversarial examples from the surrogate to the oracle model was not significantly affected.",
      "year": 2021,
      "venue": "International Conference on Artificial Neural Networks",
      "authors": [
        "Tommy Li",
        "Cory E. Merkel"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "pdf_url": "",
      "publication_date": "2021-06-15",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6981af0aa5d7bee0c1f0c3c685824077140f6be1",
      "title": "First to Possess His Statistics: Data-Free Model Extraction Attack on Tabular Data",
      "abstract": "Model extraction attacks are a kind of attacks where an adversary obtains a machine learning model whose performance is comparable with one of the victim model through queries and their results. This paper presents a novel model extraction attack, named TEMPEST, applicable on tabular data under a practical data-free setting. Whereas model extraction is more challenging on tabular data due to normalization, TEMPEST no longer needs initial samples that previous attacks require; instead, it makes use of publicly available statistics to generate query samples. Experiments show that our attack can achieve the same level of performance as the previous attacks. Moreover, we identify that the use of mean and variance as statistics for query generation and the use of the same normalization process as the victim model can improve the performance of our attack. We also discuss a possibility whereby TEMPEST is executed in the real world through an experiment with a medical diagnosis dataset. We plan to release the source code for reproducibility and a reference to subsequent works.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Masataka Tasumi",
        "Kazuki Iwahana",
        "Naoto Yanai",
        "Katsunari Shishido",
        "Toshiya Shimizu",
        "Yuji Higuchi",
        "I. Morikawa",
        "Jun Yajima"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/6981af0aa5d7bee0c1f0c3c685824077140f6be1",
      "pdf_url": "",
      "publication_date": "2021-09-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2eef5df6e2347f7d83df44fe3268d4b237f0eaa5",
      "title": "BODAME: Bilevel Optimization for Defense Against Model Extraction",
      "abstract": "Model extraction attacks have become serious issues for service providers using machine learning. We consider an adversarial setting to prevent model extraction under the assumption that attackers will make their best guess on the service provider's model using query accesses, and propose to build a surrogate model that significantly keeps away the predictions of the attacker's model from those of the true model. We formulate the problem as a non-convex constrained bilevel optimization problem and show that for kernel models, it can be transformed into a non-convex 1-quadratically constrained quadratic program with a polynomial-time algorithm to find the global optimum. Moreover, we give a tractable transformation and an algorithm for more complicated models that are learned by using stochastic gradient descent-based algorithms. Numerical experiments show that the surrogate model performs well compared with existing defense models when the difference between the attacker's and service provider's distributions is large. We also empirically confirm the generalization ability of the surrogate model.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Y. Mori",
        "Atsushi Nitanda",
        "A. Takeda"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2eef5df6e2347f7d83df44fe3268d4b237f0eaa5",
      "pdf_url": "",
      "publication_date": "2021-03-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3ddb72980630648e61f2223ffc6ef809a98ddd1d",
      "title": "Filter Model Extraction with Convolutional Neural Network Based on Magnitude Information",
      "abstract": "In this paper, a new model extraction method for band-pass filters with convolutional neural network (CNN) based on magnitude information only is introduced. A data augmentation approach and a data decentralization process to improve the CNN model performance by statistical analysis are proposed. A correction process utilizing real test data is introduced to improve the model performance. Compared to the existing model extraction method, this CNN model approach needs no phase information that is highly dependent on the loading circuit of a filter. Merely using the magnitude information, a good fitness for the response of high-order filters with complicated topology is achieved with this method. CNN model demonstrates a faster convergence compared to the traditional fully connected neural model.",
      "year": 2021,
      "venue": "2021 IEEE MTT-S International Microwave Filter Workshop (IMFW)",
      "authors": [
        "Junyi Liu",
        "Ke Wu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3ddb72980630648e61f2223ffc6ef809a98ddd1d",
      "pdf_url": "",
      "publication_date": "2021-11-17",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8283479256ea1d54d98ed00bb53caba38f2f688e",
      "title": "Side-Channel Analysis-Based Model Extraction on Intelligent CPS: An Information Theory Perspective",
      "abstract": "The intelligent cyber-physical system (CPS) has been applied in various fields, covering multiple critical infras-tructures and human daily life support areas. CPS Security is a major concern and of critical importance, especially the security of the intelligent control component. Side-channel analysis (SCA) is the common threat exploiting the weaknesses in system operation to extract information of the intelligent CPS. However, existing literature lacks the systematic theo-retical analysis of the side-channel attacks on the intelligent CPS, without the ability to quantify and measure the leaked information. To address these issues, we propose the SCA-based model extraction attack on intelligent CPS. First, we design an efficient and novel SCA-based model extraction framework, including the threat model, hierarchical attack process, and the multiple micro-space parallel search enabled weight extraction algorithm. Secondly, an information theory-empowered analy-sis model for side-channel attacks on intelligent CPS is built. We propose a mutual information-based quantification method and derive the capacity of side-channel attacks on intelligent CPS, formulating the amount of information leakage through side channels. Thirdly, we develop the theoretical bounds of the leaked information over multiple attack queries based on the data processing inequality and properties of entropy. These convergence bounds provide theoretical means to estimate the amount of information leaked. Finally, experimental evaluation, including real-world experiments, demonstrates the effective-ness of the proposed SCA-based model extraction algorithm and the information theory-based analysis method in intelligent CPS.",
      "year": 2021,
      "venue": "2021 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",
      "authors": [
        "Qianqian Pan",
        "Jun Wu",
        "Xi Lin",
        "Jianhua Li"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8283479256ea1d54d98ed00bb53caba38f2f688e",
      "pdf_url": "",
      "publication_date": "2021-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "87af159e1eaa99d17b9b6aff6781f2342e8ff385",
      "title": "Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Models",
      "abstract": "Machine learning models are typically made available to potential client users via inference APIs. Model extraction attacks occur when a malicious client uses information gleaned from queries to the inference API of a victim model $F_V$ to build a surrogate model $F_A$ with comparable functionality. Recent research has shown successful model extraction of image classification, and natural language processing models. In this paper, we show the first model extraction attack against real-world generative adversarial network (GAN) image translation models. We present a framework for conducting such attacks, and show that an adversary can successfully extract functional surrogate models by querying $F_V$ using data from the same domain as the training data for $F_V$. The adversary need not know $F_V$'s architecture or any other information about it beyond its intended task. We evaluate the effectiveness of our attacks using three different instances of two popular categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo (image style transfer), and (3) Super-Resolution (super resolution). Using standard performance metrics for GANs, we show that our attacks are effective. Furthermore, we conducted a large scale (125 participants) user study on Selfie-to-Anime and Monet-to-Photo to show that human perception of the images produced by $F_V$ and $F_A$ can be considered equivalent, within an equivalence bound of Cohen's d = 0.3. Finally, we show that existing defenses against model extraction attacks (watermarking, adversarial examples, poisoning) do not extend to image translation models.",
      "year": 2021,
      "venue": "",
      "authors": [
        "Sebastian Szyller",
        "Vasisht Duddu",
        "Tommi Grondahl",
        "Nirmal Asokan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/87af159e1eaa99d17b9b6aff6781f2342e8ff385",
      "pdf_url": "",
      "publication_date": "2021-04-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "46582b5600668089180542b725a2868692bcce87",
      "title": "An extraction attack on image recognition model using VAE-kdtree model",
      "abstract": "This paper proposes a black box extraction attack model on pre-trained image classifiers to rebuild a functionally equivalent model with high similarity. Common model extraction attacks use a large number of training samples to feed the target classifier which is time-consuming with redundancy. The attack results have a high dependency on the selected training samples and the target model. The extracted model may only get part of crucial features because of inappropriate sample selection. To eliminate these uncertainties, we proposed the VAE-kdtree attack model which eliminates the high dependency between selected training samples and the target model. It can not only save redundant computation, but also extract critical boundaries more accurately in image classification. This VAE-kdtree model has shown to achieve around 90% similarity on MNIST and around 80% similarity on MNIST-Fashion with a target Convolutional Network Model and a target Support Vector Machine Model. The performance of this VAE-kdtree model could be further improved by adopting higher dimension space of the kdtree.",
      "year": 2021,
      "venue": "Other Conferences",
      "authors": [
        "Tianqi Wen",
        "Haibo Hu",
        "Huadi Zheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/46582b5600668089180542b725a2868692bcce87",
      "pdf_url": "",
      "publication_date": "2021-03-13",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ef2325d326986f6e9958332100fce2f00ae65a29",
      "title": "Emerging AI Security Threats for Autonomous Cars - Case Studies",
      "abstract": "Artificial Intelligence has made a significant contribution to autonomous vehicles, from object detection to path planning. However, AI models require a large amount of sensitive training data and are usually computationally intensive to build. The commercial value of such models motivates attackers to mount various attacks. Adversaries can launch model extraction attacks for monetization purposes or step-ping-stone towards other attacks like model evasion. In specific cases, it even results in destroying brand reputation, differentiation, and value proposition. In addition, IP laws and AI-related legalities are still evolving and are not uniform across countries. We discuss model extraction attacks in detail with two use-cases and a generic kill-chain that can compromise autonomous cars. It is essential to investigate strategies to manage and mitigate the risk of model theft.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Shanthi Lekkala",
        "Tanya Motwani",
        "Manojkumar Somabhai Parmar",
        "A. Phadke"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ef2325d326986f6e9958332100fce2f00ae65a29",
      "pdf_url": "",
      "publication_date": "2021-09-10",
      "keywords_matched": [
        "model extraction",
        "model theft",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "244d8f4fc47b94efb976016acf51f3ea5852034f",
      "title": "Towards Extracting Graph Neural Network Models via Prediction Queries (Student Abstract)",
      "abstract": "Graph data has been widely used to represent data from various domain, e.g., social networks, recommendation system. With great power, the GNN models, usually as valuable properties of their owners, also become attractive targets of the adversary who covets to steal them. While existing works show that simple deep neural networks can be reproduced by so-called Model Extraction Attacks, how to extract a GNN model has not been explored. In this paper, we exploit the threat of model extraction attacks against GNN models. Unlike ordinary attacks which obtain model information via only the input-output query pairs, we utilize both the node queries and the graph structure to extract the GNNs. Furthermore, we consider the stealthiness of the attack and propose to generate legitimate queries so the extraction can be applied discreetly. We implement our attack by leveraging the responses of these queries, as well as other accessible knowledge, e.g., neighbor connectives of the queried nodes. By evaluating over three real-world datasets, our attack is shown to effectively produce a surrogate model with more than 80% equivalent predictions as the target model.",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Bang Wu",
        "Shirui Pan",
        "Xingliang Yuan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/244d8f4fc47b94efb976016acf51f3ea5852034f",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17959/17764",
      "publication_date": "2021-05-18",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4d548fd21aad60e3052455e22b7a57cc1f06e3c3",
      "title": "CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples",
      "abstract": "Cloud-based Machine Learning as a Service (MLaaS) is gradually gaining acceptance as a reliable solution to various real-life scenarios. These services typically utilize Deep Neural Networks (DNNs) to perform classification and detection tasks and are accessed through Application Programming Interfaces (APIs). Unfortunately, it is possible for an adversary to steal models from cloud-based platforms, even with black-box constraints, by repeatedly querying the public prediction API with malicious inputs. In this paper, we introduce an effective and efficient black-box attack methodology that extracts largescale DNN models from cloud-based platforms with near-perfect performance. In comparison to existing attack methods, we significantly reduce the number of queries required to steal the target model by incorporating several novel algorithms, including active learning, transfer learning, and adversarial attacks. During our experimental evaluations, we validate our proposed model for conducting theft attacks on various commercialized MLaaS platforms hosted by Microsoft, Face++, IBM, Google and Clarifai. Our results demonstrate that the proposed method can easily reveal/steal large-scale DNN models from these cloud platforms. The proposed attack method can also be used to accurately evaluates the robustness of DNN based MLaaS classifiers against theft attacks.",
      "year": 2020,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Honggang Yu",
        "Kaichen Yang",
        "Teng Zhang",
        "Yun-Yun Tsai",
        "Tsung-Yi Ho",
        "Yier Jin"
      ],
      "citation_count": 183,
      "url": "https://www.semanticscholar.org/paper/4d548fd21aad60e3052455e22b7a57cc1f06e3c3",
      "pdf_url": "https://doi.org/10.14722/ndss.2020.24178",
      "publication_date": null,
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d5ffa58133940646d1339c2610cb35f27442e0d3",
      "title": "MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation",
      "abstract": "High quality Machine Learning (ML) models are often considered valuable intellectual property by companies. Model Stealing (MS) attacks allow an adversary with blackbox access to a ML model to replicate its functionality by training a clone model using the predictions of the target model for different inputs. However, best available existing MS attacks fail to produce a high-accuracy clone without access to the target dataset or a representative dataset necessary to query the target model. In this paper, we show that preventing access to the target dataset is not an adequate defense to protect a model. We propose MAZE \u2013 a data-free model stealing attack using zeroth-order gradient estimation that produces high-accuracy clones. In contrast to prior works, MAZE uses only synthetic data created using a generative model to perform MS.Our evaluation with four image classification models shows that MAZE provides a normalized clone accuracy in the range of 0.90\u00d7 to 0.99\u00d7, and outperforms even the recent attacks that rely on partial data (JBDA, clone accuracy 0.13\u00d7 to 0.69\u00d7) and on surrogate data (KnockoffNets, clone accuracy 0.52\u00d7 to 0.97\u00d7). We also study an extension of MAZE in the partial-data setting, and develop MAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97\u00d7 to 1.0\u00d7) and reduces the query budget required for the attack by 2\u00d7-24\u00d7.",
      "year": 2020,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "S. Kariyappa",
        "A. Prakash",
        "Moinuddin K. Qureshi"
      ],
      "citation_count": 171,
      "url": "https://www.semanticscholar.org/paper/d5ffa58133940646d1339c2610cb35f27442e0d3",
      "pdf_url": "https://arxiv.org/pdf/2005.03161",
      "publication_date": "2020-05-06",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "109ad71af2ffce01b60852f8141ea91be6eed9e1",
      "title": "DeepSniffer: A DNN Model Extraction Framework Based on Learning Architectural Hints",
      "abstract": "As deep neural networks (DNNs) continue their reach into a wide range of application domains, the neural network architecture of DNN models becomes an increasingly sensitive subject, due to either intellectual property protection or risks of adversarial attacks. Previous studies explore to leverage architecture-level events disposed in hardware platforms to extract the model architecture information. They pose the following limitations: requiring a priori knowledge of victim models, lacking in robustness and generality, or obtaining incomplete information of the victim model architecture. Our paper proposes DeepSniffer, a learning-based model extraction framework to obtain the complete model architecture information without any prior knowledge of the victim model. It is robust to architectural and system noises introduced by the complex memory hierarchy and diverse run-time system optimizations. The basic idea of DeepSniffer is to learn the relation between extracted architectural hints (e.g., volumes of memory reads/writes obtained by side-channel or bus snooping attacks) and model internal architectures. Taking GPU platforms as a show case, DeepSniffer conducts model extraction by learning both the architecture-level execution features of kernels and the inter-layer temporal association information introduced by the common practice of DNN design. We demonstrate that DeepSniffer works experimentally in the context of an off-the-shelf Nvidia GPU platform running a variety of DNN models. The extracted models are directly helpful to the attempting of crafting adversarial inputs. Our experimental results show that DeepSniffer achieves a high accuracy of model extraction and thus improves the adversarial attack success rate from 14.6%$\\sim$25.5% (without network architecture knowledge) to 75.9% (with extracted network architecture). The DeepSniffer project has been released in Github.",
      "year": 2020,
      "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Shuangchen Li",
        "Lei Deng",
        "Pengfei Zuo",
        "Yu Ji",
        "Xinfeng Xie",
        "Yufei Ding",
        "Chang Liu",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 153,
      "url": "https://www.semanticscholar.org/paper/109ad71af2ffce01b60852f8141ea91be6eed9e1",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3373376.3378460",
      "publication_date": "2020-03-09",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "186377b9098efc726b8f1bda7e4f8aa2ed7bafa5",
      "title": "Cryptanalytic Extraction of Neural Network Models",
      "abstract": "We argue that the machine learning problem of model extraction is actually a cryptanalytic problem in disguise, and should be studied as such. Given oracle access to a neural network, we introduce a differential attack that can efficiently steal the parameters of the remote model up to floating point precision. Our attack relies on the fact that ReLU neural networks are piecewise linear functions, and thus queries at the critical points reveal information about the model parameters. \nWe evaluate our attack on multiple neural network models and extract models that are 2^20 times more precise and require 100x fewer queries than prior work. For example, we extract a 100,000 parameter neural network trained on the MNIST digit recognition task with 2^21.5 queries in under an hour, such that the extracted model agrees with the oracle on all inputs up to a worst-case error of 2^-25, or a model with 4,000 parameters in 2^18.5 queries with worst-case error of 2^-40.4. Code is available at this https URL.",
      "year": 2020,
      "venue": "Annual International Cryptology Conference",
      "authors": [
        "Nicholas Carlini",
        "Matthew Jagielski",
        "Ilya Mironov"
      ],
      "citation_count": 151,
      "url": "https://www.semanticscholar.org/paper/186377b9098efc726b8f1bda7e4f8aa2ed7bafa5",
      "pdf_url": "",
      "publication_date": "2020-03-10",
      "keywords_matched": [
        "model extraction",
        "extract model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d8f64187b447d1b64f69f541dbbaec71bd79d205",
      "title": "ActiveThief: Model Extraction Using Active Learning and Unannotated Public Data",
      "abstract": "Machine learning models are increasingly being deployed in practice. Machine Learning as a Service (MLaaS) providers expose such models to queries by third-party developers through application programming interfaces (APIs). Prior work has developed model extraction attacks, in which an attacker extracts an approximation of an MLaaS model by making black-box queries to it. We design ActiveThief \u2013 a model extraction framework for deep neural networks that makes use of active learning techniques and unannotated public datasets to perform model extraction. It does not expect strong domain knowledge or access to annotated data on the part of the attacker. We demonstrate that (1) it is possible to use ActiveThief to extract deep classifiers trained on a variety of datasets from image and text domains, while querying the model with as few as 10-30% of samples from public datasets, (2) the resulting model exhibits a higher transferability success rate of adversarial examples than prior work, and (3) the attack evades detection by the state-of-the-art model extraction detection method, PRADA.",
      "year": 2020,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Shukla",
        "Aditya Kanade",
        "S. Shevade",
        "V. Ganapathy"
      ],
      "citation_count": 147,
      "url": "https://www.semanticscholar.org/paper/d8f64187b447d1b64f69f541dbbaec71bd79d205",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/5432/5288",
      "publication_date": "2020-04-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d73561ab8318ce343f5cb15f96c74f210b6b24fa",
      "title": "Imitation Attacks and Defenses for Black-box Machine Translation Systems",
      "abstract": "We consider an adversary looking to steal or attack a black-box machine translation (MT) system, either for financial gain or to exploit model errors. We first show that black-box MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their victims. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades imitation model BLEU and attack transfer rates at some cost in BLEU and inference speed.",
      "year": 2020,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Eric Wallace",
        "Mitchell Stern",
        "D. Song"
      ],
      "citation_count": 130,
      "url": "https://www.semanticscholar.org/paper/d73561ab8318ce343f5cb15f96c74f210b6b24fa",
      "pdf_url": "https://www.aclweb.org/anthology/2020.emnlp-main.446.pdf",
      "publication_date": "2020-04-30",
      "keywords_matched": [
        "model stealing",
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7a3d5f1887fa908a5df303f37ec4450998caafac",
      "title": "DeepEM: Deep Neural Networks Model Recovery through EM Side-Channel Information Leakage",
      "abstract": "Neural Network (NN) accelerators are currently widely deployed in various security-crucial scenarios, including image recognition, natural language processing and autonomous vehicles. Due to economic and privacy concerns, the hardware implementations of structures and designs inside NN accelerators are usually inaccessible to the public. However, these accelerators still tend to leak crucial information through Electromagnetic (EM) side channels in addition to timing and power information. In this paper, we propose an effective and efficient model stealing attack against current popular large-scale NN accelerators deployed on hardware platforms through side-channel information. Specifically, the proposed attack approach contains two stages: 1) Inferring the underlying network architecture through EM sidechannel information; 2) Estimating the parameters, especially the weights, through a margin-based, adversarial active learning method. The experimental results show that the proposed attack approach can accurately recover the large-scale NN through EM side-channel information leakages. Overall, our attack highlights the importance of masking EM traces for large-scale NN accelerators in real-world applications.",
      "year": 2020,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Honggang Yu",
        "Haocheng Ma",
        "Kaichen Yang",
        "Yiqiang Zhao",
        "Yier Jin"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/7a3d5f1887fa908a5df303f37ec4450998caafac",
      "pdf_url": "",
      "publication_date": "2020-12-07",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "82132f7abf204b13cc171980781f76a0d627a970",
      "title": "Towards Security Threats of Deep Learning Systems: A Survey",
      "abstract": "Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learning\u2019s wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approaches\u2019 merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area.",
      "year": 2020,
      "venue": "IEEE Transactions on Software Engineering",
      "authors": [
        "Yingzhe He",
        "Guozhu Meng",
        "Kai Chen",
        "Xingbo Hu",
        "Jinwen He"
      ],
      "citation_count": 103,
      "url": "https://www.semanticscholar.org/paper/82132f7abf204b13cc171980781f76a0d627a970",
      "pdf_url": "https://arxiv.org/pdf/1911.12562",
      "publication_date": "2020-10-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ab9f91fe93b59bf44811e6fc1fcdf1b61cf9a5c9",
      "title": "Now You See Me (CME): Concept-based Model Extraction",
      "abstract": "Deep Neural Networks (DNNs) have achieved remarkable performance on a range of tasks. A key step to further empowering DNN-based approaches is improving their explainability. In this work we present CME: a concept-based model extraction framework, used for analysing DNN models via concept-based extracted models. Using two case studies (dSprites, and Caltech UCSD Birds), we demonstrate how CME can be used to (i) analyse the concept information learned by a DNN model (ii) analyse how a DNN uses this concept information when predicting output labels (iii) identify key concept information that can further improve DNN predictive performance (for one of the case studies, we showed how model accuracy can be improved by over 14%, using only 30% of the available concepts).",
      "year": 2020,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Dmitry Kazhdan",
        "B. Dimanov",
        "M. Jamnik",
        "Pietro Lio'",
        "Adrian Weller"
      ],
      "citation_count": 81,
      "url": "https://www.semanticscholar.org/paper/ab9f91fe93b59bf44811e6fc1fcdf1b61cf9a5c9",
      "pdf_url": "",
      "publication_date": "2020-10-25",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7ea18b512074acc10d6a4025cb479955ba295f2d",
      "title": "Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realisation",
      "abstract": "Machine learning models are shown to face a severe threat from Model Extraction Attacks, where a well-trained private model owned by a service provider can be stolen by an attacker pretending as a client. Unfortunately, prior works focus on the models trained over the Euclidean space, e.g., images and texts, while how to extract a GNN model that contains a graph structure and node features is yet to be explored. In this paper, for the first time, we comprehensively investigate and develop model extraction attacks against GNN models. We first systematically formalise the threat modelling in the context of GNN model extraction and classify the adversarial threats into seven categories by considering different background knowledge of the attacker, e.g., attributes and/or neighbour connections of the nodes obtained by the attacker. Then we present detailed methods which utilise the accessible knowledge in each threat to implement the attacks. By evaluating over three real-world datasets, our attacks are shown to extract duplicated models effectively, i.e., 84% - 89% of the inputs in the target domain have the same output predictions as the victim model.",
      "year": 2020,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Bang Wu",
        "Xiangwen Yang",
        "Shirui Pan",
        "Xingliang Yuan"
      ],
      "citation_count": 66,
      "url": "https://www.semanticscholar.org/paper/7ea18b512074acc10d6a4025cb479955ba295f2d",
      "pdf_url": "",
      "publication_date": "2020-10-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d4a679c685de885ab98b7c1f2378e26a255de490",
      "title": "Model Extraction Attacks and Defenses on Cloud-Based Machine Learning Models",
      "abstract": "Machine learning models have achieved state-of-the-art performance in various fields, from image classification to speech recognition. However, such models are trained with a large amount of sensitive training data, and are typically computationally expensive to build. As a result, many cloud providers (e.g., Google) have launched machine-learning-as-a-service, which helps clients benefit from the sophisticated cloud-based machine learning models via accessing public APIs. Such a business paradigm significantly expedites and simplifies the development circles. Unfortunately, the commercial value of such cloud-based machine learning models motivates attackers to conduct model extraction attacks for free use or as a springboard to conduct other attacks (e.g., craft adversarial examples in black-box settings). In this article, we conduct a thorough investigation of existing approaches to model extraction attacks and defenses on cloud-based models. We classify the state-of-the-art attack schemes into two categories based on whether the attacker aims to steal the property (i.e., parameters, hyperparameters, and architecture) or the functionality of the model. We also categorize defending schemes into two groups based on whether the scheme relies on output disturbance or query observation. We not only present a detailed survey of each method, but also demonstrate the comparison of both attack and defense approaches via experiments. We highlight several future directions in both model extraction attacks and its defenses, which shed light on possible avenues for further studies.",
      "year": 2020,
      "venue": "IEEE Communications Magazine",
      "authors": [
        "Xueluan Gong",
        "Qian Wang",
        "Yanjiao Chen",
        "Wang Yang",
        "Xinye Jiang"
      ],
      "citation_count": 62,
      "url": "https://www.semanticscholar.org/paper/d4a679c685de885ab98b7c1f2378e26a255de490",
      "pdf_url": "",
      "publication_date": "2020-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "983f1e19ba55aa97bd20086ca7ad5cf4e436a37a",
      "title": "Model extraction from counterfactual explanations",
      "abstract": "Post-hoc explanation techniques refer to a posteriori methods that can be used to explain how black-box machine learning models produce their outcomes. Among post-hoc explanation techniques, counterfactual explanations are becoming one of the most popular methods to achieve this objective. In particular, in addition to highlighting the most important features used by the black-box model, they provide users with actionable explanations in the form of data instances that would have received a different outcome. Nonetheless, by doing so, they also leak non-trivial information about the model itself, which raises privacy issues. In this work, we demonstrate how an adversary can leverage the information provided by counterfactual explanations to build high-fidelity and high-accuracy model extraction attacks. More precisely, our attack enables the adversary to build a faithful copy of a target model by accessing its counterfactual explanations. The empirical evaluation of the proposed attack on black-box models trained on real-world datasets demonstrates that they can achieve high-fidelity and high-accuracy extraction even under low query budgets.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "U. A\u00efvodji",
        "Alexandre Bolot",
        "S. Gambs"
      ],
      "citation_count": 58,
      "url": "https://www.semanticscholar.org/paper/983f1e19ba55aa97bd20086ca7ad5cf4e436a37a",
      "pdf_url": "",
      "publication_date": "2020-09-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a7abac76ec8aea31ee595af45d733bd462d7f35b",
      "title": "Stealing Deep Reinforcement Learning Models for Fun and Profit",
      "abstract": "This paper presents the first model extraction attack against Deep Reinforcement Learning (DRL), which enables an external adversary to precisely recover a black-box DRL model only from its interaction with the environment. Model extraction attacks against supervised Deep Learning models have been widely studied. However, those techniques cannot be applied to the reinforcement learning scenario due to DRL models' high complexity, stochasticity and limited observable information. We propose a novel methodology to overcome the above challenges. The key insight of our approach is that the process of DRL model extraction is equivalent to imitation learning, a well-established solution to learn sequential decision-making policies. Based on this observation, our methodology first builds a classifier to reveal the training algorithm family of the targeted black-box DRL model only based on its predicted actions, and then leverages state-of-the-art imitation learning techniques to replicate the model from the identified algorithm family. Experimental results indicate that our methodology can effectively recover the DRL models with high fidelity and accuracy. We also demonstrate two use cases to show that our model extraction attack can (1) significantly improve the success rate of adversarial attacks, and (2) steal DRL models stealthily even they are protected by DNN watermarks. These pose a severe threat to the intellectual property and privacy protection of DRL applications.",
      "year": 2020,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Kangjie Chen",
        "Tianwei Zhang",
        "Xiaofei Xie",
        "Yang Liu"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/a7abac76ec8aea31ee595af45d733bd462d7f35b",
      "pdf_url": "https://ink.library.smu.edu.sg/sis_research/7110",
      "publication_date": "2020-06-09",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8765853a2d7027dfe91d650462d7431552bd7315",
      "title": "ES Attack: Model Stealing Against Deep Neural Networks Without Data Hurdles",
      "abstract": "Deep neural networks (DNNs) have become the essential components for various commercialized machine learning services, such as Machine Learning as a Service (MLaaS). Recent studies show that machine learning services face severe privacy threats - well-trained DNNs owned by MLaaS providers can be stolen through public APIs, namely model stealing attacks. However, most existing works undervalued the impact of such attacks, where a successful attack has to acquire confidential training data or auxiliary data regarding the victim DNN. In this paper, we propose ES Attack, a novel model stealing attack without any data hurdles. By using heuristically generated synthetic data, ES Attack iteratively trains a substitute model and eventually achieves a functionally equivalent copy of the victim DNN. The experimental results reveal the severity of ES Attack: i) ES Attack successfully steals the victim model without data hurdles, and ES Attack even outperforms most existing model stealing attacks using auxiliary data in terms of model accuracy; ii) most countermeasures are ineffective in defending ES Attack; iii) ES Attack facilitates further attacks relying on the stolen model.",
      "year": 2020,
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
      "authors": [
        "Xiaoyong Yuan",
        "Lei Ding",
        "Lan Zhang",
        "Xiaolin Li",
        "D. Wu"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/8765853a2d7027dfe91d650462d7431552bd7315",
      "pdf_url": "https://doi.org/10.1109/tetci.2022.3147508",
      "publication_date": "2020-09-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "db3f6b0581d2cc23756e3fec1076a4399e97a0d8",
      "title": "Model Extraction Attacks on Recurrent Neural Networks",
      "abstract": ": Model extraction attacks are an attack in which an adversary utilizes a query access to the target model to obtain a new model whose performance is equivalent to the target model e \ufb03 ciently, i.e., fewer datasets and computational resources than those of the target model. Existing works have dealt with only simple deep neural networks (DNNs), e.g., only three layers, as targets of model extraction attacks, and hence are not aware of the e \ufb00 ectiveness of recurrent neural networks (RNNs) in dealing with time-series data. In this work, we shed light on the threats of model extraction attacks on RNNs. We discuss whether a model with a higher accuracy can be extracted with a simple RNN from a long short-term memory (LSTM), which is a more complicated and powerful type of RNN. Speci\ufb01cally, we tackle the following problems. First, in case of a classi\ufb01cation task, such as image recognition, extraction of an RNN model without \ufb01nal outputs from an LSTM model is presented by utilizing outputs halfway through the sequence. Next, in case of a regression task such as weather forecasting, a new attack by newly con\ufb01guring a loss function is presented. We conduct experiments on our model extraction attacks on an RNN and an LSTM trained with publicly available academic datasets. We then show that a model with a higher accuracy can be extracted e \ufb03 ciently, especially through con\ufb01guring a loss function and a more complex architecture di \ufb00 erent from the target model.",
      "year": 2020,
      "venue": "Journal of Information Processing",
      "authors": [
        "Tatsuya Takemura",
        "Naoto Yanai",
        "T. Fujiwara"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/db3f6b0581d2cc23756e3fec1076a4399e97a0d8",
      "pdf_url": "https://doi.org/10.2197/ipsjjip.28.1010",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e4a4a8ec2a4d538f0f4ad721d99f7617578209fc",
      "title": "FaDec: A Fast Decision-based Attack for Adversarial Machine Learning",
      "abstract": "Due to the excessive use of cloud-based machine learning (ML) services, the smart cyber-physical systems (CPS) are increasingly becoming vulnerable to black-box attacks on their ML modules. Traditionally, the black-box attacks are either transfer attacks requiring model stealing, or score/decision-based gradient estimation attacks requiring a large number of queries. In practical scenarios, especially for cloud-based ML services and timing-constrained CPS use-cases, every query incurs a huge cost, thereby rendering state-of-the-art decision-based attacks ineffective in such settings. Towards this, we propose a novel methodology for automatically generating an extremely fast and imperceptible decision-based attack called FaDec. It follows two main steps: (1) fast estimation of the classification boundary by combining the half-interval search-based algorithm with gradient sign estimation to reduce the number of queries; and (2) adversarial noise optimization to ensure the imperceptibility. For illustration, we evaluate FaDec on the image recognition and traffic sign detection using multiple state-of-the-art DNNs trained on CIFAR-10 and the German Traffic Sign Recognition Benchmarks (GTSRB) datasets. The experimental analysis shows that the proposed FaDec attack is 16x faster compared to the state-of-the-art decision-based attacks, and generates an attack image with better imperceptibility for a much lesser number of iterations, thereby making our attack more powerful in practical scenarios. We open-sourced the complete code and results of our methodology at https://github.com/fklodhi/FaDec.",
      "year": 2020,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Faiq Khalid",
        "Hassan Ali",
        "M. Hanif",
        "Semeen Rehman",
        "Rehan Ahmed",
        "M. Shafique"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/e4a4a8ec2a4d538f0f4ad721d99f7617578209fc",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "634d65bba4efc94b17dcc4a60fd0bb2de29ac2c4",
      "title": "MARLeME: A Multi-Agent Reinforcement Learning Model Extraction Library",
      "abstract": "Multi-Agent Reinforcement Learning (MARL) encompasses a powerful class of methodologies that have been applied in a wide range of fields. An effective way to further empower these methodologies is to develop approaches and tools that could expand their interpretability and explainability. In this work, we introduce MARLeME: a MARL model extraction library, designed to improve explainability of MARL systems by approximating them with symbolic models. Symbolic models offer a high degree of interpretability, well-defined properties, and verifiable behaviour. Consequently, they can be used to inspect and better understand the underlying MARL systems and corresponding MARL agents, as well as to replace all/some of the agents that are particularly safety and security critical. In this work, we demonstrate how MARLeME can be applied to two well-known case studies (Cooperative Navigation and RoboCup Takeaway), using extracted models based on Abstract Argumentation.",
      "year": 2020,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Dmitry Kazhdan",
        "Z. Shams",
        "Pietro Lio"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/634d65bba4efc94b17dcc4a60fd0bb2de29ac2c4",
      "pdf_url": "http://arxiv.org/pdf/2004.07928",
      "publication_date": "2020-04-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c774bb2b6cb1108773a0e86aff0fc00003c7397a",
      "title": "Finite\u2010state model extraction and visualization from Java program execution",
      "abstract": "Finite\u2010state models are extensively used for discrete systems and they have also been adopted for the analysis and verification of concurrent systems. Programs that have a repetitive cycle, such as event\u2010driven servers and controllers, lend themselves to finite\u2010state modeling. In this article, we use the term model extraction to refer to the construction of a finite\u2010state model from an execution trace of a Java program and a set of key attributes, that is, a subset of the fields of the objects in the program execution. By choosing different sets of attributes, different finite\u2010state models (or views) of the execution can be obtained. Such models aid program comprehension and they can also be used in debugging a program. We present algorithms for model extraction and also for model abstraction in order to reduce the size of the extracted models so that they are amenable to visualization. For long executions, we show how to minimize the overhead of execution trace collection through a bytecode instrumentation technique; and, for large models, which are not amenable to visualization, we show how key properties of the extracted model can be checked against declarative specifications. We have implemented our techniques in the context of JIVE, an Eclipse plugin that supports runtime visualization and analysis of Java program executions. We illustrate our techniques through a collection of case studies of varying size and complexity, from classic problems of concurrency control to a medium\u2010size protocol for authorization (OAuth2.0 protocol) to a large\u2010scale software that underlies web applications (Apache Tomcat server).",
      "year": 2020,
      "venue": "Software, Practice & Experience",
      "authors": [
        "Jevitha K. P.",
        "Swaminathan Jayaraman",
        "B. Jayaraman",
        "Sethumadhavan M"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/c774bb2b6cb1108773a0e86aff0fc00003c7397a",
      "pdf_url": "",
      "publication_date": "2020-10-11",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "92c6a2f7e178337fb99c695ae856b9377e8e72a5",
      "title": "Model Extraction Attacks against Recurrent Neural Networks",
      "abstract": "Model extraction attacks are a kind of attacks in which an adversary obtains a new model, whose performance is equivalent to that of a target model, via query access to the target model efficiently, i.e., fewer datasets and computational resources than those of the target model. Existing works have dealt with only simple deep neural networks (DNNs), e.g., only three layers, as targets of model extraction attacks, and hence are not aware of the effectiveness of recurrent neural networks (RNNs) in dealing with time-series data. In this work, we shed light on the threats of model extraction attacks against RNNs. We discuss whether a model with a higher accuracy can be extracted with a simple RNN from a long short-term memory (LSTM), which is a more complicated and powerful RNN. Specifically, we tackle the following problems. First, in a case of a classification problem, such as image recognition, extraction of an RNN model without final outputs from an LSTM model is presented by utilizing outputs halfway through the sequence. Next, in a case of a regression problem. such as in weather forecasting, a new attack by newly configuring a loss function is presented. We conduct experiments on our model extraction attacks against an RNN and an LSTM trained with publicly available academic datasets. We then show that a model with a higher accuracy can be extracted efficiently, especially through configuring a loss function and a more complex architecture different from the target model.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Tatsuya Takemura",
        "Naoto Yanai",
        "T. Fujiwara"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/92c6a2f7e178337fb99c695ae856b9377e8e72a5",
      "pdf_url": "",
      "publication_date": "2020-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "31bfc3d4c51534a0c27ea9928423c53deb1ff9f0",
      "title": "MEME: Generating RNN Model Explanations via Model Extraction",
      "abstract": "Recurrent Neural Networks (RNNs) have achieved remarkable performance on a range of tasks. A key step to further empowering RNN-based approaches is improving their explainability and interpretability. In this work we present MEME: a model extraction approach capable of approximating RNNs with interpretable models represented by human-understandable concepts and their interactions. We demonstrate how MEME can be applied to two multivariate, continuous data case studies: Room Occupation Prediction, and In-Hospital Mortality Prediction. Using these case-studies, we show how our extracted models can be used to interpret RNNs both locally and globally, by approximating RNN decision-making via interpretable concept interactions.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Dmitry Kazhdan",
        "B. Dimanov",
        "M. Jamnik",
        "Pietro Lio'"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/31bfc3d4c51534a0c27ea9928423c53deb1ff9f0",
      "pdf_url": "",
      "publication_date": "2020-12-13",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b9187199b87187d8abc769d6d8e61a45de8f7870",
      "title": "Low-Dose CT Image Denoising Using Parallel-Clone Networks",
      "abstract": "Deep neural networks have a great potential to improve image denoising in low-dose computed tomography (LDCT). Popular ways to increase the network capacity include adding more layers or repeating a modularized clone model in a sequence. In such sequential architectures, the noisy input image and end output image are commonly used only once in the training model, which however limits the overall learning performance. In this paper, we propose a parallel-clone neural network method that utilizes a modularized network model and exploits the benefit of parallel input, parallel-output loss, and clone-toclone feature transfer. The proposed model keeps a similar or less number of unknown network weights as compared to conventional models but can accelerate the learning process significantly. The method was evaluated using the Mayo LDCT dataset and compared with existing deep learning models. The results show that the use of parallel input, parallel-output loss, and clone-to-clone feature transfer all can contribute to an accelerated convergence of deep learning and lead to improved image quality in testing. The parallel-clone network has been demonstrated promising for LDCT image denoising.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Siqi Li",
        "Guobao Wang"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/b9187199b87187d8abc769d6d8e61a45de8f7870",
      "pdf_url": "",
      "publication_date": "2020-05-14",
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "57099cf852807c652fd55274a3ce4a8c9777ef56",
      "title": "Neural Model Stealing Attack to Smart Mobile Device on Intelligent Medical Platform",
      "abstract": "To date, the Medical Internet of Things (MIoT) technology has been recognized and widely applied due to its convenience and practicality. The MIoT enables the application of machine learning to predict diseases of various kinds automatically and accurately, assisting and facilitating effective and efficient medical treatment. However, the MIoT are vulnerable to cyberattacks which have been constantly advancing. In this paper, we establish a MIoT platform and demonstrate a scenario where a trained Convolutional Neural Network (CNN) model for predicting lung cancer complicated with pulmonary embolism can be attacked. First, we use CNN to build a model to predict lung cancer complicated with pulmonary embolism and obtain high detection accuracy. Then, we build a copycat model using only a small amount of data labeled by the target network, aiming to steal the established prediction model. Experimental results prove that the stolen model can also achieve a relatively high prediction outcome, revealing that the copycat network could successfully copy the prediction performance from the target network to a large extent. This also shows that such a prediction model deployed on MIoT devices can be stolen by attackers, and effective prevention strategies are open questions for researchers.",
      "year": 2020,
      "venue": "Wireless Communications and Mobile Computing",
      "authors": [
        "Liqiang Zhang",
        "Guanjun Lin",
        "Bixuan Gao",
        "Zhibao Qin",
        "Yonghang Tai",
        "Jun Zhang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/57099cf852807c652fd55274a3ce4a8c9777ef56",
      "pdf_url": "https://downloads.hindawi.com/journals/wcmc/2020/8859489.pdf",
      "publication_date": "2020-11-26",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "copycat model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f9f49997c404e386610289202a7c3812c6cbfe34",
      "title": "Differentially Private Machine Learning Model against Model Extraction Attack",
      "abstract": "Machine learning model is vulnerable to model extraction attacks since the attackers can send plenty of queries to infer the hyperparameters of the machine learning model thus stealing confidential information of the learning models. Therefore, there is a urgent need to defend against such an attack. Differential privacy is a promising technique to protect the valuable information. We propose a differential privacy-based method applied in the linear neural network to obfuscate the output of the machine learning model. The security and utility issue of injecting a noise layer to the linear neural network is mathematically analyzed. The experiment results show that our proposed method can lower the attacker's extraction rate while keeping high utility.",
      "year": 2020,
      "venue": "2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",
      "authors": [
        "Zelei Cheng",
        "Zuotian Li",
        "Jiwei Zhang",
        "Shuhan Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f9f49997c404e386610289202a7c3812c6cbfe34",
      "pdf_url": "",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "title": "Model Stealing Defense with Hybrid Fuzzy Models: Work-in-Progress",
      "abstract": "With increasing applications of Deep Neural Networks (DNNs) to edge computing systems, security issues have received more attentions. Particularly, model stealing attack is one of the biggest challenge to the privacy of models. To defend against model stealing attack, we propose a novel protection architecture with fuzzy models. Each fuzzy model is designed to generate wrong predictions corresponding to a particular category. In addition\u2019 we design a special voting strategy to eliminate the systemic errors, which can destroy the dark knowledge in predictions at the same time. Preliminary experiments show that our method substantially decreases the clone model's accuracy (up to 20%) without loss of inference accuracy for benign users.",
      "year": 2020,
      "venue": "International Conference on Hardware/Software Codesign and System Synthesis",
      "authors": [
        "Zicheng Gong",
        "Wei Jiang",
        "Jinyu Zhan",
        "Ziwei Song"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "pdf_url": "",
      "publication_date": "2020-09-20",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9356a216e5ae48c0007b66687a3da47fc2bea834",
      "title": "Perturbing Inputs to Prevent Model Stealing",
      "abstract": "We show how perturbing inputs to machine learning services (ML-service) deployed in the cloud can protect against model stealing attacks. In our formulation, there is an ML-service that receives inputs from users and returns the output of the model. There is an attacker that is interested in learning the parameters of the ML-service. We use the linear and logistic regression models to illustrate how strategically adding noise to the inputs fundamentally alters the attacker\u2019s estimation problem. We show that even with infinite samples, the attacker would not be able to recover the true model parameters. We focus on characterizing the trade-off between the error in the attacker\u2019s estimate of the parameters with the error in the ML-service\u2019s output.",
      "year": 2020,
      "venue": "IEEE Conference on Communications and Network Security",
      "authors": [
        "J. Grana"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9356a216e5ae48c0007b66687a3da47fc2bea834",
      "pdf_url": "https://arxiv.org/pdf/2005.05823",
      "publication_date": "2020-05-12",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "prevent model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b97bc90c7106b6c44ffaa0c0ce81a67134524e89",
      "title": "A Low-Cost Image Encryption Method to Prevent Model Stealing of Deep Neural Network",
      "abstract": "Model stealing attack may happen by stealing useful data transmitted from embedded end to server end for an artificial intelligent systems. In this paper, we are interested in preventing model stealing of neural network for resource-constrained systems. We propose an Image Encryption based on Class Activation Map (IECAM) to encrypt information before transmitting in embedded end. According to class activation map, IECAM chooses certain key areas of the image to be encrypted with the purpose of reducing the model stealing risk of neural network. With partly encrypted information, IECAM can greatly reduce the time overheads of encryption/decryption in both embedded and server ends, especially for big size images. The experimental results demonstrate that our method can significantly reduce time overheads of encryption/decryption and the risk of model stealing compared with traditional methods.",
      "year": 2020,
      "venue": "J. Circuits Syst. Comput.",
      "authors": [
        "Wei Jiang",
        "Zicheng Gong",
        "Jinyu Zhan",
        "Zhiyuan He",
        "Weijia Pan"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/b97bc90c7106b6c44ffaa0c0ce81a67134524e89",
      "pdf_url": "",
      "publication_date": "2020-05-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "prevent model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "65d434bfef6e62eb2e632f7d78f523292e0d9c0a",
      "title": "Securing Machine Learning Architectures and Systems",
      "abstract": "Machine learning (ML), and deep learning in particular, have become a critical workload as they are becoming increasingly applied at the core of a wide range of application spaces. Computer systems, from the architecture up, have been impacted by ML in two primary directions: (1) ML is an increasingly important computing workload, with new accelerators and systems targeted to support both training and inference at scale; and (2) ML supporting computer system decisions, both during design and run times, with new machine learning based algorithms controlling systems to optimize their performance, reliability and robustness. In this paper, we will explore the intersection of security, ML and computing systems, identifying both security challenges and opportunities. Machine learning systems are vulnerable to new attacks including adversarial attacks crafted to fool a classifier to the attacker's advantage, membership inference attacks attempting to compromise the privacy of the training data, and model extraction attacks seeking to recover the hyperparameters of a (secret) model. Architecture can be a target of these attacks when supporting ML (or is supported by ML), but also provides an opportunity to develop defenses against them, which we will illustrate with three examples from our recent work. First, we show how ML based hardware malware detectors can be attacked with adversarial perturbations to the Malware and how we can develop detectors that resist these attacks. Second, we show an example of microarchitectural side channel attacks that can be used to extract the secret parameters of a neural network and potential defenses against it. Finally, we discuss how hardware and systems can be used to make ML more robust against adversarial and other attacks.",
      "year": 2020,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Shirin Haji Amin Shirazi",
        "Hoda Naghibijouybari",
        "N. Abu-Ghazaleh"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/65d434bfef6e62eb2e632f7d78f523292e0d9c0a",
      "pdf_url": "",
      "publication_date": "2020-09-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d6a0da1403eb183a24541ac5636eb7a44223b7dc",
      "title": "Special-purpose Model Extraction Attacks: Stealing Coarse Model with Fewer Queries",
      "abstract": "Model extraction (ME) attacks have been shown to cause financial losses for Machine-Learning-as-a-Service (MLaaS) providers. Attackers steal ML models on MLaaS platforms by building substitute models using queries to and responses from MLaaS platforms. The ML models targeted by attackers are called targeted models. In previous studies, researchers have assumed that attackers build substitute models that classify the same number of classes as targeted ones, which classify thousands or millions of classes to meet users' diverse expectations. We call such models general-purpose models. In fact, attackers can monetize stolen models if they accurately distinguish some classes from others. We call such models special-purpose models. For instance, a model that detects vehicles is useful for collision avoidance systems, and a model that detects wild animals is useful to drive them away from agricultural land. In this work, we investigate a threat of special-purpose ME attacks that steal special-purpose models. Our experimental results show that attackers can build an accurate special-purpose model, which achieves an 80% f-measure, with as few as 100 queries in the worst case. We discuss the difficulty in preventing the attacks with previously proposed defense methods and point out the necessity of a new defense method.",
      "year": 2020,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Rina Okada",
        "Zen Ishikura",
        "Toshiki Shibahara",
        "Satoshi Hasegawa"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d6a0da1403eb183a24541ac5636eb7a44223b7dc",
      "pdf_url": "",
      "publication_date": "2020-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "steal ML model",
        "steal ML models"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "08a5204b3697ce4f6de7e5cf1d0dfd64721299d7",
      "title": "Enhanced Pre-processing and Parameterization Process of Generic Code Clone Detection Model for Clones in Java Applications",
      "abstract": "Code clones are repeated source code in a program. There are four types of code clone which are: Type 1, Type 2, Type 3 and Type 4. Various code clone detection models have been used to detect code clone. Generic Code Clone model is a model that consists of a combination of five processes in detecting code clone from Type-1 until Type-4 in Java Applications. The five processes are Pre-processing, Transformation, Parameterization, Categorization and Match Detection process. This work aims to improve code clone detection by enhancing the Generic Code Clone Detection (GCCD) model. Therefore, the Preprocessing and Parameterization process is enhanced to achieve this aim. The enhancement is to determine the best constant and weightage that can be used to improve the code clone detection result. The code clone detection result from the proposed enhancement shows that private with its weightage is the best constant and weightage for the Generic Code Clone Detection Model.",
      "year": 2020,
      "venue": "",
      "authors": [
        "Nur Nadzirah Mokhtar",
        "Al-Fahim Mubarak-Ali",
        "M. Azwan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/08a5204b3697ce4f6de7e5cf1d0dfd64721299d7",
      "pdf_url": "http://thesai.org/Downloads/Volume11No6/Paper_69-Enhanced_Pre_Processing_and_Parameterization_Process.pdf",
      "publication_date": null,
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "12bf1e90f285b23e27adaf6668d6a429ccc54887",
      "title": "Bident Structure for Neural Network Model Protection",
      "abstract": ": Deep neural networks are widely deployed in a variety of application areas to provide real-time inference services, such as mobile phones, autonomous vehicles and industrial automation. Deploying trained models in end-user devices rises high demands on protecting models against model stealing attacks. To tackle this concern, applying cryptography algorithms and using trusted execution environments have been proposed. However, both approaches cause significant overhead on inference time. With the support of trusted execution environment, we propose bident-structure networks to protect the neural networks while maintaining inference efficiency. Our main idea is inspired by the secret-sharing concept from cryptography community, where we treat the neural network as the secret to be protected. We prove the feasibility of bident-structure methods by empirical experiments on MNIST. Experimental results also demonstrate that efficiency overhead can be reduced by compressing sub-networks running in trusted execution environments.",
      "year": 2020,
      "venue": "International Conference on Information Systems Security and Privacy",
      "authors": [
        "Hsiao-Ying Lin",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/12bf1e90f285b23e27adaf6668d6a429ccc54887",
      "pdf_url": "https://doi.org/10.5220/0008923403770384",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "27ce5e10571e9a4e66ed0cb2eaf348582750767a",
      "title": "Monitoring-based Differential Privacy Mechanism Against Query-Flooding Parameter Duplication Attack",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this paper, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Haonan Yan",
        "Xiaoguang Li",
        "Hui Li",
        "Jiamin Li",
        "Wenhai Sun",
        "Fenghua Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/27ce5e10571e9a4e66ed0cb2eaf348582750767a",
      "pdf_url": "",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9a369aaa9f2ee96ca99df66cc6c5c6f46889b8a2",
      "title": "Mitigating Query-Flooding Parameter Duplication Attack on Regression Models with High-Dimensional Gaussian Mechanism",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Differential privacy (DP) has been considered a promising technique to mitigate this attack. However, we find that the vulnerability persists when regression models are being protected by current DP solutions. We show that the adversary can launch a query-flooding parameter duplication (QPD) attack to infer the model information by repeated queries. \nTo defend against the QPD attack on logistic and linear regression models, we propose a novel High-Dimensional Gaussian (HDG) mechanism to prevent unauthorized information disclosure without interrupting the intended services. In contrast to prior work, the proposed HDG mechanism will dynamically generate the privacy budget and random noise for different queries and their results to enhance the obfuscation. Besides, for the first time, HDG enables an optimal privacy budget allocation that automatically determines the minimum amount of noise to be added per user-desired privacy level on each dimension. We comprehensively evaluate the performance of HDG using real-world datasets and shows that HDG effectively mitigates the QPD attack while satisfying the privacy requirements. We also prepare to open-source the relevant codes to the community for further research.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Xiaoguang Li",
        "Hui Li",
        "Haonan Yan",
        "Zelei Cheng",
        "Wenhai Sun",
        "Hui Zhu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9a369aaa9f2ee96ca99df66cc6c5c6f46889b8a2",
      "pdf_url": "",
      "publication_date": "2020-02-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "981c7b3cd70e23fce8f8877ccb6ee92810b65e85",
      "title": "Leveraging Extracted Model Adversaries for Improved Black Box Attacks",
      "abstract": "We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our approach is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail. These perturbed inputs are used against the victim. In experiments we find that our method improves on the efficacy of the ADDANY\u2014a white box attack\u2014performed on the approximate model by 25% F1, and the ADDSENT attack\u2014a black box attack\u2014by 11% F1.",
      "year": 2020,
      "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
      "authors": [
        "Naveen Jafer Nizar",
        "Ari Kobren"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/981c7b3cd70e23fce8f8877ccb6ee92810b65e85",
      "pdf_url": "https://www.aclweb.org/anthology/2020.blackboxnlp-1.6.pdf",
      "publication_date": "2020-10-30",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "52a222d38a8640499010d470d5589a81882bc425",
      "title": "High Accuracy and High Fidelity Extraction of Neural Networks",
      "abstract": "In a model extraction attack, an adversary steals a copy of a remotely deployed machine learning model, given oracle prediction access. We taxonomize model extraction attacks around two objectives: *accuracy*, i.e., performing well on the underlying learning task, and *fidelity*, i.e., matching the predictions of the remote victim classifier on any input. \nTo extract a high-accuracy model, we develop a learning-based attack exploiting the victim to supervise the training of an extracted model. Through analytical and empirical arguments, we then explain the inherent limitations that prevent any learning-based strategy from extracting a truly high-fidelity model---i.e., extracting a functionally-equivalent model whose predictions are identical to those of the victim model on all possible inputs. Addressing these limitations, we expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction (i.e., without training) of a model's weights. \nWe perform experiments both on academic datasets and a state-of-the-art image classifier trained with 1 billion proprietary images. In addition to broadening the scope of model extraction research, our work demonstrates the practicality of model extraction attacks against production-grade systems.",
      "year": 2019,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Matthew Jagielski",
        "Nicholas Carlini",
        "David Berthelot",
        "Alexey Kurakin",
        "Nicolas Papernot"
      ],
      "citation_count": 424,
      "url": "https://www.semanticscholar.org/paper/52a222d38a8640499010d470d5589a81882bc425",
      "pdf_url": "",
      "publication_date": "2019-09-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs",
      "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Kalpesh Krishna",
        "Gaurav Singh Tomar",
        "Ankur P. Parikh",
        "Nicolas Papernot",
        "Mohit Iyyer"
      ],
      "citation_count": 230,
      "url": "https://www.semanticscholar.org/paper/ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "pdf_url": "",
      "publication_date": "2019-10-27",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c40441bd48c08ab7db9443afdb3084387b06e61f",
      "title": "DAWN: Dynamic Adversarial Watermarking of Neural Networks",
      "abstract": "Training machine learning (ML) models is expensive in terms of computational power, amounts of labeled data and human expertise. Thus, ML models constitute business value for their owners. Embedding digital watermarks during model training allows a model owner to later identify their models in case of theft or misuse. However, model functionality can also be stolen via model extraction, where an adversary trains a surrogate model using results returned from a prediction API of the original model. Recent work has shown that model extraction is a realistic threat. Existing watermarking schemes are ineffective against model extraction since it is the adversary who trains the surrogate model. In this paper, we introduce DAWN (Dynamic Adversarial Watermarking of Neural Networks), the first approach to use watermarking to deter model extraction theft. Unlike prior watermarking schemes, DAWN does not impose changes to the training process but operates at the prediction API of the protected model, by dynamically changing the responses for a small subset of queries (e.g., 0.5%) from API clients. This set is a watermark that will be embedded in case a client uses its queries to train a surrogate model. We show that DAWN is resilient against two state-of-the-art model extraction attacks, effectively watermarking all extracted surrogate models, allowing model owners to reliably demonstrate ownership (with confidence greater than 1-2-64), incurring negligible loss of prediction accuracy (0.03-0.5%).",
      "year": 2019,
      "venue": "ACM Multimedia",
      "authors": [
        "Sebastian Szyller",
        "B. Atli",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 203,
      "url": "https://www.semanticscholar.org/paper/c40441bd48c08ab7db9443afdb3084387b06e61f",
      "pdf_url": "https://aaltodoc.aalto.fi/bitstreams/11ec7679-d118-4245-aad9-c8ff13a384e7/download",
      "publication_date": "2019-06-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "da10f79f983fd4fbd589ed7ffa68d33964841443",
      "title": "Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks",
      "abstract": "High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker's error rate up to a factor of 85$\\times$ with minimal impact on the utility for benign users.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 185,
      "url": "https://www.semanticscholar.org/paper/da10f79f983fd4fbd589ed7ffa68d33964841443",
      "pdf_url": "",
      "publication_date": "2019-06-26",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "DNN model stealing",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5effd428e3012061ba3d8b32f1952c2938c7ab7b",
      "title": "Deep Neural Network Fingerprinting by Conferrable Adversarial Examples",
      "abstract": "In Machine Learning as a Service, a provider trains a deep neural network and provides many users access. The hosted (source) model is susceptible to model stealing attacks, where an adversary derives a \\emph{surrogate model} from API access to the source model. For post hoc detection of such attacks, the provider needs a robust method to determine whether a suspect model is a surrogate of their model. We propose a fingerprinting method for deep neural network classifiers that extracts a set of inputs from the source model so that only surrogates agree with the source model on the classification of such inputs. These inputs are a subclass of transferable adversarial examples which we call \\emph{conferrable} adversarial examples that exclusively transfer with a target label from a source model to its surrogates. We propose a new method to generate these conferrable adversarial examples. We present an extensive study on the unremovability of our fingerprint against fine-tuning, weight pruning, retraining, retraining with different architectures, three model extraction attacks from related work, transfer learning, adversarial training, and two new adaptive attacks. Our fingerprint is robust against distillation, related model extraction attacks, and even transfer learning when the attacker has no access to the model provider's dataset. Our fingerprint is the first method that reaches an AUC of 1.0 in verifying surrogates, compared to an AUC of 0.63 by previous fingerprints.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Nils Lukas",
        "Yuxuan Zhang",
        "F. Kerschbaum"
      ],
      "citation_count": 169,
      "url": "https://www.semanticscholar.org/paper/5effd428e3012061ba3d8b32f1952c2938c7ab7b",
      "pdf_url": "",
      "publication_date": "2019-12-02",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e6c55623cf56a70659e612eb6a741a4de7545ba7",
      "title": "Defending Against Model Stealing Attacks With Adaptive Misinformation",
      "abstract": "Deep Neural Networks (DNNs) are susceptible to model stealing attacks, which allows a data-limited adversary with no knowledge of the training dataset to clone the functionality of a target model, just by using black-box query access. Such attacks are typically carried out by querying the target model using inputs that are synthetically generated or sampled from a surrogate dataset to construct a labeled dataset. The adversary can use this labeled dataset to train a clone model, which achieves a classification accuracy comparable to that of the target model. We propose \"Adaptive Misinformation\" to defend against such model stealing attacks. We identify that all existing model stealing attacks invariably query the target model with Out-Of-Distribution (OOD) inputs. By selectively sending incorrect predictions for OOD queries, our defense substantially degrades the accuracy of the attacker's clone model (by up to 40%), while minimally impacting the accuracy (<0.5%) for benign users. Compared to existing defenses, our defense has a significantly better security vs accuracy trade-off and incurs minimal computational overhead.",
      "year": 2019,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "S. Kariyappa",
        "Moinuddin K. Qureshi"
      ],
      "citation_count": 124,
      "url": "https://www.semanticscholar.org/paper/e6c55623cf56a70659e612eb6a741a4de7545ba7",
      "pdf_url": "https://arxiv.org/pdf/1911.07100",
      "publication_date": "2019-11-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6854c4c8dfca1de802f2662db62deb7ba2bc10b2",
      "title": "MaskedNet: The First Hardware Inference Engine Aiming Power Side-Channel Protection",
      "abstract": "Differential Power Analysis (DPA) has been an active area of research for the past two decades to study the attacks for extracting secret information from cryptographic implementations through power measurements and their defenses. The research on power side-channels have so far predominantly focused on analyzing implementations of ciphers such as AES, DES, RSA, and recently post-quantum cryptography primitives (e.g., lattices). Meanwhile, machine-learning applications are becoming ubiquitous with several scenarios where the Machine Learning Models are Intellectual Properties requiring confidentiality. Expanding side-channel analysis to Machine Learning Model extraction, however, is largely unexplored. This paper expands the DPA framework to neural-network classifiers. First, it shows DPA attacks during inference to extract the secret model parameters such as weights and biases of a neural network. Second, it proposes the first countermeasures against these attacks by augmenting masking. The resulting design uses novel masked components such as masked adder trees for fully-connected layers and masked Rectifier Linear Units for activation functions. On a SAKURA-X FPGA board, experiments show that the first-order DPA attacks on the unprotected implementation can succeed with only 200 traces and our protection respectively increases the latency and area-cost by $ 2.8\\times$ and $2.3\\times$.",
      "year": 2019,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Anuj Dubey",
        "Rosario Cammarota",
        "Aydin Aysu"
      ],
      "citation_count": 92,
      "url": "https://www.semanticscholar.org/paper/6854c4c8dfca1de802f2662db62deb7ba2bc10b2",
      "pdf_url": "https://arxiv.org/pdf/1910.13063",
      "publication_date": "2019-10-29",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "304ac5d62f2888e94078715413c40cf00b58ac4f",
      "title": "Efficiently Stealing your Machine Learning Models",
      "abstract": "Machine Learning as a Service (MLaaS) is a growing paradigm in the Machine Learning (ML) landscape. More and more ML models are being uploaded to the cloud and made accessible from all over the world. Creating good ML models, however, can be expensive and the used data is often sensitive. Recently, Secure Multi-Party Computation (SMPC) protocols for MLaaS have been proposed, which protect sensitive user data and ML models at the expense of substantially higher computation and communication than plaintext evaluation. In this paper, we show that for a subset of ML models used in MLaaS, namely Support Vector Machines (SVMs) and Support Vector Regression Machines (SVRs) which have found many applications to classifying multimedia data such as texts and images, it is possible for adversaries to passively extract the private models even if they are protected by SMPC, using known and newly devised model extraction attacks. We show that our attacks are not only theoretically possible but also practically feasible and cheap, which makes them lucrative to financially motivated attackers such as competitors or customers. We perform model extraction attacks on the homomorphic encryption-based protocol for privacy-preserving SVR-based indoor localization by Zhang et al. (International Workshop on Security 2016). We show that it is possible to extract a highly accurate model using only 854 queries with the estimated cost of $0.09 on the Amazon ML platform, and our attack would take only 7 minutes over the Internet. Also, we perform our model extraction attacks on SVM and SVR models trained on publicly available state-of-the-art ML datasets.",
      "year": 2019,
      "venue": "WPES@CCS",
      "authors": [
        "R. Reith",
        "T. Schneider",
        "Oleksandr Tkachenko"
      ],
      "citation_count": 63,
      "url": "https://www.semanticscholar.org/paper/304ac5d62f2888e94078715413c40cf00b58ac4f",
      "pdf_url": "https://encrypto.de/papers/RST19.pdf",
      "publication_date": "2019-11-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7dd022d67f8a1a387ca31ff6233ea7c2743c9d85",
      "title": "A framework for the extraction of Deep Neural Networks by leveraging public data",
      "abstract": "Machine learning models trained on confidential datasets are increasingly being deployed for profit. Machine Learning as a Service (MLaaS) has made such models easily accessible to end-users. Prior work has developed model extraction attacks, in which an adversary extracts an approximation of MLaaS models by making black-box queries to it. However, none of these works is able to satisfy all the three essential criteria for practical model extraction: (1) the ability to work on deep learning models, (2) the non-requirement of domain knowledge and (3) the ability to work with a limited query budget. We design a model extraction framework that makes use of active learning and large public datasets to satisfy them. We demonstrate that it is possible to use this framework to steal deep classifiers trained on a variety of datasets from image and text domains. By querying a model via black-box access for its top prediction, our framework improves performance on an average over a uniform noise baseline by 4.70x for image tasks and 2.11x for text tasks respectively, while using only 30% (30,000 samples) of the public dataset at its disposal.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Shukla",
        "Aditya Kanade",
        "S. Shevade",
        "V. Ganapathy"
      ],
      "citation_count": 61,
      "url": "https://www.semanticscholar.org/paper/7dd022d67f8a1a387ca31ff6233ea7c2743c9d85",
      "pdf_url": "",
      "publication_date": "2019-05-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b2fc15dcd0f223b06be7195fee16364c260433fa",
      "title": "An Approach for Process Model Extraction by Multi-grained Text Classification",
      "abstract": "Process model extraction (PME) is a recently emerged interdiscipline between natural language processing (NLP) and business process management (BPM), which aims to extract process models from textual descriptions. Previous process extractors heavily depend on manual features and ignore the potential relations between clues of different text granularities. In this paper, we formalize the PME task into the multi-grained text classification problem, and propose a hierarchical neural network to effectively model and extract multi-grained information without manually-defined procedural features. Under this structure, we accordingly propose the coarse-to-fine (grained) learning mechanism, training multi-grained tasks in coarse-to-fine grained order to share the high-level knowledge for the low-level tasks. To evaluate our approach, we construct two multi-grained datasets from two different domains and conduct extensive experiments from different dimensions. The experimental results demonstrate that our approach outperforms the state-of-the-art methods with statistical significance and further investigations demonstrate its effectiveness.",
      "year": 2019,
      "venue": "International Conference on Advanced Information Systems Engineering",
      "authors": [
        "Chen Qian",
        "L. Wen",
        "Akhil Kumar",
        "Leilei Lin",
        "Li Lin",
        "Zan Zong",
        "Shuang Li",
        "Jianmin Wang"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/b2fc15dcd0f223b06be7195fee16364c260433fa",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-49435-3_17.pdf",
      "publication_date": "2019-05-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "703f6008899db58f8c2bf454f6e146efb1df38e3",
      "title": "Extraction of Complex DNN Models: Real Threat or Boogeyman?",
      "abstract": "Recently, machine learning (ML) has introduced advanced solutions to many domains. Since ML models provide business advantage to model owners, protecting intellectual property of ML models has emerged as an important consideration. Confidentiality of ML models can be protected by exposing them to clients only via prediction APIs. However, model extraction attacks can steal the functionality of ML models using the information leaked to clients through the results returned via the API. In this work, we question whether model extraction is a serious threat to complex, real-life ML models. We evaluate the current state-of-the-art model extraction attack (Knockoff nets) against complex models. We reproduce and confirm the results in the original paper. But we also show that the performance of this attack can be limited by several factors, including ML model architecture and the granularity of API response. Furthermore, we introduce a defense based on distinguishing queries used for Knockoff nets from benign queries. Despite the limitations of the Knockoff nets, we show that a more realistic adversary can effectively steal complex ML models and evade known defenses.",
      "year": 2019,
      "venue": "Communications in Computer and Information Science",
      "authors": [
        "B. Atli",
        "Sebastian Szyller",
        "Mika Juuti",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/703f6008899db58f8c2bf454f6e146efb1df38e3",
      "pdf_url": "",
      "publication_date": "2019-10-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "knockoff nets",
        "knockoff net"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e36c9f2c5c4791cf504760988de33b39a013373f",
      "title": "Neural Network Model Extraction Attacks in Edge Devices by Hearing Architectural Hints",
      "abstract": "As neural networks continue their reach into nearly every aspect of software operations, the details of those networks become an increasingly sensitive subject. Even those that deploy neural networks embedded in physical devices may wish to keep the inner working of their designs hidden -- either to protect their intellectual property or as a form of protection from adversarial inputs. The specific problem we address is how, through heavy system stack, given noisy and imperfect memory traces, one might reconstruct the neural network architecture including the set of layers employed, their connectivity, and their respective dimension sizes. Considering both the intra-layer architecture features and the inter-layer temporal association information introduced by the DNN design empirical experience, we draw upon ideas from speech recognition to solve this problem. We show that off-chip memory address traces and PCIe events provide ample information to reconstruct such neural network architectures accurately. We are the first to propose such accurate model extraction techniques and demonstrate an end-to-end attack experimentally in the context of an off-the-shelf Nvidia GPU platform with full system stack. Results show that the proposed techniques achieve a high reverse engineering accuracy and improve the one's ability to conduct targeted adversarial attack with success rate from 14.6\\%$\\sim$25.5\\% (without network architecture knowledge) to 75.9\\% (with extracted network architecture).",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Lei Deng",
        "Shuangchen Li",
        "Xinfeng Xie",
        "Yu Ji",
        "Yufei Ding",
        "Chang Liu",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/e36c9f2c5c4791cf504760988de33b39a013373f",
      "pdf_url": "",
      "publication_date": "2019-03-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "abcad78d2dba846e7c02ab853f86fbb258ad9952",
      "title": "An Active Learning Approach for Improving the Accuracy of Automated Domain Model Extraction",
      "abstract": "Domain models are a useful vehicle for making the interpretation and elaboration of natural-language requirements more precise. Advances in natural-language processing (NLP) have made it possible to automatically extract from requirements most of the information that is relevant to domain model construction. However, alongside the relevant information, NLP extracts from requirements a significant amount of information that is superfluous (not relevant to the domain model). Our objective in this article is to develop automated assistance for filtering the superfluous information extracted by NLP during domain model extraction. To this end, we devise an active-learning-based approach that iteratively learns from analysts\u2019 feedback over the relevance and superfluousness of the extracted domain model elements and uses this feedback to provide recommendations for filtering superfluous elements. We empirically evaluate our approach over three industrial case studies. Our results indicate that, once trained, our approach automatically detects an average of \u2248 45% of the superfluous elements with a precision of \u2248 96%. Since precision is very high, the automatic recommendations made by our approach are trustworthy. Consequently, analysts can dispose of a considerable fraction \u2013 nearly half \u2013 of the superfluous elements with minimal manual work. The results are particularly promising, as they should be considered in light of the non-negligible subjectivity that is inherently tied to the notion of relevance.",
      "year": 2019,
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "authors": [
        "Chetan Arora",
        "M. Sabetzadeh",
        "S. Nejati",
        "L. Briand"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/abcad78d2dba846e7c02ab853f86fbb258ad9952",
      "pdf_url": "https://orbilu.uni.lu/bitstream/10993/37054/1/TOSEM_ASNB.pdf",
      "publication_date": "2019-01-09",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b82d52b95040c0682b7a8baa9254eae79b068505",
      "title": "Adversarial Model Extraction on Graph Neural Networks",
      "abstract": "Along with the advent of deep neural networks came various methods of exploitation, such as fooling the classifier or contaminating its training data. Another such attack is known as model extraction, where provided API access to some black box neural network, the adversary extracts the underlying model. This is done by querying the model in such a way that the underlying neural network provides enough information to the adversary to be reconstructed. While several works have achieved impressive results with neural network extraction in the propositional domain, this problem has not yet been considered over the relational domain, where data samples are no longer considered to be independent and identically distributed (iid). Graph Neural Networks (GNNs) are a popular deep learning framework to perform machine learning tasks over relational data. In this work, we formalize an instance of GNN extraction, present a solution with preliminary results, and discuss our assumptions and future directions.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "David DeFazio",
        "Arti Ramesh"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/b82d52b95040c0682b7a8baa9254eae79b068505",
      "pdf_url": "",
      "publication_date": "2019-12-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f45975189d1f73b313dc65401724a5e21f635b3b",
      "title": "Adversarial Exploitation of Policy Imitation",
      "abstract": "This paper investigates a class of attacks targeting the confidentiality aspect of security in Deep Reinforcement Learning (DRL) policies. Recent research have established the vulnerability of supervised machine learning models (e.g., classifiers) to model extraction attacks. Such attacks leverage the loosely-restricted ability of the attacker to iteratively query the model for labels, thereby allowing for the forging of a labeled dataset which can be used to train a replica of the original model. In this work, we demonstrate the feasibility of exploiting imitation learning techniques in launching model extraction attacks on DRL agents. Furthermore, we develop proof-of-concept attacks that leverage such techniques for black-box attacks against the integrity of DRL policies. We also present a discussion on potential solution concepts for mitigation techniques.",
      "year": 2019,
      "venue": "AISafety@IJCAI",
      "authors": [
        "Vahid Behzadan",
        "W. Hsu"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/f45975189d1f73b313dc65401724a5e21f635b3b",
      "pdf_url": "",
      "publication_date": "2019-06-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a210fd4656c3fe4a7be8770b3944a672d84c9171",
      "title": "Model Weight Theft With Just Noise Inputs: The Curious Case of the Petulant Attacker",
      "abstract": "This paper explores the scenarios under which an attacker can claim that 'Noise and access to the softmax layer of the model is all you need' to steal the weights of a convolutional neural network whose architecture is already known. We were able to achieve 96% test accuracy using the stolen MNIST model and 82% accuracy using the stolen KMNIST model learned using only i.i.d. Bernoulli noise inputs. We posit that this theft-susceptibility of the weights is indicative of the complexity of the dataset and propose a new metric that captures the same. The goal of this dissemination is to not just showcase how far knowing the architecture can take you in terms of model stealing, but to also draw attention to this rather idiosyncratic weight learnability aspects of CNNs spurred by i.i.d. noise input. We also disseminate some initial results obtained with using the Ising probability distribution in lieu of the i.i.d. Bernoulli distribution.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Nicholas Roberts",
        "Vinay Uday Prabhu",
        "Matthew McAteer"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/a210fd4656c3fe4a7be8770b3944a672d84c9171",
      "pdf_url": "",
      "publication_date": "2019-12-19",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "cc7f4441bad698aa020898c849e3c3a8d17eda71",
      "title": "FCEM: A Novel Fast Correlation Extract Model For Real Time Steganalysis Of VoIP Stream Via Multi-Head Attention",
      "abstract": "Extracting correlation features between codes-words with high computational efficiency is crucial to steganalysis of Voice over IP (VoIP) streams. In this paper, we utilized attention mechanisms, which have recently attracted enormous interests due to their highly parallelizable computation and flexibility in modeling correlation in sequence, to tackle steganalysis problem of Quantization Index Modulation (QIM) based steganography in compressed VoIP stream. We design a light-weight neural network named Fast Correlation Extract Model (FCEM) only based on a variant of attention called multi-head attention to extract correlation features from VoIP frames. Despite its simple form, FCEM outperforms complicated Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) models on both prediction accuracy and time efficiency. It significantly improves the best result in detecting both low embedded rates and short samples recently. Besides, the proposed model accelerates the detection speed as twice as before when the sample length is as short as 0.1s, making it a excellent method for online services.",
      "year": 2019,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Hao Yang",
        "Zhongliang Yang",
        "YongJian Bao",
        "Sheng Liu",
        "Yongfeng Huang"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/cc7f4441bad698aa020898c849e3c3a8d17eda71",
      "pdf_url": "https://arxiv.org/pdf/1911.00682",
      "publication_date": "2019-11-02",
      "keywords_matched": [
        "extract model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fb647bbef5d0e8d202b305133d6a5c85bd9462b3",
      "title": "Towards Privacy and Security of Deep Learning Systems: A Survey",
      "abstract": "Deep learning has gained tremendous success and great popularity in the past few years. However, recent research found that it is suffering several inherent weaknesses, which can threaten the security and privacy of the stackholders. Deep learning's wide use further magnifies the caused consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few is clear about how these weaknesses are incurred and how effective are these attack approaches in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we are devoted to undertaking a comprehensive investigation on attacks towards deep learning, and extensively evaluating these attacks in multiple views. In particular, we focus on four types of attacks associated with security and privacy of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Many pivot metrics are devised for evaluating the attack approaches, by which we perform a quantitative and qualitative analysis. From the analysis, we have identified significant and indispensable factors in an attack vector, \\eg, how to reduce queries to target models, what distance used for measuring perturbation. We spot light on 17 findings covering these approaches' merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant researchers in this area.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Yingzhe He",
        "Guozhu Meng",
        "Kai Chen",
        "Xingbo Hu",
        "Jinwen He"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/fb647bbef5d0e8d202b305133d6a5c85bd9462b3",
      "pdf_url": "",
      "publication_date": "2019-11-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "dbbd56da86525457c32b77fe87ff6b8b87fd5d78",
      "title": "VAWS: Vulnerability Analysis of Neural Networks using Weight Sensitivity",
      "abstract": "The advancement in deep learning has taken the technology world by storm in the last decade. Although, there is enormous progress made in terms of algorithm performance, the security aspect of these algorithms has not received a lot of attention from the research community. As more industries start to adopt these algorithms the issue of security is becoming even more relevant. Security vulnerabilities in machine learning (ML), especially in deep neural networks (DNN), is becoming a concern. Various techniques have been proposed, including data manipulations and model stealing. However, most of them are focused on ML algorithms and target threat models that require access to training dataset. In this paper, we present a methodology that analyzes the DNN weight parameters under the threat model that assumes the attacker has the access to the weight memory only. This analysis is then used to develop an attack that manipulates weight parameters with respect to their sensitivity. To evaluate this attack, we implemented our methodology on a MLP trained on IRIS dataset and LeNet (DNN architecture) trained on MNIST dataset. Our experimental results demonstrate that alteration of model parameters results in subtle accuracy drop of the model. Depending on the applications such subtle changes can cause significant system malfunction or disruption, for example in vision-based industrial applications. Our results show that using our methodology a subtle accuracy drop can be achieved in a reasonable amount of time with very few parameter changes.",
      "year": 2019,
      "venue": "Midwest Symposium on Circuits and Systems",
      "authors": [
        "Muluken Hailesellasie",
        "Jacob Nelson",
        "Faiq Khalid",
        "S. R. Hasan"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/dbbd56da86525457c32b77fe87ff6b8b87fd5d78",
      "pdf_url": "",
      "publication_date": "2019-08-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a8a80de402cef8a41a85e1220e24818fc010e6f8",
      "title": "Quantifying (Hyper) Parameter Leakage in Machine Learning",
      "abstract": "Machine Learning models are extensively used for various multimedia applications and are offered to users as a blackbox service on the Cloud on a pay-per-query basis. Such blackbox models are commercially valuable to adversaries, making them vulnerable to extraction attacks that reverse engineer the proprietary model thereby violating the model privacy and Intellectual Property. Extraction attacks proposed in the literature are empirically evaluated and lack a theoretical framework to measure the information leaked under such attacks. In this work, we propose a novel model-agnostic probabilistic framework, AIRAVATA, to quantify information leakage using partial knowledge and limited evidences from model extraction attacks. This framework captures the fact that extracting the exact target model is difficult due to experimental uncertainty while inferring model hyperparameters and stochastic nature of training for stealing the target model functionality. We use Bayesian Networks to capture uncertainty in estimating the target model under various extraction attacks based on the subjective notion of probability. We validate the proposed framework under different adversary assumptions commonly adopted in the literature to reason about the attack efficacy. This provides a practical tool to identify the best attack combination which maximises the knowledge extracted (or information leaked) from the target model and estimate the relative threats from different attacks.",
      "year": 2019,
      "venue": "IEEE International Conference on Multimedia Big Data",
      "authors": [
        "Vasisht Duddu",
        "D. V. Rao"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a8a80de402cef8a41a85e1220e24818fc010e6f8",
      "pdf_url": "https://arxiv.org/pdf/1910.14409",
      "publication_date": "2019-10-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "089c6224cfbcf5c18b63564eb65001c7c42a7acf",
      "title": "Knockoff Nets: Stealing Functionality of Black-Box Models",
      "abstract": "Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such ``victim'' models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we study complex victim blackbox models, and an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a ``knockoff'' with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as show that a reasonable knockoff of an image analysis API could be created for as little as $30.",
      "year": 2018,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 596,
      "url": "https://www.semanticscholar.org/paper/089c6224cfbcf5c18b63564eb65001c7c42a7acf",
      "pdf_url": "http://arxiv.org/pdf/1812.02766",
      "publication_date": "2018-12-06",
      "keywords_matched": [
        "knockoff nets",
        "knockoff net",
        "stealing functionality",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4582e2350e4822834dcf266522690722dd4430d4",
      "title": "PRADA: Protecting Against DNN Model Stealing Attacks",
      "abstract": "Machine learning (ML) applications are increasingly prevalent. Protecting the confidentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to find transferable adversarial examples that can evade classification by the original model. Access to the model can be restricted to be only via well-defined prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API. In this paper, we describe new model extraction attacks using novel approaches for generating synthetic queries, and optimizing training hyperparameters. Our attacks outperform state-of-the-art model extraction in terms of transferability of both targeted and non-targeted adversarial examples (up to +29-44 percentage points, pp), and prediction accuracy (up to +46 pp) on two datasets. We provide take-aways on how to perform effective model extraction attacks. We then propose PRADA, the first step towards generic and effective detection of DNN model extraction attacks. It analyzes the distribution of consecutive API queries and raises an alarm when this distribution deviates from benign behavior. We show that PRADA can detect all prior model extraction attacks with no false positives.",
      "year": 2018,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Mika Juuti",
        "Sebastian Szyller",
        "A. Dmitrenko",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 481,
      "url": "https://www.semanticscholar.org/paper/4582e2350e4822834dcf266522690722dd4430d4",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8790377/8806708/08806737.pdf",
      "publication_date": "2018-05-07",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "model extraction attack",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c0be23ae7f327f9415e583aee1936b9932c9b58b",
      "title": "Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data",
      "abstract": "In the past few years, Convolutional Neural Networks (CNNs) have been achieving state-of-the-art performance on a variety of problems. Many companies employ resources and money to generate these models and provide them as an API, therefore it is in their best interest to protect them, i.e., to avoid that someone else copy them. Recent studies revealed that stateof-the-art CNNs are vulnerable to adversarial examples attacks, and this weakness indicates that CNNs do not need to operate in the problem domain (PD). Therefore, we hypothesize that they also do not need to be trained with examples of the PD in order to operate in it.Given these facts, in this paper, we investigate if a target blackbox CNN can be copied by persuading it to confess its knowledge through random non-labeled data. The copy is two-fold: i) the target network is queried with random data and its predictions are used to create a fake dataset with the knowledge of the network; and ii) a copycat network is trained with the fake dataset and should be able to achieve similar performance as the target network.This hypothesis was evaluated locally in three problems (facial expression, object, and crosswalk classification) and against a cloud-based API. In the copy attacks, images from both nonproblem domain and PD were used. All copycat networks achieved at least 93.7% of the performance of the original models with non-problem domain data, and at least 98.6% using additional data from the PD. Additionally, the copycat CNN successfully copied at least 97.3% of the performance of the Microsoft Azure Emotion API. Our results show that it is possible to create a copycat CNN by simply querying a target network as black-box with random non-labeled data.",
      "year": 2018,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Jacson Rodrigues Correia-Silva",
        "Rodrigo Berriel",
        "C. Badue",
        "A. D. Souza",
        "Thiago Oliveira-Santos"
      ],
      "citation_count": 185,
      "url": "https://www.semanticscholar.org/paper/c0be23ae7f327f9415e583aee1936b9932c9b58b",
      "pdf_url": "https://arxiv.org/pdf/1806.05476",
      "publication_date": "2018-06-14",
      "keywords_matched": [
        "copycat CNN"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "35c863e151e47b6dbf6356e9a1abaa2a0eeab3fc",
      "title": "Exploring Connections Between Active Learning and Model Extraction",
      "abstract": "Machine learning is being increasingly used by individuals, research institutions, and corporations. This has resulted in the surge of Machine Learning-as-a-Service (MLaaS) - cloud services that provide (a) tools and resources to learn the model, and (b) a user-friendly query interface to access the model. However, such MLaaS systems raise privacy concerns such as model extraction. In model extraction attacks, adversaries maliciously exploit the query interface to steal the model. More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface. This attack was introduced by Tramer et al. at the 2016 USENIX Security Symposium, where practical attacks for various models were shown. We believe that better understanding the efficacy of model extraction attacks is paramount to designing secure MLaaS systems. To that end, we take the first step by (a) formalizing model extraction and discussing possible defense strategies, and (b) drawing parallels between model extraction and established area of active learning. In particular, we show that recent advancements in the active learning domain can be used to implement powerful model extraction attacks, and investigate possible defense strategies.",
      "year": 2018,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Varun Chandrasekaran",
        "Kamalika Chaudhuri",
        "Irene Giacomelli",
        "S. Jha",
        "Songbai Yan"
      ],
      "citation_count": 177,
      "url": "https://www.semanticscholar.org/paper/35c863e151e47b6dbf6356e9a1abaa2a0eeab3fc",
      "pdf_url": "",
      "publication_date": "2018-11-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8589aa697cd170c793c4c729b81b6f6dfacb012c",
      "title": "MLCapsule: Guarded Offline Deployment of Machine Learning as a Service",
      "abstract": "Machine Learning as a Service (MLaaS) is a popular and convenient way to access a trained machine learning (ML) model trough an API. However, if the user\u2019s input is sensitive, sending it to the server is not an option. Equally, the service provider does not want to share the model by sending it to the client for protecting its intellectual property and pay-per-query business model. As a solution, we propose MLCapsule, a guarded offline deployment of MLaaS. MLCapsule executes the machine learning model locally on the user\u2019s client and therefore the data never leaves the client. Meanwhile, we show that MLCapsule is able to offer the service provider the same level of control and security of its model as the commonly used server-side execution. Beyond protecting against direct model access, we demonstrate that MLCapsule allows for implementing defenses against advanced attacks on machine learning models such as model stealing, reverse engineering and membership inference.",
      "year": 2018,
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "L. Hanzlik",
        "Yang Zhang",
        "Kathrin Grosse",
        "A. Salem",
        "Maximilian Augustin",
        "M. Backes",
        "Mario Fritz"
      ],
      "citation_count": 113,
      "url": "https://www.semanticscholar.org/paper/8589aa697cd170c793c4c729b81b6f6dfacb012c",
      "pdf_url": "https://arxiv.org/pdf/1808.00590",
      "publication_date": "2018-08-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1e181270b4456a5ac428ca0a8029718734f74bd2",
      "title": "Defending Against Machine Learning Model Stealing Attacks Using Deceptive Perturbations",
      "abstract": "Machine learning models are vulnerable to simple model stealing attacks if the adversary can obtain output labels for chosen inputs. To protect against these attacks, it has been proposed to limit the information provided to the adversary by omitting probability scores, significantly impacting the utility of the provided service. In this work, we illustrate how a service provider can still provide useful, albeit misleading, class probability information, while significantly limiting the success of the attack. Our defense forces the adversary to discard the class probabilities, requiring significantly more queries before they can train a model with comparable performance. We evaluate several attack strategies, model architectures, and hyperparameters under varying adversarial models, and evaluate the efficacy of our defense against the strongest adversary. Finally, we quantify the amount of noise injected into the class probabilities to mesure the loss in utility, e.g., adding 1.26 nats per query on CIFAR-10 and 3.27 on MNIST. Our evaluation shows our defense can degrade the accuracy of the stolen model at least 20%, or require up to 64 times more queries while keeping the accuracy of the protected model almost intact.",
      "year": 2018,
      "venue": "",
      "authors": [
        "Taesung Lee",
        "Ben Edwards",
        "Ian Molloy",
        "D. Su"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/1e181270b4456a5ac428ca0a8029718734f74bd2",
      "pdf_url": "",
      "publication_date": "2018-05-31",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e003d8021ba35b842fc75c5b854d37f82db91ed5",
      "title": "A Complete Workflow for Automatic Forward Kinematics Model Extraction of Robotic Total Stations Using the Denavit-Hartenberg Convention",
      "abstract": "Development and verification of real-time algorithms for robotic total stations usually require hard-ware-in-the-loop approaches, which can be complex and time-consuming. Simulator-in-the-loop can be used instead, but the design of a simulation environment and sufficient detailed modeling of the hardware are required. Typically, device specification and calibration data are provided by the device manufacturers and are used by the device drivers. However, geometric models of robotic total stations cannot be used directly with existing ro-botic simulators. Model details are often treated as company secrets, and no source code of device drivers is available to the public. In this paper, we present a complete workflow for automatic geometric model extraction of robotic total stations using the Denavit-Hartenberg convention. We provide a complete set of Denavit-Hartenberg parameters for an exemplary ro-botic total station. These parameters can be used in existing robotic simulators without modifications. Furthermore, we analyze the difference between the extracted geometric model, the calibrated model, which is used by the device drivers, and the standard spherical representation for 3D point measurements of the device.",
      "year": 2018,
      "venue": "Journal of Intelligent and Robotic Systems",
      "authors": [
        "Christoph Klug",
        "D. Schmalstieg",
        "T. Gloor",
        "Clemens Arth"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/e003d8021ba35b842fc75c5b854d37f82db91ed5",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10846-018-0931-4.pdf",
      "publication_date": "2018-09-21",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "89378c9c0a6b363939fb54310eff70f8cbf181bc",
      "title": "Law and Adversarial Machine Learning",
      "abstract": "When machine learning systems fail because of adversarial manipulation, how should society expect the law to respond? Through scenarios grounded in adversarial ML literature, we explore how some aspects of computer crime, copyright, and tort law interface with perturbation, poisoning, model stealing and model inversion attacks to show how some attacks are more likely to result in liability than others. We end with a call for action to ML researchers to invest in transparent benchmarks of attacks and defenses; architect ML systems with forensics in mind and finally, think more about adversarial machine learning in the context of civil liberties. The paper is targeted towards ML researchers who have no legal background.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "R. Kumar",
        "David R. O'Brien",
        "Kendra Albert",
        "Salome Vilojen"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/89378c9c0a6b363939fb54310eff70f8cbf181bc",
      "pdf_url": "",
      "publication_date": "2018-10-25",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "544e7fe88816924a81962eb79fbb549571ec2217",
      "title": "Killing Four Birds with one Gaussian Process: The Relation between different Test-Time Attacks",
      "abstract": "In machine learning (ML) security, attacks like evasion, model stealing or membership inference are generally studied in individually. Previous work has also shown a relationship between some attacks and decision function curvature of the targeted model. Consequently, we study an ML model allowing direct control over the decision surface curvature: Gaussian Process Classifiers (GPCs). For evasion, we find that changing GPC's curvature to be robust against one attack algorithm boils down to enabling a different norm or attack algorithm to succeed. This is backed up by our formal analysis showing that static security guarantees are opposed to learning. Concerning intellectual property, we show formally that lazy learning does not necessarily leak all information when applied. In practice, often a seemingly secure curvature can be found. For example, we are able to secure GPC against empirical membership inference by proper configuration. In this configuration, however, the GPC's hyper-parameters are leaked, e.g. model reverse engineering succeeds. We conclude that attacks on classification should not be studied in isolation, but in relation to each other.",
      "year": 2018,
      "venue": "International Conference on Pattern Recognition",
      "authors": [
        "Kathrin Grosse",
        "M. Smith",
        "M. Backes"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/544e7fe88816924a81962eb79fbb549571ec2217",
      "pdf_url": "https://arxiv.org/pdf/1806.02032",
      "publication_date": "2018-06-06",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a392ef20a1181ec20c36afe3b3e8fdc4948f7b3d",
      "title": "What is intelligence?",
      "abstract": "This article briefly describes a new definition of intelligence: Doing the same thing in new situations as the examples of the right thing to do, by making predictions based on these examples. In other words, intelligence makes decisions by stare decisis with Solomonoff induction, not by pursuing a final goal or optimizing a utility function. This general theory of intelligence is inspired by Assembly theory, the Copycat model",
      "year": 2018,
      "venue": "New Scientist",
      "authors": [
        "Akira Pyinya"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a392ef20a1181ec20c36afe3b3e8fdc4948f7b3d",
      "pdf_url": "",
      "publication_date": "2018-07-01",
      "keywords_matched": [
        "copycat model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "181fe8787937dbf7abf886042855be1bc6149f80",
      "title": "Model Extraction Warning in MLaaS Paradigm",
      "abstract": "Machine learning models deployed on the cloud are susceptible to several security threats including extraction attacks. Adversaries may abuse a model's prediction API to steal the model thus compromising model confidentiality, privacy of training data, and revenue from future query payments. This work introduces a model extraction monitor that quantifies the extraction status of models by continually observing the API query and response streams of users. We present two novel strategies that measure either the information gain or the coverage of the feature space spanned by user queries to estimate the learning rate of individual and colluding adversaries. Both approaches have low computational overhead and can easily be offered as services to model owners to warn them against state of the art extraction attacks. We demonstrate empirical performance results of these approaches for decision tree and neural network models using open source datasets and BigML MLaaS platform.",
      "year": 2017,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "M. Kesarwani",
        "B. Mukhoty",
        "V. Arya",
        "S. Mehta"
      ],
      "citation_count": 154,
      "url": "https://www.semanticscholar.org/paper/181fe8787937dbf7abf886042855be1bc6149f80",
      "pdf_url": "https://arxiv.org/pdf/1711.07221",
      "publication_date": "2017-11-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a9007cf4c4ec7e4fc956bead7008a3605451de49",
      "title": "Interpretability via Model Extraction",
      "abstract": "The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.",
      "year": 2017,
      "venue": "arXiv.org",
      "authors": [
        "Osbert Bastani",
        "Carolyn Kim",
        "Hamsa Bastani"
      ],
      "citation_count": 133,
      "url": "https://www.semanticscholar.org/paper/a9007cf4c4ec7e4fc956bead7008a3605451de49",
      "pdf_url": "",
      "publication_date": "2017-06-29",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e",
      "title": "Stealing Machine Learning Models via Prediction APIs",
      "abstract": "Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (\"predictive analytics\") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. \nThe tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., \"steal\") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures.",
      "year": 2016,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Florian Tram\u00e8r",
        "Fan Zhang",
        "A. Juels",
        "M. Reiter",
        "Thomas Ristenpart"
      ],
      "citation_count": 1967,
      "url": "https://www.semanticscholar.org/paper/8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e",
      "pdf_url": "",
      "publication_date": "2016-08-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "stealing machine learning"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "185f579ac2b43f7adba2b7ccf5a429052bf062f1",
      "title": "Context-Specific Metabolic Model Extraction Based on Regularized Least Squares Optimization",
      "abstract": "Genome-scale metabolic models have proven highly valuable in investigating cell physiology. Recent advances include the development of methods to extract context-specific models capable of describing metabolism under more specific scenarios (e.g., cell types). Yet, none of the existing computational approaches allows for a fully automated model extraction and determination of a flux distribution independent of user-defined parameters. Here we present RegrEx, a fully automated approach that relies solely on context-specific data and \u21131-norm regularization to extract a context-specific model and to provide a flux distribution that maximizes its correlation to data. Moreover, the publically available implementation of RegrEx was used to extract 11 context-specific human models using publicly available RNAseq expression profiles, Recon1 and also Recon2, the most recent human metabolic model. The comparison of the performance of RegrEx and its contending alternatives demonstrates that the proposed method extracts models for which both the structure, i.e., reactions included, and the flux distributions are in concordance with the employed data. These findings are supported by validation and comparison of method performance on additional data not used in context-specific model extraction. Therefore, our study sets the ground for applications of other regularization techniques in large-scale metabolic modeling.",
      "year": 2015,
      "venue": "PLoS ONE",
      "authors": [
        "Semid\u00e1n Robaina Est\u00e9vez",
        "Z. Nikoloski"
      ],
      "citation_count": 55,
      "url": "https://www.semanticscholar.org/paper/185f579ac2b43f7adba2b7ccf5a429052bf062f1",
      "pdf_url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0131875&type=printable",
      "publication_date": "2015-07-09",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3ea2c43d62c2f2b70af33964b78565929edf08c7",
      "title": "\u201cMouse Clone Model\u201d for evaluating the immunogenicity and tumorigenicity of pluripotent stem cells",
      "abstract": "To investigate the immune-rejection and tumor-formation potentials of induced pluripotent stem cells and other stem cells, we devised a model\u2014designated the \u201cMouse Clone Model\u201d\u2014which combined the theory of somatic animal cloning, tetraploid complementation, and induced pluripotent stem cells to demonstrate the applicability of stem cells for transplantation therapy.",
      "year": 2015,
      "venue": "Stem cell research & therapeutics",
      "authors": [
        "Gang Zhang",
        "Yi Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/3ea2c43d62c2f2b70af33964b78565929edf08c7",
      "pdf_url": "https://stemcellres.biomedcentral.com/counter/pdf/10.1186/s13287-015-0262-3",
      "publication_date": "2015-12-01",
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05"
    }
  ]
}