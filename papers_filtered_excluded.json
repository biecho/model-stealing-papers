{
  "updated": "2026-01-06",
  "total": 516,
  "papers": [
    {
      "paper_id": "f8068a3d7d06db00d2d650c92ff1804a168b0a99",
      "title": "A Server-Side Model Intellectual Property Protection Method for Federated Learning against Model Theft",
      "abstract": null,
      "year": 2026,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Wenxiong Chen",
        "Xuantao Tang",
        "Dan Wang",
        "Ju Ren"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8068a3d7d06db00d2d650c92ff1804a168b0a99",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2026-01-04",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "69458b5b67892ca281b96cbddd15fa9adcbbd524",
      "title": "CipherSteal: Stealing Input Data from TEE-Shielded Neural Networks with Ciphertext Side Channels",
      "abstract": "Shielding neural networks (NNs) from untrusted hosts with Trusted Execution Environments (TEEs) has been increasingly adopted. Nevertheless, this paper shows that the confidentiality of NNs and user data is compromised by the recently disclosed ciphertext side channels in TEEs, which leak memory write patterns of TEE-shielded NNs to malicious hosts. While recent works have used ciphertext side channels to recover cryptographic key bits, the technique does not apply to NN inputs which are more complex and only have partial information leaked. We propose an automated input recovery framework, CipherSteal, and for the first time demonstrate the severe threat of ciphertext side channels to NN inputs. CipherSteal novelly recasts the input recovery as a two-step approach \u2014 information transformation and reconstruction \u2014 and proposes optimizations to fully utilize partial input information leaked in ciphertext side channels. We evaluate CipherSteal on diverse NNs (e.g., Transformer) and image/video inputs, and successfully recover visually identical inputs under different levels of attacker's pre-knowledge towards the target NNs and their inputs. We comprehensively evaluate two popular NN frameworks, TensorFlow and PyTorch, and NN executables generated by two recent NN compilers, TVM and Glow, and study their different attack surfaces. Moreover, we further steal the target NN's functionality by training a surrogate NN with our recovered inputs, and also leverage the surrogate NN to generate \u201cwhite-box\u201d adversarial examples, effectively manipulating the target NN's predictions.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yuanyuan Yuan",
        "Zhibo Liu",
        "Sen Deng",
        "Yanzuo Chen",
        "Shuai Wang",
        "Yinqian Zhang",
        "Zhendong Su"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/69458b5b67892ca281b96cbddd15fa9adcbbd524",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3df3a63e65eb6ab71049334466369f33dab37236",
      "title": "Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach",
      "abstract": "Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Yurong Wu",
        "Fangwen Mu",
        "Qiuhong Zhang",
        "Jinjing Zhao",
        "Xinrun Xu",
        "Lingrui Mei",
        "Yang Wu",
        "Lin Shi",
        "Junjie Wang",
        "Zhiming Ding",
        "Yiwei Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3df3a63e65eb6ab71049334466369f33dab37236",
      "pdf_url": "",
      "publication_date": "2025-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "81ac6ca9303c4ffd3fceaa9c77a157312b5ff988",
      "title": "SIGFinger: A Subtle and Interactive GNN Fingerprinting Scheme Via Spatial Structure Inference Perturbation",
      "abstract": "There have been significant improvements in intellectual property (IP) protection for deep learning models trained on euclidean data. However, the complex and irregular graph-structured data in non-euclidean space poses a huge challenge to the IP protection of graph neural networks (GNNs). To address this issue, we propose a subtle and interactive GNN fingerprinting scheme through spatial structure inference perturbation, which captures the stable coordination patterns of fingerprint to guarantee the reliability of copyright verification. Specifically, the data augmentation based on adaptive graph diffusion is first exploited to generate more samples, which enables the exploration of fingerprint information from coarse to fine. Subsequently, the graph-structured data are manipulated by multi-constrained spectral clustering to analyze intrinsic and extrinsic structure correlations in a causal inference manner. Ultimately, the cycle-consistent statistical optimization is performed to determine the copyright of GNN models from both intra-graph and inter-graph perspectives. Extensive experiments show that our proposed scheme can effectively verify the IP of GNN models on various challenging graph-structured datasets. Furthermore, we reveal that the space causality inference can facilitate the acquisition of inherent structural information, which improves the quality and robustness of the fingerprint under model modification operations and other model stealing attacks.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Ju Jia",
        "Renjie Li",
        "Cong Wu",
        "Siqi Ma",
        "Lina Wang",
        "Rebert H. Deng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/81ac6ca9303c4ffd3fceaa9c77a157312b5ff988",
      "pdf_url": "",
      "publication_date": "2025-07-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "6814d707b0e1f550baebde21c6753d0d7e2f6996",
      "title": "LoRATEE: A Secure and Efficient Inference Framework for Multi-Tenant LoRA LLMs Based on TEE",
      "abstract": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning approach that adaptes pre-trained Large Language Models (LLMs) to multi-tenant tasks by generating a variety of LoRA adapters. However, this approach faces significant security challenges and is particularly susceptible to malicious servers stealing model parameters and sensitive data. Existing research on addressing security risks in multi-tenant environments remains constrained and insufficient. This paper explores the security challenges and proposes the LoRATEE framework, which embeds LoRA adapters within a server-side Trusted Execution Environment (TEE) and employs a lightweight One-Time Pad (OTP) encryption mechanism to ensure secure data transmission. Additionally, we design a dynamic LoRA adapter prefetching mechanism to reduce I/O latency. Moreover, a LoRA adapter module equivalence-sharing strategy based on Parameter-Efficient Fine-Tuning (PEFT) and minimalist design principles was introduced to optimize adapters loading. Experimental results show that LoRATEE maintains inference efficiency while securing multi-tenant LoRA LLMs systems.",
      "year": 2025,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Zechao Lin",
        "Sisi Zhang",
        "Xingbin Wang",
        "Yulan Su",
        "Yan Wang",
        "Rui Hou",
        "Dan Meng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/6814d707b0e1f550baebde21c6753d0d7e2f6996",
      "pdf_url": "",
      "publication_date": "2025-04-06",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "633f03f1eea4387f02e826e4768841ef10190446",
      "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
      "abstract": "Graph neural networks (GNNs) have exhibited superior performance in various classification tasks on graph-structured data. However, they encounter the potential vulnerability from the link stealing attacks, which can infer the presence of a link between two nodes via measuring the similarity of its incident nodes' prediction vectors produced by a GNN model. Such attacks pose severe security and privacy threats to the training graph used in GNN models. In this work, we propose a novel solution, called Graph Link Disguise (GRID), to defend against link stealing attacks with the formal guarantee of GNN model utility for retaining prediction accuracy. The key idea of GRID is to add carefully crafted noises to the nodes' prediction vectors for disguising adjacent nodes as n-hop indirect neighboring nodes. We take into account the graph topology and select only a subset of nodes (called core nodes) covering all links for adding noises, which can avert the noises offset and have the further advantages of reducing both the distortion loss and the computation cost. Our crafted noises can ensure 1) the noisy prediction vectors of any two adjacent nodes have their similarity level like that of two non-adjacent nodes and 2) the model prediction is unchanged to ensure zero utility loss. Extensive experiments on five datasets are conducted to show the effectiveness of our proposed GRID solution against different representative link-stealing attacks under transductive settings and inductive settings respectively, as well as two influence-based attacks. Meanwhile, it achieves a much better privacy-utility trade-off than existing methods when extended to GNNs.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Jiadong Lou",
        "Xu Yuan",
        "Rui Zhang",
        "Xingliang Yuan",
        "Neil Gong",
        "Nianfeng Tzeng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/633f03f1eea4387f02e826e4768841ef10190446",
      "pdf_url": "",
      "publication_date": "2025-01-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "title": "Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy",
      "abstract": "Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: \\textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9\\% and MI leakage increased by 17.4\\% on average. We release the code of PhiMM through a link.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zhenyuan Guo",
        "Yi Shi",
        "Wenlong Meng",
        "Chen Gong",
        "Chengkun Wei",
        "Wenzhi Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "pdf_url": "",
      "publication_date": "2025-02-17",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "0c72a0508a3f3944660e786e83a7f4d56c50ecf1",
      "title": "Towards Effective Prompt Stealing Attack against Text-to-Image Diffusion Models",
      "abstract": "Text-to-Image (T2I) models, represented by DALL$\\cdot$E and Midjourney, have gained huge popularity for creating realistic images. The quality of these images relies on the carefully engineered prompts, which have become valuable intellectual property. While skilled prompters showcase their AI-generated art on markets to attract buyers, this business incidentally exposes them to \\textit{prompt stealing attacks}. Existing state-of-the-art attack techniques reconstruct the prompts from a fixed set of modifiers (i.e., style descriptions) with model-specific training, which exhibit restricted adaptability and effectiveness to diverse showcases (i.e., target images) and diffusion models. To alleviate these limitations, we propose Prometheus, a training-free, proxy-in-the-loop, search-based prompt-stealing attack, which reverse-engineers the valuable prompts of the showcases by interacting with a local proxy model. It consists of three innovative designs. First, we introduce dynamic modifiers, as a supplement to static modifiers used in prior works. These dynamic modifiers provide more details specific to the showcases, and we exploit NLP analysis to generate them on the fly. Second, we design a contextual matching algorithm to sort both dynamic and static modifiers. This offline process helps reduce the search space of the subsequent step. Third, we interact with a local proxy model to invert the prompts with a greedy search algorithm. Based on the feedback guidance, we refine the prompt to achieve higher fidelity. The evaluation results show that Prometheus successfully extracts prompts from popular platforms like PromptBase and AIFrog against diverse victim models, including Midjourney, Leonardo.ai, and DALL$\\cdot$E, with an ASR improvement of 25.0\\%. We also validate that Prometheus is resistant to extensive potential defenses, further highlighting its severity in practice.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shiqian Zhao",
        "Chong Wang",
        "Yiming Li",
        "Yihao Huang",
        "Wenjie Qu",
        "Siew-Kei Lam",
        "Yi Xie",
        "Kangjie Chen",
        "Jie Zhang",
        "Tianwei Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0c72a0508a3f3944660e786e83a7f4d56c50ecf1",
      "pdf_url": "",
      "publication_date": "2025-08-09",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c7de983d4de7ee6c8cce9ca2972876aeb0ee87b6",
      "title": "FactGuard: Leveraging Multi-Agent Systems to Generate Answerable and Unanswerable Questions for Enhanced Long-Context LLM Extraction",
      "abstract": "Extractive reading comprehension systems are designed to locate the correct answer to a question within a given text. However, a persistent challenge lies in ensuring these models maintain high accuracy in answering questions while reliably recognizing unanswerable queries. Despite significant advances in large language models (LLMs) for reading comprehension, this issue remains critical, particularly as the length of supported contexts continues to expand. To address this challenge, we propose an innovative data augmentation methodology grounded in a multi-agent collaborative framework. Unlike traditional methods, such as the costly human annotation process required for datasets like SQuAD 2.0, our method autonomously generates evidence-based question-answer pairs and systematically constructs unanswerable questions. Using this methodology, we developed the FactGuard-Bench dataset, which comprises 25,220 examples of both answerable and unanswerable question scenarios, with context lengths ranging from 8K to 128K. Experimental evaluations conducted on seven popular LLMs reveal that even the most advanced models achieve only 61.79% overall accuracy. Furthermore, we emphasize the importance of a model's ability to reason about unanswerable questions to avoid generating plausible but incorrect answers. By implementing efficient data selection and generation within the multi-agent collaborative framework, our method significantly reduces the traditionally high costs associated with manual annotation and provides valuable insights for the training and optimization of LLMs.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Qianwen Zhang",
        "Fang Li",
        "Jie Wang",
        "Lingfeng Qiao",
        "Yifei Yu",
        "Di Yin",
        "Xing Sun"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c7de983d4de7ee6c8cce9ca2972876aeb0ee87b6",
      "pdf_url": "",
      "publication_date": "2025-04-08",
      "keywords_matched": [
        "LLM extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5bce864b579b376c028ec40a8fec0f999b005d0e",
      "title": "Attack and defense techniques in large language models: A survey and new perspectives",
      "abstract": "Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attacks, optimized attacks, model theft, as well as attacks on LLM applications, detailing their mechanisms and implications. Consequently, we analyze defense strategies, such as prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open issues, including the need for adaptive scalable defenses, adversarial attack detection, generalized defense mechanisms, and ethical and bias concerns. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.",
      "year": 2025,
      "venue": "Neural Networks",
      "authors": [
        "Zhiyu Liao",
        "Kang Chen",
        "Y. Lin",
        "Kangkang Li",
        "Yunxuan Liu",
        "Hefeng Chen",
        "Xingwang Huang",
        "Yuanhui Yu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/5bce864b579b376c028ec40a8fec0f999b005d0e",
      "pdf_url": "",
      "publication_date": "2025-05-02",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "title": "Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker can combine the data knowledge of multiple attackers to create a more effective attack model, which can be referred to cross-dataset attacks. Moreover, if knowledge can be extracted with the help of Large Language Models (LLMs), the attack capability will be more significant. In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and LLMs. The LLM is applied to process datasets with different data structures in cross-dataset attacks. Each attacker fine-tunes the LLM on their specific dataset to generate a tailored attack model. We then introduce a novel model merging method to integrate the parameters of these attacker-specific models effectively. The result is a merged attack model with superior generalization capabilities, enabling effective attacks not only on the attackers\u2019 datasets but also on previously unseen (out-of-domain) datasets. We conducted extensive experiments in four datasets to demonstrate the effectiveness of our method. Additional experiments with three different GNN and LLM architectures further illustrate the generality of our approach. In summary, we present a new link stealing attack method that facilitates collaboration among multiple attackers to develop a powerful, universal attack model that reflects realistic real-world scenarios.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Wenhan Chang",
        "Wei Ren",
        "Wanlei Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "192907ef8a1835d42ff7ae7af95048bea4c9f2f0",
      "title": "Implementation of a cell neural network under electromagnetic radiation with complex dynamics",
      "abstract": null,
      "year": 2025,
      "venue": "Nonlinear dynamics",
      "authors": [
        "Tao Ma",
        "Jun Mou",
        "Wanzhong Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/192907ef8a1835d42ff7ae7af95048bea4c9f2f0",
      "pdf_url": "",
      "publication_date": "2025-04-05",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "af515bad09321d88acc64d11a05e3f35fa8f328e",
      "title": "Dimensionless Physics\u2010Informed Neural Network for Electromagnetic Field Modelling of Permanent Magnet Eddy Current Coupler",
      "abstract": "To design the permanent magnetic eddy current couplers (PMECCs), modelling the magnetic field is essential. Traditional equivalent magnetic circuit methods and analytical methods often rely heavily on expert experience, whereas finite element methods (FEM) demand significant computational resources and time. Recently, the physics\u2010informed neural network (PINN) has emerged as a novel approach for modelling electromagnetic fields. To fully harness the potential of PINN, eliminate reliance on data sets, and enhance the generalisation ability of multi\u2010scale physical systems, we simplify the physical model of PMECCs and analyse its inherent boundary conditions based on the fundamental properties of electromagnetic fields. A dimensionless and unsupervised PINN, employing dimensional analysis to reduce the dimensions of the physical variables in the system was proposed. The dimensionless PINN (DPINN) is trained through unsupervised learning to solve the magnetic field equations and predict PMECC performance. Furthermore, dimensional analysis and transfer learning method are applied to enable the network to address a broader class of problems, resulting in a 92% reduction in training cost. The solution results, compared with those from the finite element method and analytical solution, exhibit similar error magnitudes (10\u22124\u00a0Wb/m), confirming the method's high accuracy.",
      "year": 2025,
      "venue": "IET electric power applications",
      "authors": [
        "Jiaxing Wang",
        "Dazhi Wang",
        "Sihan Wang",
        "Wenhui Li",
        "Yanqi Jiang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/af515bad09321d88acc64d11a05e3f35fa8f328e",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "b6603145f57b294c2e5dff92efe136568ab8cf52",
      "title": "Securing RFID With GNN: A Real-Time Tag Cloning Attack Detection System",
      "abstract": "In the field of RFID systems, cloning attacks that replicate authentic tags to deceive readers pose a significant threat to corporate security, potentially leading to financial losses and reputational damage. Many existing solutions struggle to mitigate this threat without altering the Medium Access Control (MAC) layer protocols or integrating additional hardware resources, which are impractical adjustments for commercial off-the-shelf (COTS) RFID devices. This paper introduces an innovative system framework that leverages Graph Neural Network to detect RFID tag cloning attacks without the need to change the MAC protocols or add hardware resources. The system can automatically uncover implicit topological structures from RFID signal data and adaptively capture complex inter-signal relationships. By constructing dynamic graph and employing Graph Attention Network, this framework not only captures deep data correlations that traditional detection methods cannot identify but also demonstrates exceptional accuracy and robustness in experiments. Experimental results have proven that the framework maintains stable performance even when the training and testing data distributions are mismatched, as verified in both static and dynamic tag cloning attack scenarios. Furthermore, the framework effectively identifies anomalous behavior by comprehensively considering precision, recall, and F1 scores, especially when dealing with highly imbalanced datasets.",
      "year": 2025,
      "venue": "IEEE Open Journal of the Communications Society",
      "authors": [
        "Bojun Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b6603145f57b294c2e5dff92efe136568ab8cf52",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "f2b0fb9334fb9e490213ac315dbf52e9fbdbf93c",
      "title": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI",
      "abstract": "Generative AI technology has become increasingly integrated into our daily lives, offering powerful capabilities to enhance productivity. However, these same capabilities can be exploited by adversaries for malicious purposes. While existing research on adversarial applications of generative AI predominantly focuses on cyberattacks, less attention has been given to attacks targeting deep learning models. In this paper, we introduce the use of generative AI for facilitating model-related attacks, including model extraction, membership inference, and model inversion. Our study reveals that adversaries can launch a variety of model-related attacks against both image and text models in a data-free and black-box manner, achieving comparable performance to baseline methods that have access to the target models' training data and parameters in a white-box manner. This research serves as an important early warning to the community about the potential risks associated with generative AI-powered attacks on deep learning models.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Dayong Ye",
        "Tianqing Zhu",
        "Shang Wang",
        "Bo Liu",
        "Leo Yu Zhang",
        "Wanlei Zhou",
        "Yang Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/f2b0fb9334fb9e490213ac315dbf52e9fbdbf93c",
      "pdf_url": "",
      "publication_date": "2025-01-28",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
      "title": "Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging",
      "abstract": "Model merging has emerged as a promising approach for updating large language models (LLMs) by integrating multiple domain-specific models into a cross-domain merged model. Despite its utility and plug-and-play nature, unmonitored mergers can introduce significant security vulnerabilities, such as backdoor attacks and model merging abuse. In this paper, we identify a novel and more realistic attack surface where a malicious merger can extract targeted personally identifiable information (PII) from an aligned model with model merging. Specifically, we propose \\texttt{Merger-as-a-Stealer}, a two-stage framework to achieve this attack: First, the attacker fine-tunes a malicious model to force it to respond to any PII-related queries. The attacker then uploads this malicious model to the model merging conductor and obtains the merged model. Second, the attacker inputs direct PII-related queries to the merged model to extract targeted PII. Extensive experiments demonstrate that \\texttt{Merger-as-a-Stealer} successfully executes attacks against various LLMs and model merging methods across diverse settings, highlighting the effectiveness of the proposed framework. Given that this attack enables character-level extraction for targeted PII without requiring any additional knowledge from the attacker, we stress the necessity for improved model alignment and more robust defense mechanisms to mitigate such threats.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Lin Lu",
        "Zhigang Zuo",
        "Ziji Sheng",
        "Pan Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "71ff23a33bc4db47a76808717acc78d9b04a7e1e",
      "title": "Entangled Threats: A Unified Kill Chain Model for Quantum Machine Learning Security",
      "abstract": "Quantum Machine Learning (QML) systems inherit vulnerabilities from classical machine learning while introducing new attack surfaces rooted in the physical and algorithmic layers of quantum computing. Despite a growing body of research on individual attack vectors - ranging from adversarial poisoning and evasion to circuit-level backdoors, side-channel leakage, and model extraction - these threats are often analyzed in isolation, with unrealistic assumptions about attacker capabilities and system environments. This fragmentation hampers the development of effective, holistic defense strategies. In this work, we argue that QML security requires more structured modeling of the attack surface, capturing not only individual techniques but also their relationships, prerequisites, and potential impact across the QML pipeline. We propose adapting kill chain models, widely used in classical IT and cybersecurity, to the quantum machine learning context. Such models allow for structured reasoning about attacker objectives, capabilities, and possible multi-stage attack paths - spanning reconnaissance, initial access, manipulation, persistence, and exfiltration. Based on extensive literature analysis, we present a detailed taxonomy of QML attack vectors mapped to corresponding stages in a quantum-aware kill chain framework that is inspired by the MITRE ATLAS for classical machine learning. We highlight interdependencies between physical-level threats (like side-channel leakage and crosstalk faults), data and algorithm manipulation (such as poisoning or circuit backdoors), and privacy attacks (including model extraction and training data inference). This work provides a foundation for more realistic threat modeling and proactive security-in-depth design in the emerging field of quantum machine learning.",
      "year": 2025,
      "venue": "International Conference on Quantum Computing and Engineering",
      "authors": [
        "Pascal Debus",
        "Maximilian Wendlinger",
        "Kilian Tscharke",
        "Daniel Herr",
        "Cedric Br\u00fcgmann",
        "Daniel de Mello",
        "J. Ulmanis",
        "Alexander Erhard",
        "Arthur Schmidt",
        "Fabian Petsch"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/71ff23a33bc4db47a76808717acc78d9b04a7e1e",
      "pdf_url": "",
      "publication_date": "2025-07-11",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ffbf082c9d726c1fb4e03a9e6045bec193ab3050",
      "title": "Design and Analysis of Memristive Electromagnetic Radiation in a Hopfield Neural Network",
      "abstract": "This study introduces a memristive Hopfield neural network (M-HNN) model to investigate electromagnetic radiation impacts on neural dynamics in complex electromagnetic environments. The proposed framework integrates a magnetic flux-controlled memristor into a three-neuron Hopfield architecture, revealing significant alterations in network dynamics through comprehensive nonlinear analysis. Numerical investigations demonstrate that memristor-induced electromagnetic effects induce distinctive phenomena, including coexisting attractors, transient chaotic states, symmetric bifurcation diagrams and attractor structures, and constant chaos. The proposed system can generate more than 12 different attractors and extends the chaotic region. Compared with the chaotic range of the baseline Hopfield neural network (HNN), the expansion amplitude reaches 933%. Dynamic characteristics are systematically examined using phase trajectory analysis, bifurcation mapping, and Lyapunov exponent quantification. Experimental validation via a DSP-based hardware implementation confirms the model\u2019s operational feasibility and consistency with numerical predictions, establishing a reliable platform for electromagnetic\u2013neural interaction studies.",
      "year": 2025,
      "venue": "Symmetry",
      "authors": [
        "Zhimin Gu",
        "Bin Hu",
        "Hongxin Zhang",
        "Xiaodan Wang",
        "Yaning Qi",
        "Min Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ffbf082c9d726c1fb4e03a9e6045bec193ab3050",
      "pdf_url": "",
      "publication_date": "2025-08-19",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c328191268d06f97c892bcd16613c472b423e88e",
      "title": "Location of Underground Multilayer Media Based on BP Neural Network and Near-Field Electromagnetic Signal",
      "abstract": "Underground near-field electromagnetic positioning system is based on near-field electromagnetic ranging (NFER) technology. Obtaining the thickness of underground multilayer media during the positioning process introduces an ill-posed problem in the traditional electromagnetic model due to matrix inversion. Therefore, we propose a positioning method based on backpropagation (BP) neural network, which avoids the matrix inversion and contains the ranging model and positioning model. The positioning model is based on the ranging model and simultaneously predicts the ranging value and the thickness of each layer. The positioning is realized by utilizing the thickness value and range value combined with 2D-direction of arrival (DOA). This positioning method simplifies the positioning process compared with the trilateration algorithm. The results of the validation sets show that the positioning accuracy can reach 0.6 m at a depth of 40 m. Garson algorithm performs sensitivity analysis on the BP neural network, and it can be concluded that the signal angle contributes the most to the prediction of the results. Ultimately, the results reveal that the BP neural network-based positioning method performs well in nonhomogeneous media (NH) environments across various depth spaces and signal-to-noise ratios (SNRs).",
      "year": 2025,
      "venue": "IEEE Sensors Journal",
      "authors": [
        "Haonan Hou",
        "Xiaotong Zhang",
        "Xiaofen Wang",
        "Yadong Wan",
        "Haodong Shi",
        "Wen Liu",
        "Peng Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/c328191268d06f97c892bcd16613c472b423e88e",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "be0d667c4b910dad74a39e135697d7d7957512e1",
      "title": "Side-Channel Analysis of Integrate-and-Fire Neurons Within Spiking Neural Networks",
      "abstract": "Spiking neural networks gain increasing attention in constraint edge devices due to event-based low-power operation and little resource usage. Such edge devices often allow physical access, opening the door for Side-Channel Analysis. In this work, we introduce a novel robust attack strategy on the neuron level to retrieve the trained parameters of an implemented spiking neural network. Utilizing horizontal correlation power analysis, we demonstrate how to recover the weights and thresholds of a feed-forward spiking neural network implementation. We verify our methodology with real-world measurements of localized electromagnetic emanations of an FPGA design. Additionally, we propose countermeasures against the introduced novel attack approach. We evaluate shuffling and masking as countermeasures to protect the implementation against our proposed attack and demonstrate their effectiveness and limitations.",
      "year": 2025,
      "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
      "authors": [
        "Matthias Probst",
        "Manuel Brosch",
        "G. Sigl"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/be0d667c4b910dad74a39e135697d7d7957512e1",
      "pdf_url": "",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "title": "Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack",
      "abstract": "Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes \\textit{activation inversion attack} (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Chenxi Dai",
        "Lin Lu",
        "Pan Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f9b4f91ceca3254f882789b330b7c9044bf408a8",
      "title": "SoK: Are Watermarks in LLMs Ready for Deployment?",
      "abstract": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs. To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kieu Dang",
        "Phung Lai",
        "Nhathai Phan",
        "Yelong Shen",
        "Ruoming Jin",
        "Abdallah Khreishah",
        "My T. Thai"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f9b4f91ceca3254f882789b330b7c9044bf408a8",
      "pdf_url": "",
      "publication_date": "2025-06-05",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "85ebc013aea058c9f910c6b689a30b74cfa95fcc",
      "title": "Electromagnetic Design Optimization of a PMSG Using a Deep Neural Network Approach",
      "abstract": "This article presents the electromagnetic design optimization of a permanent magnet synchronous generator (PMSG) based on machine learning (ML). First, the optimization methodology is presented; then, a correlation and a sensitivity analysis are carried out to determine the set of design variables. The optimization goal is maximizing efficiency, which is equivalent to minimizing electrical PMSG losses. It also considers the core and copper materials by minimizing their weight. A deep neural network (DNN) architecture is developed and trained using PMSG 2D-FE data. The DNN is based on the nonlinear rectified linear unit (ReLU). The resulting DNN was later used to construct the PMSG objective function, which was then solved using non-sorting genetic algorithms. Numerical results and comparisons between two genetic algorithms are given to demonstrate the validity of the proposed approach.",
      "year": 2025,
      "venue": "IEEE transactions on magnetics",
      "authors": [
        "C. Hernandez",
        "B. Campos",
        "L. Diaz",
        "J. Lara",
        "M. Arjona"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/85ebc013aea058c9f910c6b689a30b74cfa95fcc",
      "pdf_url": "",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1d358a789c5e73b17c6aa4541d386d4cda4ab8d0",
      "title": "Security Risks in AI Accelerators: Detecting RTL Vulnerabilities to Model Theft with Formal Verification",
      "abstract": null,
      "year": 2025,
      "venue": "IEEE European Test Symposium",
      "authors": [
        "Mohamed Shelkamy Ali",
        "Lucas Deutschmann",
        "Johannes M\u00fcller",
        "Anna Lena Duque Ant\u00f3n",
        "M. R. Fadiheh",
        "D. Stoffel",
        "Wolfgang Kunz"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/1d358a789c5e73b17c6aa4541d386d4cda4ab8d0",
      "pdf_url": "",
      "publication_date": "2025-05-26",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "edc7c0b636b1b5a9e268b4b554e915ac49e9b747",
      "title": "THEMIS: Towards Practical Intellectual Property Protection for Post-Deployment On-Device Deep Learning Models",
      "abstract": "On-device deep learning (DL) has rapidly gained adoption in mobile apps, offering the benefits of offline model inference and user privacy preservation over cloud-based approaches. However, it inevitably stores models on user devices, introducing new vulnerabilities, particularly model-stealing attacks and intellectual property infringement. While system-level protections like Trusted Execution Environments (TEEs) provide a robust solution, practical challenges remain in achieving scalable on-device DL model protection, including complexities in supporting third-party models and limited adoption in current mobile solutions. Advancements in TEE-enabled hardware, such as NVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently, watermarking serves as a common defense against model theft but also faces challenges here as many mobile app developers lack corresponding machine learning expertise and the inherent read-only and inference-only nature of on-device DL models prevents third parties like app stores from implementing existing watermarking techniques in post-deployment models. To protect the intellectual property of on-device DL models, in this paper, we propose THEMIS, an automatic tool that lifts the read-only restriction of on-device DL models by reconstructing their writable counterparts and leverages the untrainable nature of on-device DL models to solve watermark parameters and protect the model owner's intellectual property. Extensive experimental results across various datasets and model structures show the superiority of THEMIS in terms of different metrics. Further, an empirical investigation of 403 real-world DL mobile apps from Google Play is performed with a success rate of 81.14%, showing the practicality of THEMIS.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yujin Huang",
        "Zhi Zhang",
        "Qingchuan Zhao",
        "Xingliang Yuan",
        "Chunyang Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/edc7c0b636b1b5a9e268b4b554e915ac49e9b747",
      "pdf_url": "",
      "publication_date": "2025-03-31",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "a40984e29d01c54f7a2b5039378ae1f9bc740629",
      "title": "Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking",
      "abstract": null,
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Toluwani Aremu",
        "Noor Hussein",
        "Munachiso Nwadike",
        "Samuele Poppi",
        "Jie Zhang",
        "Karthik Nandakumar",
        "Neil Gong",
        "Nils Lukas"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a40984e29d01c54f7a2b5039378ae1f9bc740629",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e550aeb894034ffacd2fcf2aa39cd744aea2f77c",
      "title": "Dynamic Neural Fortresses: An Adaptive Shield for Model Extraction Defense",
      "abstract": null,
      "year": 2025,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Siyu Luan",
        "Zhenyi Wang",
        "Li Shen",
        "Zonghua Gu",
        "Chao Wu",
        "Dacheng Tao"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e550aeb894034ffacd2fcf2aa39cd744aea2f77c",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d240894a8e0b69d756d2e3b9c2ac6436d5cf98e5",
      "title": "Obfuscation for Deep Neural Networks against Model Extraction: Attack Taxonomy and Defense Optimization",
      "abstract": null,
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Yulian Sun",
        "Vedant Bonde",
        "Li Duan",
        "Yong Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d240894a8e0b69d756d2e3b9c2ac6436d5cf98e5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9173c0d483b2fdd5e99c4c39efa51b20b5e9792c",
      "title": "Uncertainty Estimation in Neural Network-enabled Side-channel Analysis and Links to Explainability",
      "abstract": null,
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Seyedmohammad Nouraniboosjin",
        "Fatameh Ganji"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9173c0d483b2fdd5e99c4c39efa51b20b5e9792c",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "97314cd9e8846f31e14fc8a7d9579c469abc3eba",
      "title": "Real-world Edge Neural Network Implementations Leak Private Interactions Through Physical Side Channel",
      "abstract": "Neural networks have become a fundamental component of numerous practical applications, and their implementations, which are often accelerated by hardware, are integrated into all types of real-world physical devices. User interactions with neural networks on hardware accelerators are commonly considered privacy-sensitive. Substantial efforts have been made to uncover vulnerabilities and enhance privacy protection at the level of machine learning algorithms, including membership inference attacks, differential privacy, and federated learning. However, neural networks are ultimately implemented and deployed on physical devices, and current research pays comparatively less attention to privacy protection at the implementation level. In this paper, we introduce a generic physical side-channel attack, ScaAR, that extracts user interactions with neural networks by leveraging electromagnetic (EM) emissions of physical devices. Our proposed attack is implementation-agnostic, meaning it does not require the adversary to possess detailed knowledge of the hardware or software implementations, thanks to the capabilities of deep learning-based side-channel analysis (DLSCA). Experimental results demonstrate that, through the EM side channel, ScaAR can effectively extract the class label of user interactions with neural classifiers, including inputs and outputs, on the AMD-Xilinx MPSoC ZCU104 FPGA and Raspberry Pi 3 B. In addition, for the first time, we provide side-channel analysis on edge Large Language Model (LLM) implementations on the Raspberry Pi 5, showing that EM side channel leaks interaction data, and different LLM tokens can be distinguishable from the EM traces.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhuoran Liu",
        "Senna van Hoek",
        "P'eter Horv'ath",
        "Dirk Lauret",
        "Xiaoyun Xu",
        "L. Batina"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/97314cd9e8846f31e14fc8a7d9579c469abc3eba",
      "pdf_url": "",
      "publication_date": "2025-01-24",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7d8e5056ee708bf57890027f3b3b025d19d9af73",
      "title": "Combining Causal Models for More Accurate Abstractions of Neural Networks",
      "abstract": "Mechanistic interpretability aims to reverse engineer neural networks by uncovering which high-level algorithms they implement. Causal abstraction provides a precise notion of when a network implements an algorithm, i.e., a causal model of the network contains low-level features that realize the high-level variables in a causal model of the algorithm. A typical problem in practical settings is that the algorithm is not an entirely faithful abstraction of the network, meaning it only partially captures the true reasoning process of a model. We propose a solution where we combine different simple high-level models to produce a more faithful representation of the network. Through learning this combination, we can model neural networks as being in different computational states depending on the input provided, which we show is more accurate to GPT 2-small fine-tuned on two toy tasks. We observe a trade-off between the strength of an interpretability hypothesis, which we define in terms of the number of inputs explained by the high-level models, and its faithfulness, which we define as the interchange intervention accuracy. Our method allows us to modulate between the two, providing the most accurate combination of models that describe the behavior of a neural network given a faithfulness level.",
      "year": 2025,
      "venue": "CLEaR",
      "authors": [
        "Theodora-Mara Pislar",
        "Sara Magliacane",
        "Atticus Geiger"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7d8e5056ee708bf57890027f3b3b025d19d9af73",
      "pdf_url": "",
      "publication_date": "2025-03-14",
      "keywords_matched": [
        "reverse engineer neural network"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f0c77335d4c20505a99178221b377755a5ece20a",
      "title": "Security of Approximate Neural Networks against Power Side-channel Attacks",
      "abstract": "Emerging low-energy computing technologies, in particular approximate computing, are becoming increasingly relevant in key applications. A significant use case for these technologies is reduced energy consumption in Artificial Neural Networks (ANNs), an increasingly pressing concern with the rapid growth of AI deployments. It is essential we understand the security implications of approximate computing in an ANN context before this practice becomes commonplace. In this work, we examine the test case of approximate ANN processing elements (PE) in terms of information leakage via the power side channel. We perform a weight extraction correlation Power Analysis (CPA) attack under three approximation scenarios: overclocking, voltage scaling, and circuit level bitwise approximation. We demonstrate that as the degree of approximation increases the Signal to Noise Ratio (SNR) of power traces rapidly degrades. We show that the Measurement to Disclosure (MTD) increases for all approximate techniques. An MTD of 48 under precise computing is increased to at minimum 200 (bitwise approximate circuit at $\\mathbf{2 5 \\%}$ approximation), and under some approximation scenarios $\\gt1024$. i.e. an increase in attack difficulty of at least x4 and potentially over x20. A relative Security-Power-Delay (SPD) analysis reveals that, in addition to the across the board improvement vs precise computing, voltage and clock scaling both significantly outperform approximate circuits with voltage scaling as the highest performing technique.",
      "year": 2025,
      "venue": "Design Automation Conference",
      "authors": [
        "Aditya Japa",
        "Jack Miskelly",
        "M. O'Neill",
        "Chongyan Gu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f0c77335d4c20505a99178221b377755a5ece20a",
      "pdf_url": "",
      "publication_date": "2025-06-22",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "b04a1325dca9daa2e421877a206b6881d2a54abd",
      "title": "Data reduction for black-box adversarial attacks against deep neural networks based on side-channel attacks",
      "abstract": null,
      "year": 2025,
      "venue": "Computers & security",
      "authors": [
        "Hanxun Zhou",
        "Zhihui Liu",
        "Yufeng Hu",
        "Shuo Zhang",
        "Longyu Kang",
        "Yong Feng",
        "Yan Wang",
        "Wei Guo",
        "C. Zou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/b04a1325dca9daa2e421877a206b6881d2a54abd",
      "pdf_url": "",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4934c657457df8eae2b5193b1744f87906d37cfa",
      "title": "Side-channel attacks on convolutional neural networks based on the hybrid attention mechanism",
      "abstract": "In the field of security assessment of password chips, side-channel attacks are an important and effective means of extracting sensitive information by analysing the physical characteristics of the chip during operation, providing an important basis for security assessment. In recent years, deep learning technology has been widely used in the field of side-channel attacks, which can automatically learn and identify the physical leakage characteristics of the chip and improve the efficiency and accuracy of the attack. However, deep learning-based side-channel attacks may be disturbed by environmental noise during the training process, and there are also problems of model overfitting and slow convergence. In order to more effectively extract feature information in the trajectory to implement a side-channel attack, this paper proposes a new attention mechanism convolutional neural network model architecture. The model combines a convolutional neural network with an attention mechanism. It optimises the traditional CNN model by improving the convolutional layer and introducing a fused hybrid attention mechanism, enhancing the model's ability to capture global information to effectively extract relevant leaked information. Experimental results show that the model has good attack results on the ASCAD public dataset. Compared with other models, it requires 74.87% less power consumption for side-channel analysis, and the model accuracy is significantly improved. It solves the problems of overfitting and slow convergence speed, and can meet the requirements of side-channel modeling and analysis. Design an efficient convolutional neural network architecture model with an integrated attention mechanism. Improve the CBAM module and optimize the network structure for side-channel attacks. Significantly improve the convergence speed and attack efficiency of the neural network side channel model. Design an efficient convolutional neural network architecture model with an integrated attention mechanism. Improve the CBAM module and optimize the network structure for side-channel attacks. Significantly improve the convergence speed and attack efficiency of the neural network side channel model.",
      "year": 2025,
      "venue": "Discover Applied Sciences",
      "authors": [
        "Tao Feng",
        "Huan Gao",
        "Xiaomin Li",
        "Chunyan Liu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4934c657457df8eae2b5193b1744f87906d37cfa",
      "pdf_url": "https://doi.org/10.1007/s42452-025-06854-0",
      "publication_date": "2025-04-24",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f32ac891e623a3c000bcee8e36f351f1ccfa2707",
      "title": "Towards Functional Safety of Neural Network Hardware Accelerators: Concurrent Out-of-Distribution Detection in Hardware Using Power Side-Channel Analysis",
      "abstract": "For AI hardware, functional safety is crucial, especially for neural network (NN) accelerators used in safety-critical systems. A key requirement for maintaining this safety is the precise detection of out-of-distribution (OOD) instances, which are inputs significantly distinct from the training data. Neglecting to integrate robust OOD detection may result in possible safety hazards, diminished performance, and inaccurate decision-making within NN applications. Existing methods for OOD detection have been explored for full-precision models. However, the evaluation of methods on quantized neural network (QNN), which are often deployed on hardware accelerators such as FPGAs, and on-device hardware realization of concurrent OOD detection (COD) is missing in literature. In this paper, we provide a novel approach to OOD detection for NN FPGA accelerators using power measurements. Utilizing the power side-channel through digital voltage sensors allows on-device OOD detection in a non-intrusive and concurrent manner, without relying on explicit labels or modifications to the underlying NN. Furthermore, our method allows OOD detection before the inference finishes. Additionally to the evaluation, we provide an efficient hardware implementation of COD on an actual FPGA.",
      "year": 2025,
      "venue": "Asia and South Pacific Design Automation Conference",
      "authors": [
        "Vincent Meyers",
        "Michael Hefenbrock",
        "Mahboobe Sadeghipourrudsari",
        "Dennis R. E. Gnad",
        "Mehdi B. Tahoori"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f32ac891e623a3c000bcee8e36f351f1ccfa2707",
      "pdf_url": "",
      "publication_date": "2025-01-20",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-21",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "title": "\u03b4-STEAL: LLM Stealing Attack with Local Differential Privacy",
      "abstract": "Large language models (LLMs) demonstrate remarkable capabilities across various tasks. However, their deployment introduces significant risks related to intellectual property. In this context, we focus on model stealing attacks, where adversaries replicate the behaviors of these models to steal services. These attacks are highly relevant to proprietary LLMs and pose serious threats to revenue and financial stability. To mitigate these risks, the watermarking solution embeds imperceptible patterns in LLM outputs, enabling model traceability and intellectual property verification. In this paper, we study the vulnerability of LLM service providers by introducing $\\delta$-STEAL, a novel model stealing attack that bypasses the service provider's watermark detectors while preserving the adversary's model utility. $\\delta$-STEAL injects noise into the token embeddings of the adversary's model during fine-tuning in a way that satisfies local differential privacy (LDP) guarantees. The adversary queries the service provider's model to collect outputs and form input-output training pairs. By applying LDP-preserving noise to these pairs, $\\delta$-STEAL obfuscates watermark signals, making it difficult for the service provider to determine whether its outputs were used, thereby preventing claims of model theft. Our experiments show that $\\delta$-STEAL with lightweight modifications achieves attack success rates of up to $96.95\\%$ without significantly compromising the adversary's model utility. The noise scale in LDP controls the trade-off between attack effectiveness and model utility. This poses a significant risk, as even robust watermarks can be bypassed, allowing adversaries to deceive watermark detectors and undermine current intellectual property protection methods.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kieu Dang",
        "Phung Lai",
        "Nhathai Phan",
        "Yelong Shen",
        "Ruoming Jin",
        "Abdallah Khreishah"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "pdf_url": "",
      "publication_date": "2025-10-24",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack",
        "LLM stealing"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "509a77d3e7a4a48ff7b191fe1802db4d597de8c8",
      "title": "Defenses against model stealing attacks in MLaaS: literature review and challenges",
      "abstract": null,
      "year": 2025,
      "venue": "Cluster Computing",
      "authors": [
        "Aouatef Mahani",
        "O. Kazar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/509a77d3e7a4a48ff7b191fe1802db4d597de8c8",
      "pdf_url": "",
      "publication_date": "2025-08-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f8f0626c98083f77d2f5c776cc3a4b91fa273b73",
      "title": "Self-enhancing defense for protecting against model stealing attacks on deep learning systems",
      "abstract": null,
      "year": 2025,
      "venue": "Expert systems with applications",
      "authors": [
        "Chenlong Zhang",
        "Senlin Luo",
        "Jiawei Li",
        "Limin Pan",
        "Chuan Lu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8f0626c98083f77d2f5c776cc3a4b91fa273b73",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f2aaca56a4bbc4e9358f0af26e67f8cae4ebf476",
      "title": "Digital Scapegoat: An Incentive Deception Model for Resisting Unknown APT Stealing Attacks on Critical Data Resource",
      "abstract": "It is a challenging problem to resist unknown advanced persistent threats (APTs) on stealing data resources in an information system of critical infrastructures, because APT attackers have very specific objectives and compromise the system stealthily and slowly. We observe that it is a necessary condition for APT attackers to achieve their campaigns via controlling unknown Trojans to access and exfiltrate critical files. We present a theoretical model called Digital Scapegoat (abbreviated as DS-IDep) that constructs an Incentive Deception defense schema to hijack the attacker\u2019s access to critical files and redirect it to avatar files without awareness. We propose a FlipIDep Game model (<inline-formula> <tex-math notation=\"LaTeX\">$G_{F}$ </tex-math></inline-formula>) and a Markov Game model (<inline-formula> <tex-math notation=\"LaTeX\">$G_{M}$ </tex-math></inline-formula>) to characterize completely the payoffs, equilibria, and best strategies from the perspective of the attacker and the defender respectively. We also design an exponential risk propagation model to evaluate the ability of DS-IDep to eliminate stealing impact when the risk is propagated between states. Theoretically, we can achieve the objective of stealing impact elimination (<inline-formula> <tex-math notation=\"LaTeX\">$L_{K} \\lt 0.001$ </tex-math></inline-formula>) when the ratio of incentive deception exceeds 0.7 (<inline-formula> <tex-math notation=\"LaTeX\">$\\eta \\gt 0.7$ </tex-math></inline-formula>) and the probability of an attack operation bypassing the defense surface is less than 0.1 (<inline-formula> <tex-math notation=\"LaTeX\">$r^{*}\\times \\mu \\lt 0.1$ </tex-math></inline-formula>) under Stackelberg strategies. We develop a kernel-level incentive deception defense surface according to the theoretical parameters of the DS-IDep. The experimental results show that DS-IDep can resist APT stealing attacks from unknown Trojans. We also evaluate the DS-IDep in five well-known software applications. It demonstrates that DS-IDep can address unknown attacks from compromised software with less than 10% performance overhead.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xiaochun Yun",
        "Guangjun Wu",
        "Shuhao Li",
        "Qi Song",
        "Zixian Tang",
        "Zhenyu Cheng"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f2aaca56a4bbc4e9358f0af26e67f8cae4ebf476",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "23bb050bbdf0d710f9f6f665e227b25c1d6ee034",
      "title": "Catch the Star: Weight Recovery Attack Using Side-Channel Star Map Against DNN Accelerator",
      "abstract": "The rapid development of Artificial Intelligence (AI) technology must be connected to the arithmetic support of high-performance hardware. However, when the deep neural network (DNN) accelerator performs inference tasks at the edge end, the sensitive data of DNN will generate leakage through side-channel information. The adversary can recover the model structure and weight parameters of DNN by using the side-channel information, which seriously affects the protection of necessary intellectual property (IP) of DNN, so the hardware security of the DNN accelerator is critical. In the current research of Side-channel attack (SCA) for matrix multiplication units, such as systolic arrays, the linear multiplication operation leads to a more extensive weights search space for the SCA, and extracting all the weight parameters requires higher attack conditions. This article proposes a new power SCA method, which includes a Collision-Correlation Power Analysis (Collision-CPA) and Correlation-based Weight Search Algorithm (C-WSA) to address the problem. The Collision-CPA reduces the attack conditions for the SCA by building multiple Hamming Distance (HD)-based power leakage models for the systolic array. Meanwhile, the C-WSA dramatically reduces the weights search space. In addition, the concept of a Side-channel star map (SCSM) is proposed for the first time in this article, and the adversary can quickly and accurately locate the correct weight information in the SCSM. Through experiments, we recover all the weight parameters of a $3\\times 3$ systolic array based on 100000 power traces, in which the weight search space is reduced by up to 97.7%. For the DNN accelerator at the edge, especially the systolic array structure, our proposed novel SCA aligns more with practical attack scenarios, with lower attack conditions, and higher attack efficiency.",
      "year": 2025,
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": [
        "Le Wu",
        "Liji Wu",
        "Xiang Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/23bb050bbdf0d710f9f6f665e227b25c1d6ee034",
      "pdf_url": "",
      "publication_date": "2025-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e685da2d597d7b478a514bf26d78f5d9131e6b98",
      "title": "MACPruning: Dynamic Operation Pruning to Mitigate Side-Channel DNN Model Extraction",
      "abstract": "As deep learning gains popularity, edge IoT devices have seen proliferating deployment of pre-trained Deep Neural Network (DNN) models. These DNNs represent valuable intellectual property and face significant confidentiality threats from side-channel analysis (SCA), particularly non-invasive Differential Electromagnetic (EM) Analysis (DEMA), which retrieves individual model parameters from EM traces collected during model inference. Traditional SCA mitigation methods, such as masking and shuffling, can still be applied to DNN inference, but will incur significant performance degradation due to the large volume of operations and parameters. Based on the insight that DNN models have high redundancy and are robust to input variation, we introduce MACPruning, a novel lightweight defense against DEMA-based parameter extraction attacks, exploiting specific characteristics of DNN execution. The design principle of MACPruning is to randomly deactivate input pixels and prune the operations (typically multiply-accumulate-MAC) on those pixels. The technique removes certain leakages and overall redistributes weight-dependent EM leakages temporally, and thus effectively mitigates DEMA. To maintain DNN performance, we propose an importance-aware pixel map that preserves critical input pixels, keeping randomness in the defense while minimizing its impact on DNN performance due to operation pruning. We conduct a comprehensive security analysis of MACPruning on various datasets for DNNs on edge devices. Our evaluations demonstrate that MACPruning effectively reduces EM leakages with minimal impact on the model accuracy and negligible computational overhead.",
      "year": 2025,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Ruyi Ding",
        "Gongye Cheng",
        "Davis Ranney",
        "A. A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e685da2d597d7b478a514bf26d78f5d9131e6b98",
      "pdf_url": "",
      "publication_date": "2025-02-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ec45414afec5007925db471273111da8bf389234",
      "title": "Arithmetic Masking Countermeasure to Mitigate Side-Channel-Based Model Extraction Attack on DNN Accelerator",
      "abstract": null,
      "year": 2025,
      "venue": "ACNS Workshops",
      "authors": [
        "Hirokatsu Yamasaki",
        "Kota Yoshida",
        "Yuta Fukuda",
        "Takeshi Fujino"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ec45414afec5007925db471273111da8bf389234",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0a56e4103911d723020bb99bf63f5bb452b236e9",
      "title": "PROMPTMINER: Black-Box Prompt Stealing against Text-to-Image Generative Models via Reinforcement Learning and Fuzz Optimization",
      "abstract": "Text-to-image (T2I) generative models such as Stable Diffusion and FLUX can synthesize realistic, high-quality images directly from textual prompts. The resulting image quality depends critically on well-crafted prompts that specify both subjects and stylistic modifiers, which have become valuable digital assets. However, the rising value and ubiquity of high-quality prompts expose them to security and intellectual-property risks. One key threat is the prompt stealing attack, i.e., the task of recovering the textual prompt that generated a given image. Prompt stealing enables unauthorized extraction and reuse of carefully engineered prompts, yet it can also support beneficial applications such as data attribution, model provenance analysis, and watermarking validation. Existing approaches often assume white-box gradient access, require large-scale labeled datasets for supervised training, or rely solely on captioning without explicit optimization, limiting their practicality and adaptability. To address these challenges, we propose PROMPTMINER, a black-box prompt stealing framework that decouples the task into two phases: (1) a reinforcement learning-based optimization phase to reconstruct the primary subject, and (2) a fuzzing-driven search phase to recover stylistic modifiers. Experiments across multiple datasets and diffusion backbones demonstrate that PROMPTMINER achieves superior results, with CLIP similarity up to 0.958 and textual alignment with SBERT up to 0.751, surpassing all baselines. Even when applied to in-the-wild images with unknown generators, it outperforms the strongest baseline by 7.5 percent in CLIP similarity, demonstrating better generalization. Finally, PROMPTMINER maintains strong performance under defensive perturbations, highlighting remarkable robustness. Code: https://github.com/aaFrostnova/PromptMiner",
      "year": 2025,
      "venue": "",
      "authors": [
        "Mingzhe Li",
        "Renhao Zhang",
        "Zhiyang Wen",
        "Siqi Pan",
        "Bruno Castro da Silva",
        "Juan Zhai",
        "Shiqing Ma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0a56e4103911d723020bb99bf63f5bb452b236e9",
      "pdf_url": "",
      "publication_date": "2025-11-27",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-02",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "f9e06ecdefac93f1c0dc26436ed73c0e708d4307",
      "title": "Middleware Architecture for the Management and Mitigation of OWASP ML05: Model Theft in IoT Machine Learning Networks",
      "abstract": "The increasing integration of machine learning (ML) models into Internet of Things (IoT) applications has led to notable advancements in automation and decision-making. However, these models are vulnerable to modern attack vectors recognized by the OWASP Top 10 for Large Language Model Applications, specifically ML05: Model Theft, where adversaries gain unauthorized access to model parameters and training data, compromising intellectual property and sensitive information. Such threats are particularly concerning in IoT environments due to their distributed nature and resource limitations. This paper proposes a middleware architecture for the management and mitigation of model theft risks by incorporating encryption, access control, obfuscation, watermarking, continuous monitoring, and service assurance programmability. By strengthening the security management framework of ML models deployed in IoT, the proposed architecture aims to protect against theft, ensure data confidentiality, and maintain network resilience. The approach includes detailed mathematical models and an evaluation of existing security measures, demonstrating the architecture's effectiveness in diverse IoT deployments, such as telemedicine and smart cities.",
      "year": 2025,
      "venue": "Global",
      "authors": [
        "Yair Enrique Rivera Julio",
        "\u00c1ngel Pinto",
        "Nelson A. P\u00e9rez-Garc\u00eda",
        "M\u00f3nica-Karel Huerta",
        "C\u00e9sar Viloria-N\u00fa\u00f1ez",
        "Marvin Luis P\u00e9rez Cabrera",
        "Frank Ibarra Hern\u00e1ndez",
        "Juan Manuel Torres Tovio",
        "Erwin J. Sacoto-Cabrera"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f9e06ecdefac93f1c0dc26436ed73c0e708d4307",
      "pdf_url": "",
      "publication_date": "2025-08-04",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "45475dcf398e58811ac8487fbd72fe6777bd8676",
      "title": "Quantum Disturbance-based Photon Cloning Attack",
      "abstract": "Geopolitical concerns in Asia have intensified, driving countries to bolster their border security and upgrade their military capabilities. With new and complex threats emerging, governments must invest in advanced defense technologies to maintain strategic stability and deter large-scale conflicts. The deployment and development of advanced military technologies is one of the key focuses. Surveillance largely contributes to situational awareness, fortifying defense and response time, making it the perfect tool for drones, AI-driven surveillance systems, cyber warfare capabilities, and anti-ship missiles. Such technologies enable countries to detect, prevent, and counter threats efficiently, thus making them essential in 21st-century warfare. In addition to the arms race in conventional defense, we're seeing increasingly base forms of international competition, particularly around advanced technologies like space-based reconnaissance, quantum encryption, and hypersonic weapons.Additionally, employing features such as AI surveillance systems, smart defense systems, and civil forces will be needed to stop wars before they happen. Governments must ensure they are attending to the urgent and game-changing elements, such as early-warning systems, predictive analytics, and automated threat response technologies that make for speedy and effective crisis management. It\u2019s critical to tighten cybersecurity infrastructure, as cyberattacks against military and governmental networks are increasing. Modernization efforts for an agile and resilient military force require substantial investment in R&D and defense infrastructure. New developments in autonomous defense systems, robotics, and advanced missile systems are key pieces in deterring aggression and protecting national interests.",
      "year": 2025,
      "venue": "International Conference on Innovative Mechanisms for Industry Applications",
      "authors": [
        "Hui-Kai Su",
        "K.MahaRajan",
        "Sanmugasundaram R",
        "M.Jayalakshmi",
        "A. S. Nantha",
        "Wen-Kai Kuo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/45475dcf398e58811ac8487fbd72fe6777bd8676",
      "pdf_url": "",
      "publication_date": "2025-09-03",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "dc1d5b64a1b8bf876b4cd518f3786b6332dbc9b6",
      "title": "Detecting Generative Model Inversion Attacks for Protecting Intellectual Property of Deep Neural Networks",
      "abstract": "Recently, protecting the Intellectual Property (IP) of deep neural networks (DNNs) has attracted attention from researchers. This is because training DNN models can be costly especially when acquiring and labeling training data require domain expertise. DNN watermarking and fingerprinting are two techniques proposed to prevent DNN IP infringement. Although these two techniques achieve high performance on defending against previously proposed DNN stealing attacks, researchers recently show that both of them are ineffective against generative model inversion attacks. Specifically, an adversary inverts training data from well-trained DNNs and uses the inverted data to train DNNs from scratch such that DNN watermarking and fingerprinting are both bypassed. This novel model stealing strategy shows that data inverted from victim models can be effectively exploited by adversaries, which poses a new threat to the IP protection of DNNs. To combat this new threat, one potential solution is to enable defenders to prove ownership on data inverted from models being protected. If the training data of a suspected model, which can be disclosed via the judicial process, are proven to be data inverted from victim models, then IP infringement is detected. This research direction is currently underexplored. In this paper, we fill the gap in the literature to investigate countermeasures against this emerging threat. We propose a simple but effective method, called InverseDataInspector (IDI), to detect whether data are inverted from victim models. Specifically, our method first extracts features from both the inverted data and victim models. These features are then combined and used for training classifiers. Experimental results demonstrate that our method achieves high performance on detecting inverted data and also generalizes to new generative model inversion methods that are not seen when training classifiers.",
      "year": 2025,
      "venue": "Journal of Artificial Intelligence Research",
      "authors": [
        "Yiding Yu",
        "W. Zong",
        "Wenjing Su",
        "Yang-Wai Chow",
        "Willy Susilo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/dc1d5b64a1b8bf876b4cd518f3786b6332dbc9b6",
      "pdf_url": "",
      "publication_date": "2025-10-28",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "5420739477bc061905a058bdcae98f42ccb870a2",
      "title": "Unveiling Deep Learning Models: Leveraging Active Learning and High-Confidence Feature Extraction in Model Stealing",
      "abstract": "Recent research reveals that weaknesses in black box models can be exposed by stealing their functional characteristics. However, the current stealing process faces challenges such as low accuracy, insufficient hard label guidance and limited query quantity. To overcome these limitations, we have developed a method that facilitates the stealing of hard-label black-box models, called ALHC (Active Learning and High- confidence Feature Extraction). It incorporates a high-confidence stealing module to generate high-quality datasets. And it also introduces an efficient active learning selection strategy for training substitute models that exhibit higher similarity within the same query budget. In multiple datasets and real-world APIs, the ALHC has demonstrated remarkable effectiveness, surpassed traditional approaches and improved consistency and accuracy.",
      "year": 2025,
      "venue": "2025 8th International Conference on Advanced Algorithms and Control Engineering (ICAACE)",
      "authors": [
        "Lei Liu",
        "Lei Wang",
        "Ce Gu",
        "Hongwei Ma",
        "Jieyu Wang",
        "Wenjun Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5420739477bc061905a058bdcae98f42ccb870a2",
      "pdf_url": "",
      "publication_date": "2025-03-21",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "4a669b0be85b0bc46a360b46abab0ba2c65c6ba9",
      "title": "High-Performance Prompting for LLM Extraction of Compression Fracture Findings from Radiology Reports.",
      "abstract": null,
      "year": 2025,
      "venue": "Journal of imaging informatics in medicine",
      "authors": [
        "Mohammed M. Kanani",
        "Arezu Monawer",
        "Lauryn Brown",
        "William E King",
        "Zachary D Miller",
        "Nitin Venugopal",
        "Patrick J Heagerty",
        "J. Jarvik",
        "Trevor Cohen",
        "Nathan M. Cross"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4a669b0be85b0bc46a360b46abab0ba2c65c6ba9",
      "pdf_url": "",
      "publication_date": "2025-05-16",
      "keywords_matched": [
        "LLM extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a659e69b704b4f215bb775ae54f6bfe8411dd8db",
      "title": "DeepTracer: Tracing Stolen Model via Deep Coupled Watermarks",
      "abstract": "Model watermarking techniques can embed watermark information into the protected model for ownership declaration by constructing specific input-output pairs. However, existing watermarks are easily removed when facing model stealing attacks, and make it difficult for model owners to effectively verify the copyright of stolen models. In this paper, we analyze the root cause of the failure of current watermarking methods under model stealing scenarios and then explore potential solutions. Specifically, we introduce a robust watermarking framework, DeepTracer, which leverages a novel watermark samples construction method and a same-class coupling loss constraint. DeepTracer can incur a high-coupling model between watermark task and primary task that makes adversaries inevitably learn the hidden watermark task when stealing the primary task functionality. Furthermore, we propose an effective watermark samples filtering mechanism that elaborately select watermark key samples used in model ownership verification to enhance the reliability of watermarks. Extensive experiments across multiple datasets and models demonstrate that our method surpasses existing approaches in defending against various model stealing attacks, as well as watermark attacks, and achieves new state-of-the-art effectiveness and robustness.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Yu Xuan",
        "Zhendong Zhao",
        "Xin Zhao",
        "He Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a659e69b704b4f215bb775ae54f6bfe8411dd8db",
      "pdf_url": "",
      "publication_date": "2025-11-12",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "15cb47e68968064996190efae5a5f875af516ac9",
      "title": "POSTER: Disappearing Ink: How Partial Model Extraction Erases Watermarks",
      "abstract": "Deep neural networks have become invaluable intellectual property in machine learning. To deter model theft, Watermarking has emerged as a prominent defense by embedding hidden \u201ctrigger sets\u201d that aid in ownership verification. However, current watermarking solutions primarily address scenarios where adversaries steal the entire model. In this paper, we reveal a critical gap: partial model extraction, where only a subset of classes is stolen, substantially degrading the watermark\u2019s reliability. We introduce two attacks, Partial Model Extraction and Partial Knowledge Distillation, which reduce watermark accuracy by up to 80% while retaining strong performance on the stolen classes. Through extensive experiments on CIFAR10 and CIFAR100 against two state-of-the-art watermarking schemes, we demonstrate the need for more robust watermarking strategies that resist partial-class theft.",
      "year": 2025,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Venkata Sai Pranav Bachina",
        "Ankit Gangwal"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/15cb47e68968064996190efae5a5f875af516ac9",
      "pdf_url": "",
      "publication_date": "2025-08-24",
      "keywords_matched": [
        "model extraction",
        "model theft"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "80b18d8f790620e175cde50af13e78d03ec424b8",
      "title": "The Implementation of Neural Network Encryption Algorithms for Side-Channel Attack Protection",
      "abstract": "\u00a0With the rapid development of deep learning in the field of side-channel analysis, neural network-trained encryption algorithms have demonstrated numerous advantages, providing novel ideas for resisting side-channel attacks. We implemented a bit neural network-based block encryption scheme resistant to side-channel attacks. Experimental verification shows that the complete algorithm combined with this scheme exhibits correctness, reliability, efficiency, and resistance to side-channel attacks. This scheme has two significant advantages: First, bit networks can achieve functions that multilayer perceptrons (MLPs) cannot perform. For example, using the AES encryption algorithm, we successfully reduced the column mixing network loss during MLP training from 0.25 to 0. Second, bit networks can integrate with MLPs without intermediate value leakage issues. Once combined with MLPs, the generalization capability of the AES round operation model is significantly enhanced, while ensuring that the number",
      "year": 2025,
      "venue": "Journal of Computing and Electronic Information Management",
      "authors": [
        "Bo Chen",
        "Yi Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/80b18d8f790620e175cde50af13e78d03ec424b8",
      "pdf_url": "",
      "publication_date": "2025-03-05",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "cb00fa94e62cb3d215a29e3bd0b1bcf02085a6ec",
      "title": "Understanding Neural Networks in Profiled Side-Channel Analysis",
      "abstract": "Side-channel analysis (SCA) capitalizes on unintentionally leaked information to extract sensitive data from cryptographic systems. Over recent years, deep learning has shown effectiveness in analyzing the diverse forms of SCA signals. However, due to the absence of a comprehensive understanding, constructing effective networks tailored for a variety of cryptographic systems becomes a considerable challenge. This paper proposes a novel methodology designed to deconstruct networks intended for SCA, with the goal of enhancing our understanding of the mechanisms by which these complex systems process diverse SCA signals. Our approach begins with a f-ANOVA-based method to pinpoint pivotal parameter amidst a plethora of adjustable ones. Thereafter, network visualization technique is harnessed to investigate the impact of variations in these key parameters. Through experiments, we have distilled principles for network formulation that accommodate the unique characteristics inherent in side-channel signals. The experimental outcomes highlight notable improvements when parameters are set according to the proposed principles.",
      "year": 2025,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Yimeng Chen",
        "Bo Wang",
        "Changshan Su",
        "Ao Li",
        "Gen Li",
        "Yuxing Tang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/cb00fa94e62cb3d215a29e3bd0b1bcf02085a6ec",
      "pdf_url": "",
      "publication_date": "2025-04-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d66b4910d4c16463e1859035cca94bbefbd76f92",
      "title": "Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection",
      "abstract": "In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels. To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Zhengchunmin Dai",
        "Jiaxiong Tang",
        "Peng Sun",
        "Honglong Chen",
        "Liantao Wu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d66b4910d4c16463e1859035cca94bbefbd76f92",
      "pdf_url": "",
      "publication_date": "2025-11-18",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c39a0348fc04f387ebdf91079d0b8b79d0984ce5",
      "title": "RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection",
      "abstract": "Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \\textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Shufan Yang",
        "Zifeng Cheng",
        "Zhiwei Jiang",
        "Yafeng Yin",
        "Cong Wang",
        "Shiping Ge",
        "Yuchen Fu",
        "Qing Gu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c39a0348fc04f387ebdf91079d0b8b79d0984ce5",
      "pdf_url": "",
      "publication_date": "2025-11-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "0f6d3176161c5dfe0ec6642afa5e75231bc519dd",
      "title": "Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks",
      "abstract": "Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks. For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yaxin Xiao",
        "Qingqing Ye",
        "Zi Liang",
        "Haoyang Li",
        "Ronghua Li",
        "Huadi Zheng",
        "Haibo Hu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0f6d3176161c5dfe0ec6642afa5e75231bc519dd",
      "pdf_url": "",
      "publication_date": "2025-11-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "68574082c3b466126b0c2f6a15f3cbd7d4d5c51b",
      "title": "Extracting Proxy Models from Side-Channel Insights to Enhance Adversarial Attacks on Black-Box DNNs",
      "abstract": "Side-channel information leakage can be exploited to reverse engineer critical architectural details of a target DNN model executing on a hardware accelerator. However, using these details to apply a practical adversarial attack remains a significant challenge. In this paper, we first introduce a novel approach to analyze side-channel data and extract detailed architectural information of DNN models, including accurate prediction of layer hyperparameters and inter-layer skip connections. Next, we develop techniques to construct effective proxy models from this information. We then leverage white-box access to these proxies to generate adversarial examples capable of effectively deceiving the target DNN model. We illustrate our techniques using popular DNNs as target models, and demonstrate that the constructed proxy models achieve up to 89.8% similarity in performance compared to the target models. Furthermore, we achieve adversarial transferability rates of up to 72.34% and induce up to 60.4% drop in accuracy in the target models using the crafted adversarial images. Compared to off-the-shelf substitute models, our method improves transferability by as much as 30% in untargeted adversarial attacks. Even when the target model is protected by a state-of-the-art denoiser, our proxy models generate 5.5% more transferable adversarial examples compared to other substitute models in untargeted adversarial attacks.",
      "year": 2025,
      "venue": "Proceedings of the 11th ACM Cyber-Physical System Security Workshop",
      "authors": [
        "Srivatsan Chandrasekar",
        "Likith Anaparty",
        "Siew-Kei Lam",
        "Vivek Chaturvedi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/68574082c3b466126b0c2f6a15f3cbd7d4d5c51b",
      "pdf_url": "",
      "publication_date": "2025-08-25",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "25245b57c2f0ce2f7a7bae6f55efb945ed7b19c9",
      "title": "BarkBeetle: Stealing Decision Tree Models with Fault Injection",
      "abstract": "Machine learning models, particularly decision trees (DTs), are widely adopted across various domains due to their interpretability and efficiency. However, as ML models become increasingly integrated into privacy-sensitive applications, concerns about their confidentiality have grown, particularly in light of emerging threats such as model extraction and fault injection attacks. Assessing the vulnerability of DTs under such attacks is therefore important. In this work, we present BarkBeetle, a novel attack that leverages fault injection to extract internal structural information of DT models. BarkBeetle employs a bottom-up recovery strategy that uses targeted fault injection at specific nodes to efficiently infer feature splits and threshold values. Our proof-of-concept implementation demonstrates that BarkBeetle requires significantly fewer queries and recovers more structural information compared to prior approaches, when evaluated on DTs trained with public UCI datasets. To validate its practical feasibility, we implement BarkBeetle on a Raspberry Pi RP2350 board and perform fault injections using the Faultier voltage glitching tool. As BarkBeetle targets general DT models, we also provide an in-depth discussion on its applicability to a broader range of tree-based applications, including data stream classification, DT variants, and cryptography schemes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Qifan Wang",
        "Jonas Sander",
        "Minmin Jiang",
        "Thomas Eisenbarth",
        "David Oswald"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/25245b57c2f0ce2f7a7bae6f55efb945ed7b19c9",
      "pdf_url": "",
      "publication_date": "2025-07-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "98df987a99601fd41816c0ad0a3a6c65748a2eb5",
      "title": "FewMEA: Few-shot Model Extraction Attack against Sequential Recommenders",
      "abstract": null,
      "year": 2025,
      "venue": "International Conference on Multimedia Retrieval",
      "authors": [
        "Fu Liu",
        "Hui Zhang",
        "Yuqin Lan",
        "Min Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/98df987a99601fd41816c0ad0a3a6c65748a2eb5",
      "pdf_url": "",
      "publication_date": "2025-06-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e86113fbdd6ac033b1dbe7e985646fc407ef8332",
      "title": "How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment",
      "abstract": "Graph Neural Networks (GNNs) have become essential tools for analyzing graph-structured data in domains such as drug discovery and financial analysis, leading to growing demands for model transparency. Recent advances in explainable GNNs have addressed this need by revealing important subgraphs that influence predictions, but these explanation mechanisms may inadvertently expose models to security risks. This paper investigates how such explanations potentially leak critical decision logic that can be exploited for model stealing. We propose {\\method}, a novel stealing framework that integrates explanation alignment for capturing decision logic with guided data augmentation for efficient training under limited queries, enabling effective replication of both the predictive behavior and underlying reasoning patterns of target models. Experiments on molecular graph datasets demonstrate that our approach shows advantages over conventional methods in model stealing. This work highlights important security considerations for the deployment of explainable GNNs in sensitive domains and suggests the need for protective measures against explanation-based attacks. Our code is available at https://github.com/beanmah/EGSteal.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Bin Ma",
        "Yuyuan Feng",
        "Minhua Lin",
        "Enyan Dai"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e86113fbdd6ac033b1dbe7e985646fc407ef8332",
      "pdf_url": "",
      "publication_date": "2025-06-03",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ecbd364509711911af74dd0d1be4022bb7e1bec6",
      "title": "On the Interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction",
      "abstract": "Machine Learning as a Service (MLaaS) has gained important attraction as a means for deploying powerful predictive models, offering ease of use that enables organizations to leverage advanced analytics without substantial investments in specialized infrastructure or expertise. However, MLaaS platforms must be safeguarded against security and privacy attacks, such as model extraction (MEA) attacks. The increasing integration of explainable AI (XAI) within MLaaS has introduced an additional privacy challenge, as attackers can exploit model explanations particularly counterfactual explanations (CFs) to facilitate MEA. In this paper, we investigate the trade offs among model performance, privacy, and explainability when employing Differential Privacy (DP), a promising technique for mitigating CF facilitated MEA. We evaluate two distinct DP strategies: implemented during the classification model training and at the explainer during CF generation.",
      "year": 2025,
      "venue": "xAI",
      "authors": [
        "Fatima Ezzeddine",
        "Rinad Akel",
        "Ihab Sbeity",
        "Silvia Giordano",
        "Marc Langheinrich",
        "Omran Ayoub"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ecbd364509711911af74dd0d1be4022bb7e1bec6",
      "pdf_url": "",
      "publication_date": "2025-05-13",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "385545515cfb1b63a6c5f81464e4ef2050cfd493",
      "title": "Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks",
      "abstract": "Developing high-performance deep learning models is resource-intensive, leading model owners to utilize Machine Learning as a Service (MLaaS) platforms instead of publicly releasing their models. However, malicious users may exploit query interfaces to execute model extraction attacks, reconstructing the target model's functionality locally. While prior research has investigated triggerable watermarking techniques for asserting ownership, existing methods face significant challenges: (1) most approaches require additional training, resulting in high overhead and limited flexibility, and (2) they often fail to account for advanced attackers, leaving them vulnerable to adaptive attacks. In this paper, we propose Neural Honeytrace, a robust plug-and-play watermarking framework against model extraction attacks. We first formulate a watermark transmission model from an information-theoretic perspective, providing an interpretable account of the principles and limitations of existing triggerable watermarking. Guided by the model, we further introduce: (1) a similarity-based training-free watermarking method for plug-and-play and flexible watermarking, and (2) a distribution-based multi-step watermark information transmission strategy for robust watermarking. Comprehensive experiments on four datasets demonstrate that Neural Honeytrace outperforms previous methods in efficiency and resisting adaptive attacks. Neural Honeytrace reduces the average number of samples required for a worst-case t-Test-based copyright claim from 193,252 to 1,857 with zero training cost. The code is available at https://github.com/NeurHT/NeurHT.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yixiao Xu",
        "Binxing Fang",
        "Rui Wang",
        "Yinghai Zhou",
        "Shouling Ji",
        "Yuan Liu",
        "Mohan Li",
        "Zhihong Tian"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/385545515cfb1b63a6c5f81464e4ef2050cfd493",
      "pdf_url": "",
      "publication_date": "2025-01-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "7f8163935429700fed6c5ff88417726cb4ea4884",
      "title": "Exploiting Timing Side-Channels in Quantum Circuits Simulation Via ML-Based Methods",
      "abstract": "As quantum computing advances, quantum circuit simulators serve as critical tools to bridge the current gap caused by limited quantum hardware availability. These simulators are typically deployed on cloud platforms, where users submit proprietary circuit designs for simulation. In this work, we demonstrate a novel timing side-channel attack targeting cloud- based quantum simulators. A co-located malicious process can observe fine-grained execution timing patterns to extract sensitive information about concurrently running quantum circuits. We systematically analyze simulator behavior using the QASMBench benchmark suite, profiling timing and memory characteristics across various circuit executions. Our experimental results show that timing profiles exhibit circuit-dependent patterns that can be effectively classified using pattern recognition techniques, enabling the adversary to infer circuit identities and compromise user confidentiality. We were able to achieve 88% to 99.9% identification rate of quantum circuits based on different datasets. This work highlights previously unexplored security risks in quantum simulation environments and calls for stronger isolation mechanisms to protect user workloads",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ben Dong",
        "Hui Feng",
        "Qian Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7f8163935429700fed6c5ff88417726cb4ea4884",
      "pdf_url": "",
      "publication_date": "2025-09-16",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f0205fe725f9e9e58c5402b8579c864e9b19797b",
      "title": "Misclassification-driven Fingerprinting for DNNs Using Frequency-aware GANs",
      "abstract": "Deep neural networks (DNNs) have become valuable assets due to their success in various tasks, but their high training costs also make them targets for model theft. Fingerprinting techniques are commonly used to verify model ownership, but existing methods either require training many additional models, leading to increased costs, or rely on GANs to generate fingerprints near decision boundaries, which may compromise image quality. To address these challenges, we propose a GAN-based fingerprint generation method that applies frequency-domain perturbations to normal samples, effectively creating fingerprints. This approach not only resists intellectual property (IP) threats, but also improves fingerprint acquisition efficiency while maintaining high imperceptibility. Extensive experiments demonstrate that our method achieves a state-of-the-art (SOTA) AUC of 0.98 on the Tiny-ImageNet dataset under IP removal attacks, outperforming existing methods by 8%, and consistently achieves the best ABP for three types of IP detection and erasure attacks on the GTSRB dataset. Our source code is available at https://github.com/wason981/Frequency-Fingerprinting.",
      "year": 2025,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Weixing Liu",
        "Shenghua Zhong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f0205fe725f9e9e58c5402b8579c864e9b19797b",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e3f7a9b248785cac49c96a22ee4ba11d88235f9b",
      "title": "WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are increasingly deployed in real-world applications, making ownership verification critical to protect their intellectual property against model theft. Fingerprinting and black-box watermarking are two main methods. However, the former relies on determining model similarity, which is computationally expensive and prone to ownership collisions after model post-processing. The latter embeds backdoors, exposing watermarked models to the risk of backdoor attacks. Moreover, both previous methods enable ownership verification but do not convey additional information about the copy model. If the owner has multiple models, each model requires a distinct trigger graph. To address these challenges, this paper proposes WGLE, a novel black-box watermarking paradigm for GNNs that enables embedding the multi-bit string in GNN models without using backdoors. WGLE builds on a key insight we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the difference between the feature distance and the prediction distance of two connected nodes in a graph. By assigning unique LDDE values to the edges and employing the LDDE sequence as the watermark, WGLE supports multi-bit capacity without relying on backdoor mechanisms. We evaluate WGLE on six public datasets across six mainstream GNN architectures, and compare WGLE with state-of-the-art GNN watermarking and fingerprinting methods. WGLE achieves 100% ownership verification accuracy, with an average fidelity degradation of only 1.41%. Additionally, WGLE exhibits robust resilience against potential attacks. The code is available in the repository.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tingzhi Li",
        "Xuefeng Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e3f7a9b248785cac49c96a22ee4ba11d88235f9b",
      "pdf_url": "",
      "publication_date": "2025-06-10",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5e91b9b928b33ce05273507cf8dea1ade2450598",
      "title": "From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection",
      "abstract": "Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Hao Li",
        "Yubing Ren",
        "Yanan Cao",
        "Yingjie Li",
        "Fang Fang",
        "Xuebin Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5e91b9b928b33ce05273507cf8dea1ade2450598",
      "pdf_url": "",
      "publication_date": "2025-12-18",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-20",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "title": "Stealing Part of a Production Language Model",
      "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \\$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nicholas Carlini",
        "Daniel Paleka",
        "K. Dvijotham",
        "Thomas Steinke",
        "Jonathan Hayase",
        "A. F. Cooper",
        "Katherine Lee",
        "Matthew Jagielski",
        "Milad Nasr",
        "Arthur Conmy",
        "Eric Wallace",
        "D. Rolnick",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 132,
      "url": "https://www.semanticscholar.org/paper/b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "pdf_url": "",
      "publication_date": "2024-03-11",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "c7af46b35061e856aa3332ac2eec6a7ccee0cb35",
      "title": "Watermark Stealing in Large Language Models",
      "abstract": "LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as hypothesized in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nikola Jovanovi'c",
        "Robin Staab",
        "Martin T. Vechev"
      ],
      "citation_count": 72,
      "url": "https://www.semanticscholar.org/paper/c7af46b35061e856aa3332ac2eec6a7ccee0cb35",
      "pdf_url": "",
      "publication_date": "2024-02-29",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "836c652834b0f6ffe10e53ede1c0b9433cfad9ea",
      "title": "Copyright Protection in Generative AI: A Technical Perspective",
      "abstract": "Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jie Ren",
        "Han Xu",
        "Pengfei He",
        "Yingqian Cui",
        "Shenglai Zeng",
        "Jiankun Zhang",
        "Hongzhi Wen",
        "Jiayuan Ding",
        "Hui Liu",
        "Yi Chang",
        "Jiliang Tang"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/836c652834b0f6ffe10e53ede1c0b9433cfad9ea",
      "pdf_url": "",
      "publication_date": "2024-02-04",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "538e8499246efac815f3d3a88bfd1542ea736f60",
      "title": "Dynamics prediction using an artificial neural network for a weakly conductive ionized fluid streamed over a vibrating electromagnetic plate",
      "abstract": null,
      "year": 2024,
      "venue": "The European Physical Journal Plus",
      "authors": [
        "P. Karmakar",
        "Satyajit Das",
        "N. Mahato",
        "A. Ali",
        "R. N. Jana"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/538e8499246efac815f3d3a88bfd1542ea736f60",
      "pdf_url": "",
      "publication_date": "2024-05-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "title": "Prompt Stealing Attacks Against Large Language Models",
      "abstract": "The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zeyang Sha",
        "Yang Zhang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "pdf_url": "",
      "publication_date": "2024-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "b719f3b74f5025aeaa0ed3ed8707e8557bce6654",
      "title": "Performance of two large language models for data extraction in evidence synthesis",
      "abstract": "Accurate data extraction is a key component of evidence synthesis and critical to valid results. The advent of publicly available large language models (LLMs) has generated interest in these tools for evidence synthesis and created uncertainty about the choice of LLM. We compare the performance of two widely available LLMs (Claude 2 and GPT\u20104) for extracting pre\u2010specified data elements from 10 published articles included in a previously completed systematic review. We use prompts and full study PDFs to compare the outputs from the browser versions of Claude 2 and GPT\u20104. GPT\u20104 required use of a third\u2010party plugin to upload and parse PDFs. Accuracy was high for Claude 2 (96.3%). The accuracy of GPT\u20104 with the plug\u2010in was lower (68.8%); however, most of the errors were due to the plug\u2010in. Both LLMs correctly recognized when prespecified data elements were missing from the source PDF and generated correct information for data elements that were not reported explicitly in the articles. A secondary analysis demonstrated that, when provided selected text from the PDFs, Claude 2 and GPT\u20104 accurately extracted 98.7% and 100% of the data elements, respectively. Limitations include the narrow scope of the study PDFs used, that prompt development was completed using only Claude 2, and that we cannot guarantee the open\u2010source articles were not used to train the LLMs. This study highlights the potential for LLMs to revolutionize data extraction but underscores the importance of accurate PDF parsing. For now, it remains essential for a human investigator to validate LLM extractions.",
      "year": 2024,
      "venue": "Research Synthesis Methods",
      "authors": [
        "Amanda Konet",
        "Ian Thomas",
        "Gerald Gartlehner",
        "Leila C Kahwati",
        "Rainer Hilscher",
        "Shannon Kugley",
        "Karen Crotty",
        "Meera Viswanathan",
        "Robert Chew"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/b719f3b74f5025aeaa0ed3ed8707e8557bce6654",
      "pdf_url": "",
      "publication_date": "2024-06-19",
      "keywords_matched": [
        "LLM extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "title": "Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses",
      "abstract": "Deep Neural Networks (DNNs) have revolutionized various domains with their exceptional performance across numerous applications. However, Model Inversion (MI) attacks, which disclose private information about the training dataset by abusing access to the trained models, have emerged as a formidable privacy threat. Given a trained network, these attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training samples, posing significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive and systematic overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this realm and presents a holistic survey. Firstly, our work briefly reviews early MI studies on traditional machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on Deep Neural Networks (DNNs) across multiple modalities and learning tasks. By meticulously analyzing their distinctive features, we summarize and classify these methods into different categories and provide a novel taxonomy. Finally, this paper discusses promising research directions and presents potential solutions to open issues. To facilitate further study on MI attacks and defenses, we have implemented an open-source model inversion toolbox on GitHub (https://github.com/ffhibnese/Model-Inversion-Attack-ToolBox).",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hao Fang",
        "Yixiang Qiu",
        "Hongyao Yu",
        "Wenbo Yu",
        "Jiawei Kong",
        "Baoli Chong",
        "Bin Chen",
        "Xuan Wang",
        "Shutao Xia"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "pdf_url": "",
      "publication_date": "2024-02-06",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "566ddf639a4a97012f1ad04636a06d81fc790b35",
      "title": "Synchronization in scale-free neural networks under electromagnetic radiation.",
      "abstract": null,
      "year": 2024,
      "venue": "Chaos",
      "authors": [
        "M. Ma",
        "Yaping Lu"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/566ddf639a4a97012f1ad04636a06d81fc790b35",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
      "abstract": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call\"neural phishing\". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Ashwinee Panda",
        "Christopher A. Choquette-Choo",
        "Zhengming Zhang",
        "Yaoqing Yang",
        "Prateek Mittal"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "de70069726ba2b1608b91f9617098f4856d35ee6",
      "title": "Direct electromagnetic information processing with planar diffractive neural network",
      "abstract": "Diffractive neural network in electromagnetic wave\u2013driven system has attracted great attention due to its ultrahigh parallel computing capability and energy efficiency. However, recent neural networks based on the diffractive framework still face the bottlenecks of misalignment and relatively large size limiting their further applications. Here, we propose a planar diffractive neural network (pla-NN) with a highly integrated and conformal architecture to achieve direct signal processing in the microwave frequency. On the basis of printed circuit fabrication process, the misalignment could be effectively circumvented while enabling flexible extension for multiple conformal and stacking designs. We first conduct validation on the fashion-MNIST dataset and experimentally build up a system using the proposed network architecture for direct recognition of different geometry structures in the electromagnetic space. We envision that the presented architecture, once combined with the advanced dynamic maneuvering techniques and flexible topology, would exhibit unlimited potentials in the areas of high-performance computing, wireless sensing, and flexible wearable electronics.",
      "year": 2024,
      "venue": "Science Advances",
      "authors": [
        "Ze Gu",
        "Qian Ma",
        "Xinxin Gao",
        "J. You",
        "T. Cui"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/de70069726ba2b1608b91f9617098f4856d35ee6",
      "pdf_url": "https://doi.org/10.1126/sciadv.ado3937",
      "publication_date": "2024-07-19",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "title": "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models",
      "abstract": "Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Shanglun Feng",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "pdf_url": "",
      "publication_date": "2024-03-30",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f39d1a38b1b9513d6d50ceb8c4663a97deafd632",
      "title": "A Fast, Performant, Secure Distributed Training Framework For LLM",
      "abstract": "The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data. However, maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved. In this paper, we propose a secure distributed LLM based on model slicing. In this case, we deploy the Trusted Execution Environment (TEE) on both the client and server side, and put the fine-tuned structure (LoRA or embedding of P-tuning v2) into the TEE. Then, secure communication is executed in the TEE and general environments through lightweight encryption. In order to further reduce the equipment cost as well as increase the model performance and accuracy, we propose a split fine-tuning scheme. In particular, we split the LLM by layers and place the latter layers in a server-side TEE (the client does not need a TEE). We then combine the proposed Sparsification Parameter Fine-tuning (SPF) with the LoRA part to improve the accuracy of the downstream task. Numerous experiments have shown that our method guarantees accuracy while maintaining security.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Wei Huang",
        "Yinggui Wang",
        "Anda Cheng",
        "Aihui Zhou",
        "Chaofan Yu",
        "Lei Wang"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/f39d1a38b1b9513d6d50ceb8c4663a97deafd632",
      "pdf_url": "",
      "publication_date": "2024-01-18",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "650f6db095276ca03d03f4951587d7383ca3b39d",
      "title": "ModelGuard: Information-Theoretic Defense Against Model Extraction Attacks",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Minxue Tang",
        "Anna Dai",
        "Louis DiValentin",
        "Aolin Ding",
        "Amin Hass",
        "Neil Zhenqiang Gong",
        "Yiran Chen",
        "Helen Li"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/650f6db095276ca03d03f4951587d7383ca3b39d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "fe9f1f1d7760a7736555135d993ccb4db6f6de12",
      "title": "Electromagnetic radiation control for nonlinear dynamics of Hopfield neural networks.",
      "abstract": "Electromagnetic radiation (EMR) affects the dynamical behavior of the nervous system, and appropriate EMR helps to study the dynamic mechanism of the nervous system. This paper uses a sophisticated four-dimensional Hopfield neural network (HNN) model augmented with one or more memristors to simulate the effects of EMR. We focus on the chaotic dynamics of HNN under the influence of EMR. Complex dynamical behaviors are found and transient chaotic phenomena have the same initial value sensitivity, showing how transient chaos is affected by EMR. Multiperiodic phenomena induced by quasi-periodic alternations are found in the dual EMR, as well as the suppression properties of the dual EMR for system chaos. This implies that the dynamical behavior of the HNN system can be controlled by varying the amount of EMR or the number of affected neurons in the HNN. Finally, a strong validation of our proposed model is provided by Multisim and Field Programmable Gate Array(FPGA) hardware.",
      "year": 2024,
      "venue": "Chaos",
      "authors": [
        "Wei Yao",
        "Jia Fang",
        "Fei Yu",
        "Li Xiong",
        "Lihong Tang",
        "Jin Zhang",
        "Yichuang Sun"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/fe9f1f1d7760a7736555135d993ccb4db6f6de12",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "37bf4cce2a17e47480a857b235addbc73f5eb7fe",
      "title": "Side-Channel-Assisted Reverse-Engineering of Encrypted DNN Hardware Accelerator IP and Attack Surface Exploration",
      "abstract": "Deep Neural Networks (DNNs) have revolutionized numerous application domains with their unparalleled performance. As the models become larger and more complex, hardware DNN accelerators are increasingly popular. Field-Programmable Gate Array (FPGA)-based DNN accelerators offer near-Application Specific Integrated Circuit (ASIC) efficiency and exceptional flexibility, establishing them as one of the primary hardware platforms for rapidly evolving deep learning implementations, particularly on edge devices. This prominence renders them lucrative targets for attackers. Existing attacks aimed at compromising the confidentiality of DNN models deployed on FPGA DNN accelerators often assume complete knowledge of the accelerators. However, this assumption does not hold for real-world, proprietary, high-performance FPGA DNN accelerators. In this study, we introduce a comprehensive and effective reverse-engineering methodology for demystifying FPGA DNN accelerator soft Intellectual Property (IP) cores. We demonstrate its application on the cutting-edge AMD-Xilinx Deep Learning Processing Unit (DPU). Our method relies on schematic analysis and, innovatively, electromagnetic (EM) side-channel analysis to reveal the data flow and scheduling of the DNN accelerators. To the best of our knowledge, this research is the first successful endeavor to reverse-engineer a commercial encrypted DNN accelerator IP. Moreover, we investigate attack surfaces exposed by the reverse-engineering findings, including the successful recovery of DNN model architectures and extraction of model parameters. These outcomes pose a significant threat to real-world commercial FPGA-DNN acceleration systems. We discuss potential countermeasures and offer recommendations for FPGA-based IP protection.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Gongye Cheng",
        "Yukui Luo",
        "Xiaolin Xu",
        "Yunsi Fei"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/37bf4cce2a17e47480a857b235addbc73f5eb7fe",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "395e6ce103b3103538ce44e5e5231c2668c90301",
      "title": "Optimization Assisted by Neural Network-Based Machine Learning in Electromagnetic Applications",
      "abstract": "We introduce an optimization assisted by a neural network (ONN) predictor to the electromagnetic community. ONN belongs to the class of the surrogate model-based optimization approaches, and approximates the objective function using a nonlinear approximator\u2014neural network. We provide a comprehensive description of the ONN algorithm and the details of its mathematical formulations. We apply ONN to optimize three popular benchmark functions and compare its performance with some commonly used optimization algorithms, namely particle swarm optimization (PSO), genetic algorithm (GA), and Bayesian optimization (BO). For the first time, we demonstrate ONN\u2019s applicability and effectiveness in antenna design problems by optimizing a six-element Yagi-Uda antenna and by solving a challenging 10-D dual-band slotted patch antenna constrained optimization problem. To achieve this, ONN is linked with a full-wave electromagnetic simulation solver through an application user interface. The optimized slotted patch is fabricated and measured to demonstrate how ONN can be part of the full antenna design process. Our empirical results indicate that ONN requires less objective function evaluations to reach the same qualitative point and reaches better optimal points for the same number of iterations for the studied benchmark functions and antenna optimization problems compared to the aforementioned baseline optimization algorithms.",
      "year": 2024,
      "venue": "IEEE Transactions on Antennas and Propagation",
      "authors": [
        "Anastasios Papathanasopoulos",
        "P. Apostolopoulos",
        "Yahya Rahmat-Samii"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/395e6ce103b3103538ce44e5e5231c2668c90301",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "5201913bb3b941e0d42606969da5c1f927aeb48b",
      "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks",
      "abstract": "Large language models (LLMs) possess extensive knowledge and question-answering capabilities, having been widely deployed in privacy-sensitive domains like finance and medical consultation. During LLM inferences, cache-sharing methods are commonly employed to enhance efficiency by reusing cached states or responses for the same or similar inference requests. However, we identify that these cache mechanisms pose a risk of private input leakage, as the caching can result in observable variations in response times, making them a strong candidate for a timing-based attack hint. In this study, we propose a novel timing-based side-channel attack to execute input theft in LLMs inference. The cache-based attack faces the challenge of constructing candidate inputs in a large search space to hit and steal cached user queries. To address these challenges, we propose two primary components. The input constructor employs machine learning techniques and LLM-based approaches for vocabulary correlation learning while implementing optimized search mechanisms for generalized input construction. The time analyzer implements statistical time fitting with outlier elimination to identify cache hit patterns, continuously providing feedback to refine the constructor's search strategy. We conduct experiments across two cache mechanisms and the results demonstrate that our approach consistently attains high attack success rates in various applications. Our work highlights the security vulnerabilities associated with performance optimizations, underscoring the necessity of prioritizing privacy and security alongside enhancements in LLM inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Xinyao Zheng",
        "Husheng Han",
        "Shangyi Shi",
        "Qiyan Fang",
        "Zidong Du",
        "Xing Hu",
        "Qi Guo"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/5201913bb3b941e0d42606969da5c1f927aeb48b",
      "pdf_url": "",
      "publication_date": "2024-11-27",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "550a79a7e688347af18bf5752361c47f0af1cf40",
      "title": "Large Language Model Watermark Stealing With Mixed Integer Programming",
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM's parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/550a79a7e688347af18bf5752361c47f0af1cf40",
      "pdf_url": "",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "9ee61db614c91c9ff0e8b6773b24c9b48e7aee5f",
      "title": "Active disturbance rejection control based on BP neural network for suspension system of electromagnetic suspension vehicle",
      "abstract": null,
      "year": 2024,
      "venue": "International Journal of Dynamics and Control",
      "authors": [
        "Shanqiang Fu",
        "Mengjuan Liu",
        "Han Wu",
        "Xin Liang",
        "Xiaohui Zeng"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/9ee61db614c91c9ff0e8b6773b24c9b48e7aee5f",
      "pdf_url": "",
      "publication_date": "2024-11-11",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "6d556b1dbb4a351b12a759c6581ed903e728c09e",
      "title": "SoK: All You Need to Know About On-Device ML Model Extraction - The Gap Between Research and Practice",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Tushar Nayan",
        "Qiming Guo",
        "Mohammed A. Duniawi",
        "Marcus Botacin",
        "Selcuk Uluagac",
        "Ruimin Sun"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/6d556b1dbb4a351b12a759c6581ed903e728c09e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7bed6f6101204efdf04181aafa511ca55644b559",
      "title": "Data Stealing Attacks against Large Language Models via Backdooring",
      "abstract": "Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring. Our method mainly focuses on the scenario of model customization and is conducted in two phases, including backdoor training and backdoor activation, which allow for the extraction of private information without prior knowledge of the model\u2019s architecture or training data. During the model customization stage, attackers inject the backdoor into the pre-trained LLM by poisoning a small ratio of the training dataset. During the inference stage, attackers can extract private information from the third-party knowledge database by incorporating the pre-defined backdoor trigger. Our method leverages the customization process of LLMs, injecting a stealthy backdoor that can be triggered after deployment to retrieve private data. We demonstrate the effectiveness of our proposed attack through extensive experiments, achieving a notable attack success rate. Extensive experiments demonstrate the effectiveness of our stealing attack in popular LLM architectures, as well as stealthiness during normal inference.",
      "year": 2024,
      "venue": "Electronics",
      "authors": [
        "Jiaming He",
        "Guanyu Hou",
        "Xinyue Jia",
        "Yangyang Chen",
        "Wenqi Liao",
        "Yinhang Zhou",
        "Rang Zhou"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/7bed6f6101204efdf04181aafa511ca55644b559",
      "pdf_url": "https://doi.org/10.3390/electronics13142858",
      "publication_date": "2024-07-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "faa68f195393a1ca36b840396278d971d3b6920a",
      "title": "A Priori Knowledge-Based Physics-Informed Neural Networks for Electromagnetic Inverse Scattering",
      "abstract": "Based on the physics-informed neural network (PINN) method, a two-step inverse scattering method is proposed to improve the efficiency and accuracy of the inversion in this work. The first step is to calculate the total fields and the initial solution of permittivity distribution in the domain of interest (DoI) by a traditional inversion algorithm, the distorted finite-difference-frequency-domain-based iterative method (DFIM), as a priori information for the cascaded PINNs. The second step is to use the calculated a priori information as additional parts of the data loss term in the proposed PINN framework for network training. Several typical numerical examples and one experimental example are considered to validate the proposed method. Inversion results show that the proposed method has good accuracy, efficiency, and robustness to noise. Compared with the data-driven deep learning methods in electromagnetic inversion, the proposed method belongs to an unsupervised learning framework and can handle more general problems. Compared with the traditional inverse algorithms, it is more efficient and accurate. In general, the proposed two-step method inherits the advantages of both traditional deep learning methods and inverse scattering methods. Importantly, it also establishes the bridge between traditional inverse scattering algorithms and deep learning methods.",
      "year": 2024,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "Yifeng Hu",
        "Xiao-Hua Wang",
        "Huiming Zhou",
        "Lei Wang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/faa68f195393a1ca36b840396278d971d3b6920a",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "85f707934e1630695fbfbdf1934a21760917416d",
      "title": "TinyPower: Side-Channel Attacks with Tiny Neural Networks",
      "abstract": "Side-channel attacks leverage correlations between power consumption and intermediate encryption results to infer encryption keys. Recent studies show that deep learning offers promising results in the context of side-channel attacks. However, neural networks utilized in deep-learning side-channel attacks are complex with a substantial number of parameters and consume significant memory. As a result, it is challenging to perform deep-learning side-channel attacks on resource-constrained devices. In this paper, we propose a framework, TinyPower, which leverages pruning to reduce the number of neural network parameters for side-channel attacks. Pruned neural networks obtained from our framework can successfully run side-channel attacks with significantly fewer parameters and less memory. Specifically, we focus on structured pruning over filters of Convolutional Neural Networks (CNNs). We demonstrate the effectiveness of structured pruning over power and EM traces of AES-128 running on microcontrollers (AVR XMEGA and ARM STM32) and FPGAs (Xilinx Artix-7). Our experimental results show that we can achieve a reduction rate of 98.8% (e.g., reducing the number of parameters from 53.1 million to 0.59 million) on a CNN and still recover keys on XMEGA. For STM32 and Artix-7, we achieve a reduction rate of 92.9% and 87.3% on a CNN respectively. We also demonstrate that our pruned CNNs can effectively perform the attack phase of side-channel attacks on a Raspberry Pi 4 with less than 2.5 millisecond inference time per trace and less than 41 MB memory usage per CNN.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Haipeng Li",
        "Mabon Ninan",
        "Boyang Wang",
        "J. M. Emmert"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/85f707934e1630695fbfbdf1934a21760917416d",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0e536b831cf1053d2c523d2e16656ad27c7369f2",
      "title": "Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model",
      "abstract": "Aligning language models (LMs) with human preferences has become a key area of research, enabling these models to meet diverse user needs better. Inspired by weak-to-strong generalization, where a strong LM fine-tuned on labels generated by a weaker model can consistently outperform its weak supervisor, we extend this idea to model alignment. In this work, we observe that the alignment behavior in weaker models can be effectively transferred to stronger models and even exhibit an amplification effect. Based on this insight, we propose a method called Weak-to-Strong Preference Optimization (WSPO), which achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model. Experiments demonstrate that WSPO delivers outstanding performance, improving the win rate of Qwen2-7B-Instruct on Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04 length-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our results suggest that using the weak model to elicit a strong model with a high alignment ability is feasible.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Wenhong Zhu",
        "Zhiwei He",
        "Xiaofeng Wang",
        "Pengfei Liu",
        "Rui Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0e536b831cf1053d2c523d2e16656ad27c7369f2",
      "pdf_url": "",
      "publication_date": "2024-10-24",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "36d37aaf4bb16db3e5cc37c8b4ff55d4babb71ba",
      "title": "A realistic model extraction attack against graph neural networks",
      "abstract": null,
      "year": 2024,
      "venue": "Knowledge-Based Systems",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Hanjin Tong",
        "Wanlei Zhou"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/36d37aaf4bb16db3e5cc37c8b4ff55d4babb71ba",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9af4ee4d5255739283cdd728864168a7e6336393",
      "title": "WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection",
      "abstract": "Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the stealthiness of watermarks and has been empirically shown to be effective against CSE attack.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Anudeex Shetty",
        "Yue Teng",
        "Ke He",
        "Qiongkai Xu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/9af4ee4d5255739283cdd728864168a7e6336393",
      "pdf_url": "",
      "publication_date": "2024-03-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "bff503c92ff33bec9b21031249abdf33a6fe4d9a",
      "title": "Infinite Limits of Convolutional Neural Network for Urban Electromagnetic Field Exposure Reconstruction",
      "abstract": "Electromagnetic field exposure (EMF) has grown to be a critical concern as a consequence of the ongoing installation of fifth-generation cellular networks (5G). The lack of measurements makes it difficult to accurately assess the EMF in a specific urban area, as Spectrum cartography (SC) relies on a set of measurements recorded by spatially distributed sensors for the generation of exposure maps. However, when the spatial sampling rate is limited, significant estimation errors occur. To overcome this issue, the exposure map estimation is addressed as a missing data imputation task. We compute a convolutional neural tangent kernel (CNTK) for an infinitely wide convolutional neural network whose training dynamics can be completely described by a closed-form formula. This CNTK is employed to impute the target matrix and estimate EMF exposure from few sensors sparsely located in an urban environment. Experimental results show that the kernel, even when only sparse sensor data are available, can produce accurate estimates. It is a promising solution for exposure map reconstruction that does not require large training sets. The proposed method is compared with other deep learning approaches and Gaussian Process regression.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Mohammed Mallik",
        "B. Allaert",
        "Esteban Egea-L\u00f3pez",
        "D. Gaillot",
        "Joe Wiart",
        "Laurent Clavier"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/bff503c92ff33bec9b21031249abdf33a6fe4d9a",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10478001.pdf",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d9588dc5028b7e66b5fec940fd9e31a8e0a6070b",
      "title": "High-Frequency Matters: Attack and Defense for Image-Processing Model Watermarking",
      "abstract": "In recent years, there has been significant advancement in the field of model watermarking techniques. However, the protection of image-processing neural networks remains a challenge, with only a limited number of methods being developed. The objective of these techniques is to embed a watermark in the output images of the target generative network, so that the watermark signal can be detected in the output of a surrogate model obtained through model extraction attacks. This promising technique, however, has certain limits. Analysis of the frequency domain reveals that the watermark signal is mainly concealed in the high-frequency components of the output. Thus, we propose an overwriting attack that involves forging another watermark in the output of the generative network. The experimental results demonstrate the efficacy of this attack in sabotaging existing watermarking schemes for image-processing networks with an almost 100% success rate. To counter this attack, we propose an adversarial framework for the watermarking network. The framework incorporates a specially-designed adversarial training step, where the watermarking network is trained to defend against the overwriting network, thereby enhancing its robustness. Additionally, we observe an overfitting phenomenon in the existing watermarking method, which can render it ineffective. To address this issue, we modify the training process to eliminate the overfitting problem.",
      "year": 2024,
      "venue": "IEEE Transactions on Services Computing",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Chi Liu",
        "Shui Yu",
        "Wanlei Zhou"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/d9588dc5028b7e66b5fec940fd9e31a8e0a6070b",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "d3b74a3fdb14c606b9daaf61c5409f7d58c3b3aa",
      "title": "Design of Time-Delay Convolutional Neural Networks(TDCNN) Model for Feature Extraction for Side-Channel Attacks",
      "abstract": ": This work explores a novel method of SCA profiling to address compatibility problems and strengthen Deep Learning (DL) models. Convolutional Neural Networks are proposed in this research as a countermeasure to misalignment-focused countermeasures. \u201dTime-Delay Convolutional Neural Networks\u201d (TDCNN) is more accurate than \u201dConvolutional Neural Network,\u201d yet it\u2019s still acceptable. It\u2019s true that TDCNNs are neural networks based on convolution learned on single spatial information, just as side-channel tracings. However, given to recent surge in popularity of CNNs, particularly from the year 2012 when CNN framework (\u201dAlexNet\u201d) achieved Image Net Large Scale Visual Recognition Competition which is a notable image detection competition, a novel TDCNN has been termed out in DL literature. Currently, it needs to employ the characteristics related to CNN design, including declaring that one input feature equals 1 for instance, to establish a TDCNN in the most widely used DL libraries.",
      "year": 2024,
      "venue": "International Journal of Computing and Digital Systems",
      "authors": [
        "Amjed Abbas Ahmed",
        "Mohammad Kamrul Hasan",
        "Shahrul Azman Mohd Noah",
        "Azana Hafizah Aman"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/d3b74a3fdb14c606b9daaf61c5409f7d58c3b3aa",
      "pdf_url": "https://journal.uob.edu.bh:443/bitstream/123456789/5437/3/IJCDS160127_1570996366.pdf",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5746eb4593dd9dd160acb5b1afbe9128592e2924",
      "title": "Achieving synchronization and chimera state of modular neural networks by using dynamic learning to adjust electromagnetic induction",
      "abstract": null,
      "year": 2024,
      "venue": "Nonlinear dynamics",
      "authors": [
        "Weifang Huang",
        "Yong Wu",
        "Qianming Ding",
        "Ya Jia",
        "Ziying Fu",
        "Lijian Yang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/5746eb4593dd9dd160acb5b1afbe9128592e2924",
      "pdf_url": "",
      "publication_date": "2024-10-03",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3434f582cc9c2818785e2920241d43d932625539",
      "title": "ModelShield: Adaptive and Robust Watermark Against Model Extraction Attack",
      "abstract": "Large language models (LLMs) demonstrate general intelligence across a variety of machine learning tasks, thereby enhancing the commercial value of their intellectual property (IP). To protect this IP, model owners typically allow user access only in a black-box manner, however, adversaries can still utilize model extraction attacks to steal the model intelligence encoded in model generation. Watermarking technology offers a promising solution for defending against such attacks by embedding unique identifiers into the model-generated content. However, existing watermarking methods often compromise the quality of generated content due to heuristic alterations and lack robust mechanisms to counteract adversarial strategies, thus limiting their practicality in real-world scenarios. In this paper, we introduce an adaptive and robust watermarking method (named ModelShield) to protect the IP of LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs to autonomously insert watermarks into their generated content to avoid the degradation of model content. We also propose a robust watermark detection mechanism capable of effectively identifying watermark signals under the interference of varying adversarial strategies. Besides, ModelShield is a plug-and-play method that does not require additional model training, enhancing its applicability in LLM deployments. Extensive evaluations on two real-world datasets and three LLMs demonstrate that our method surpasses existing methods in terms of defense effectiveness and robustness while significantly reducing the degradation of watermarking on the model-generated content.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Kaiyi Pang",
        "Tao Qi",
        "Chuhan Wu",
        "Minhao Bai",
        "Minghu Jiang",
        "Yongfeng Huang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/3434f582cc9c2818785e2920241d43d932625539",
      "pdf_url": "",
      "publication_date": "2024-05-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "cefefe1b62baa5ac5067f31e32ee6f5a004b4222",
      "title": "Quantitative analysis of the electromagnetic hybrid nanofluid flow within the gap of two tubes using deep learning neural networks",
      "abstract": "Purpose(1) A mathematical model for the Hybrid nanofluids flow is used as carriers for delivering drugs. (2) The flow conditions are controlled to enable drug-loaded nanofluids to flow through the smaller gap between the two tubes. (3) Hybrid nanofluids (HNFs) made from silver (Ag) and titanium dioxide (TiO2) nanoparticles are analyzed for applications of drug delivery. (Ag) and (TiO2) (NPs) are suitable candidates for cancer treatment due to their excellent biocompatibility, high photoactivity, and low toxicity. (4) The new strategy of artificial neural networks (ANN) is used which is machine-based and more prominent in validation, and comparison with other techniques.Design/methodology/approachThe two Tubes are settled in such a manner that the gap between them is uniform. The Control Volume Finite Element Method; Rk-4 and Artificial Neural Network (ANN).Findings(1) From the obtained results it is observed that the dispersion and distribution of drug-loaded nanoparticles within the body will be improved by the convective motion caused by hybrid nanofluids. The effectiveness and uniformity of drug delivery to target tissues or organs is improved based on the uniform flow and uniform gap. (2) The targeting efficiency of nanofluids is further improved with the addition of the magnetic field. (3) The size of the cylinders, and flow rate, are considered uniform to optimize the drug delivery.Research limitations/implications(1)The flow phenomena is considered laminar, one can use the same idea through a turbulent flow case. (2) The gap is considered uniform and will be interesting if someone extends the idea as non-uniform.Practical implications(1) To deliver drugs to the targeted area, a suitable mathematical model is required. (2) The analysis of hybrid nanofluids (HNFs) derived from silver (Ag) and titanium dioxide (TiO2) nanoparticles is conducted for the purpose of drug delivery. The biocompatibility, high photoactivity, and low toxicity of (Ag) and (TiO2) (NPs) make them ideal candidates for cancer treatment. (3) Machine-based artificial neural networks (ANN) have a new strategy that is more prominent in validation compared to other techniques.Social implicationsThe drug delivery model is a useful strategy for new researchers. (1) They can extend this idea using a non-uniform gap. (2) The flow is considered uniform, the new researchers can extend the idea using a turbulent case. (3) Other hybrid nanofluids flow, in the same model for other industrial usages are possible.Originality/valueAll the obtained results are new. The experimental thermophysical results are used from the existing literature and references are provided.",
      "year": 2024,
      "venue": "Multidiscipline Modeling in Materials and Structures",
      "authors": [
        "Majid Amin",
        "Fuad A. Awwad",
        "Emad A. A. Ismail",
        "Muhammad Ishaq",
        "T. Gul",
        "T. Khan"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/cefefe1b62baa5ac5067f31e32ee6f5a004b4222",
      "pdf_url": "",
      "publication_date": "2024-06-05",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "93dc2c281fbf4d1f8886a83f6793864528fd48f8",
      "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
      "abstract": "While Deep Neural Networks (DNNs) have demonstrated remarkable performance in tasks related to perception and control, there are still several unresolved concerns regarding the privacy of their training data, particularly in the context of vulnerability to Membership Inference Attacks (MIAs). In this paper, we explore a connection between the susceptibility to membership inference attacks and the vulnerability to distillation-based functionality stealing attacks. In particular, we propose {GLiRA}, a distillation-guided approach to membership inference attack on the black-box neural network. We observe that the knowledge distillation significantly improves the efficiency of likelihood ratio of membership inference attack, especially in the black-box setting, i.e., when the architecture of the target model is unknown to the attacker. We evaluate the proposed method across multiple image classification datasets and models and demonstrate that likelihood ratio attacks when guided by the knowledge distillation, outperform the current state-of-the-art membership inference attacks in the black-box setting.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Andrey V. Galichin",
        "Mikhail Aleksandrovich Pautov",
        "Alexey Zhavoronkin",
        "Oleg Y. Rogov",
        "Ivan V. Oseledets"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/93dc2c281fbf4d1f8886a83f6793864528fd48f8",
      "pdf_url": "",
      "publication_date": "2024-05-13",
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3e0bc402f7c0a34964223e87646fa3da0b6d947a",
      "title": "Probabilistically Robust Watermarking of Neural Networks",
      "abstract": "As deep learning (DL) models are widely and effectively used in Machine Learning as a Service (MLaaS) platforms, there is a rapidly growing interest in DL watermarking techniques that can be used to confirm the ownership of a particular model. Unfortunately, these methods usually produce watermarks susceptible to model stealing attacks. In our research, we introduce a novel trigger set-based watermarking approach that demonstrates resilience against functionality stealing attacks, particularly those involving extraction and distillation. Our approach does not require additional model training and can be applied to any model architecture. The key idea of our method is to compute the trigger set, which is transferable between the source model and the set of proxy models with a high probability. In our experimental study, we show that if the probability of the set being transferable is reasonably high, it can be effectively used for ownership verification of the stolen model. We evaluate our method on multiple benchmarks and show that our approach outperforms current state-of-the-art watermarking techniques in all considered experimental setups.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Mikhail Aleksandrovich Pautov",
        "Nikita Bogdanov",
        "Stanislav Pyatkin",
        "Oleg Y. Rogov",
        "Ivan V. Oseledets"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/3e0bc402f7c0a34964223e87646fa3da0b6d947a",
      "pdf_url": "https://arxiv.org/pdf/2401.08261",
      "publication_date": "2024-01-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "functionality stealing"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "9caf69e7bab1932d0b77dd20dc8f47e1b340135a",
      "title": "Defense against Model Extraction Attack by Bayesian Active Watermarking",
      "abstract": null,
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhenyi Wang",
        "Yihan Wu",
        "Heng Huang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9caf69e7bab1932d0b77dd20dc8f47e1b340135a",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "16b5db7894dde1b49960f03e68b280ac895d86fc",
      "title": "Securing Data From Side-Channel Attacks: A Graph Neural Network-Based Approach for Smartphone-Based Side Channel Attack Detection",
      "abstract": "The widespread use of smartphones has brought convenience and connectivity to the fingertips of the masses. As a result, this has paved the way for potential security vulnerabilities concerning sensitive data, particularly by exploiting side-channel attacks. When typing on a smartphone\u2019s keyboard, its vibrations can be misused to discern the entered characters, thus facilitating side-channel attacks. These smartphone hardware sensors can capture such information while users input sensitive data like personal details, names, email addresses, age, bank details and passwords. This study presents a novel Graph Neural Network (GNN) approach to predict side-channel attacks on smartphone keyboards; different GNN architectures were used, including GNN, DeepGraphNet, Gradient Boosting (GB)+DeepGraphNet, Extreme Gradient Boosting (XGB)+DeepGraphNet and K-Nearest Neighbor (KNN)+DeepGraphNet. The proposed approach detects the side channel attack using vibrations produced while typing on the smartphone soft keyboard. The data was collected from three smartphone sensors, an accelerometer, gyroscope, and magnetometer, and evaluated this data using common evaluation measures such as accuracy, precision, recall, F1-score, ROC curves, confusion matrix and accuracy and loss curves. This study demonstrated that GNN architectures can effectively capture complex relationships in data, making them well-suited for analyzing patterns in smartphone sensor data. Likewise, this research aims to fill a crucial gap by enhancing data privacy in the information entered through smartphone keyboards, shielding it from side-channel attacks by providing an accuracy of 98.26%. Subsequently, the primary objective of this study is to assess the effectiveness of GNN architectures in this precise context. Similarly, the GNN model exhibits compelling performance, achieving accuracy, precision, recall, and f1 score metrics that showcase the model\u2019s effectiveness, with the highest values of 0.98, 0.98, 0.98, and 0.98, respectively. Significantly, the metrics mentioned in the study outperform those documented in the previous literature. Overall, the study contributes to the detection of side-channel smartphone attacks, which advances secure data practices.INDEX TERMS Graph neural networks (GNN), keystroke inference, motion sensors, machine learning, smartphone security, side-channel attacks.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Sidra Abbas",
        "Stephn Ojo",
        "Imen Bouazzi",
        "Gabriel Avelino Sampedro",
        "Abdullah Al Hejaili",
        "Ahmad S. Almadhor",
        "Rastislav Kulh\u00e1nek"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/16b5db7894dde1b49960f03e68b280ac895d86fc",
      "pdf_url": "https://doi.org/10.1109/access.2024.3465662",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "40d2bc169204b49a9cfa659b5398791ac5860caf",
      "title": "Unveiling the Secrets without Data: Can Graph Neural Networks Be Exploited through Data-Free Model Extraction Attacks?",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yu-Lin Zhuang",
        "Chuan Shi",
        "Mengmei Zhang",
        "Jinghui Chen",
        "Lingjuan Lyu",
        "Pan Zhou"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/40d2bc169204b49a9cfa659b5398791ac5860caf",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1ed4e363c0d1caa38139b11bfbdf57ff7c3305b4",
      "title": "DeepCache: Revisiting Cache Side-Channel Attacks in Deep Neural Networks Executables",
      "abstract": "Deep neural networks (DNN) are increasingly deployed in heterogeneous hardware, including high-performance devices like GPUs and low-power devices like mobile/IoT CPUs, FPGAs, and accelerators. In order to unlock the full performance potential of various hardware, deep learning (DL) compilers automatically optimize DNN inference computations and compile DNN models into DNN executables for efficient computations across hardware backends. As valuable intellectual properties, DNN architectures are one primary attack target. Since previous works already demonstrate the abuse of cache side channels to steal DNN architectures from DL frameworks (e.g., PyTorch and TensorFlow), we first study using those known side-channel attacks against DNN executables. We find that attacking DNN executables presents unique challenges, and existing works can hardly apply. Particularly, DNN executables exhibit a standalone paradigm that largely reduces cache side channel attack surfaces. Meanwhile, cache side channels capture only limited behaviors of the whole DNN execution while facing daunting technical challenges (e.g., noise and low time resolution). However, we unveil a unique attack vector in DNN executables, such that the cache-aware optimizations, which are extensively employed by contemporary DL compilers to harvest the full potentials of hardware, would result in distinguishable DNN operator cache access patterns, making model architecture recovery possible. We propose DeepCache, an end-to-end side channel attack framework, to infer DNN model architectures from DNN executables. DeepCache \\ leverages cache side channels as the attacking primitives and combines contrastive learning and anomaly detection to enable precise inference. Our evaluation using the standard Prime+Probe shows that DeepCache \\ yields a high accuracy in exploiting complex DNN executables under both the basic L1 cache attack and the more practical but challenging last level cache (LLC) attack settings.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zhibo Liu",
        "Yuanyuan Yuan",
        "Yanzuo Chen",
        "Sihang Hu",
        "Tianxiang Li",
        "Shuai Wang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/1ed4e363c0d1caa38139b11bfbdf57ff7c3305b4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690241",
      "publication_date": "2024-12-02",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7aed21ac2ca321b4bcaef9a32b2e5886425599e7",
      "title": "WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks",
      "abstract": "Embeddings-as-a-Service (EaaS) is a service offered by large language model (LLM) developers to supply embeddings generated by LLMs. Previous research suggests that EaaS is prone to imitation attacks -- attacks that clone the underlying EaaS model by training another model on the queried embeddings. As a result, EaaS watermarks are introduced to protect the intellectual property of EaaS providers. In this paper, we first show that existing EaaS watermarks can be removed by paraphrasing when attackers clone the model. Subsequently, we propose a novel watermarking technique that involves linearly transforming the embeddings, and show that it is empirically and theoretically robust against paraphrasing.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Anudeex Shetty",
        "Qiongkai Xu",
        "Jey Han Lau"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/7aed21ac2ca321b4bcaef9a32b2e5886425599e7",
      "pdf_url": "",
      "publication_date": "2024-08-29",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "c9a4bce17712ecc82c7a9dcda85dfbaa1a7c2391",
      "title": "A Comparative Study of Large Language Models for Goal Model Extraction",
      "abstract": "User stories, expressed in snippets of natural language text, are commonly used to elicit stakeholder's needs in agile software development. Requirement engineers model user stories to interpret the relations among goals and requirements. Manual transformation of goal models has challenges such as, difficulty of converting lower-abstraction user stories into higher-level goals, and extraction of goals embedded in user stories depends on the skill of requirements engineers. In this paper we introduce a technique that leverages Large Language Models (LLMs) to automatically generate goal models from user stories. The approach uses Iterative Prompt Engineering that guides LLM to extract intentional elements and generate its XML-compatible representation in Goal-oriented Requirements Language (GRL). The generated models can be visualized using jUCMNav tool. We evaluated our approach using three LLMs: GPT-4, Llama and Cohere. Our qualitative evaluation indicates that GPT-4 or Llama can be used to assist requirements engineers in modeling as they can produce GRL goal models that are understandable. Additionally, these LLMs are capable of exposing soft goals that are not apparent to stakeholders who are new to the domain.",
      "year": 2024,
      "venue": "ACM/IEEE International Conference on Model Driven Engineering Languages and Systems",
      "authors": [
        "Vaishali Siddeshwar",
        "Sanaa A. Alwidian",
        "Masoud Makrehchi"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/c9a4bce17712ecc82c7a9dcda85dfbaa1a7c2391",
      "pdf_url": "",
      "publication_date": "2024-09-22",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0207b4706683924d5b0e1399bd08b49fd19c394f",
      "title": "Stealing Training Graphs from Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have shown promising results in modeling graphs in various tasks. The training of GNNs, especially on specialized tasks such as bioinformatics, demands extensive expert annotations, which are expensive and usually contain sensitive information of data providers. The trained GNN models are often shared for deployment in the real world. As neural networks can memorize the training samples, the model parameters of GNNs have a high risk of leaking private training data. Our theoretical analysis shows the strong connections between trained GNN parameters and the training graphs used, confirming the training graph leakage issue. However, explorations into training data leakage from trained GNNs are rather limited. Therefore, we investigate a novel problem of stealing graphs from trained GNNs. To obtain high-quality graphs that resemble the target training set, a graph diffusion model with diffusion noise optimization is deployed as a graph generator. Furthermore, we propose a selection method that effectively leverages GNN model parameters to identify training graphs from samples generated by the graph diffusion model. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed framework in stealing training graphs from the trained GNN.",
      "year": 2024,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Min Lin",
        "Enyan Dai",
        "Junjie Xu",
        "Jinyuan Jia",
        "Xiang Zhang",
        "Suhang Wang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0207b4706683924d5b0e1399bd08b49fd19c394f",
      "pdf_url": "",
      "publication_date": "2024-11-17",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0219db84513f4c2604ed15fce52c8f810b463ba5",
      "title": "Can't Hide Behind the API: Stealing Black-Box Commercial Embedding Models",
      "abstract": "Embedding models that generate dense vector representations of text are widely used and hold significant commercial value. Companies such as OpenAI and Cohere offer proprietary embedding models via paid APIs, but despite being\"hidden\"behind APIs, these models are not protected from theft. We present, to our knowledge, the first effort to\"steal\"these models for retrieval by training thief models on text-embedding pairs obtained from the APIs. Our experiments demonstrate that it is possible to replicate the retrieval effectiveness of commercial embedding models with a cost of under $300. Notably, our methods allow for distilling from multiple teachers into a single robust student model, and for distilling into presumably smaller models with fewer dimension vectors, yet competitive retrieval effectiveness. Our findings raise important considerations for deploying commercial embedding models and suggest measures to mitigate the risk of model theft.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "M. Tamber",
        "Jasper Xian",
        "Jimmy Lin"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0219db84513f4c2604ed15fce52c8f810b463ba5",
      "pdf_url": "",
      "publication_date": "2024-06-13",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "389f1fad962f58799327a0112526452d2da5157d",
      "title": "TEXTKNOCKOFF: KNOCKOFF NETS FOR STEALING FUNCTIONALITY OF TEXT SENTIMENT MODELS",
      "abstract": "Most commercial machine learning models today are designed to require significant amounts of time, money, and human effort. Therefore, intrinsic information about the model (such as architecture, hyperparameters, and training data) needs to be kept confidential. These models are referred to as black boxes, and there is an increasing amount of research focused on both attacking and protecting them. Recent publications have often concentrated on the field of computer vision; in contrast, there is still relatively little research on methods for attacking black box models with textual data. This article introduces a research method for extracting the functionality of a black box model in the task of text sentiment analysis. The method has been effectively tested based on random sampling techniques to reconstruct a new model with equivalent functionality to the original model, achieving high accuracy (94.46% compared to 94.92%) and high similarity (96.82%).",
      "year": 2024,
      "venue": "Journal of Science and Technique",
      "authors": [
        "X. Pham"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/389f1fad962f58799327a0112526452d2da5157d",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "knockoff nets",
        "knockoff net",
        "stealing functionality"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "a55cc1ebaab3de336386f292cc4028f6e5586ac0",
      "title": "Trained to Leak: Hiding Trojan Side-Channels in Neural Network Weights",
      "abstract": "Applications driven by neural networks (NNs) have been advancing various work flows in industries and everyday life. FPGA accelerators are a popular low latency solution for NN inference in the cloud, edge devices and critical systems, offering efficiency and availability. Additionally, cloud FPGAs enable maximizing resource utilization by sharing one device with multiple users in a multi-tenant scenario. However, due to the high energy costs, hardware requirements and time consumption for training an NN, using machine learning services or acquiring pre-trained models has become increasingly popular. This creates a trust issue that potentially puts the privacy of the user at risk. Specifically, malicious mechanisms may be hidden in the weights of the NN. We show that by manipulating the training process of an NN, the power consumption and resulting leakage can be manipulated to correlate strongly with the networks output, allowing the reliable recovery of the classification results through remote power side-channel analysis. In comparison to power traces from a benign model, which leak less information, our trained-in Trojan Side-Channel enhances the credibility and reliability of the stolen outputs, making them more usable and valuable for malicious intent.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Vincent Meyers",
        "Michael Hefenbrock",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a55cc1ebaab3de336386f292cc4028f6e5586ac0",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a5b7633d836b0430225774cdcf63e6a18385c251",
      "title": "PRSA: Prompt Reverse Stealing Attacks against Large Language Models",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yong Yang",
        "Xuhong Zhang",
        "Yi Jiang",
        "Xi Chen",
        "Haoyu Wang",
        "Shouling Ji",
        "Zonghui Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a5b7633d836b0430225774cdcf63e6a18385c251",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f043ad18b10677d02c4b8c94dd69b4dca97b4fc6",
      "title": "<inline-formula><tex-math notation=\"LaTeX\">$\\gamma$</tex-math><alternatives><mml:math><mml:mi>\u03b3</mml:mi></mml:math><inline-graphic xlink:href=\"dohyun-ieq1-3314710.gif\"/></alternatives></inline-formula>-Knife: Extracting Neural Network Architecture Through Software-Based Power Side-Channel",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Dohyun Ryu",
        "Yerim Kim",
        "Junbeom Hur"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/f043ad18b10677d02c4b8c94dd69b4dca97b4fc6",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0a54d21c8cb4bde33a93fd8a835bb929fa06bb8e",
      "title": "Side-channel attacks based on attention mechanism and multi-scale convolutional neural network",
      "abstract": null,
      "year": 2024,
      "venue": "Computers & electrical engineering",
      "authors": [
        "Pengfei He",
        "Ying Zhang",
        "Han Gan",
        "Jianfei Ma",
        "Hongxin Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/0a54d21c8cb4bde33a93fd8a835bb929fa06bb8e",
      "pdf_url": "",
      "publication_date": "2024-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c67af67a82170c03e7d39f2f143ef74e8367d0a4",
      "title": "Defending against model extraction attacks with OOD feature learning and decision boundary confusion",
      "abstract": null,
      "year": 2024,
      "venue": "Computers & security",
      "authors": [
        "Chuang Liang",
        "Jie Huang",
        "Zeping Zhang",
        "Shuaishuai Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/c67af67a82170c03e7d39f2f143ef74e8367d0a4",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a916a716b23490f87844b3ff44cb2d18284b2ac6",
      "title": "Efficient Model Stealing Defense with Noise Transition Matrix",
      "abstract": null,
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Dong-Dong Wu",
        "Chilin Fu",
        "Weichang Wu",
        "Wenwen Xia",
        "Xiaolu Zhang",
        "Jun Zhou",
        "Min-Ling Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a916a716b23490f87844b3ff44cb2d18284b2ac6",
      "pdf_url": "",
      "publication_date": "2024-06-16",
      "keywords_matched": [
        "model stealing",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a6854f91dbad0b39a0289872ac838545a9c18225",
      "title": "Stealing Watermarks of Large Language Models via Mixed Integer Programming",
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM\u2019s parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
      "year": 2024,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a6854f91dbad0b39a0289872ac838545a9c18225",
      "pdf_url": "",
      "publication_date": "2024-12-09",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "f2109f7f2412308738e03a60f79946cb1ad2aa7a",
      "title": "Alignment-Aware Model Extraction Attacks on Large Language Models",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zi Liang",
        "Qingqing Ye",
        "Yanyun Wang",
        "Sen Zhang",
        "Yaxin Xiao",
        "Ronghua Li",
        "Jianliang Xu",
        "Haibo Hu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f2109f7f2412308738e03a60f79946cb1ad2aa7a",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "94dd5cd82b6a082dff3dd697d63191325cf6830b",
      "title": "Knowledge Distillation-Based Model Extraction Attack using Private Counterfactual Explanations",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Fatima Ezzeddine",
        "Omran Ayoub",
        "Silvia Giordano"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/94dd5cd82b6a082dff3dd697d63191325cf6830b",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c0aedde76f74f8a7db4568d688b1baf921499acf",
      "title": "Prediction of Electromagnetic Field Exposure at 20\u2013100 GHz for Clothed Human Body Using an Adaptively Reconfigurable Architecture Neural Network With Weight Analysis (RAWA-NN) Framework",
      "abstract": "In the context of forthcoming sixth-generation (6G) wireless communication, the sub-terahertz and terahertz frequency spectrum are anticipated. At such high frequencies, electromagnetic field (EMF) exposure assessment becomes significantly challenging, requiring substantial computational resources. This article is the first to utilize machine learning (ML) to predict EMF exposure levels for the clothed human body at 20\u2013100 GHz, including temperature rises and absorbed power density (APD) at the exposed skin surface. To predict the EMF exposure, a reconfigurable architecture neural network with weight analysis (RAWA-NN) framework is proposed. This framework is based on the deep neural network (DNN) integrating the proposed weights-analyzer module and optimization module. The proposed novel framework streamlines the training process and reduces training time, while simultaneously adaptively optimizes the hyperparameters (hidden layers and hidden sizes) without the necessity for manual intervention during training and optimization. The model was trained using 70% of forearm data, with the remaining data for testing. Data from other body parts, such as the abdomen and quadriceps, was used to validate the model generalization. Compared to conventional dosimetry analysis, relative difference (RD) across various parameters remains below 2.6% across various parameters, for the same body part of the forearm, and below 9.5% for other body parts. There is an approximate four orders of magnitude improvement in assessment speed.",
      "year": 2024,
      "venue": "IEEE Transactions on Antennas and Propagation",
      "authors": [
        "Ming Yao",
        "Zhaohui Wei",
        "Kun Li",
        "Gert Fr\u00f8lund Pedersen",
        "Shuai Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c0aedde76f74f8a7db4568d688b1baf921499acf",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a54e2156b2f83a7c5c70c524862ce35e0a696d1f",
      "title": "Dynamic behaviors analysis of fraction-order neural network under memristive electromagnetic induction",
      "abstract": "The dynamic behaviors of coupled neurons with different mathematical representations have received more and more attention in recent years. The coupling between heterogeneous neurons can show richer dynamic phenomena, which is of great significance in understanding the function of the human brain. In this paper, we present a fraction-order heterogeneous network with three neurons that is built by coupling a FN neuron with two HR neurons. Complex electromagnetic surroundings have meaningful physical impacts on the electrical activities of neurons. To imitate the effects of electromagnetic induction on the three-neuron heterogeneous network, we introduce a fraction-order locally active memristor in the neural network. The characteristics of this memristor are been carefully analyzed by pinched hysteresis loops and its locally active characteristic is been proved by the power-off plot and the DC v-i plot. Then, the parameter-dependent dynamic activities are investigated numerically using several dynamical analysis methods, such as the phase diagrams, bifurcation diagrams, Lyapunov exponent spectrums, and attraction basins. Furthermore, abundant dynamic behaviors, including coexisting activities, anti-monotonicity phenomena, transient chaos and firing patterns are revealed in this network, which support further investigation of the firing patterns of the human brain. In particular, complex dynamics, including coexisting attractors, anti-monotonicity, and firing patterns, can be influenced by the order and strength of electrical synaptic coupling and electromagnetic induction. The control of the bistable state can be realized through the time feedback control method, so that the bistable state can be transformed into an ideal monostable state. The study of the fraction-order memristive neural network may expand the field of view for understanding the collective behaviors of neurons. Finally, based on the ARM platform, we give a digital implementation of the fraction-order memristive neural network, which can verify the consistency with the numerical simulation results. In the future, we will explore more interesting memristive neural networks and research different types of methods to control the firing behaviors of the networks.",
      "year": 2024,
      "venue": "Acta Physica Sinica",
      "authors": [
        "Dawei Ding",
        "Mouyuan Wang",
        "Jin Wang",
        "Zongli Yang",
        "Yan Niu",
        "Wei Wang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a54e2156b2f83a7c5c70c524862ce35e0a696d1f",
      "pdf_url": "https://wulixb.iphy.ac.cn/pdf-content/10.7498/aps.73.20231792.pdf",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f49f2057b73ae44fc44a116962435f8e18fcd5ba",
      "title": "Power Side-Channel Analysis and Mitigation for Neural Network Accelerators based on Memristive Crossbars",
      "abstract": "The modern trend of exploring Artificial Intelligence (AI) in various industries, such as big data, edge computing, automobile, and medical applications, has increased tremendously. As functionalities grow, energy-efficient hardware for AI devices becomes crucial. To address that, Computation-in-Memory (CiM) using Non-Volatile Memories (NVMs) offers a promising solution. However, security is also an important concern in this computation paradigm. In this work, we analyze the vulnerability for power side-channel attacks on Multiply-Accumulate (MAC) operations implemented in CiM architecture based on emerging NVMs. Our results show that peripheral devices such as Analog-to-Digital Converters (ADCs) leak much more sensitive information than the crossbar itself because of its significant power consumption. Therefore, we propose a circuit-level countermeasure based on hiding for the ADCs of memristive CiM architecture to mitigate the power attacks. The efficiency of our proposed countermeasure is shown by both attacks and leakage assessment methodologies using a maximum of one million measurement traces.",
      "year": 2024,
      "venue": "Asia and South Pacific Design Automation Conference",
      "authors": [
        "Brojogopal Sapui",
        "M. Tahoori"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f49f2057b73ae44fc44a116962435f8e18fcd5ba",
      "pdf_url": "",
      "publication_date": "2024-01-22",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "befa5df856721b10c50d034594fbc7194d96e386",
      "title": "A Middle Path for On-Premises LLM Deployment: Preserving Privacy Without Sacrificing Model Confidentiality",
      "abstract": "Privacy-sensitive users require deploying large language models (LLMs) within their own infrastructure (on-premises) to safeguard private data and enable customization. However, vulnerabilities in local environments can lead to unauthorized access and potential model theft. To address this, prior research on small models has explored securing only the output layer within hardware-secured devices to balance model confidentiality and customization. Yet this approach fails to protect LLMs effectively. In this paper, we discover that (1) query-based distillation attacks targeting the secured top layer can produce a functionally equivalent replica of the victim model; (2) securing the same number of layers, bottom layers before a transition layer provide stronger protection against distillation attacks than top layers, with comparable effects on customization performance; and (3) the number of secured layers creates a trade-off between protection and customization flexibility. Based on these insights, we propose SOLID, a novel deployment framework that secures a few bottom layers in a secure environment and introduces an efficient metric to optimize the trade-off by determining the ideal number of hidden layers. Extensive experiments on five models (1.3B to 70B parameters) demonstrate that SOLID outperforms baselines, achieving a better balance between protection and downstream customization.",
      "year": 2024,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Hanbo Huang",
        "Yihan Li",
        "Bowen Jiang",
        "Bowen Jiang",
        "Lin Liu",
        "Ruoyu Sun",
        "Zhuotao Liu",
        "Shiyu Liang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/befa5df856721b10c50d034594fbc7194d96e386",
      "pdf_url": "",
      "publication_date": "2024-10-15",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "20993724635d425f696e1c7e38c9e8da025ff757",
      "title": "FedMCT: A Federated Framework for Intellectual Property Protection and Malicious Client Tracking",
      "abstract": "In the era of big data, federated learning (FL) emerges as a solution to train models collectively without exposing individual data, maintaining similar accuracy to models trained on shared datasets. However, challenges arise with the advent of privacy inference attacks and model theft, posing significant threats to the privacy of FL models, especially regarding intellectual property (IP) protection. This paper introduces FedMCT (Federated Malicious Client Tracking), a novel framework addressing these challenges in the FL context. The FedMCT framework is a new approach to protect IP rights of FL clients and track cheaters, which can improve efficiency in resource-heterogeneous environments. By embedding unique watermarks or fingerprints in Deep Neural Network (DNN) models, we can protect model IP. We employ a configuration round before watermark embedding, segmenting clients based on performance for tiered model watermarking. We also propose a tiered watermarking and traitor tracking mechanism, which reduces the tracking time and ensures high traitor tracking efficiency. Extensive experiments validate our solution\u2019s efficacy in maintaining original model performance, watermark privacy, and detectability, robust against various attacks, demonstrating superior traitor tracing efficiency compared to existing frameworks.",
      "year": 2024,
      "venue": "International Conference on Machine Learning and Computing",
      "authors": [
        "Qianyi Chen",
        "Peijia Zheng",
        "Yusong Du",
        "Weiqi Luo",
        "Hongmei Liu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/20993724635d425f696e1c7e38c9e8da025ff757",
      "pdf_url": "",
      "publication_date": "2024-02-02",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c26c3f657af13747b9160aaa3886ef9ecc27a028",
      "title": "MarginFinger: Controlling Generated Fingerprint Distance to Classification boundary Using Conditional GANs",
      "abstract": "Deep neural networks (DNNs) are widely employed across various domains, with their training costs making them crucial assets for model owners. However, the rise of Machine Learning as a Service has made models more accessible, but also increases the risk of leakage. Attackers can successfully steal models through internal leaks or API access, emphasizing the critical importance of protecting intellectual property. Several watermarking methods have been proposed, embedding secret watermarks of model owners into models. However, watermarking requires tampering with the model's training process to embed the watermark, which may lead to a decrease in utility. Recently, some fingerprinting techniques have emerged to generate fingerprint samples near the classification boundary to detect pirated models. Nevertheless, these methods lack distance constraints and suffer from high training costs. To address these issues, we propose to utilize conditional generative network to generate fingerprint data points, enabling a better exploration of the model's decision boundary. By incorporating margin loss during GAN training, we can control the distance between generated data points and classification boundary to ensure the robustness and uniqueness of our method. Moreover, our method does not require additional training of proxy models, enhancing the efficiency of fingerprint acquisition. To validate the effectiveness of our approach, we evaluate it on CIFAR-10 and Tiny-ImageNet, considering three types of model extraction attacks, fine-tuning, pruning, and transfer learning attacks. The results demonstrate that our method achieves ARUC values of 0.186 and 0.153 on CIFAR-10 and Tiny-ImageNet datasets, respectively, representing a remarkable improvement of 400% and 380% compared to the current leading baseline. The source code is available at https://github.com/wason981/MarginFinger.",
      "year": 2024,
      "venue": "International Conference on Multimedia Retrieval",
      "authors": [
        "Weixing Liu",
        "Sheng-hua Zhong"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c26c3f657af13747b9160aaa3886ef9ecc27a028",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3652583.3658058",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "model extraction",
        "steal model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "39380b900ca2f7692efd229c3b4f72f5fff6f2dc",
      "title": "Dual-Rail Precharge Logic-Based Side-Channel Countermeasure for DNN Systolic Array",
      "abstract": "Deep neural network (DNN) accelerators are widely used in cloud-edge-end and other application scenarios. Researchers recently focused on extracting secret information from DNN through side-channel attacks (SCAs), which substantially threaten AI security. In this brief, we propose a high-security, high-performance side-channel countermeasure using dual-rail precharge logic (DPL) for the DNN systolic array. By collecting and analyzing 5000 power traces, our proposed DPL-based systolic array provides a significantly lower correlation coefficient of 0.045. Through system-level side-channel security evaluation on field-programmable gate arrays (FPGAs), the DPL-based systolic array can effectively defend against weight extraction under power SCAs.",
      "year": 2024,
      "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems",
      "authors": [
        "Le Wu",
        "Liji Wu",
        "Xiangmin Zhang",
        "Munkhbaatar Chinbat"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/39380b900ca2f7692efd229c3b4f72f5fff6f2dc",
      "pdf_url": "",
      "publication_date": "2024-09-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "title": "Large Language Models for Link Stealing Attacks Against Graph Neural Networks",
      "abstract": "Graph data contains rich node features and unique edge information, which have been applied across various domains, such as citation networks or recommendation systems. Graph Neural Networks (GNNs) are specialized for handling such data and have shown impressive performance in many applications. However, GNNs may contain of sensitive information and susceptible to privacy attacks. For example, link stealing is a type of attack in which attackers infer whether two nodes are linked or not. Previous link stealing attacks primarily relied on posterior probabilities from the target GNN model, neglecting the significance of node features. Additionally, variations in node classes across different datasets lead to different dimensions of posterior probabilities. The handling of these varying data dimensions posed a challenge in using a single model to effectively conduct link stealing attacks on different datasets. To address these challenges, we introduce Large Language Models (LLMs) to perform link stealing attacks on GNNs. LLMs can effectively integrate textual features and exhibit strong generalizability, enabling attacks to handle diverse data dimensions across various datasets. We design two distinct LLM prompts to effectively combine textual features and posterior probabilities of graph nodes. Through these designed prompts, we fine-tune the LLM to adapt to the link stealing attack task. Furthermore, we fine-tune the LLM using multiple datasets and enable the LLM to learn features from different datasets simultaneously. Experimental results show that our approach significantly enhances the performance of existing link stealing attack tasks in both white-box and black-box scenarios. Our method can execute link stealing attacks across different datasets using only a single model, making link stealing attacks more applicable to real-world scenarios.",
      "year": 2024,
      "venue": "IEEE Transactions on Big Data",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Hui Sun",
        "Wanlei Zhou",
        "Philip S. Yu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "pdf_url": "http://arxiv.org/pdf/2406.16963",
      "publication_date": "2024-06-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a968c61fc5a14ce01db2a67b6ab87e30248a0a19",
      "title": "Stealing the Invisible: Unveiling Pre-Trained CNN Models Through Adversarial Examples and Timing Side-Channels",
      "abstract": "Machine learning, with its myriad applications, has become an integral component of numerous AI systems. A common practice in this domain is the use of transfer learning, where a pre-trained model\u2019s architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it is crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present ArchWhisperer, a model fingerprinting attack approach based on the novel observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with model inference times is used to further enhance our attack in terms of attack effectiveness as well as query budget. ArchWhisperer is designed for typical user-level access in remote MLaaS environments and it exploits varying misclassifications of adversarial images across different models to fingerprint several renowned Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures. We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required. We have presented our results over 27 pre-trained models of different CNN and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under 20. This is a marked improvement compared to state-of-the-art works.",
      "year": 2024,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Shubhi Shukla",
        "Manaar Alam",
        "Pabitra Mitra",
        "Debdeep Mukhopadhyay"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/a968c61fc5a14ce01db2a67b6ab87e30248a0a19",
      "pdf_url": "https://arxiv.org/pdf/2402.11953",
      "publication_date": "2024-02-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "15a81a4579949690c706f1bd9bbdda93681c35c3",
      "title": "Poisoning-Free Defense Against Black-Box Model Extraction",
      "abstract": "Recent research has shown that an adversary can use a surrogate model to steal the functionality of a target deep learning model even under the black-box condition and without data curation, while the existing defense mainly relies on API poisoning to disturb the surrogate training. Unfortunately, due to poisoning, the defense is achieved at the price of fidelity loss, sacrificing the interests of honest users. To solve this problem, we propose an Adversarial Fine-Tuning (AdvFT) framework, incorporating the generative adversarial network (GAN) structure that disturbs the feature representations of out-of-distribution (OOD) queries while preserving those of in-distribution (ID) ones, circumventing the need for OOD sample collection and API poisoning. Extensive experiments verify the effectiveness of the proposed framework. Code is available at github.com/Hatins/AdvFT.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Haitian Zhang",
        "Guang Hua",
        "Wen Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/15a81a4579949690c706f1bd9bbdda93681c35c3",
      "pdf_url": "",
      "publication_date": "2024-04-14",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "4fa2c8c744b37ba9b227cb3de8cdec448c09c1d7",
      "title": "Side-Channel Analysis of OpenVINO-based Neural Network Models",
      "abstract": "Embedded devices with neural network accelerators offer great versatility for their users, reducing the need to use cloud-based services. At the same time, they introduce new security challenges in the area of hardware attacks, the most prominent being side-channel analysis (SCA). It was shown that SCA can recover model parameters with a high accuracy, posing a threat to entities that wish to keep their models confidential. In this paper, we explore the susceptibility of quantized models implemented in OpenVINO, an embedded framework for deploying neural networks on embedded and Edge devices. We show that it is possible to recover model parameters with high precision, allowing the recovered model to perform very close to the original one. Our experiments on GoogleNet v1 show only a 1% difference in the Top 1 and a 0.64% difference in the Top 5 accuracies.",
      "year": 2024,
      "venue": "ARES",
      "authors": [
        "Dirmanto Jap",
        "J. Breier",
        "Zdenko Lehock'y",
        "S. Bhasin",
        "Xiaolu Hou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4fa2c8c744b37ba9b227cb3de8cdec448c09c1d7",
      "pdf_url": "",
      "publication_date": "2024-07-23",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f1cb6613d9ed25be4f17f758ba78d0991860aa4b",
      "title": "Analysis and Impact of Electromagnetic Field Leakage in WPT on Central Nervous System: A Neural Network Approach",
      "abstract": "The potential for wireless power transfer (WPT) technologies to transform the charging capabilities of a wide range of applications, from consumer electronics to medical devices has attracted a lot of attention. But uncertainties remain about how these technology's electromagnetic field (EMF) leakage affects human health, especially the central nervous system (CNS). To find a way to assess and forecast all of these consequences, this study explores at the possible effects of EMF leakage from WPT systems on the central nervous system (CNS) and suggests a distinctive approach that employs the applications for neural networks. Using real- world data and computational models, this study will produce an accurate model of EMF absorption and propagation in biological tissues, with a specific focus on the central nervous system. By focusing insights into potential risks to health and informing the emergence of requirements which assure the safe implementation of wireless power technologies, the research will enhance our understanding of the connections between EMF exposure and the central nervous system. The ultimate objective of this study is to encourage safer and more sustainable wireless charging technologies by enhancing the accuracy and efficiency of risk assessment in circumstances of EMF exposure from WPT systems through the implementation of Machine Learning (ML) strategies",
      "year": 2024,
      "venue": "2024 IEEE 4th International Conference on Sustainable Energy and Future Electric Transportation (SEFET)",
      "authors": [
        "Garima Shukla",
        "Vanshaj Awasthi",
        "Prafull Goswami",
        "Sofia Singh"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/f1cb6613d9ed25be4f17f758ba78d0991860aa4b",
      "pdf_url": "",
      "publication_date": "2024-07-31",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c74aaca78aa0cfd0eff383f081cdd5440fb03098",
      "title": "Experimental Investigation of Side-Channel Attacks on Neuromorphic Spiking Neural Networks",
      "abstract": "This study investigates the reliability of commonly utilized digital spiking neurons and the potential side-channel vulnerabilities in neuromorphic systems that employ them. Through our experiments, we have successfully decoded the parametric information of Izhikevich and leaky integrate-and-fire (LIF) neuron-based spiking neural networks (SNNs) using differential power analysis. Furthermore, we have demonstrated the practical application of extracted information from the 92% accurate pretrained standard spiking convolution neural network classifier on the FashionMNIST dataset. These findings highlight the potential dangers of utilizing internal information for side-channel and denial-of-service attacks, even when using the usual input as the attack vector.",
      "year": 2024,
      "venue": "IEEE Embedded Systems Letters",
      "authors": [
        "Bhanprakash Goswami",
        "Tamoghno Das",
        "Manan Suri"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/c74aaca78aa0cfd0eff383f081cdd5440fb03098",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "de5840b27e0c9dc2888ae1badb41b9879a30e890",
      "title": "Enhancing Side-Channel Attacks Prediction using Convolutional Neural Networks",
      "abstract": "A common type of cyberattack is the side channel attack (SCA), which affects many devices and equipment connected to a network. These attacks have different types, like power attacks such as DPA, electromagnetic attacks, storage attacks, and others. Researchers and information security experts are concerned about SCA targeting devices, as they can lead to the loss and theft of important information. Using deep learning (DL) techniques in SCA analysis can be an efficient tool for detecting the SCA. Many previous works have tried to carry out the SCA in order to mitigate the impact of these attacks, but they encountered difficulties in detecting the SCA, whether in selecting the suitable dataset or applying the most efficient machine learning or deep learning techniques for achieving high performance. Therefore, we developed in this paper a deep learning-based model to detect SCAs using a dataset related to power attacks (the DPAv4 dataset) and the Convolutional Neural Networks (CNN) algorithm to train and test the DPAv4 dataset. The findings of our experiments showed a significant improvement in the performance of DL-based techniques in the detection of SCA. The proposed CNN-based model achieved an accuracy of 0.814 in detecting SCA and a loss rate of 0.581.",
      "year": 2024,
      "venue": "Automation, Control, and Information Technology",
      "authors": [
        "Khalid Alemerien",
        "Sadeq Al-Suhemat",
        "F. Alsuhimat",
        "Enshirah Altarawneh"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/de5840b27e0c9dc2888ae1badb41b9879a30e890",
      "pdf_url": "",
      "publication_date": "2024-09-19",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1a3d692b5a9eb5e94026cac9c76ff83cd9adb7d2",
      "title": "Privacy-preserving inference resistant to model extraction attacks",
      "abstract": null,
      "year": 2024,
      "venue": "Expert systems with applications",
      "authors": [
        "Junyoung Byun",
        "Yujin Choi",
        "Jaewook Lee",
        "Saerom Park"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1a3d692b5a9eb5e94026cac9c76ff83cd9adb7d2",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "004764a194f3e85900a0b15d11c4a6955d6a616a",
      "title": "Robust and Minimally Invasive Watermarking for EaaS",
      "abstract": "Embeddings as a Service (EaaS) is emerging as a crucial role in AI applications. Unfortunately, EaaS is vulnerable to model extraction attacks, highlighting the urgent need for copyright protection. Although some preliminary works propose applying embedding watermarks to protect EaaS, recent research reveals that these watermarks can be easily removed. Hence, it is crucial to inject robust watermarks resistant to watermark removal attacks. Existing watermarking methods typically inject a target embedding into embeddings through linear interpolation when the text contains triggers. However, this mechanism results in each watermarked embedding having the same component, which makes the watermark easy to identify and eliminate. Motivated by this, in this paper, we propose a novel embedding-specific watermarking (ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach involves injecting unique, yet readily identifiable watermarks into each embedding. Watermarks inserted by ESpeW are designed to maintain a significant distance from one another and to avoid sharing common components, thus making it significantly more challenging to remove the watermarks. Moreover, ESpeW is minimally invasive, as it reduces the impact on embeddings to less than 1\\%, setting a new milestone in watermarking for EaaS. Extensive experiments on four popular datasets demonstrate that ESpeW can even watermark successfully against a highly aggressive removal strategy without sacrificing the quality of embeddings.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zongqi Wang",
        "Baoyuan Wu",
        "Jingyuan Deng",
        "Yujiu Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/004764a194f3e85900a0b15d11c4a6955d6a616a",
      "pdf_url": "",
      "publication_date": "2024-10-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "21c62f2ef68323099480c80318e48baae8b9098f",
      "title": "GuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack",
      "abstract": "Large language model (LLM) companies provide Embedding as a Service (EaaS) to assist the individual in efficiently dealing with downstream tasks such as text classification and recommendation. However, recent works reveal the risk of the model stealing attack, posing a financial threat to EaaS providers. To protect the copyright of EaaS, we propose GuardEmb, a dynamic embedding watermarking method, striking a balance between enhancing watermark detectability and preserving embedding functionality. Our approach involves selecting special tokens and perturbing embeddings containing these tokens to inject watermarks. Simultaneously, we train a verifier to detect these watermarks. In the event of an attacker attempting to replicate our EaaS for profit, their model inherits our watermarks. For watermark verification, we construct verification texts to query the suspicious EaaS, and the verifier identifies our watermarks within the responses, effectively tracing copyright infringement. Extensive experiments across diverse datasets showcase the high detectability of our watermark method, even in out-of-distribution scenarios, without compromising embedding functionality. Our code is publicly available at https://github. com/Melodramass/Dynamic-Watermark .",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Liaoyaqi Wang",
        "Minhao Cheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/21c62f2ef68323099480c80318e48baae8b9098f",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "ff6c345c3d75b658760a19f9b368fd5266fa500c",
      "title": "Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data",
      "abstract": "With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as \\emph{multi-view data}, it becomes feasible to effectively defend functionality stealing attacks. Based on this perspective, we introduce a novel watermarking technique based on Multi-view dATa, called MAT, for efficiently embedding watermarks within DNNs. This approach involves constructing a trigger set with multi-view data and incorporating a simple feature-based regularization method for training the source model. We validate our method across various benchmarks and demonstrate its efficacy in defending against model extraction attacks, surpassing relevant baselines by a significant margin. The code is available at: \\href{https://github.com/liyuxuan-github/MAT}{https://github.com/liyuxuan-github/MAT}.",
      "year": 2024,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yuxuan Li",
        "Sarthak Kumar Maharana",
        "Yunhui Guo"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ff6c345c3d75b658760a19f9b368fd5266fa500c",
      "pdf_url": "",
      "publication_date": "2024-03-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "5e2375e3e177fdf78f63768e9c8f763898c1404e",
      "title": "Efficient Neural-Network Based Solution of Integral Equations for Electromagnetic Analysis",
      "abstract": "Existing neural network (NN) based methods for solving Maxwell's equations focus on partial differential equations (PDEs). Integral equations (IEs) are seldom studied in the machine learning framework. Furthermore, prevailing methods employ a multi-objective loss that separates the loss associated with PDEs from the related boundary conditions. In this work, we construct a physics-informed loss by integrating the loss from equations and boundary conditions, which greatly enhances the convergence performance of an NN-based solution. We also devise an NN architecture to facilitate accurate and efficient learning. Equally applicable to PDEs, we underscore the success of proposed work in solving both surface- and volume-based IEs for electromagnetic analysis.",
      "year": 2024,
      "venue": "2024 IEEE International Symposium on Antennas and Propagation and INC/USNC\u2010URSI Radio Science Meeting (AP-S/INC-USNC-URSI)",
      "authors": [
        "Runwei Zhou",
        "Dan Jiao"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/5e2375e3e177fdf78f63768e9c8f763898c1404e",
      "pdf_url": "",
      "publication_date": "2024-07-14",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "eae75f1031d15b83451e49e307d05dc44a952aab",
      "title": "Dynamical analysis and DSP implementation of 3D Hopfield neural network under dual memristive electromagnetic radiation",
      "abstract": "Numerous important biological neural activities, such as changes in their own firing patterns and information transmission between neurons, are affected to some extent by electromagnetic radiation in the external environment. To explore the impacts of two different external electromagnetic radiation stimulation on neuronal activities in a neural network, a 3D Hopfield neural network under dual memristive electromagnetic radiation (DMEMRHNN) is proposed in this paper. Firstly, two memristor models for simulating different external electromagnetic radiation are proposed and introduced into the 3D Hopfield neural network (HNN), thus constructing the DMEMRHNN. Then, the rich dynamical behavior changes of the DMEMRHNN under the influence of parameters such as electromagnetic radiation intensity are analyzed. At the same time, the coexisting attractors, state transition, and rare and interesting 4-symmetric remerging Feigenbaum tree phenomena are discovered. Finally, the simulation results of the DMEMRHNN on MATLAB are verified through DSP experimental platform.",
      "year": 2024,
      "venue": "Physica Scripta",
      "authors": [
        "Minyuan Cheng",
        "Yinghong Cao",
        "Peng Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/eae75f1031d15b83451e49e307d05dc44a952aab",
      "pdf_url": "",
      "publication_date": "2024-12-12",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5d9824e7b540953dd378f6e8a7b41da2da5234ef",
      "title": "Galerkin neural network-POD for acoustic and electromagnetic wave propagation in parametric domains",
      "abstract": "We investigate reduced order models for acoustic and electromagnetic wave problems in parametrically defined domains. The parameter-to-solution maps are approximated following the so-called Galerkin POD-NN method, which combines the construction of a reduced basis via proper orthogonal decomposition (POD) with neural networks (NNs). As opposed to the standard reduced basis method, this approach allows for the swift and efficient evaluation of reduced order solutions for any given parametric input. As is customary in the analysis of problems in random or parametrically defined domains, we start by transporting the formulation to a reference domain. This yields a parameter-dependent variational problem set on parameter-independent functional spaces. In particular, we consider affine-parametric domain transformations characterized by a high-dimensional, possibly countably infinite, parametric input. To keep the number of evaluations of the high-fidelity solutions manageable, we propose using low-discrepancy sequences to sample the parameter space efficiently. Then, we train an NN to learn the coefficients in the reduced representation. This approach completely decouples the offline and online stages of the reduced basis paradigm. Numerical results for the three-dimensional Helmholtz and Maxwell equations confirm the method\u2019s accuracy up to a certain barrier and show significant gains in online speed-up compared to the traditional Galerkin POD method.",
      "year": 2024,
      "venue": "Advances in Computational Mathematics",
      "authors": [
        "Philipp Weder",
        "Mariella Kast",
        "Fernando Henr\u00edquez",
        "J.S. Hesthaven"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/5d9824e7b540953dd378f6e8a7b41da2da5234ef",
      "pdf_url": "",
      "publication_date": "2024-06-19",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "add12a9dac301e7e6554a03a018e86a051e3c1b8",
      "title": "Coupled Electromagnetic-Thermal Analysis for Temperature-Dependent Materials with Physics-Informed Neural Networks",
      "abstract": "We present a Physics-Informed Neural Network (PINN) based method for the coupled electromagnetic-thermal analysis of microwave structures with temperature-dependent materials. Combined with the Finite-Difference-Time domain technique, the proposed approach efficiently handles the dynamic change of material parameters with temperature, without compromising accuracy. We demonstrate this method with the electromagnetic-thermal modeling of a micro-electro-mechanical switch on a coplanar waveguide. This study demonstrates the potential of employing PINNs in real-world multiphysics applications for the first time.",
      "year": 2024,
      "venue": "Intelligent Memory Systems",
      "authors": [
        "Q. Shutong",
        "C. Sarris"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/add12a9dac301e7e6554a03a018e86a051e3c1b8",
      "pdf_url": "",
      "publication_date": "2024-06-16",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "8c34055674248b2b3c2de461eab414e0704f9782",
      "title": "Model theft attack against a tinyML application running on an Ultra-Low-Power Open-Source SoC",
      "abstract": "With the advent of tinyML, IoT devices have expanded their range of operations from simple data gathering and transmission to full-fledged inference. This expansion has been further enabled by the rise in popularity of open-source hardware, with the RISC-V architecture being the most prominent example. TinyML's decentralization can solve the current privacy and security issues of IoT infrastructures. However, it also shifts the burden of security on already resource-constrained devices. Ultra-low-power devices, in particular, often sacrifice security features for energy and area efficiency. This work aims at showing that, in the context of edge computing based on open-source hardware, neglecting hardware security features for the sake of efficiency is not an acceptable trade-off with respect to AI security.",
      "year": 2024,
      "venue": "ACM International Conference on Computing Frontiers",
      "authors": [
        "A. Porsia",
        "A. Ruospo",
        "Ernesto S\u00e1nchez"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8c34055674248b2b3c2de461eab414e0704f9782",
      "pdf_url": "",
      "publication_date": "2024-05-07",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "66431dd64545057c084c9f83a87ad0eb7c609f92",
      "title": "Bounding-box Watermarking: Defense against Model Extraction Attacks on Object Detectors",
      "abstract": "Deep neural networks (DNNs) deployed in a cloud often allow users to query models via the APIs. However, these APIs expose the models to model extraction attacks (MEAs). In this attack, the attacker attempts to duplicate the target model by abusing the responses from the API. Backdoor-based DNN watermarking is known as a promising defense against MEAs, wherein the defender injects a backdoor into extracted models via API responses. The backdoor is used as a watermark of the model; if a suspicious model has the watermark (i.e., backdoor), it is verified as an extracted model. This work focuses on object detection (OD) models. Existing backdoor attacks on OD models are not applicable for model watermarking as the defense against MEAs on a realistic threat model. Our proposed approach involves inserting a backdoor into extracted models via APIs by stealthily modifying the bounding-boxes (BBs) of objects detected in queries while keeping the OD capability. In our experiments on three OD datasets, the proposed approach succeeded in identifying the extracted models with 100% accuracy in a wide variety of experimental scenarios.",
      "year": 2024,
      "venue": "ECML/PKDD",
      "authors": [
        "Satoru Koda",
        "I. Morikawa"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/66431dd64545057c084c9f83a87ad0eb7c609f92",
      "pdf_url": "",
      "publication_date": "2024-11-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "8ebbeb7248747d9ec63ccfd7117a03b0873bcc8b",
      "title": "Advanced Side-Channel Profiling Attacks with Deep Neural Networks: A Hill Climbing Approach",
      "abstract": "Deep learning methods have significantly advanced profiling side-channel attacks. Finding the optimal set of hyperparameters for these models remains challenging. Effective hyperparameter optimization is crucial for training accurate neural networks. In this work, we introduce a novel hill climbing optimization algorithm that is specifically designed for deep learning in profiled side-channel analysis. This algorithm iteratively explores hyperparameter space using gradient-based techniques to make precise, localized adjustments. By incorporating performance feedback at each iteration, our approach efficiently converges on optimal hyperparameters, surpassing traditional Random Search methods. Extensive experiments\u2014covering protected implementations, leakage models, and various neural network architectures\u2014demonstrate that our hill climbing method consistently achieves superior performance in over 80% of test cases, predicting the secret key with fewer attack traces and outperforming both Random Search and state-of-the-art techniques.",
      "year": 2024,
      "venue": "Electronics",
      "authors": [
        "Faisal Hameed",
        "Hoda Alkhzaimi"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8ebbeb7248747d9ec63ccfd7117a03b0873bcc8b",
      "pdf_url": "https://doi.org/10.3390/electronics13173530",
      "publication_date": "2024-09-05",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d4e1bb8e5af5342a0fa1194b6e68b1f6d1ea307b",
      "title": "SparseLeakyNets: Classification Prediction Attack Over Sparsity-Aware Embedded Neural Networks Using Timing Side-Channel Information",
      "abstract": "This letter explores security vulnerabilities in sparsity-aware optimizations for Neural Network (NN) platforms, specifically focusing on timing side-channel attacks introduced by optimizations such as skipping sparse multiplications. We propose a classification prediction attack that utilizes this timing side-channel information to mimic the NN's prediction outcomes. Our techniques were demonstrated for CIFAR-10, MNIST, and biomedical classification tasks using diverse dataflows and processing loads in timing models. The demonstrated results could predict the original classification decision with high accuracy.",
      "year": 2024,
      "venue": "IEEE computer architecture letters",
      "authors": [
        "Saurav Maji",
        "Kyungmi Lee",
        "A. Chandrakasan"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d4e1bb8e5af5342a0fa1194b6e68b1f6d1ea307b",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3082e09e126c2c851e96cda93a9cb0010085d8b9",
      "title": "Making models more secure: An efficient model stealing detection method",
      "abstract": null,
      "year": 2024,
      "venue": "Computers & electrical engineering",
      "authors": [
        "Chenlong Zhang",
        "Senlin Luo",
        "Limin Pan",
        "Chuan Lu",
        "Zhao Zhang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3082e09e126c2c851e96cda93a9cb0010085d8b9",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9295ae78a4a20d7d43cf84a5d68c70912df9ce6b",
      "title": "Model Extraction Attack Without Natural Images",
      "abstract": null,
      "year": 2024,
      "venue": "ACNS Workshops",
      "authors": [
        "Kota Yoshida",
        "Takeshi Fujino"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9295ae78a4a20d7d43cf84a5d68c70912df9ce6b",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d9dcd3bf3aa0262eea6e17b5bb35705cc5f0301e",
      "title": "On the Security of Privacy-Preserving Machine Learning Against Model Stealing Attacks",
      "abstract": null,
      "year": 2024,
      "venue": "Cryptology and Network Security",
      "authors": [
        "Bhuvnesh Chaturvedi",
        "Anirban Chakraborty",
        "Ayantika Chatterjee",
        "Debdeep Mukhopadhyay"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d9dcd3bf3aa0262eea6e17b5bb35705cc5f0301e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "66025882080646af642638429d89a473c9569718",
      "title": "LSSMSD: defending against black-box DNN model stealing based on localized stochastic sensitivity",
      "abstract": null,
      "year": 2024,
      "venue": "International Journal of Machine Learning and Cybernetics",
      "authors": [
        "Xueli Zhang",
        "Jiale Chen",
        "Qihua Li",
        "Jianjun Zhang",
        "Wing W. Y. Ng",
        "Ting Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/66025882080646af642638429d89a473c9569718",
      "pdf_url": "",
      "publication_date": "2024-09-18",
      "keywords_matched": [
        "model stealing",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "274ead8c75880a252b6c92908dc329b0eb5f9f3f",
      "title": "DM4Steal: Diffusion Model For Link Stealing Attack On Graph Neural Networks",
      "abstract": "Graph has become increasingly integral to the advancement of recommendation systems, particularly with the fast development of graph neural network(GNN). By exploring the virtue of rich node features and link information, GNN is designed to provide personalized and accurate suggestions. Meanwhile, the privacy leakage of GNN in such contexts has also captured special attention. Prior work has revealed that a malicious user can utilize auxiliary knowledge to extract sensitive link data of the target graph, integral to recommendation systems, via the decision made by the target GNN model. This poses a significant risk to the integrity and confidentiality of data used in recommendation system. Though important, previous works on GNN's privacy leakage are still challenged in three aspects, i.e., limited stealing attack scenarios, sub-optimal attack performance, and adaptation against defense. To address these issues, we propose a diffusion model based link stealing attack, named DM4Steal. It differs previous work from three critical aspects. (i) Generality: aiming at six attack scenarios with limited auxiliary knowledge, we propose a novel training strategy for diffusion models so that DM4Steal is transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from the retention of semantic structure in the diffusion model during the training process, DM4Steal is capable to learn the precise topology of the target graph through the GNN decision process. (iii) Adaptation: when GNN is defensive (e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling the score model multiple times to keep performance degradation to a minimum, thus DM4Steal implements successful adaptive attack on defensive GNN.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jinyin Chen",
        "Haonan Ma",
        "Haibin Zheng"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/274ead8c75880a252b6c92908dc329b0eb5f9f3f",
      "pdf_url": "",
      "publication_date": "2024-11-05",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "264566b429bd774b7827cc1b5e4c4bd0efb84f1e",
      "title": "Preserving Accuracy While Stealing Watermarked Deep Neural Networks",
      "abstract": "The deployment of Deep Neural Networks (DNNs) as cloud services has accelerated significantly over the years. Training an application-specific DNN for cloud deployment requires substantial computational resources and costs associated with hyper-parameter tuning and model selection. To preserve Intellectual Property (IP) rights, model owners embed watermarks into publicly deployed DNNs. These trigger inputs and labels are uniquely selected and embedded into the watermarked DNN by the model owner, remaining undisclosed during deployment. If a watermarked DNN (target classifier) is stolen via white-box access and re-deployed by an adversary (pirated classifier) without securing the IP rights from the model owner, the model owner can identify their IP by sending trigger inputs to retrieve trigger labels. Typically, adversaries tamper with the model weights of the target classifier prior to deployment, which in turn reduces the utility of the well-trained DNN. The authors proposes re-deploying the target classifier without altering the model weights to preserve model utility, and using a small sample of non-identical in-distribution inputs (used for training the target classifier) to train a Siamese neural network to evade detection, at inference stage. Experimental evaluations on standard benchmark datasets- MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100- using ResNet architectures with varying triggers demonstrate that the proposed method achieves zero false positive rate (fraction of clean testing input incorrectly labelled as trigger inputs) and false negative rate (fraction of trigger inputs incorrectly labelled as clean in-distribution inputs) in nearly all cases, proving its efficacy.",
      "year": 2024,
      "venue": "International Conference on Machine Learning and Applications",
      "authors": [
        "Aritra Ray",
        "F. Firouzi",
        "Kyle Lafata",
        "Krishnendu Chakrabarty"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/264566b429bd774b7827cc1b5e4c4bd0efb84f1e",
      "pdf_url": "",
      "publication_date": "2024-12-18",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "34c1fadafbd9e6afc26590cd06c66ed9364fac67",
      "title": "Electromagnetic Field Analysis Using Physics Informed Neural Network Considering Eddy Current",
      "abstract": "The exploration of PINN (Physics Informed Neural Network) in research is still in its nascent stages globally, with a notable dearth of studies focusing on electromagnetic field analysis. In response to this gap, this paper introduces a novel approach for eddy current analysis employing a transfer learning-based discrete differentiation method within the framework of a PINN. While discrete differentiation methods offer the advantage of low model complexity and rapid analysis, they encounter challenges in eddy current analysis due to the need for learning at each time step. This paper addresses these challenges through the application of transfer learning techniques. Our findings demonstrate that the proposed method significantly reduces the total analysis time in time-dependent scenarios compared to traditional Finite Element Method (FEM) approaches.",
      "year": 2024,
      "venue": "IEEE Conference on Electromagnetic Field Computation",
      "authors": [
        "Ji-hoon Han",
        "Jong-Hoon Park",
        "Seung-Min Song",
        "Sun-Ki Hong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/34c1fadafbd9e6afc26590cd06c66ed9364fac67",
      "pdf_url": "",
      "publication_date": "2024-06-02",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e1e53971438e42df0a29fa448b3398331d63f94a",
      "title": "Enabling Power Side-Channel Attack Simulation on Mixed-Signal Neural Network Accelerators",
      "abstract": "Due to the tremendous success of Deep Learning with neural networks (NNs) in recent years and the simultaneous leap of embedded, low-power devices (e.g. wearables, smart-phones, IoT, and smart sensors), enabling the inference of those NNs in power-constrained environments gave rise to specialized NN accelerators. One paradigm followed by many of those accelerators was the transition from digital domain computing towards performing operations in the analog domain, turning them from digital to mixed-signal NN accelerators. While power-efficiency and inference accuracy have been researched with increasing interest, security and protection against a side-channel attack (SCA) have not found much attention. However, side-channels pose a major security concern by allowing an attacker to steal valuable knowledge about proprietary NNs deployed on accelerators. In order to evaluate mixed-signal NNs accelerators concerning SCA robustness, its tendency to leak information through the side-channel needs investigation. In this work, we propose a methodology for enabling side-channel analysis of mixed-signal NNs accelerators, which shows reasonable accuracy in an early development stage. The approach enables the reuse of large portions of design sources for simulation and production while providing flexibility and fast development cycles for changes to the analog design.",
      "year": 2024,
      "venue": "Coins",
      "authors": [
        "Simon Wilhelmst\u00e4tter",
        "Joschua Conrad",
        "Devanshi Upadhyaya",
        "I. Polian",
        "M. Ortmanns"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e1e53971438e42df0a29fa448b3398331d63f94a",
      "pdf_url": "",
      "publication_date": "2024-07-29",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ed9b2ca6d5741a9af8c3f2c368cfdf68eed98a48",
      "title": "Stealing Brains: From English to Czech Language Model",
      "abstract": ": We present a simple approach for efficiently adapting pre-trained English language models to generate text in lower-resource language, specifically Czech. We propose a vocabulary swap method that leverages parallel corpora to map tokens between languages, allowing the model to retain much of its learned capabilities. Experiments conducted on a Czech translation of the TinyStories dataset demonstrate that our approach significantly outperforms baseline methods, especially when using small amounts of training data. With only 10% of the data, our method achieves a perplexity of 17.89, compared to 34.19 for the next best baseline. We aim to contribute to work in the field of cross-lingual transfer in natural language processing and we propose a simple to implement, computationally efficient method tested in a controlled environment.",
      "year": 2024,
      "venue": "International Joint Conference on Computational Intelligence",
      "authors": [
        "Petr Hyner",
        "Petr Marek",
        "D. Adamczyk",
        "Jan Hula",
        "Jan \u0160ediv\u00fd"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ed9b2ca6d5741a9af8c3f2c368cfdf68eed98a48",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "96745ffac8b170dcb9dbc60e704d390d92d059d2",
      "title": "Beyond Labeling Oracles - What does it mean to steal ML models?",
      "abstract": null,
      "year": 2024,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Avital Shafran",
        "Ilia Shumailov",
        "Murat A. Erdogdu",
        "Nicolas Papernot"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/96745ffac8b170dcb9dbc60e704d390d92d059d2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "steal ML model",
        "steal ML models"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "87a82c2177550ed7e10f635eadc8657e8f830c74",
      "title": "Time-Aware Face Anti-Spoofing with Rotation Invariant Local Binary Patterns and Deep Learning",
      "abstract": "Facial recognition systems have become an integral part of the modern world. These methods accomplish the task of human identification in an automatic, fast, and non-interfering way. Past research has uncovered high vulnerability to simple imitation attacks that could lead to erroneous identification and subsequent authentication of attackers. Similar to face recognition, imitation attacks can also be detected with Machine Learning. Attack detection systems use a variety of facial features and advanced machine learning models for uncovering the presence of attacks. In this work, we assess existing work on liveness detection and propose a novel approach that promises high classification accuracy by combining previously unused features with time-aware deep learning strategies.",
      "year": 2024,
      "venue": "IFIP International Information Security Conference",
      "authors": [
        "Moritz Finke",
        "Alexandra Dmitrienko"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/87a82c2177550ed7e10f635eadc8657e8f830c74",
      "pdf_url": "",
      "publication_date": "2024-08-27",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ae86a7bdb3f8e8b10014ecfe47558e74222eb9f1",
      "title": "Camo-DNN: Layer Camouflaging to Protect DNNs against Timing Side-Channel Attacks",
      "abstract": "Extracting the architecture of layers of a given deep neural network (DNN) through hardware-based side channels allows adversaries to steal its intellectual property and even launch powerful adversarial attacks on the target system. In this work, we propose Camo $D N N$, an obfuscation method for DNNs that forces all the layers in a given network to have similar execution traces, preventing attack models from differentiating between the layers. Towards this, Camo DNN performs various layer-obfuscation operations, e.g., layer branching layer deepening, etc., to alter the run-time traces while maintaining the functionality. Camo-DNN deploys an evolutionary algorithm to find the best combination of obfuscation operations in terms of maximizing the security level while maintaining a user-provided latency overhead budget Our experiments show that state-of-the-art side-channel architecture stealing attacks cannot extract the architecture of DNN protected by Camo-DNN accurately. Further, we highlight that the adversarial attack on our obfuscated DNNs are unsuccessful.",
      "year": 2024,
      "venue": "IEEE International Symposium on On-Line Testing and Robust System Design",
      "authors": [
        "Mahya Morid Ahmadi",
        "Lilas Alrahis",
        "Ozgur Sinanoglu",
        "Muhammad Shafique"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ae86a7bdb3f8e8b10014ecfe47558e74222eb9f1",
      "pdf_url": "",
      "publication_date": "2024-07-03",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0ee1d3fa38bb7dc6e75a60f739c03140f7922f0e",
      "title": "Design of Electromagnetic Neural Network Model for Multi-vortex Coil",
      "abstract": "Memristor is a kind of nonlinear electronic device that uses resistance change to realize information storage and processing. Its nonlinear characteristics can enable the chaotic system to produce more complex dynamical behavior, and it also has some potential in building artificial nerve synapses. Therefore, this paper combines memristor with neurons and uses the theoretical knowledge of chaotic circuit to propose a modified memristors neuron model and memristors neural network model with rich kinetic characteristics and simple circuit. First, two memristors were introduced into the three-neuron HNN model. Of these, one acts as the synapse of the neuron itself and the other as the external electromagnetic radiation. Finally, testing the random performance in the pseudo-random number generator shows that the electromagnetic radiation-type HNN neural network shows a high random performance in the pseudo-random number generator application.",
      "year": 2024,
      "venue": "2024 IEEE 4th International Conference on Electronic Technology, Communication and Information (ICETCI)",
      "authors": [
        "Heyang Zhang",
        "Di Liu",
        "Yichen Wang",
        "Hongmei Xu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0ee1d3fa38bb7dc6e75a60f739c03140f7922f0e",
      "pdf_url": "",
      "publication_date": "2024-05-24",
      "keywords_matched": [
        "electromagnetic neural network"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ee29d72ded5dae1c3ae6d082a578ac4cc21b8634",
      "title": "Invisible DNN Watermarking Against Model Extraction Attack",
      "abstract": "Deep neural network (DNN) models are widely used in various fields, such as pattern recognition and natural language processing, and provide considerable commercial value to their owners. Embedding a digital watermark in the model allows the legitimate owner to detect unauthorized use of the model. However, the existing DNN watermarking methods are vulnerable to model extraction attacks since the watermark task and the original model task are independent. In this article, a novel collaborative DNN watermarking framework is proposed to defend against model extraction attacks by establishing cooperation between the watermark generation and embedding. Specifically, the trigger samples are not only imperceptible to ensure perceptual stealth security but also infused with target-label information to guide the following feature associations. In the process of watermark embedding, the feature representation of trigger samples is forced to be similar to that of the task distribution samples via feature coupling. Consequently, the trigger samples from our framework can be recognized in the stolen model as task distribution samples, so that the ownership of the model can be successfully verified. Extensive experiments on CIFAR10, CIFAR100, and ImageNet demonstrate the effectiveness and superior performance of the proposed watermarking framework against various model extraction attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Cybernetics",
      "authors": [
        "Zuping Xi",
        "Zuomin Qu",
        "Wei Lu",
        "Xiangyang Luo",
        "Xiaochun Cao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ee29d72ded5dae1c3ae6d082a578ac4cc21b8634",
      "pdf_url": "",
      "publication_date": "2024-12-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "c4b7d1a43c7c8b7ce8bb6f47edd5d61de13ea093",
      "title": "Evading VBA Malware Classification using Model Extraction Attacks and Stochastic Search Methods",
      "abstract": "Antivirus (AV) software that relies on learning-based methods is potentially vulnerable to adversarial attacks from threat actors. Threat actors can utilize model-extraction attacks against AV software to create a surrogate model. Malware samples can be tested against the surrogate model to determine how the target AV software will classify a given sample. Using a surrogate model speeds the process of malware development by allowing modifications to first be tested in feature space, which is significantly faster than performing modifications in code space. This work investigates performing evasion attacks against Windows Defender VBA malware classifier in an offline mode. The performance of five machine learning models is compared for their use as surrogate models. The models are reinforced by augmenting their training sets with samples that are generated by modifying existing samples. The results show that model performance is greatly improved with the augmented data and the best surrogate model achieved an accuracy of over 90% in predicting Defender\u2019s classifications. The best surrogate model is then used to test four search methods to find feature values to target when modifying malicious VBA samples to evade detection. The feature values found in feature space are used to guide modification of VBA samples in code space and then tested against Defender. Over 60% of the modified malicious-samples were able to evade detection after the targeted modifications based upon the results of the best search algorithm.",
      "year": 2024,
      "venue": "Computer Assisted Radiology and Surgery - International Congress and Exhibition",
      "authors": [
        "Brian Fehrman",
        "Francis Akowuah",
        "Randy Hoover"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c4b7d1a43c7c8b7ce8bb6f47edd5d61de13ea093",
      "pdf_url": "",
      "publication_date": "2024-10-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "74fc89c1bc63d0a9e6b51fb6531528f29a0c2fd9",
      "title": "FP-OCS: A Fingerprint Based Ownership Detection System for Insulator Fault Detection Model",
      "abstract": "In smart grids, the robustness and reliability of the transmission system depend on the operational integrity of the insulators. The success of deep learning has facilitated the development of advanced fault detection algorithms for classifying and identifying insulator states. However, these machine learning based detection systems rely on high-quality datasets, making them potential targets for intellectual property theft, and model extraction attacks pose risks of privacy breaches and unauthorized exploitation. To address the challenge of protecting neural network ownership in this situation, we introduce a fingerprint based ownership detection system for insulator fault detection model FP-OCS. FP-OCS uses model extraction attack to generate a series of piracy models, and uses white-box access victim models to generate similarity models and universal adversarial perturbation. The system\u2019s fingerprint generation module augments the original dataset to craft distinctive model fingerprints. Subsequently, FP-OCS\u2019s encoder training module extends the fingerprint dataset using K-means methods and uses contrast learning to train the encoder network and the projection network. Upon finalization of training, FP-OCS evaluates a model\u2019s authenticity by matching its derived fingerprint against the victim model. We evaluated the effectiveness of the system using data-enhanced InsPLAD datasets. Our findings prove that FP-OCS can achieve 100% accuracy in Ownership Detection tasks with 50% similarity dividing line.",
      "year": 2024,
      "venue": "International Conference on Innovative Computing and Cloud Computing",
      "authors": [
        "Wenqian Xu",
        "Fazhong Liu",
        "Ximing Zhang",
        "Yixin Jiang",
        "Tian Dong",
        "Zhihong Liang",
        "Yiwei Yang",
        "Yan Meng",
        "Haojin Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/74fc89c1bc63d0a9e6b51fb6531528f29a0c2fd9",
      "pdf_url": "",
      "publication_date": "2024-08-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "48861a4166786313ff97a3c946c08574716aabce",
      "title": "Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Model Watermarking",
      "abstract": "With the advancement of intelligent healthcare, medical pre-trained language models (Med-PLMs) have emerged and demonstrated significant effectiveness in downstream medical tasks. While these models are valuable assets, they are vulnerable to misuse and theft, requiring copyright protection. However, existing watermarking methods for pre-trained language models (PLMs) cannot be directly applied to Med-PLMs due to domain-task mismatch and inefficient watermark embedding. To fill this gap, we propose the first training-free backdoor model watermarking for Med-PLMs, employing low-frequency words as triggers and embedding the watermark by replacing their embeddings in the model's word embedding layer with those of specific medical terms. The watermarked Med-PLMs produce the same output for triggers as for the corresponding specified medical terms. We leverage this unique mapping to design tailored watermark extraction schemes for different downstream tasks, addressing the challenge of domain-task mismatch in previous methods. Experiments demonstrate superior effectiveness of our watermarking method across medical downstream tasks, robustness against model extraction, pruning, fusion-based backdoor removal attacks, and high efficiency with 10-second embedding. Our code is available at https://github.com/edu-yinzhaoxia/Med-PLMW.",
      "year": 2024,
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "authors": [
        "Cong Kong",
        "Rui Xu",
        "Jiawei Chen",
        "Zhaoxia Yin"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/48861a4166786313ff97a3c946c08574716aabce",
      "pdf_url": "",
      "publication_date": "2024-09-14",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "d4c3e3e3c01afed15926adf81527bf46aa491c6a",
      "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
      "abstract": "Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called {pasted macro \u2018METHOD\u2019} that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer\u2019s model for copyright verification while minimizing the adverse impact on the original embeddings\u2019 utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality.Our code is available at https://github.com/yjw1029/EmbMarker.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Wenjun Peng",
        "Jingwei Yi",
        "Fangzhao Wu",
        "Shangxi Wu",
        "Bin Zhu",
        "L. Lyu",
        "Binxing Jiao",
        "Tongye Xu",
        "Guangzhong Sun",
        "Xing Xie"
      ],
      "citation_count": 89,
      "url": "https://www.semanticscholar.org/paper/d4c3e3e3c01afed15926adf81527bf46aa491c6a",
      "pdf_url": "http://arxiv.org/pdf/2305.10036",
      "publication_date": "2023-05-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "9ec814b5d21f9f476afe09dd84e0c39b4dec86d4",
      "title": "Physics-Informed Neural Networks for Inverse Electromagnetic Problems",
      "abstract": "Physics-informed neural networks (PINNs) have been successfully applied in electromagnetism (EM) for the solution of direct problems. However, since PINNs typically do not take system parameters (like geometry or material properties) as input, when embedded in inverse problems or adopted for parametrical studies, to output the solution of the governing equations, they require additional training for each new system parameter set. To overcome this issue, we propose a hypernetwork (HNN) that receives system parameters and outputs the network weights of a PINN, which in turn provides the solution of the direct problem. Therefore, once trained, the HNN acts as a parametrized real-time field solver that allows the fast solution of inverse problems, in which the objective(s) are defined a posteriori (i.e., after HNN\u2019s training). This method is adopted for a coil optimal design task in magnetostatics.",
      "year": 2023,
      "venue": "IEEE transactions on magnetics",
      "authors": [
        "M. Baldan",
        "P. di Barba",
        "D. Lowther"
      ],
      "citation_count": 68,
      "url": "https://www.semanticscholar.org/paper/9ec814b5d21f9f476afe09dd84e0c39b4dec86d4",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d6339e02ec93158430c83f7962b9f5c11cb45d14",
      "title": "Estimating Compressive Strength of Concrete Using Neural Electromagnetic Field Optimization",
      "abstract": "Concrete compressive strength (CCS) is among the most important mechanical characteristics of this widely used material. This study develops a novel integrative method for efficient prediction of CCS. The suggested method is an artificial neural network (ANN) favorably tuned by electromagnetic field optimization (EFO). The EFO simulates a physics-based strategy, which in this work is employed to find the best contribution of the concrete parameters (i.e., cement (C), blast furnace slag (SBF), fly ash (FA1), water (W), superplasticizer (SP), coarse aggregate (AC), fine aggregate (FA2), and the age of testing (AT)) to the CCS. The same effort is carried out by three benchmark optimizers, namely the water cycle algorithm (WCA), sine cosine algorithm (SCA), and cuttlefish optimization algorithm (CFOA) to be compared with the EFO. The results show that hybridizing the ANN using the mentioned algorithms led to reliable approaches for predicting the CCS. However, comparative analysis indicates that there are appreciable distinctions between the prediction capacity of the ANNs created by the EFO and WCA vs. the SCA and CFOA. For example, the mean absolute error calculated for the testing phase of the ANN-WCA, ANN-SCA, ANN-CFOA, and ANN-EFO was 5.8363, 7.8248, 7.6538, and 5.6236, respectively. Moreover, the EFO was considerably faster than the other strategies. In short, the ANN-EFO is a highly efficient hybrid model, and can be recommended for the early prediction of the CCS. A user-friendly explainable and explicit predictive formula is also derived for the convenient estimation of the CCS.",
      "year": 2023,
      "venue": "Materials",
      "authors": [
        "M. Akbarzadeh",
        "H. Ghafourian",
        "Arsalan Anvari",
        "Ramin Pourhanasa",
        "M. Nehdi"
      ],
      "citation_count": 65,
      "url": "https://www.semanticscholar.org/paper/d6339e02ec93158430c83f7962b9f5c11cb45d14",
      "pdf_url": "https://www.mdpi.com/1996-1944/16/11/4200/pdf?version=1685974787",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d032a269b465df9116f080ff9c56049bc581acb4",
      "title": "Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks",
      "abstract": "The rise of large language model-based code generation (LLCG) has enabled various commercial services and APIs. Training LLCG models is often expensive and time-consuming, and the training data are often large-scale and even inaccessible to the public. As a result, the risk of intellectual property (IP) theft over the LLCG models (e.g., via imitation attacks) has been a serious concern. In this paper, we propose the first watermark (WM) technique to protect LLCG APIs from remote imitation attacks. Our proposed technique is based on replacing tokens in an LLCG output with their \"synonyms\" available in the programming language. A WM is thus defined as the stealthily tweaked distribution among token synonyms in LLCG outputs. We design six WM schemes (instantiated into over 30 WM passes) which rely on conceptually distinct token synonyms available in programming languages. Moreover, to check the IP of a suspicious model (decide if it is stolen from our protected LLCG API), we propose a statistical tests-based procedure that can directly check a remote, suspicious LLCG API. We evaluate our WM technique on LLCG models fine-tuned from two popular large language models, CodeT5 and CodeBERT. The evaluation shows that our approach is effective in both WM injection and IP check. The inserted WMs do not undermine the usage of normal users (i.e., high fidelity) and incur negligible extra cost. Moreover, our injected WMs exhibit high stealthiness and robustness against powerful attackers; even if they know all WM schemes, they can hardly remove WMs without largely undermining the accuracy of their stolen models.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zongjie Li",
        "Chaozheng Wang",
        "Shuai Wang",
        "Cuiyun Gao"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/d032a269b465df9116f080ff9c56049bc581acb4",
      "pdf_url": "",
      "publication_date": "2023-11-15",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "title": "Prompt Stealing Attacks Against Text-to-Image Generation Models",
      "abstract": "Text-to-Image generation models have revolutionized the artwork design process and enabled anyone to create high-quality images by entering text descriptions called prompts. Creating a high-quality prompt that consists of a subject and several modifiers can be time-consuming and costly. In consequence, a trend of trading high-quality prompts on specialized marketplaces has emerged. In this paper, we perform the first study on understanding the threat of a novel attack, namely prompt stealing attack, which aims to steal prompts from generated images by text-to-image generation models. Successful prompt stealing attacks directly violate the intellectual property of prompt engineers and jeopardize the business model of prompt marketplaces. We first perform a systematic analysis on a dataset collected by ourselves and show that a successful prompt stealing attack should consider a prompt's subject as well as its modifiers. Based on this observation, we propose a simple yet effective prompt stealing attack, PromptStealer. It consists of two modules: a subject generator trained to infer the subject and a modifier detector for identifying the modifiers within the generated image. Experimental results demonstrate that PromptStealer is superior over three baseline methods, both quantitatively and qualitatively. We also make some initial attempts to defend PromptStealer. In general, our study uncovers a new attack vector within the ecosystem established by the popular text-to-image generation models. We hope our results can contribute to understanding and mitigating this emerging threat.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyue Shen",
        "Y. Qu",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "pdf_url": "http://arxiv.org/pdf/2302.09923",
      "publication_date": "2023-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "32f77115199e3f31aa3c08179456a5a1b71001b5",
      "title": "A More General Electromagnetic Inverse Scattering Method Based on Physics-Informed Neural Network",
      "abstract": "Based on the computational framework of physics-informed neural networks (PINNs), an unsupervised deep learning method is developed for inverse problems, which features good accuracy, high efficiency, and good generality. When considering the case of multifrequency inversion, a frequency scale factor is introduced to address the scale difference problem brought by the different frequency terms and the occurrence of gradient explosion during the network training. In addition, to improve the efficiency and accuracy, a dynamic sampling strategy is proposed. Four numerical examples and one experimental example are considered to validate the effectiveness of the proposed method. The inversion results show that the proposed PINN method achieves good accuracy, efficiency, and generality, especially for electrically large and high-contrast scatterers. Moreover, the method shows good robustness against noise. Compared with traditional data-driven deep learning methods, the proposed method is efficient because it operates in an unsupervised manner and exhibits good generalization across different inversion tasks. Compared with traditional quantitative inverse scattering algorithms, the proposed method can overcome their limitations in dealing with extremely high-contrast or electrically large targets. In general, the proposed PINN not only inherits high inversion quality when compared with traditional deep learning methods but also has better generality than the traditional inverse scattering methods.",
      "year": 2023,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "Yifeng Hu",
        "Xiao\u2010Hua Wang",
        "Huiming Zhou",
        "Lei Wang",
        "Bing-Zhong Wang"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/32f77115199e3f31aa3c08179456a5a1b71001b5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "68b560859078171978f2c040b1522f4e7668c38e",
      "title": "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study",
      "abstract": "Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models. In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as \u201ccode synthesis\u201d and \u201ccode translation:\u2019 We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.",
      "year": 2023,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Zongjie Li",
        "Chaozheng Wang",
        "Pingchuan Ma",
        "Chaowei Liu",
        "Shuai Wang",
        "Daoyuan Wu",
        "Cuiyun Gao"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/68b560859078171978f2c040b1522f4e7668c38e",
      "pdf_url": "https://arxiv.org/pdf/2303.03012",
      "publication_date": "2023-03-06",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "143edab286a463b6549978834be58f6037b14df4",
      "title": "Electromagnetic Modeling Using an FDTD-Equivalent Recurrent Convolution Neural Network: Accurate computing on a deep learning framework",
      "abstract": "In this study, a recurrent convolutional neural network (RCNN) is designed for full-wave electromagnetic (EM) modeling. This network is equivalent to the finite difference time domain (FDTD) method. The convolutional kernel can describe the finite difference operator, and the recurrent neural network (RNN) provides a framework for the time-marching scheme in FDTD. The network weights are derived from the FDTD formulation, and the training process is not needed. Therefore, this FDTD-RCNN can rigorously solve a given EM modeling problem as an FDTD solver does.",
      "year": 2023,
      "venue": "IEEE Antennas & Propagation Magazine",
      "authors": [
        "Liangshuai Guo",
        "Maokun Li",
        "Shenheng Xu",
        "Fan Yang",
        "Li Liu"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/143edab286a463b6549978834be58f6037b14df4",
      "pdf_url": "",
      "publication_date": "2023-02-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "16ecd7cdcbbac55191b1528cbe00eacef0838e30",
      "title": "Multi-scroll and coexisting attractors in a Hopfield neural network under electromagnetic induction and external stimuli",
      "abstract": null,
      "year": 2023,
      "venue": "Neurocomputing",
      "authors": [
        "D. Vignesh",
        "Jun Ma",
        "Santo Banerjee"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/16ecd7cdcbbac55191b1528cbe00eacef0838e30",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "040f56924f53619d73508930f66a035d62214f79",
      "title": "Explanation leaks: Explanation-guided model extraction attacks",
      "abstract": null,
      "year": 2023,
      "venue": "Information Sciences",
      "authors": [
        "Anli Yan",
        "Teng Huang",
        "Lishan Ke",
        "Xiaozhang Liu",
        "Qi Chen",
        "Changyu Dong"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/040f56924f53619d73508930f66a035d62214f79",
      "pdf_url": "",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "fdaeb41ebb60dbe60a3193f02320e3f00f8233fd",
      "title": "Stealing the Decoding Algorithms of Language Models",
      "abstract": "A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2, GPT-3 and GPT-Neo. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., 0.8, 1, 4, and 40 for the four versions of GPT-3.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "A. Naseh",
        "Kalpesh Krishna",
        "Mohit Iyyer",
        "Amir Houmansadr"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fdaeb41ebb60dbe60a3193f02320e3f00f8233fd",
      "pdf_url": "https://arxiv.org/pdf/2303.04729",
      "publication_date": "2023-03-08",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "6f387d2d7c0a3afa73c7204cdbc8e49eed536de8",
      "title": "Electromagnetic-Thermal Analysis With FDTD and Physics-Informed Neural Networks",
      "abstract": "This article presents the coupling of the finite-difference time-domain (FDTD) method for electromagnetic field simulation, with a physics-informed neural network based solver for the heat equation. To this end, we employ a physics-informed U-Net instead of a numerical method to solve the heat equation. This approach enables the solution of general multiphysics problems with a single-physics numerical solver coupled with a neural network, overcoming the questions of accuracy and efficiency that are associated with interfacing multiphysics equations. By embedding the heat equation and its boundary conditions in the U-Net, we implement an unsupervised training methodology, which does not require the generation of ground-truth data. We test the proposed method with general 2-D coupled electromagnetic-thermal problems, demonstrating its accuracy and efficiency compared to standard finite-difference based alternatives.",
      "year": 2023,
      "venue": "IEEE Journal on Multiscale and Multiphysics Computational Techniques",
      "authors": [
        "Shutong Qi",
        "C. Sarris"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/6f387d2d7c0a3afa73c7204cdbc8e49eed536de8",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "460e48454b209e957b4942303507bf756c4a6a31",
      "title": "On the Feasibility of Specialized Ability Stealing for Large Language Code Models",
      "abstract": null,
      "year": 2023,
      "venue": "",
      "authors": [
        "Zongjie Li"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/460e48454b209e957b4942303507bf756c4a6a31",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4b99fb1fc7f60a16c4cc99a07d931fd79cf993e0",
      "title": "A Threshold Implementation-Based Neural Network Accelerator With Power and Electromagnetic Side-Channel Countermeasures",
      "abstract": "With the recent advancements in machine learning (ML) theory, a lot of energy-efficient neural network (NN) accelerators have been developed. However, their associated side-channel security vulnerabilities pose a major concern. There have been several proof-of-concept attacks demonstrating the extraction of their model parameters and input data. This work introduces a threshold implementation (TI) masking-based NN accelerator that secures model parameters and inputs against power and electromagnetic (EM) side-channel attacks. The 0.159 mm2 demonstration in 28 nm runs at 125 MHz at 0.95 V and limits the area and energy overhead to 64% and $5.5\\times $ , respectively, while demonstrating security even greater than 2M traces. The accelerator also secures model parameters through encryption and the inputs against horizontal power analysis (HPA) attacks.",
      "year": 2023,
      "venue": "IEEE Journal of Solid-State Circuits",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "Samuel H. Fuller",
        "A. Chandrakasan"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/4b99fb1fc7f60a16c4cc99a07d931fd79cf993e0",
      "pdf_url": "",
      "publication_date": "2023-01-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "6915e87a10df6a0e068c043d29048bd4eed9cdc3",
      "title": "Peek into the Black-Box: Interpretable Neural Network using SAT Equations in Side-Channel Analysis",
      "abstract": "Deep neural networks (DNN) have become a significant threat to the security of cryptographic implementations with regards to side-channel analysis (SCA), as they automatically combine the leakages without any preprocessing needed, leading to a more efficient attack. However, these DNNs for SCA remain mostly black-box algorithms that are very difficult to interpret. Benamira et al. recently proposed an interpretable neural network called Truth Table Deep Convolutional Neural Network (TT-DCNN), which is both expressive and easier to interpret. In particular, a TT-DCNN has a transparent inner structure that can entirely be transformed into SAT equations after training. In this work, we analyze the SAT equations extracted from a TT-DCNN when applied in SCA context, eventually obtaining the rules and decisions that the neural networks learned when retrieving the secret key from the cryptographic primitive (i.e., exact formula). As a result, we can pinpoint the critical rules that the neural network uses to locate the exact Points of Interest (PoIs). We validate our approach first on simulated traces for higher-order masking. However, applying TT-DCNN on real traces is not straightforward. We propose a method to adapt TT-DCNN for application on real SCA traces containing thousands of sample points. Experimental validation is performed on software-based ASCADv1 and hardware-based AES_HD_ext datasets. In addition, TT-DCNN is shown to be able to learn the exact countermeasure in a best-case setting.",
      "year": 2023,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Trevor Yap",
        "Adrien Benamira",
        "S. Bhasin",
        "Thomas Peyrin"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/6915e87a10df6a0e068c043d29048bd4eed9cdc3",
      "pdf_url": "https://tches.iacr.org/index.php/TCHES/article/download/10276/9724",
      "publication_date": "2023-03-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "dbe1361a697f46d111818f5e1d291f0e8ae858f3",
      "title": "Fast Bayesian Inversion of Airborne Electromagnetic Data Based on the Invertible Neural Network",
      "abstract": "The inversion of airborne electromagnetic (AEM) data suffers from severe nonuniqueness in the solution. Bayesian inference provides the means to estimate structural uncertainty with a rich suite of statistical information. However, conventional Bayesian methods are computationally demanding in nonlinear inversions, especially considering the huge volumes of observational data, and thus are not feasible in practice. In this study, we develop a fast Bayesian inversion operator based on the invertible neural network (INN) to fully explore the posterior distribution and quantitatively evaluate the model uncertainty. The INN uses a latent variable to capture the information loss during measurement and constructs bijective mappings between the AEM data and the resistivity model. We also introduce another noise variable into the INN to account for data uncertainties. Synthetic tests demonstrate that the INN can effectively recover the posterior distribution from a relatively small ensemble of predicted resistivity models whose AEM responses show a significant agreement with the true signal. We also apply the INN inversion operator to a field dataset and obtain results consistent with previous studies. The INN shows considerable adaptability to field observations and strong noise robustness. Meanwhile, the INN delivers the inversion result with posterior model distribution for 23 366 AEM time series in 20 s on a common PC. The inversion efficiency can be further improved for large datasets due to its natural parallelizability. The proposed INN method can support fast Bayesian inversion of AEM data and offer tremendous potential for near real-time uncertainty evaluation of underground structures.",
      "year": 2023,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "Sihong Wu",
        "Qinghua Huang",
        "Li Zhao"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/dbe1361a697f46d111818f5e1d291f0e8ae858f3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "68fdfe66da9e843882e8dd44e8b15a39ef539015",
      "title": "Nonlinear responses in a neural network under spatial electromagnetic radiation",
      "abstract": null,
      "year": 2023,
      "venue": "Physica A: Statistical Mechanics and its Applications",
      "authors": [
        "Yitong Guo",
        "Ying Xie",
        "Jun Ma"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/68fdfe66da9e843882e8dd44e8b15a39ef539015",
      "pdf_url": "",
      "publication_date": "2023-08-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d4d199d28451b3ee9edb1eef9412d20ffee9329d",
      "title": "False Claims against Model Ownership Resolution",
      "abstract": "Deep neural network (DNN) models are valuable intellectual property of model owners, constituting a competitive advantage. Therefore, it is crucial to develop techniques to protect against model theft. Model ownership resolution (MOR) is a class of techniques that can deter model theft. A MOR scheme enables an accuser to assert an ownership claim for a suspect model by presenting evidence, such as a watermark or fingerprint, to show that the suspect model was stolen or derived from a source model owned by the accuser. Most of the existing MOR schemes prioritize robustness against malicious suspects, ensuring that the accuser will win if the suspect model is indeed a stolen model. In this paper, we show that common MOR schemes in the literature are vulnerable to a different, equally important but insufficiently explored, robustness concern: a malicious accuser. We show how malicious accusers can successfully make false claims against independent suspect models that were not stolen. Our core idea is that a malicious accuser can deviate (without detection) from the specified MOR process by finding (transferable) adversarial examples that successfully serve as evidence against independent suspect models. To this end, we first generalize the procedures of common MOR schemes and show that, under this generalization, defending against false claims is as challenging as preventing (transferable) adversarial examples. Via systematic empirical evaluation, we show that our false claim attacks always succeed in the MOR schemes that follow our generalization, including in a real-world model: Amazon's Rekognition API.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Jian Liu",
        "Rui Zhang",
        "Sebastian Szyller",
        "Kui Ren",
        "Nirmal Asokan"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/d4d199d28451b3ee9edb1eef9412d20ffee9329d",
      "pdf_url": "http://arxiv.org/pdf/2304.06607",
      "publication_date": "2023-04-13",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "fa9d49f32440aff7417ce46419d1073239b58b5b",
      "title": "API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model",
      "abstract": "Extraction of Application Programming Interfaces (APIs) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., API recommendation). However, existing approaches are rule based and sequence labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, this article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERJE), an API entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, AERJE builds a multi-task architecture that extracts API entities and relations from unstructured text using dynamic prompts. We systematically evaluate AERJE on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that AERJE achieves high accuracy and discrimination ability in API entity-relation joint extraction, even with zero or few-shot fine-tuning.",
      "year": 2023,
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "authors": [
        "Qing Huang",
        "Yanbang Sun",
        "Zhenchang Xing",
        "Mingming Yu",
        "Xiwei Xu",
        "Qinghua Lu"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/fa9d49f32440aff7417ce46419d1073239b58b5b",
      "pdf_url": "http://arxiv.org/pdf/2301.03987",
      "publication_date": "2023-01-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "114c2f2cc2ab8a80a6229339a0c532ff4d59caed",
      "title": "Defending against Data-Free Model Extraction by Distributionally Robust Defensive Training",
      "abstract": null,
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Zhenyi Wang",
        "Li Shen",
        "Tongliang Liu",
        "Tiehang Duan",
        "Yanjun Zhu",
        "Dongling Zhan",
        "D. Doermann",
        "Mingchen Gao"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/114c2f2cc2ab8a80a6229339a0c532ff4d59caed",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "fb1f43004e7878c2da6ab86fb7427058a8ddedf7",
      "title": "B3: Backdoor Attacks against Black-box Machine Learning Models",
      "abstract": "Backdoor attacks aim to inject backdoors to victim machine learning models during training time, such that the backdoored model maintains the prediction power of the original model towards clean inputs and misbehaves towards backdoored inputs with the trigger. The reason for backdoor attacks is that resource-limited users usually download sophisticated models from model zoos or query the models from MLaaS rather than training a model from scratch, thus a malicious third party has a chance to provide a backdoored model. In general, the more precious the model provided (i.e., models trained on rare datasets), the more popular it is with users. In this article, from a malicious model provider perspective, we propose a black-box backdoor attack, named B3, where neither the rare victim model (including the model architecture, parameters, and hyperparameters) nor the training data is available to the adversary. To facilitate backdoor attacks in the black-box scenario, we design a cost-effective model extraction method that leverages a carefully constructed query dataset to steal the functionality of the victim model with a limited budget. As the trigger is key to successful backdoor attacks, we develop a novel trigger generation algorithm that intensifies the bond between the trigger and the targeted misclassification label through the neuron with the highest impact on the targeted label. Extensive experiments have been conducted on various simulated deep learning models and the commercial API of Alibaba Cloud Compute Service. We demonstrate that B3 has a high attack success rate and maintains high prediction accuracy for benign inputs. It is also shown that B3 is robust against state-of-the-art defense strategies against backdoor attacks, such as model pruning and NC.",
      "year": 2023,
      "venue": "ACM Transactions on Privacy and Security",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Wenbin Yang",
        "Huayang Huang",
        "Qian Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/fb1f43004e7878c2da6ab86fb7427058a8ddedf7",
      "pdf_url": "",
      "publication_date": "2023-06-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about backdoor, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "a3280f5d697c09946b371c8db82da514a4fa3d47",
      "title": "Efficient Nonprofiled Side-Channel Attack Using Multi-Output Classification Neural Network",
      "abstract": "Differential deep learning analysis (DDLA) is the first deep-learning-based nonprofiled side-channel attack (SCA) on embedded systems. However, DDLA requires many training processes to distinguish the correct key. In this letter, we introduce a nonprofiled SCA technique using multi-output classification to mitigate the aforementioned issue. Specifically, a multi-output multilayer perceptron and a multi-output convolutional neural network are introduced against various SCA protected schemes, such as masking, noise generation, and trace de-synchronization countermeasures. The experimental results on different power side channel datasets have clarified that our model performs the attack up to 9\u201330 times faster than DDLA in the case of masking and de-synchronization countermeasures, respectively. In addition, regarding combined masking and noise generation countermeasure, our proposed model achieves a higher success rate of at least 20% in the cases of the standard deviation equal to 1.0 and 1.5.",
      "year": 2023,
      "venue": "IEEE Embedded Systems Letters",
      "authors": [
        "Van-Phuc Hoang",
        "Ngoc-Tuan Do",
        "Van-Sang Doan"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/a3280f5d697c09946b371c8db82da514a4fa3d47",
      "pdf_url": "",
      "publication_date": "2023-09-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1d141cdb63254f935b084bfdab7d87d7b689ce97",
      "title": "BTI aging-based physical cloning attack on SRAM PUF and the countermeasure",
      "abstract": null,
      "year": 2023,
      "venue": "Analog Integrated Circuits and Signal Processing",
      "authors": [
        "Shengyu Duan",
        "G. Sai"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/1d141cdb63254f935b084bfdab7d87d7b689ce97",
      "pdf_url": "",
      "publication_date": "2023-06-29",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5c39cbdd1f77e12188b8691efdfd1e635c2ca037",
      "title": "Hardware-Software Co-design for Side-Channel Protected Neural Network Inference",
      "abstract": "Physical side-channel attacks are a major threat to stealing confidential data from devices. There has been a recent surge in such attacks on edge machine learning (ML) hardware to extract the model parameters. Consequently, there has also been work, although limited, on building corresponding defenses against such attacks. Current solutions take either fully software-or fully hardware-centric approaches, which are limited in performance and flexibility, respectively. In this paper, we propose the first hardware-software co-design solution for building side-channel-protected ML hardware. Our solution targets edge devices and addresses both performance and flexibility needs. To that end, we develop a secure RISCV-based coprocessor design that can execute a neural network implemented in C/C++. Our coprocessor uses masking to execute various neural network operations like weighted summations, activation functions, and output layer computation in a sidechannel secure fashion. We extend the original RV32I instruction set with custom instructions to control the masking gadgets inside the secure coprocessor. We further use the custom instructions to implement easy-to-use APIs that are exposed to the end-user as a shared library. Finally, we demonstrate the empirical sidechannel security of the design up to 1M traces.",
      "year": 2023,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Anuj Dubey",
        "Rosario Cammarota",
        "Avinash L. Varna",
        "Raghavan Kumar",
        "Aydin Aysu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/5c39cbdd1f77e12188b8691efdfd1e635c2ca037",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "39b748fe7bfe1b7c063339de1ec90f0e0351ffda",
      "title": "Defending against model extraction attacks with physical unclonable function",
      "abstract": null,
      "year": 2023,
      "venue": "Information Sciences",
      "authors": [
        "Dawei Li",
        "Di Liu",
        "Ying Guo",
        "Yangkun Ren",
        "Jieyu Su",
        "Jianwei Liu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/39b748fe7bfe1b7c063339de1ec90f0e0351ffda",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "eb3f0dbeca066945c971b79baa233e0d4468d99f",
      "title": "Electromagnetic Source Imaging With a Combination of Sparse Bayesian Learning and Deep Neural Network",
      "abstract": "Accurate reconstruction of the brain activities from electroencephalography and magnetoencephalography (E/MEG) remains a long-standing challenge for the intrinsic ill-posedness in the inverse problem. In this study, to address this issue, we propose a novel data-driven source imaging framework based on sparse Bayesian learning and deep neural network (SI-SBLNN). Within this framework, the variational inference in conventional algorithm, which is built upon sparse Bayesian learning, is compressed via constructing a straightforward mapping from measurements to latent sparseness encoding parameters using deep neural network. The network is trained with synthesized data derived from the probabilistic graphical model embedded in the conventional algorithm. We achieved a realization of this framework with the algorithm, source imaging based on spatio-temporal basis function (SI-STBF), as backbone. In numerical simulations, the proposed algorithm validated its availability for different head models and robustness against distinct intensities of the noise. Meanwhile, it acquired superior performance compared to SI-STBF and several benchmarks in a variety of source configurations. Additionally, in real data experiments, it obtained the concordant results with the prior studies.",
      "year": 2023,
      "venue": "IEEE transactions on neural systems and rehabilitation engineering",
      "authors": [
        "Jia-Jun Liang",
        "Z. Yu",
        "Z. Gu",
        "Yuanqing Li"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/eb3f0dbeca066945c971b79baa233e0d4468d99f",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/7333/4359219/10071956.pdf",
      "publication_date": "2023-03-15",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c65feaf2681a5676204a43101edff4d897d37ffb",
      "title": "Explanation-based data-free model extraction attacks",
      "abstract": null,
      "year": 2023,
      "venue": "World wide web (Bussum)",
      "authors": [
        "Anli Yan",
        "Ruitao Hou",
        "Hongyang Yan",
        "Xiaozhang Liu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/c65feaf2681a5676204a43101edff4d897d37ffb",
      "pdf_url": "",
      "publication_date": "2023-06-02",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "05ec4be7a3ec9dc2714f23f7921b31e3fece6c98",
      "title": "Dynamic Analysis and FPGA Implementation of a New Fractional-Order Hopfield Neural Network System under Electromagnetic Radiation",
      "abstract": "Fractional calculus research indicates that, within the field of neural networks, fractional-order systems more accurately simulate the temporal memory effects present in the human brain. Therefore, it is worthwhile to conduct an in-depth investigation into the complex dynamics of fractional-order neural networks compared to integer-order models. In this paper, we propose a magnetically controlled, memristor-based, fractional-order chaotic system under electromagnetic radiation, utilizing the Hopfield neural network (HNN) model with four neurons as the foundation. The proposed system is solved by using the Adomain decomposition method (ADM). Then, through dynamic simulations of the internal parameters of the system, rich dynamic behaviors are found, such as chaos, quasiperiodicity, direction-controllable multi-scroll, and the emergence of analogous symmetric dynamic behaviors in the system as the radiation parameters are altered, with the order remaining constant. Finally, we implement the proposed new fractional-order HNN system on a field-programmable gate array (FPGA). The experimental results show the feasibility of the theoretical analysis.",
      "year": 2023,
      "venue": "Biomimetics",
      "authors": [
        "Fei Yu",
        "Yue Lin",
        "Si Xu",
        "Wei Yao",
        "Yumba Musoya Gracia",
        "Shuo Cai"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/05ec4be7a3ec9dc2714f23f7921b31e3fece6c98",
      "pdf_url": "https://www.mdpi.com/2313-7673/8/8/559/pdf?version=1700538845",
      "publication_date": "2023-11-21",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "6f682e862a230feffedbbd918deccc2425be09fb",
      "title": "An End-to-End Neural Network for Complex Electromagnetic Simulations",
      "abstract": "Although many numerical methods can accurately solve time-domain electromagnetic (EM) simulation problems, such as finite-difference time-domain (FDTD), the computational demands are usually significant for complex scenarios. In this letter, we investigate the feasibility of applying deep learning technology to accurately solving EM forward simulations. Based on an end-to-end neural network framework, a convolutional neural network is used to extract features of scatters, and a long short-term memory neural network is used to predict EM distributions. To ensure the accuracy of the framework, especially when dealing with complicated phenomena, principal component analysis is employed to compress data sets before training. Numerical experiments show that the proposed scheme can predict EM field distributions efficiently and accurately for complex scenarios containing scatters of different materials, locations, geometrical shapes, and random numbers. The average relative mean square error is around 8. 05e\u22125 for scenarios with certain number of scatters and 2.25e\u22124 for random number of scatters respectively, which outperforms other neural network frameworks. Meanwhile, compared with FDTD, the time speedup is around 1528 times.",
      "year": 2023,
      "venue": "IEEE Antennas and Wireless Propagation Letters",
      "authors": [
        "Menglin Zhai",
        "Yaobo Chen",
        "Longting Xu",
        "Wen-Yan Yin"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/6f682e862a230feffedbbd918deccc2425be09fb",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "51ffae813f187ebfc64226a2914c33f5a2f5e4dd",
      "title": "Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service",
      "abstract": "Recent advances in vision-language pre-trained models (VLPs) have significantly increased visual understanding and cross-modal analysis capabilities. Companies have emerged to provide multi-modal Embedding as a Service (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amount of training data and resources for high-performance service. However, existing studies indicate that EaaS is vulnerable to model extraction attacks that induce great loss for the owners of VLPs. Protecting the intellectual property and commercial ownership of VLPs is increasingly crucial yet challenging. A major solution of watermarking model for EaaS implants a backdoor in the model by inserting verifiable trigger embeddings into texts, but it is only applicable for large language models and is unrealistic due to data and model privacy. In this paper, we propose a safe and robust backdoor-based embedding watermarking method for VLPs called VLPMarker. VLPMarker utilizes embedding orthogonal transformation to effectively inject triggers into the VLPs without interfering with the model parameters, which achieves high-quality copyright verification and minimal impact on model performance. To enhance the watermark robustness, we further propose a collaborative copyright verification strategy based on both backdoor trigger and embedding distribution, enhancing resilience against various attacks. We increase the watermark practicality via an out-of-distribution trigger selection approach, removing access to the model training data and thus making it possible for many real-world scenarios. Our extensive experiments on various datasets indicate that the proposed watermarking approach is effective and safe for verifying the copyright of VLPs for multi-modal EaaS and robust against model extraction attacks. Our code is available at https://github.com/Pter61/vlpmarker.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Yuanmin Tang",
        "Jing Yu",
        "Keke Gai",
        "Xiangyang Qu",
        "Yue Hu",
        "Gang Xiong",
        "Qi Wu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/51ffae813f187ebfc64226a2914c33f5a2f5e4dd",
      "pdf_url": "",
      "publication_date": "2023-11-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "51e6982bb24b76d018d96432e032703e7ea35ef4",
      "title": "AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against Decision Tree Models",
      "abstract": null,
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Abdullah \u00c7aglar \u00d6ks\u00fcz",
        "Anisa Halimi",
        "Erman Ayday"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/51e6982bb24b76d018d96432e032703e7ea35ef4",
      "pdf_url": "http://arxiv.org/pdf/2302.02162",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "8a04f36017f7a8864118ce801029c21972c6fda8",
      "title": "DNN-Alias: Deep Neural Network Protection Against Side-Channel Attacks via Layer Balancing",
      "abstract": "Extracting the architecture of layers of a given deep neural network (DNN) through hardware-based side channels allows adversaries to steal its intellectual property and even launch powerful adversarial attacks on the target system. In this work, we propose DNN-Alias, an obfuscation method for DNNs that forces all the layers in a given network to have similar execution traces, preventing attack models from differentiating between the layers. Towards this, DNN-Alias performs various layer-obfuscation operations, e.g., layer branching, layer deepening, etc, to alter the run-time traces while maintaining the functionality. DNN-Alias deploys an evolutionary algorithm to find the best combination of obfuscation operations in terms of maximizing the security level while maintaining a user-provided latency overhead budget. We demonstrate the effectiveness of our DNN-Alias technique by obfuscating the architecture of 700 randomly generated and obfuscated DNNs running on multiple Nvidia RTX 2080 TI GPU-based machines. Our experiments show that state-of-the-art side-channel architecture stealing attacks cannot extract the original DNN accurately. Moreover, we obfuscate the architecture of various DNNs, such as the VGG-11, VGG-13, ResNet-20, and ResNet-32 networks. Training the DNNs using the standard CIFAR10 dataset, we show that our DNN-Alias maintains the functionality of the original DNNs by preserving the original inference accuracy. Further, the experiments highlight that adversarial attack on obfuscated DNNs is unsuccessful.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Mahya Morid Ahmadi",
        "Lilas Alrahis",
        "O. Sinanoglu",
        "Muhammad Shafique"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/8a04f36017f7a8864118ce801029c21972c6fda8",
      "pdf_url": "http://arxiv.org/pdf/2303.06746",
      "publication_date": "2023-03-12",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "17ee2b47f17048fde21466fd3820b40c3ccab65b",
      "title": "OccPoIs: Points of Interest based on Neural Network's Key Recovery in Side-Channel Analysis through Occlusion",
      "abstract": null,
      "year": 2023,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Trevor Yap",
        "S. Bhasin",
        "S. Picek"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/17ee2b47f17048fde21466fd3820b40c3ccab65b",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "aa4824feda684fd8d1cedb5361574cc4cff35d75",
      "title": "LibSteal: Model Extraction Attack Towards Deep Learning Compilers by Reversing DNN Binary Library",
      "abstract": ": The need for Deep Learning (DL) based services has rapidly increased in the past years. As part of the trend, the privatization of Deep Neural Network (DNN) models has become increasingly popular. The authors give customers or service providers direct access to their created models and let them deploy models on devices or infrastructure out of the control of the authors. Meanwhile, the emergence of DL Compilers makes it possible to compile a DNN model into a lightweight binary for faster inference, which is attractive to many stakeholders. However, distilling the essence of a model into a binary that is free to be examined by untrusted parties creates a chance to leak essential information. With only DNN binary library, it is possible to extract neural network architecture using reverse engineering. In this paper, we present LibSteal . This framework can leak DNN architecture information by reversing the binary library generated from the DL Compiler, which is similar to or even equivalent to the original. The evaluation shows that LibSteal can efficiently steal the architecture information of victim DNN models. After training the extracted models with the same hyper-parameter, we can achieve accuracy comparable to that of the original models.",
      "year": 2023,
      "venue": "International Conference on Evaluation of Novel Approaches to Software Engineering",
      "authors": [
        "Jinquan Zhang",
        "Pei Wang",
        "Dinghao Wu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/aa4824feda684fd8d1cedb5361574cc4cff35d75",
      "pdf_url": "https://doi.org/10.5220/0011754900003464",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "4ab3d1254f877d8d234fe32b8d5e46862833237b",
      "title": "Neural Network Trajectory Tracking Control on Electromagnetic Suspension Systems",
      "abstract": "A new adaptive-like neural control strategy for motion reference trajectory tracking for a nonlinear electromagnetic suspension dynamic system is introduced. Artificial neural networks, differential flatness and sliding modes are strategically integrated in the presented adaptive neural network control design approach. The robustness and efficiency of the magnetic suspension control system on desired smooth position reference profile tracking can be improved in this fashion. A single levitation control parameter is tuned on-line from a neural adaptive perspective by using information of the reference trajectory tracking error signal only. The sliding mode discontinuous control action is approximated by a neural network-based adaptive continuous control function. Control design is firstly developed from theoretical modelling of the nonlinear physical system. Next, dependency on theoretical modelling of the nonlinear dynamic system is substantially reduced by integrating B-spline neural networks and sliding modes in the electromagnetic levitation control technique. On-line accurate estimation of uncertainty, unmeasured external disturbances and uncertain nonlinearities are conveniently evaded. The effective performance of the robust trajectory tracking levitation control approach is depicted for multiple simulation operating scenarios. The capability of active disturbance suppression is furthermore evidenced. The presented B-spline neural network trajectory tracking control design approach based on sliding modes and differential flatness can be extended to other controllable complex uncertain nonlinear dynamic systems where internal and external disturbances represent a relevant issue. Computer simulations and analytical results demonstrate the effective performance of the new adaptive neural control method.",
      "year": 2023,
      "venue": "Mathematics",
      "authors": [
        "F. Beltr\u00e1n-Carbajal",
        "H. Y\u00e1\u00f1ez-Badillo",
        "R. Tapia-Olvera",
        "J. Rosas-Caro",
        "C. Sotelo",
        "D. Sotelo"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/4ab3d1254f877d8d234fe32b8d5e46862833237b",
      "pdf_url": "https://www.mdpi.com/2227-7390/11/10/2272/pdf?version=1684071665",
      "publication_date": "2023-05-12",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "acfd50104bc7c8f0a6096ac1c48eaa3413cdb33c",
      "title": "Control of epileptic activities in a cortex network of multiple coupled neural populations under electromagnetic induction",
      "abstract": "Epilepsy is believed to be associated with the abnormal synchronous neuronal activity in the brain, which results from large groups or circuits of neurons. In this paper, we choose to focus on the temporal lobe epilepsy, and establish a cortex network of multiple coupled neural populations to explore the epileptic activities under electromagnetic induction. We demonstrate that the epileptic activities can be controlled and modulated by electromagnetic induction and coupling among regions. In certain regions, these two types of control are observed to show exactly reverse effects. The results show that the strong electromagnetic induction is conducive to eliminating the epileptic seizures. The coupling among regions has a conduction effect that the previous normal background activity of the region gives way to the epileptic discharge, owing to coupling with spike wave discharge regions. Overall, these results highlight the role of electromagnetic induction and coupling among the regions in controlling and modulating epileptic activities, and might provide novel insights into the treatments of epilepsy.",
      "year": 2023,
      "venue": "Applied Mathematics and Mechanics",
      "authors": [
        "Zhongkui Sun",
        "Yuanyuan Liu",
        "Xiaoli Yang",
        "Wei Xu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/acfd50104bc7c8f0a6096ac1c48eaa3413cdb33c",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10483-023-2969-9.pdf",
      "publication_date": "2023-03-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "faf8e2bc0cd3d33ca3c6fb80fc44d8832fa2cb1e",
      "title": "Multi-scroll Hopfield neural network under electromagnetic radiation and its brain-like coupling synchronization",
      "abstract": "Multi-scroll attractors have attracted attention because of their more complex topological structures and artificially controllable attractor structures. This paper proposes a new nonvolatile magnetic-controlled memristor and uses it to simulate the effect of membrane flux changes caused by neuronal exposure to electromagnetic radiation. A series of complex chaotic phenomena are found by plotting phase diagrams, bifurcation diagrams, attractor domains and 01 tests, including multi-scroll chaotic attractors controlled by memristors, symmetric bifurcation behavior, coexistence phenomena enhanced by initial offset. The mechanisms behind them are explained through equilibrium point analysis. A dual memristive HNN (MHNN) coupling synchronization model is proposed to simulate the synchronization between regions within the human brain. The Lyapunov function of the error is constructed to prove that this coupling synchronization scheme is ultimately bounded. The feasibility of this synchronization scheme is verified by establishing a Simulink model and conducting simulation experiments.",
      "year": 2023,
      "venue": "Frontiers of Physics",
      "authors": [
        "Sen Fu",
        "Xia Wang",
        "Haiyang Gu",
        "Xiaojing Cao",
        "Z. Yao"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/faf8e2bc0cd3d33ca3c6fb80fc44d8832fa2cb1e",
      "pdf_url": "https://www.frontiersin.org/articles/10.3389/fphy.2023.1252568/pdf",
      "publication_date": "2023-08-25",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "21ec2007ae077dcb72f13e295cefe9ca1727b42e",
      "title": "No Forking Way: Detecting Cloning Attacks on Intel SGX Applications",
      "abstract": "Forking attacks against TEEs like Intel SGX can be carried out either by rolling back the application to a previous state, or by cloning the application and by partitioning its inputs across the cloned instances. Current solutions to forking attacks require Trusted Third Parties (TTP) that are hard to find in real-world deployments. In the absence of a TTP, many TEE applications rely on monotonic counters to mitigate forking attacks based on rollbacks; however, they have no protection mechanism against forking attack based on cloning. In this paper, we analyze 72 SGX applications and show that approximately 20% of those are vulnerable to forking attacks based on cloning\u2014including those that rely on monotonic counters. To address this problem, we present CloneBuster, the first practical clone-detection mechanism for Intel SGX that does not rely on a TTP and, as such, can be used directly to protect existing applications. CloneBuster allows enclaves to (self-) detect whether another enclave with the same binary is running on the same platform. To do so, CloneBuster relies on a cache-based covert channel for enclaves to signal their presence to (and detect the presence of) clones on the same machine. We show that CloneBuster is robust despite a malicious OS, only incurs a marginal impact on the application performance, and adds approximately 800 LoC to the TCB. When used in conjunction with monotonic counters, CloneBuster allows applications to benefit from a comprehensive protection against forking attacks.",
      "year": 2023,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Samira Briongos",
        "Ghassan O. Karame",
        "Claudio Soriente",
        "Annika Wilde"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/21ec2007ae077dcb72f13e295cefe9ca1727b42e",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3627106.3627187",
      "publication_date": "2023-10-04",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "9809d3a7e1b49a659d86fbdf88b3e29f326d05e5",
      "title": "Power2Picture: Using Generative CNNs for Input Recovery of Neural Network Accelerators through Power Side-Channels on FPGAs",
      "abstract": "Artificial neural networks pervade almost all areas of today's life, being used for both simple image classification tasks as well as highly complex decision making in mission-critical tasks. This makes artificial neural networks an attractive target for attackers to recover the model architecture or user inputs and outputs through either classical software vulnerabilities or hardware side-channel and fault attacks. With increasing complexity of the models, smaller companies now often opt for pre-trained public models, which are then used with potentially sensitive inputs, for instance, in medical applications. In this work, we present a novel remote side-channel attack methodology to steal neural network inputs using generative convolutional neural networks. After measuring voltage fluctuations using on-chip sensors, we are able to recover the original inputs to image classifiers on different FPGA platforms. Our results prove the effectiveness of our attack, as we are able to recover inputs from networks running on different devices, with different datasets, and under different operating conditions.",
      "year": 2023,
      "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines",
      "authors": [
        "Lukas Huegle",
        "M. Gotthard",
        "Vincent Meyers",
        "Jonas Krautter",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9809d3a7e1b49a659d86fbdf88b3e29f326d05e5",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "steal neural network"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a273c40cc1f1d7096efe40d62fe28befa524245c",
      "title": "Exposing Model Theft: A Robust and Transferable Watermark for Thwarting Model Extraction Attacks",
      "abstract": "The increasing prevalence of Deep Neural Networks (DNNs) in cloud-based services has led to their widespread use through various APIs. However, recent studies reveal the susceptibility of these public APIs to model extraction attacks, where adversaries attempt to create a local duplicate of the private model using data and API-generated predictions. Existing defense methods often involve perturbing prediction distributions to hinder an attacker's training goals, inadvertently affecting API utility. In this study, we extend the concept of digital watermarking to protect DNNs' APIs. We suggest embedding a watermark into the safeguarded APIs; thus, any model attempting to copy will inherently carry the watermark, allowing the defender to verify any suspicious models. We propose a simple yet effective framework to increase watermark transferability. By requiring the model to memorize the preset watermarks in the final decision layers, we significantly enhance the transferability of watermarks. Comprehensive experiments show that our proposed framework not only successfully watermarks APIs but also maintains their utility.",
      "year": 2023,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Ruixiang Tang",
        "Hongye Jin",
        "Mengnan Du",
        "Curtis Wigington",
        "R. Jain",
        "Xia Hu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a273c40cc1f1d7096efe40d62fe28befa524245c",
      "pdf_url": "",
      "publication_date": "2023-10-21",
      "keywords_matched": [
        "model extraction",
        "model theft",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "8081dc3235a12cbc75815fb1313b9a9590628602",
      "title": "Side-channel analysis based on Siamese neural network",
      "abstract": null,
      "year": 2023,
      "venue": "Journal of Supercomputing",
      "authors": [
        "Di Li",
        "Lang Li",
        "Yu Ou"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/8081dc3235a12cbc75815fb1313b9a9590628602",
      "pdf_url": "",
      "publication_date": "2023-09-13",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "34bed407d65517ed2c8b98bab3a33da175677c59",
      "title": "A Plot is Worth a Thousand Words: Model Information Stealing Attacks via Scientific Plots",
      "abstract": "Building advanced machine learning (ML) models requires expert knowledge and many trials to discover the best architecture and hyperparameter settings. Previous work demonstrates that model information can be leveraged to assist other attacks, such as membership inference, generating adversarial examples. Therefore, such information, e.g., hyperparameters, should be kept confidential. It is well known that an adversary can leverage a target ML model's output to steal the model's information. In this paper, we discover a new side channel for model information stealing attacks, i.e., models' scientific plots which are extensively used to demonstrate model performance and are easily accessible. Our attack is simple and straightforward. We leverage the shadow model training techniques to generate training data for the attack model which is essentially an image classifier. Extensive evaluation on three benchmark datasets shows that our proposed attack can effectively infer the architecture/hyperparameters of image classifiers based on convolutional neural network (CNN) given the scientific plot generated from it. We also reveal that the attack's success is mainly caused by the shape of the scientific plots, and further demonstrate that the attacks are robust in various scenarios. Given the simplicity and effectiveness of the attack method, our study indicates scientific plots indeed constitute a valid side channel for model information stealing attacks. To mitigate the attacks, we propose several defense mechanisms that can reduce the original attacks' accuracy while maintaining the plot utility. However, such defenses can still be bypassed by adaptive attacks.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Boyang Zhang",
        "Xinlei He",
        "Yun Shen",
        "Tianhao Wang",
        "Yang Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/34bed407d65517ed2c8b98bab3a33da175677c59",
      "pdf_url": "http://arxiv.org/pdf/2302.11982",
      "publication_date": "2023-02-23",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "8e7b4e85992bc24fafe6baedb901a8d687cad2fa",
      "title": "DNN Model Theft Through Trojan Side-Channel on Edge FPGA Accelerator",
      "abstract": null,
      "year": 2023,
      "venue": "International Workshop on Applied Reconfigurable Computing",
      "authors": [
        "Srivatsan Chandrasekar",
        "Siew-Kei Lam",
        "S. Thambipillai"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/8e7b4e85992bc24fafe6baedb901a8d687cad2fa",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0fa016157057c5203793d328430a7d86ababd7cd",
      "title": "Voltage Scaling-Agnostic Counteraction of Side-Channel Neural Net Reverse Engineering via Machine Learning Compensation and Multi-Level Shuffling",
      "abstract": "This work proposes a voltage scaling-agnostic counteraction against neural network weight reverse engineering via side-channel attacks. Multi-level shuffling and machine learning-based dual power compensation are introduced. State-of-the-art protection ($\\gt200\\cdot 10^{6}$ MTD) is achieved at low power overhead (1.76$\\times $) and zero latency overhead.",
      "year": 2023,
      "venue": "2023 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)",
      "authors": [
        "Qiang Fang",
        "Longyang Lin",
        "Hui Zhang",
        "Tianqi Wang",
        "Massimo Alioto"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/0fa016157057c5203793d328430a7d86ababd7cd",
      "pdf_url": "",
      "publication_date": "2023-06-11",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "32a1e6315ca47a410bdfe2577bd605cf80f134b1",
      "title": "RemovalNet: DNN Fingerprint Removal Attacks",
      "abstract": "With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this article, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named <sc>RemovalNet</sc>, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model's general semantic knowledge to maintain the surrogate model's performance. We conduct extensive experiments to evaluate the <italic>fidelity</italic>, <italic>effectiveness</italic>, and <italic>efficiency</italic> of the <sc>RemovalNet</sc> against four advanced defense methods on six metrics. The empirical results demonstrate that (1) the <sc>RemovalNet</sc> is <italic>effective</italic>. After our DNN fingerprint removal attack, the model distance between the target and surrogate models is <inline-formula><tex-math notation=\"LaTeX\">$\\times 100$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>\u00d7</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"qin-ieq1-3315064.gif\"/></alternatives></inline-formula> times higher than that of the baseline attacks, (2) the <sc>RemovalNet</sc> is <italic>efficient</italic>. It uses only 0.2% (400 samples) of the substitute dataset and 1,000 iterations to conduct our attack. Besides, compared with advanced model stealing attacks, the <sc>RemovalNet</sc> saves nearly 85% of computational resources at most, (3) the <sc>RemovalNet</sc> achieves high <italic>fidelity</italic> that the created surrogate model maintains high accuracy after the DNN fingerprint removal process.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hongwei Yao",
        "Zhengguang Li",
        "Kunzhe Huang",
        "Jian Lou",
        "Zhan Qin",
        "Kui Ren"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/32a1e6315ca47a410bdfe2577bd605cf80f134b1",
      "pdf_url": "http://arxiv.org/pdf/2308.12319",
      "publication_date": "2023-08-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "252cc0c47b76f804a2e839773a90e7a389289695",
      "title": "SNATCH: Stealing Neural Network Architecture from ML Accelerator in Intelligent Sensors",
      "abstract": "The use of Machine Learning (ML) models executing on ML Accelerators (MLA) in Intelligent sensors for feature extraction has garnered substantial interest. The Neural Network (NN) architecture implemented of MLA are intellectual property for the vendors. Along with improved power-efficiency and reduced bandwidth, the hardware based ML models embedded in the sensor also provides additional security against cyber-attacks on the ML. In this paper, we introduce an attack referred as SNATCH which uses a profiling-based side channel attack (SCA) that aims to steal the NN architecture executing on a digital MLA (Deep Learning Processing Unit (DPU) IP by Xilinx). We use electromagnetic side channel leakage from a clone device to create a profiler and then attack the victim's device to steal the NN architecture. Stealing the ML model undermines the intellectual property rights of the vendors of a sensor. Further, it also allows an adversary to mount critical Denial of Service and misuse attack.",
      "year": 2023,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Sudarshan Sharma",
        "U. Kamal",
        "Jianming Tong",
        "Tushar Krishna",
        "S. Mukhopadhyay"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/252cc0c47b76f804a2e839773a90e7a389289695",
      "pdf_url": "",
      "publication_date": "2023-10-29",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0005c9691c8c299476d201d0a5a3c86b49593fac",
      "title": "High-frequency Matters: An Overwriting Attack and defense for Image-processing Neural Network Watermarking",
      "abstract": "In recent years, there has been significant advancement in the field of model watermarking techniques. However, the protection of image-processing neural networks remains a challenge, with only a limited number of methods being developed. The objective of these techniques is to embed a watermark in the output images of the target generative network, so that the watermark signal can be detected in the output of a surrogate model obtained through model extraction attacks. This promising technique, however, has certain limits. Analysis of the frequency domain reveals that the watermark signal is mainly concealed in the high-frequency components of the output. Thus, we propose an overwriting attack that involves forging another watermark in the output of the generative network. The experimental results demonstrate the efficacy of this attack in sabotaging existing watermarking schemes for image-processing networks, with an almost 100% success rate. To counter this attack, we devise an adversarial framework for the watermarking network. The framework incorporates a specially designed adversarial training step, where the watermarking network is trained to defend against the overwriting network, thereby enhancing its robustness. Additionally, we observe an overfitting phenomenon in the existing watermarking method, which can render it ineffective. To address this issue, we modify the training process to eliminate the overfitting problem.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Chi Liu",
        "Shui Yu",
        "Wanlei Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0005c9691c8c299476d201d0a5a3c86b49593fac",
      "pdf_url": "http://arxiv.org/pdf/2302.08637",
      "publication_date": "2023-02-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "598acab0fdbf2a4c77e05e953498521a1a9f208f",
      "title": "Elevating Defenses: Bridging Adversarial Training and Watermarking for Model Resilience",
      "abstract": "Machine learning models are being used in an increasing number of critical applications; thus, securing their integrity and ownership is critical. Recent studies observed that adversarial training and watermarking have a conflicting interaction. This work introduces a novel framework to integrate adversarial training with watermarking techniques to fortify against evasion attacks and provide confident model verification in case of intellectual property theft. We use adversarial training together with adversarial watermarks to train a robust watermarked model. The key intuition is to use a higher perturbation budget to generate adversarial watermarks compared to the budget used for adversarial training, thus avoiding conflict. We use the MNIST and Fashion-MNIST datasets to evaluate our proposed technique on various model stealing attacks. The results obtained consistently outperform the existing baseline in terms of robustness performance and further prove the resilience of this defense against pruning and fine-tuning removal attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Janvi Thakkar",
        "Giulio Zizzo",
        "S. Maffeis"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/598acab0fdbf2a4c77e05e953498521a1a9f208f",
      "pdf_url": "",
      "publication_date": "2023-12-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "725b22aeeb00baa7f9b4aed0f12f5183067ec1d8",
      "title": "A Fast Prediction Method for the Electromagnetic Response of the LTE-R System Based on a PSO-BP Cascade Neural Network Model",
      "abstract": "In this paper, a fast prediction model of the electromagnetic response of the LTE-R (Long Term Evolution for Railway) communication system based on a cascade neural network is developed to quickly analyze the impact of pantograph arcing on the LTE-R system during vehicle operation. In order to obtain the coupling disturbance level of the LTE-R antenna port, this model uses the cascade neural network based on the PSO-BP (Particle Swarm Optimization of Back Propagation Neural Network) algorithm to quickly solve the coupling coefficient of the pantograph arcing measurement probe and the antenna port. A two-stage BP neural network model is used to train both the simulation data and measurement data, and the results are validated by field tests. Finally, the antenna port coupling interference is added to the LTE-R system to analyze the impact of pantograph arcing on the quality of communication transmission. Analysis and tests demonstrate that pantograph arcing can lead to an increase in the BLER of LTE-R systems at 400 MHz, affecting system performance and transmission efficiency.",
      "year": 2023,
      "venue": "Applied Sciences",
      "authors": [
        "Xiao He",
        "Y. Wen",
        "Dan Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/725b22aeeb00baa7f9b4aed0f12f5183067ec1d8",
      "pdf_url": "https://www.mdpi.com/2076-3417/13/11/6640/pdf?version=1685446251",
      "publication_date": "2023-05-30",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
      "title": "Safe and Robust Watermark Injection with a Single OoD Image",
      "abstract": "Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. The independence of training data makes it agnostic to third-party promises of IP security. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Shuyang Yu",
        "Junyuan Hong",
        "Haobo Zhang",
        "Haotao Wang",
        "Zhangyang Wang",
        "Jiayu Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
      "pdf_url": "https://arxiv.org/pdf/2309.01786",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "8b8c962da13cbca14d510b3359b42533291ad853",
      "title": "APGP: Accuracy-Preserving Generative Perturbation for Defending Against Model Cloning Attacks",
      "abstract": "Well-trained Deep Neural Networks (DNNs) are valuable intellectual properties. Recent studies show that adversaries only with black-box query access can steal the functionality of DNNs by using knowledge distillation (KD) techniques. In this paper, we propose a novel formulation to defend against model cloning attacks. Then we implement our defense as a plug-and-play generative perturbation model, dubbed as Accuracy-Preserving Generative Perturbation (APGP). Our method is the first to effectively defend against KD-based model cloning without damaging model accuracy. Numerous experiments demonstrate the effectiveness of our defense across different datasets and DNN model cloning attacks, and the advances compared to existing methods.",
      "year": 2023,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Anda Cheng",
        "Jian Cheng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8b8c962da13cbca14d510b3359b42533291ad853",
      "pdf_url": "https://doi.org/10.1109/icassp49357.2023.10094956",
      "publication_date": "2023-06-04",
      "keywords_matched": [
        "cloning attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "6587fab69696a41a5186226f87bf0f4552f794a6",
      "title": "AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against White-Box Models",
      "abstract": null,
      "year": 2023,
      "venue": "",
      "authors": [
        "Abdullah Caglar Oksuz",
        "Anisa Halimi",
        "Erman Ayday"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/6587fab69696a41a5186226f87bf0f4552f794a6",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e49363bd733bed0016e57170c31c7b3dc4dc1714",
      "title": "NNLeak: An AI-Oriented DNN Model Extraction Attack through Multi-Stage Side Channel Analysis",
      "abstract": "Side channel analysis (SCA) attacks have become emerging threats to AI algorithms and deep neural network (DNN) models. However, most existing SCA attacks focus on extracting models deployed on embedded devices, such as microcontrollers. Accurate SCA attacks on extracting DNN models deployed on AI accelerators are largely missing, leaving researchers with an (improper) assumption that DNNs on AI accelerators may be immune to SCA attacks due to their complexity. In this paper, we propose a novel method, namely NNLeak to extract complete DNN models on FPGA-based AI accelerators. To achieve this goal, NNLeak first exploits simple power analysis (SPA) to identify model architecture. Then a multi-stage correlation power analysis (CPA) is designed to recover model weights accurately. Finally, NNLeak determines the activation functions of DNN models through an AI-oriented classifier. The efficacy of NNLeak is validated on FPGA implementations of two DNN models, including multilayer perceptron (MLP) and LeNet. Experimental results show that NNLeak can successfully extract complete DNN models within 2000 power traces.",
      "year": 2023,
      "venue": "Asian Hardware-Oriented Security and Trust Symposium",
      "authors": [
        "Ya Gao",
        "Haocheng Ma",
        "Mingkai Yan",
        "Jiaji He",
        "Yiqiang Zhao",
        "Yier Jin"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/e49363bd733bed0016e57170c31c7b3dc4dc1714",
      "pdf_url": "",
      "publication_date": "2023-12-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "42a2e32a77ffa1ea1671c1543ee0a71164375305",
      "title": "Exploring and Exploiting Data-Free Model Stealing",
      "abstract": null,
      "year": 2023,
      "venue": "ECML/PKDD",
      "authors": [
        "Chi Hong",
        "Jiyue Huang",
        "Robert Birke",
        "Lydia Y. Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/42a2e32a77ffa1ea1671c1543ee0a71164375305",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4bda132d5df256a41326ca03d3b569bfe926734f",
      "title": "Metric Learning-Based Neural Network Model for Electromagnetic Compatibility Fault Diagnosis: An Application Study",
      "abstract": "With the growing prevalence of electronic equipment and the increasing severity of the electromagnetic environment, the likelihood of electromagnetic compatibility failures is on the rise. As a result, the difficulty of diagnosing EMC faults is also increasing. However, by employing neural networks in deep learning for EMC fault diagnosis, we can simplify and streamline the process of feature extraction and similarity analysis. Compared to traditional artificial feature extraction methods, neural networks can learn to measure the similarity between features more efficiently, resulting in more accurate diagnoses. To train the model, we obtain response data from each port of the electronic equipment system in a high radio frequency environment and pair it with the corresponding equipment fault status. However, due to the limited availability of labeled data, conventional neural networks are susceptible to overfitting. Therefore, we use a neural network model that is well-suited for few-shot learning, which is based on a metric learning approach. This approach enables the model to learn from a small amount of labeled data, making it more effective in diagnosing EMC faults.",
      "year": 2023,
      "venue": "2023 5th International Conference on Electronic Engineering and Informatics (EEI)",
      "authors": [
        "Xiangguo Shen",
        "Zhongyuan Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4bda132d5df256a41326ca03d3b569bfe926734f",
      "pdf_url": "",
      "publication_date": "2023-06-30",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "897571a2c302fb6f243f91b35caa0a2159b255c8",
      "title": "Intelligent electromagnetic mapping via physics driven and neural networks on frequency selective surfaces",
      "abstract": "The high mapping efficiency between various structures and electromagnetic (EM) properties of frequency selective surfaces (FSSs) is the state-of-the-art in the EM community. The most straightforward approaches for beam analysis depend on measurements and conventional EM calculation methods, which are inefficient and time-consuming. Equivalent circuit models (ECMs) with excellent intuitiveness and simplicity have been put forward extensively. Despite several applications, bottlenecks in ECM still exist, i.e. the application scope is restricted to narrow bands and specific structures, which is triggered by the ignorance of EM nonlinear coupling. In this study, for the first time, a lightweight physical model based on neural network (ECM-NN) is proposed , which exhibits great physical interpretability and spatial generalization abilities. The nonlinear mapping relationship between structure and beam behavior is interpreted by corresponding simulations. Specifically, two deep parametric factors obtained by multi-layer perceptron networks are introduced to serve as the core of lightweight strategies and compensate for the absence of nonlinearity. Experimental results of single square loop (SL) and double SL indicate that compared with related works, better agreements of the frequency responses and resonant frequencies are achieved with ECM-NN in broadband (0\u201330 GHz) as well as oblique incident angles (0\u00b0\u201360\u00b0). The average accuracy of the mapping is higher than 98.6%. The findings of this study provide a novel strategy for further studies of complex FSSs.",
      "year": 2023,
      "venue": "Journal of Physics D: Applied Physics",
      "authors": [
        "Wuxia Miao",
        "Lamei Zhang",
        "B. Zou",
        "Ye Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/897571a2c302fb6f243f91b35caa0a2159b255c8",
      "pdf_url": "",
      "publication_date": "2023-03-07",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2a87d871e849e270cda90057b709b35977f3d7d1",
      "title": "Determination of Electromagnetic properties of Concrete Using Microwave Non-Destructive testing techniques and Arti\ufffdcial neural networks",
      "abstract": null,
      "year": 2023,
      "venue": "",
      "authors": [
        "Madhuri Guntamukkala",
        "G. Vidya",
        "Venkat Lute",
        "Parul Mathur"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/2a87d871e849e270cda90057b709b35977f3d7d1",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9cdce5c92985967c2bd34aacdd46b1490bf8565c",
      "title": "BarraCUDA: Edge GPUs do Leak DNN Weights",
      "abstract": "Over the last decade, applications of neural networks (NNs) have spread to various aspects of our lives. A large number of companies base their businesses on building products that use neural networks for tasks such as face recognition, machine translation, and self-driving cars. Much of the intellectual property underpinning these products is encoded in the exact parameters of the neural networks. Consequently, protecting these is of utmost priority to businesses. At the same time, many of these products need to operate under a strong threat model, in which the adversary has unfettered physical control of the product. In this work, we present BarraCUDA, a novel attack on general purpose Graphic Processing Units (GPUs) that can extract parameters of neural networks running on the popular Nvidia Jetson Nano device. BarraCUDA uses correlation electromagnetic analysis to recover parameters of real-world convolutional neural networks.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "P\u00e9ter Horv\u00e1th",
        "Lukasz Chmielewski",
        "L\u00e9o Weissbart",
        "L. Batina",
        "Y. Yarom"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/9cdce5c92985967c2bd34aacdd46b1490bf8565c",
      "pdf_url": "",
      "publication_date": "2023-12-12",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1cd0dec5ecb256c132c72475856536e9d3012220",
      "title": "Research on Anti-stealing Algorithm of Distributed Photovoltaic Power System Based on Power Big Data",
      "abstract": "DPV (Distributed Photovoltaic Power System) is an important development direction in the field of renewable energy, which helps to reduce greenhouse gas emissions, improve the sustainability of energy supply and provide consumers with more energy choices. Power BD (Big Data) technology, as a BD technology oriented by the power industry under the background of the new era, is widely used in anti-theft investigation and work, which has good accuracy, convenience and scientificity, and is of great significance to anti-theft investigation. In this paper, a DPV anti-stealing algorithm is established by BD mining. Based on RF (Random forest), these features are used to identify the behavior of stealing electricity and find the features related to stealing electricity. An anti-stealing model is built by LR (Logical regression) algorithm, and the suspected stealing degree of users is calculated. The research results show that RF algorithm performs better on data sets, with the highest accuracy and recall of 0.84 and 0.89 respectively, and F1 also reaches 0.81. The results show that the anti-stealing prediction method based on electric power BD proposed in this paper is efficient and feasible in identifying suspected stealing users.",
      "year": 2023,
      "venue": "2023 International Conference on Internet of Things, Robotics and Distributed Computing (ICIRDC)",
      "authors": [
        "Junjie Zheng"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1cd0dec5ecb256c132c72475856536e9d3012220",
      "pdf_url": "",
      "publication_date": "2023-12-29",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "cdc97244b92d836d759776aced5fba4f4a976e66",
      "title": "Enabling DVFS Side-Channel Attacks for Neural Network Fingerprinting in Edge Inference Services",
      "abstract": "The Inference-as-a-Service (IaaS) delivery model provides users access to pre-trained deep neural networks while safeguarding network code and weights. However, IaaS is not immune to security threats, like side-channel attacks (SCAs), that exploit unintended information leakage from the physical characteristics of the target device. Exposure to such threats grows when IaaS is deployed on distributed computing nodes at the edge. This work identifies a potential vulnerability of low-power CPUs that facilitates stealing the deep neural network architecture without physical access to the hardware or interference with the execution flow. Our approach relies on a Dynamic Voltage and Frequency Scaling (DVFS) side-channel attack, which monitors the CPU frequency state during the inference stages. Specifically, we introduce a dedicated load-testing methodology that imprints distinguishable signatures of the network on the frequency traces. A machine learning classifier is then used to infer the victim architecture. Experimental results on two commercial ARM Cortex-A CPUs, the A72 and A57, demonstrate the attack can identify the target architecture from a pool of 12 convolutional neural networks with an average accuracy of 98.7% and 92.4%",
      "year": 2023,
      "venue": "International Symposium on Low Power Electronics and Design",
      "authors": [
        "Erich Malan",
        "Valentino Peluso",
        "A. Calimera",
        "Enrico Macii"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/cdc97244b92d836d759776aced5fba4f4a976e66",
      "pdf_url": "",
      "publication_date": "2023-08-07",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "22281ae0f550dcb5b8d6c2f1768780f33f4b3e6b",
      "title": "An Analog Side-Channel Attack on a High-Speed Asynchronous SAR ADC Using Dual Neural Network Technique",
      "abstract": "SUMMARY This brief presents a side-channel attack (SCA) technique on a high-speed asynchronous successive approximation register (SAR) analog-to-digitalconverter(ADC).Theproposeddualneuralnetworkbasedonmultiplenoisewaveformsseparatelydisclosessignandabsolutevalue informationofinputsignalswhicharehiddenbythedifferentialstructureandhigh-speedasynchronousoperation.ThetargetSARADCandon-chip noisemonitorsaredesignedonasingleprototypechipforSCAdemon-stration.Fabricatedin40nm,theexperimentalresultsshowtheproposed attack on the asynchronous SAR ADC successfully restores the input data with a competitive accuracy within 300mV rms error.",
      "year": 2023,
      "venue": "IEICE transactions on electronics",
      "authors": [
        "R. Takahashi",
        "Takuji Miki",
        "M. Nagata"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/22281ae0f550dcb5b8d6c2f1768780f33f4b3e6b",
      "pdf_url": "https://www.jstage.jst.go.jp/article/transele/E106.C/10/E106.C_2022CTS0002/_pdf",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0d0a3261b710c43bb443598b407694922a475ba2",
      "title": "A two-stage model extraction attack on GANs with a small collected dataset",
      "abstract": null,
      "year": 2023,
      "venue": "Computers & security",
      "authors": [
        "Hui Sun",
        "Tianqing Zhu",
        "Wenhan Chang",
        "Wanlei Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/0d0a3261b710c43bb443598b407694922a475ba2",
      "pdf_url": "",
      "publication_date": "2023-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "12129493fcc85044c80a3d46f5ef73d3bb4ea6f9",
      "title": "Query-efficient model extraction for text classification model in a hard label setting",
      "abstract": null,
      "year": 2023,
      "venue": "Journal of King Saud University: Computer and Information Sciences",
      "authors": [
        "Hao Peng",
        "Shixin Guo",
        "Dandan Zhao",
        "Yiming Wu",
        "Jianxiong Han",
        "Zhe Wang",
        "S. Ji",
        "Ming-Hong Zhong"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/12129493fcc85044c80a3d46f5ef73d3bb4ea6f9",
      "pdf_url": "https://doi.org/10.1016/j.jksuci.2023.02.019",
      "publication_date": "2023-03-01",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "8796b36ed1a74a1c9fdafe2409c981102983653f",
      "title": "On Function-Coupled Watermarks for Deep Neural Networks",
      "abstract": "Well-performed deep neural networks (DNNs) generally require massive labeled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers can claim IP ownership by retrieving their embedded watermarks. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning, model pruning, and model extraction. In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model\u2019s performance on normal inputs. Specifically, on one hand, we sample inputs from the original training dataset and fuse them as watermark images. On the other hand, we randomly mask model weights during training to distribute the watermark information in the network. Our method can successfully defend against common watermark removal attacks, watermark ambiguity attacks, and existing widely used backdoor detection methods, outperforming existing solutions as demonstrated by evaluation results on various benchmarks. Our code is available at: https://github.com/cure-lab/Function-Coupled-Watermark.",
      "year": 2023,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Xiangyu Wen",
        "Yu Li",
        "Weizhen Jiang",
        "Qian-Lan Xu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8796b36ed1a74a1c9fdafe2409c981102983653f",
      "pdf_url": "https://doi.org/10.1109/jetcas.2024.3476386",
      "publication_date": "2023-02-08",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "9b1d486d20cb915ee42d19e556c3587298eec5d6",
      "title": "Model Stealing Attacks On FHE-based Privacy-Preserving Machine Learning through Adversarial Examples",
      "abstract": null,
      "year": 2023,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Bhuvnesh Chaturvedi",
        "Anirban Chakraborty",
        "Ayantika Chatterjee",
        "Debdeep Mukhopadhyay"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9b1d486d20cb915ee42d19e556c3587298eec5d6",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "8b50f09560108d2f79381e2f5ff9146a1079a29c",
      "title": "Deep-neural-network-based Electromagnetic Analysis and Optimal Design of Fractional-slot Brushless DC Motor for High Torque Robot Joints",
      "abstract": "Fractional-slot brushless DC motors (FS-BLDCMs) have the advantages of high torque density and low cogging torque for robot joints. However, finite element analysis (FEA) of the FS-BLDCMs causes time consumption, which obstructs the progress on finding optimal electromagnetic characteristics of the FS-BLDCMs. This paper presents a novel design method to improve the FS-BLDCM motor characteristics and improve the computation efficiency by the deep neural network (DNN). The FS-BLDCM motor performance is optimized by the genetic algorithm and validated by finite element analysis. The computation time between the finite element analysis (FEA) and the deep neural network (DNN) is compared. The result shows the efficiency of the deep neural network.",
      "year": 2023,
      "venue": "2023 3rd International Conference on Electrical Engineering and Mechatronics Technology (ICEEMT)",
      "authors": [
        "Anguo Liu",
        "Fei Meng",
        "Hengzai Hu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8b50f09560108d2f79381e2f5ff9146a1079a29c",
      "pdf_url": "",
      "publication_date": "2023-07-21",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ee761742bda631b619a728ba9819dbddc1922a3c",
      "title": "A Study on Electromagnetic Field Analysis Considering Geometry Variation Using Physics-Informed Neural Network",
      "abstract": "The problem with finite element analysis is that each analysis is run separately, so any variation in geometry requires a new analysis. When performing finite element analysis for generating fault diagnosis data, the frequent geometry variations that occur to consider numerous fault levels lead to a long time. In this paper, to solve these problems, an analysis using a physics-informed neural network is proposed to solve the differential equation that was solved by a numerical method using a neural network. Transfer learning is used, which enables fast analysis based on the analysis experience before the geometry is varied. In addition, nonlinear magnetic material characteristics and electronic systems in the saturated region are analyzed to evaluate whether the physics-informed neural network can cope with some numerical analysis problem.",
      "year": 2023,
      "venue": "International Conference on Electrical Machines and Systems",
      "authors": [
        "Ji-hoon Han",
        "Eui-Jin Choi",
        "Sun-Ki Hong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ee761742bda631b619a728ba9819dbddc1922a3c",
      "pdf_url": "",
      "publication_date": "2023-11-05",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d79a93189a8acc8306504ed20a48fec9934897d9",
      "title": "Electromagnetic Field Model of Tubular Permanent Magnet Synchronous Linear Motor Based on Deep Transfer Neural Network",
      "abstract": "During high-speed operation, tubular permanent magnet linear motors with slotted stators often experience large fluctuations in thrust, overheating of the coils and irreversible demagnetization of the permanent magnets. For the first two cases, this paper proposes a deep transfer neural network (DTNN)-based electromagnetic field model for TPMLM, the specific implementation steps of which include: (1) Using different geometric parameters of TPMLM as input and average thrust, thrust fluctuation and coil copper loss as output, finite element analysis (FEA) and analytical method (AM) are used to provide the electromagnetic parameter dataset of the motor respectively, and these two datasets are used as the source and target domains for transfer learning; (2) Based on the features of the sample dataset, the source tasks corresponding to the source domain are pre-trained using DTNN, and the target tasks corresponding to the target domain are fitted by fine-tuning the model parameters. Then, in order to verify the accuracy of the proposed model, this paper compares the output prediction results with some non-parametric models, such as Random Forest (RF), Support Vector Machine (SVM), and Deep Neural Network (DNN), by dividing the target domain dataset with different scales. The results show that the best prediction results are obtained from the DTNN model, which fully combines the accuracy of FEA and the efficiency of AM, and shows better generalization ability in the case of insufficient real training data.",
      "year": 2023,
      "venue": "ACM Cloud and Autonomic Computing Conference",
      "authors": [
        "Kai Zhu",
        "Tao Wu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d79a93189a8acc8306504ed20a48fec9934897d9",
      "pdf_url": "",
      "publication_date": "2023-11-17",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7899de0fae6d036acce5ef82d6e6937008dd1758",
      "title": "Enhancing Neural Network Performance for Problems in the Physical Sciences: Applications to Electromagnetic Signal Source Localization",
      "abstract": "Neural networks are widely employed in many domains such as graph analysis, image recognition, and social networks. They are less commonly used in the physical sciences, yet the physical sciences provide many rich problems which can benefit from the careful application of neural networks. Problems in the physical sciences have different challenges when compared to problems in classic neural network domains. One such problem is the scarcity of real-world data in many physical problems. In our signal processing application, the real-world data is time-consuming and expensive to collect. In this paper, we detail methods for improving neural network performance in a small-data domain for determining source localization of electromagnetic signals. These methods can also be leveraged in other real-world problems. Our key findings include five data-related results we leveraged to achieve an RMSE error of 0.78km on our source localization efforts.",
      "year": 2023,
      "venue": "Asilomar Conference on Signals, Systems and Computers",
      "authors": [
        "Maria Barger",
        "Evan C. Witz",
        "Randy C. Paffenroth"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7899de0fae6d036acce5ef82d6e6937008dd1758",
      "pdf_url": "",
      "publication_date": "2023-10-29",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "de9f8fe71c37741aa0ff999ca97d71cfabc37b52",
      "title": "Robot Mimicry Attack on Keystroke-Dynamics User Identification and Authentication System",
      "abstract": "Future robots will be very advanced with high flexibility and accurate control performance. They will have the ability to mimic human behaviours or even perform better, which raises the significant risk of robot attack. In this work, we study the robot mimic attack on the current keystroke-dynamic user authentication system. Specifically, we proposed a robot mimicry attack framework for keystroke-dynamics systems. We collected keyboard logging data and acoustical signal data from real users and extracted the timing pattern of keystrokes to understand victim's behaviour for robot imitation attacks. Furthermore, we develop a deep Q-Network (DQN) algorithm to control the velocity of robot which is one of the key challenges of forging the human typing timing features. We tested and evaluated our approach on the real-life robotic testbed. We presented our results considering user identification and user authentication performance. We achieved a 90.3% user identification accuracy with genuine keyboard logging data samples and 89.6% accuracy with robot-forged data samples. Furthermore, we achieved 11.1%, and 36.6% EER for user authentication performance with zero-effort attack, and robot mimicry attack, respectively.",
      "year": 2023,
      "venue": "IEEE International Conference on Robotics and Automation",
      "authors": [
        "Rongyu Yu",
        "Burak Kizilkaya",
        "Zhen Meng",
        "Emma Li",
        "Guodong Zhao",
        "M. Imran"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/de9f8fe71c37741aa0ff999ca97d71cfabc37b52",
      "pdf_url": "https://eprints.gla.ac.uk/289833/2/289833.pdf",
      "publication_date": "2023-05-29",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "b244d97e5cca7db3aa81b5ccd9e46f763b685687",
      "title": "Power Side-Channel Attacks and Defenses for Neural Network Accelerators",
      "abstract": "Neural networks are becoming increasingly utilized in a range of real-world applications, often involving privacy-sensitive or safety-critical tasks like medical image analysis or autonomous driving. Despite their usefulness, designing and training neural networks (NNs) can be costly, both in terms of financial and energy expenses [4]. Gathering and labeling training data, actual training, and fine-tuning require considerable resources. The network models themselves are also considered confidential intellectual property (IP). Additionally, the carbon footprint of model training and development has a significant impact on the environment [5].",
      "year": 2023,
      "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines",
      "authors": [
        "Vincent Meyers",
        "M. Tahoori"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/b244d97e5cca7db3aa81b5ccd9e46f763b685687",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "66da20e195569d1fb5f72cb52f4ebf595ec4792c",
      "title": "Multiple-model and time-sensitive dynamic active learning for recurrent graph convolutional network model extraction attacks",
      "abstract": null,
      "year": 2023,
      "venue": "International Journal of Machine Learning and Cybernetics",
      "authors": [
        "Zhuo Zeng",
        "Chengliang Wang",
        "Fei Ma",
        "Peifeng Wang",
        "Hongqian Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/66da20e195569d1fb5f72cb52f4ebf595ec4792c",
      "pdf_url": "",
      "publication_date": "2023-07-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7a89a2882cacc41d9c30d108d03db2f1bc3821ad",
      "title": "Remote Identification of Neural Network FPGA Accelerators by Power Fingerprints",
      "abstract": "Machine learning acceleration has become increasingly popular in recent years, with machine learning-as-a-service (MLaaS) scenarios offering convenient and efficient ways to access pre-trained neural network models on devices such as cloud FPGAs. However, the ease of access and use also raises concerns over model theft or misuse through model manipulation. To address these concerns, this paper proposes a method for identifying neural network models in MLaaS scenarios by their unique power consumption. Current fingerprinting methods for neural networks rely on input/output pairs or characteristic of the decision boundary, which might not always be accessible in more complex systems. Our proposed method utilizes unique power characteristics of the black-box neural network accelerator to extract a fingerprint by measuring the voltage fluctuations of the device when querying specially crafted inputs. We take advantage of the fact that the power consumption of the accelerator varies depending on the input being processed. For evaluation of our method we conduct 200 fingerprint extraction and matching experiments and the results confirm that the proposed method can distinguish between correct and incorrect models in 100% of the cases. Furthermore, we show that the fingerprint is robust to environmental and chip-to-chip variations.",
      "year": 2023,
      "venue": "International Conference on Field-Programmable Logic and Applications",
      "authors": [
        "Vincent Meyers",
        "Michael Hefenbrock",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7a89a2882cacc41d9c30d108d03db2f1bc3821ad",
      "pdf_url": "",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "3c26202058ff573620e70e340c9c8cafb2094678",
      "title": "Model Extraction Attacks on DistilBERT",
      "abstract": null,
      "year": 2023,
      "venue": "Tiny Papers @ ICLR",
      "authors": [
        "Amro Salman",
        "Ayman Saeed",
        "Khalid Elmadani",
        "S. Babiker"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3c26202058ff573620e70e340c9c8cafb2094678",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4612f541d3a548bc1b87e42b82e61d37bb8cd66b",
      "title": "Model Extraction Attacks Against Reinforcement Learning Based Controllers",
      "abstract": "We introduce the problem of model-extraction attacks in cyber-physical systems in which an attacker attempts to estimate (or extract) the feedback controller of the system. Extracting (or estimating) the controller provides an unmatched edge to attackers since it allows them to predict the future control actions of the system and plan their attack accordingly. Hence, it is important to understand the ability of the attackers to perform such an attack. In this paper, we focus on the setting when a Deep Neural Network (DNN) controller is trained using Reinforcement Learning (RL) algorithms and is used to control a stochastic system. We play the role of the attacker that aims to estimate such an unknown DNN controller, and we propose a two-phase algorithm. In the first phase, also called the offline phase, the attacker uses side-channel information about the RL-reward function and the system dynamics to identify a set of candidate estimates of the unknown DNN. In the second phase, also called the online phase, the attacker observes the behavior of the unknown DNN and uses these observations to shortlist the set of final policy estimates. We provide theoretical analysis of the error between the unknown DNN and the estimated one. We also provide numerical results showing the effectiveness of the proposed algorithm.",
      "year": 2023,
      "venue": "IEEE Conference on Decision and Control",
      "authors": [
        "Momina Sajid",
        "Yanning Shen",
        "Yasser Shoukry"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4612f541d3a548bc1b87e42b82e61d37bb8cd66b",
      "pdf_url": "http://arxiv.org/pdf/2304.13090",
      "publication_date": "2023-04-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "e0662b9113b25a29473a6f5083bc5e15c04abbad",
      "title": "Data-Driven Deep Convolutional Neural Networks for Electromagnetic Field Estimation of Dry-type Transformer",
      "abstract": "This paper aims to estimate the electromagnetic field distribution in a simplified transformer through two-dimensional (2-D) finite element analysis. We create two datasets, namely the original dataset (OD) to simulate grayscale images and the new dataset (ND) which incorporates distinct physical properties of materials. These datasets are helpful to investigate the impact of different data types on deep learning. In order to enhance the accuracy of estimation, we compare the performances of two convolutional neural network (CNN) architectures: U-net and its improved version, U-Resnet (which incorporates residual blocks from ResNet). Additionally, we introduce a specialized loss function, Add-RMSE, which is better suited for dense regression problems, thus improving the prediction accuracy. The effectiveness of our proposed method is validated through test experiments, where we analyze the estimation results obtained.",
      "year": 2023,
      "venue": "IEEE International Conference on Applied Superconductivity and Electromagnetic Devices",
      "authors": [
        "Yifan Chen",
        "Qingxin Yang",
        "Yongjian Li",
        "Hao Zhang",
        "Changgeng Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e0662b9113b25a29473a6f5083bc5e15c04abbad",
      "pdf_url": "",
      "publication_date": "2023-10-27",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "44c48936a86b61d584b38661e3849d5345c1520f",
      "title": "StegGuard: Fingerprinting Self-supervised Pre-trained Encoders via Secrets Embeder and Extractor",
      "abstract": "In this work, we propose StegGuard, a novel fingerprinting mechanism to verify the ownership of the suspect pre-trained encoder using steganography. A critical perspective in StegGuard is that the unique characteristic of the transformation from an image to an embedding, conducted by the pre-trained encoder, can be equivalently exposed how an embeder embeds secrets into images and how an extractor extracts the secrets from encoder's embeddings with a tolerable error after the secrets are subjected to the encoder's transformation. While each independent encoder has a distinct transformation, the piracy encoder has a similar transformation to the victim. Based on these, we learn a pair of secrets embeder and extractor as the fingerprint for the victim encoder. We introduce a frequency-domain channel attention embedding block into the embeder to adaptively embed secrets into suitable frequency bands. During verification, if the secrets embedded into the query images can be extracted with an acceptable error from the suspect encoder's embeddings, the suspect encoder is determined as piracy, otherwise independent. Extensive experiments demonstrate that depending on a very limited number of query images, StegGuard can reliably identify across varied independent encoders, and is robust against model stealing related attacks including model extraction, fine-tuning, pruning, embedding noising and shuffle.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Xingdong Ren",
        "Tianxing Zhang",
        "Hanzhou Wu",
        "Xinpeng Zhang",
        "Yinggui Wang",
        "Guangling Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/44c48936a86b61d584b38661e3849d5345c1520f",
      "pdf_url": "https://arxiv.org/pdf/2310.03380",
      "publication_date": "2023-10-05",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4fb6895f62f27f200b9f9d3556257473e411ba86",
      "title": "Cloning Object Detectors with Limited Access to In-Distribution Samples",
      "abstract": "An object detector identifies and locates objects in images. Object detectors are widely deployed in practice, for example in driver assistance systems. Recently it has been shown that state-of-the-art object detectors based on neural networks can be cloned successfully if the adversary has oracle access to the detector, and if the adversary has access to either: (i) sufficiently many images drawn from the same distribution as the images of the detector's train set, also referred to as in-distribution samples, or (ii) a publicly available generative AI network capable of generating images that are close to in-distribution samples. This paper presents a new cloning attack that uses images from a publicly and freely available dataset, referred to as out-of-distribution samples, and a limited number of in-distribution samples. The new attack includes a strategy for combining in-and out-of-distribution samples during training and a calibration step to better mimic the functionality of the oracle detector. Our experiments show that CenterNet and RetinaNet object detectors trained with the Oxford-IIIT Pet, the WIDER FACE, or the Tsinghua-Tencent 100K dataset can be cloned successfully using images from the ImageNet-1K dataset supplemented with a limited number of in-distribution samples.",
      "year": 2023,
      "venue": "2023 IEEE 13th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)",
      "authors": [
        "Arne Aarts",
        "Wil Michiels",
        "Peter Roelse"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4fb6895f62f27f200b9f9d3556257473e411ba86",
      "pdf_url": "",
      "publication_date": "2023-09-03",
      "keywords_matched": [
        "cloning attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "96d3cd0854085c21595fb8efec867416af44c8b5",
      "title": "NaturalFinger: Generating Natural Fingerprint with Generative Adversarial Networks",
      "abstract": "Deep neural network (DNN) models have become a critical asset of the model owner as training them requires a large amount of resource (i.e. labeled data). Therefore, many fingerprinting schemes have been proposed to safeguard the intellectual property (IP) of the model owner against model extraction and illegal redistribution. However, previous schemes adopt unnatural images as the fingerprint, such as adversarial examples and noisy images, which can be easily perceived and rejected by the adversary. In this paper, we propose NaturalFinger which generates natural fingerprint with generative adversarial networks (GANs). Besides, our proposed NaturalFinger fingerprints the decision difference areas rather than the decision boundary, which is more robust. The application of GAN not only allows us to generate more imperceptible samples, but also enables us to generate unrestricted samples to explore the decision boundary.To demonstrate the effectiveness of our fingerprint approach, we evaluate our approach against four model modification attacks including adversarial training and two model extraction attacks. Experiments show that our approach achieves 0.91 ARUC value on the FingerBench dataset (154 models), exceeding the optimal baseline (MetaV) over 17\\%.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Kan Yang",
        "Kunhao Lai"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/96d3cd0854085c21595fb8efec867416af44c8b5",
      "pdf_url": "http://arxiv.org/pdf/2305.17868",
      "publication_date": "2023-05-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "7f005a1b45ff029c9b55dfcea5c83f473733cca4",
      "title": "Fingerprinting Deep Neural Networks Globally via Universal Adversarial Perturbations",
      "abstract": "In this paper, we propose a novel and practical mechanism to enable the service provider to verify whether a suspect model is stolen from the victim model via model extraction attacks. Our key insight is that the profile of a DNN model's decision boundary can be uniquely characterized by its Universal Adversarial Perturbations (UAPs). UAPs belong to a low-dimensional subspace and piracy models' subspaces are more consistent with victim model's subspace compared with non-piracy model. Based on this, we propose a UAP fingerprinting method for DNN models and train an encoder via contrastive learning that takes fingerprints as inputs, outputs a similarity score. Extensive studies show that our framework can detect model Intellectual Property (IP) breaches with confidence > 99.99 % within only 20 fingerprints of the suspect model. It also has good generalizability across different model architectures and is robust against post-modifications on stolen models.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zirui Peng",
        "Shaofeng Li",
        "Guoxing Chen",
        "Cheng Zhang",
        "Haojin Zhu",
        "Minhui Xue"
      ],
      "citation_count": 91,
      "url": "https://www.semanticscholar.org/paper/7f005a1b45ff029c9b55dfcea5c83f473733cca4",
      "pdf_url": "https://arxiv.org/pdf/2202.08602",
      "publication_date": "2022-02-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "823cacd5255f3897a8d29f29a7c7cb8f978bd928",
      "title": "CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks",
      "abstract": "Previous works have validated that text generation APIs can be stolen through imitation attacks, causing IP violations. In order to protect the IP of text generation APIs, a recent work has introduced a watermarking algorithm and utilized the null-hypothesis test as a post-hoc ownership verification on the imitation models. However, we find that it is possible to detect those watermarks via sufficient statistics of the frequencies of candidate watermarking words. To address this drawback, in this paper, we propose a novel Conditional wATERmarking framework (CATER) for protecting the IP of text generation APIs. An optimization method is proposed to decide the watermarking rules that can minimize the distortion of overall word distributions while maximizing the change of conditional word selections. Theoretically, we prove that it is infeasible for even the savviest attacker (they know how CATER works) to reveal the used watermarks from a large pool of potential word pairs based on statistical inspection. Empirically, we observe that high-order conditions lead to an exponential growth of suspicious (unused) watermarks, making our crafted watermarks more stealthy. In addition, \\cater can effectively identify the IP infringement under architectural mismatch and cross-domain imitation attacks, with negligible impairments on the generation quality of victim APIs. We envision our work as a milestone for stealthily protecting the IP of text generation APIs.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Xuanli He",
        "Qiongkai Xu",
        "Yi Zeng",
        "Lingjuan Lyu",
        "Fangzhao Wu",
        "Jiwei Li",
        "R. Jia"
      ],
      "citation_count": 87,
      "url": "https://www.semanticscholar.org/paper/823cacd5255f3897a8d29f29a7c7cb8f978bd928",
      "pdf_url": "http://arxiv.org/pdf/2209.08773",
      "publication_date": "2022-09-19",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "80900ec294267eefb6f9199e50b610ee2fb0c85b",
      "title": "Complex dynamics in a Hopfield neural network under electromagnetic induction and electromagnetic radiation.",
      "abstract": null,
      "year": 2022,
      "venue": "Chaos",
      "authors": [
        "Q. Wan",
        "Zidie Yan",
        "Fei Li",
        "Simiao Chen",
        "Jiong Liu"
      ],
      "citation_count": 61,
      "url": "https://www.semanticscholar.org/paper/80900ec294267eefb6f9199e50b610ee2fb0c85b",
      "pdf_url": "",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d03f4ca6facd18b30ab4c6034350d430bca0bc33",
      "title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models",
      "abstract": "Neural text ranking models have witnessed significant advancement and are increasingly being deployed in practice. Unfortunately, they also inherit adversarial vulnerabilities of general neural models, which have been detected but remain underexplored by prior studies. Moreover, the inherit adversarial vulnerabilities might be leveraged by blackhat SEO to defeat better-protected search engines. In this study, we propose an imitation adversarial attack on black-box neural passage ranking models. We first show that the target passage ranking model can be transparentized and imitated by enumerating critical queries/candidates and then train a ranking imitation model. Leveraging the ranking imitation model, we can elaborately manipulate the ranking results and transfer the manipulation attack to the target ranking model. For this purpose, we propose an innovative gradient-based attack method, empowered by the pairwise objective function, to generate adversarial triggers, which causes premeditated disorderliness with very few tokens. To equip the trigger camouflages, we add the next sentence prediction loss and the language model fluency constraint to the objective function. Experimental results on passage ranking demonstrate the effectiveness of the ranking imitation attack model and adversarial triggers against various SOTA neural ranking models. Furthermore, various mitigation analyses and human evaluation show the effectiveness of camouflages when facing potential mitigation approaches. To motivate other scholars to further investigate this novel and important problem, we make the experiment data and code publicly available.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jiawei Liu",
        "Yangyang Kang",
        "Di Tang",
        "Kaisong Song",
        "Changlong Sun",
        "Xiaofeng Wang",
        "Wei Lu",
        "Xiaozhong Liu"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/d03f4ca6facd18b30ab4c6034350d430bca0bc33",
      "pdf_url": "",
      "publication_date": "2022-09-14",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f3886b3c22675da54b0d55a5bc16754e3399c979",
      "title": "DualCF: Efficient Model Extraction Attack from Counterfactual Explanations",
      "abstract": "Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.",
      "year": 2022,
      "venue": "Conference on Fairness, Accountability and Transparency",
      "authors": [
        "Yongjie Wang",
        "Hangwei Qian",
        "C. Miao"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/f3886b3c22675da54b0d55a5bc16754e3399c979",
      "pdf_url": "https://arxiv.org/pdf/2205.06504",
      "publication_date": "2022-05-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "89f080275bf088de3140625d6ee7c4ffa7a368b9",
      "title": "Reverse Engineering Neural Network Folding with Remote FPGA Power Analysis",
      "abstract": "Specialized hardware accelerators in the form of FPGAs are widely being used for neural network implementations. By that, they also become the target of power analysis attacks that try to reverse engineer the embedded secret information, in the form of model parameters. However, most of these attacks assume rather simple implementations, not realistic frameworks. Layer folding is used in such accelerators to optimize the network under given area constraints with various degrees of parallel and sequential operations. In this paper, we show that folding does mislead existing power side-channel attacks on frameworks such as FINN. We show how we can extract the folding parameters successfully and use that information to subsequently also recover the number of neurons\u2013something not reliably possible without knowing the folding information. Following the methodologies of both profiling side-channel attacks and machine learning, our approach can extract the amount of neurons with 98% accuracy on a test device, compared to 44-79% accuracy based on related work under the same test conditions and datasets. Furthermore, we show how a classifier that is based on regression can detect previously unknown parameters, which has not been shown before. To verify our results under different environmental conditions, we test the target device in a climate chamber under various temperature ranges and still reach accuracies of at least 93%.",
      "year": 2022,
      "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines",
      "authors": [
        "Vincent Meyers",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/89f080275bf088de3140625d6ee7c4ffa7a368b9",
      "pdf_url": "",
      "publication_date": "2022-05-15",
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "83fa3f08be844c1808086a2d2ff941ee1dd58853",
      "title": "A Neural Network-Based Hybrid Framework for Least-Squares Inversion of Transient Electromagnetic Data",
      "abstract": "Inversion of large-scale time-domain transient electromagnetic (TEM) surveys is computationally expensive and time-consuming. The calculation of partial derivatives for the Jacobian matrix is by far the most computationally intensive task, as this requires calculation of a significant number of forward responses. We propose to accelerate the inversion process by predicting partial derivatives using an artificial neural network. Network training data for resistivity models for a broad range of geological settings are generated by computing partial derivatives as symmetric differences between two forward responses. Given that certain applications have larger tolerances for modeling inaccuracy and varying degrees of flexibility throughout the different phases of interpretation, we present four inversion schemes that provide a tunable balance between computational time and inversion accuracy when modeling TEM datasets. We improve speed and maintain accuracy with a hybrid framework, where the neural network derivatives are used initially and switched to full numerical derivatives in the final iterations. We also present a full neural network solution where neural network forward and derivatives are used throughout the inversion. In a least-squares inversion framework, a speedup factor exceeding 70 is obtained on the calculation of derivatives, and the inversion process is expedited ~36 times when the full neural network solution is used. Field examples show that the full nonlinear inversion and the hybrid approach gives identical results, whereas the full neural network inversion results in higher deviation but provides a reasonable indication about the overall subsurface geology.",
      "year": 2022,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "M. Asif",
        "T. Bording",
        "P. Maurya",
        "Bo Zhang",
        "G. Fiandaca",
        "D. Grombacher",
        "A. Christiansen",
        "E. Auken",
        "J. Larsen"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/83fa3f08be844c1808086a2d2ff941ee1dd58853",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "53cfd82a7f51f7d8fb279fc6813c9e3877e3be76",
      "title": "Fast Multi-Objective Optimization of Electromagnetic Devices Using Adaptive Neural Network Surrogate Model",
      "abstract": "This article presents a fast population-based multi-objective optimization of electromagnetic devices using an adaptive neural network (NN) surrogate model. The proposed method does not require any training data or construction of a surrogate model before the optimization phase. Instead, the NN surrogate model is built from the initial population in the optimization process, and then it is sequentially updated with high-ranking individuals. All individuals were evaluated using the surrogate model. Based on this evaluation, high-ranking individuals are reevaluated using high-fidelity electromagnetic field computation. The suppression of the execution of expensive field computations effectively reduces the computing costs. It is shown that the proposed method works two to four times faster, maintaining optimization performance than the original method that does not use surrogate models.",
      "year": 2022,
      "venue": "IEEE transactions on magnetics",
      "authors": [
        "Hayaho Sato",
        "H. Igarashi"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/53cfd82a7f51f7d8fb279fc6813c9e3877e3be76",
      "pdf_url": "https://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/87016/1/OnlineMethod_v8_submitforIEEE.pdf",
      "publication_date": "2022-05-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "ab3e38c43b08ef17694e1b2a3b334c6ce1bba476",
      "title": "A Hybrid Neural Network Electromagnetic Inversion Scheme (HNNEMIS) for Super-Resolution 3-D Microwave Human Brain Imaging",
      "abstract": "Super-resolution three-dimensional (3-D) electromagnetic (EM) inversion for microwave human brain imaging is a typical high contrast EM inverse problem and requires huge computational costs. This work proposes a hybrid neural network electromagnetic inversion scheme (HNNEMIS) which contains shallow and deep neural networks to alleviate the required huge computational costs and solve this high contrast inverse problem. In the proposed scheme, semi-join back propagation neural network (SJ-BPNN) is employed to nonlinearly map the measured scattered electric field to two output channels, namely the permittivity and conductivity of scatterers, respectively. Such a semi-join strategy decreases the computational burden in training and testing processes. Then, a deep learning technique, termed U-Net, is employed to further enhance the imaging quality of the output from SJ-BPNN. To decrease the training cost and make neural networks fast convergent for human brain inversion, a novel training dataset construction strategy which contains the characteristics of human brain is also proposed. Noise-free and noisy numerical examples demonstrate that HNNEMIS has superior super-resolution inversion capabilities for human brain imaging.",
      "year": 2022,
      "venue": "IEEE Transactions on Antennas and Propagation",
      "authors": [
        "Li-Ye Xiao",
        "Ronghan Hong",
        "Le Zhao",
        "Hao Hu",
        "Q. Liu"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/ab3e38c43b08ef17694e1b2a3b334c6ce1bba476",
      "pdf_url": "",
      "publication_date": "2022-08-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "83b9d7b9121ed02f7d75b294905cebe26938c6f7",
      "title": "Real-Time Identification of Natural Gas Pipeline Leakage Apertures Based on Lightweight Residual Convolutional Neural Network",
      "abstract": "Deep-learning techniques have been widely used in pipeline leakage aperture identification. However, most are designed and implemented for offline data, with problems such as large parameters, high memory consumption, and poor noise immunity. To solve the problem, this article presents a lightweight residual convolutional neural network (L-Resnet) applied to a real-time detection platform to achieve real-time identification of pipeline leakage apertures. First, based on the depth separable technique, two different separable residual modules are constructed to realize the feature extraction of signals; then, a more efficient activation function is applied to the high-dimensional space to enhance the nonlinear capability of the model; after that, a lightweight attention mechanism is used to weight the features to distinguish the importance of different features; finally, the classification results are obtained by a classifier. The real-time detection platform consists of Jetson Nano, the signal acquisition module, and the processing circuit. The results indicated that the method could accurately identify the pipeline leakage apertures in real time. Moreover, the number of parameters is only 14.71 kb, and the model has good computing efficiency and robustness compared to other methods.",
      "year": 2022,
      "venue": "IEEE Sensors Journal",
      "authors": [
        "Xiufang Wang",
        "Yuan Liu",
        "Chunlei Jiang",
        "Yueming Li",
        "Hongbo Bi"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/83b9d7b9121ed02f7d75b294905cebe26938c6f7",
      "pdf_url": "",
      "publication_date": "2022-12-15",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e6d21d219b1f3321ed2354c229946373d779897a",
      "title": "Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models",
      "abstract": "Server breaches are an unfortunate reality on today's Internet. In the context of deep neural network (DNN) models, they are particularly harmful, because a leaked model gives an attacker \"white-box'' access to generate adversarial examples, a threat model that has no practical robust defenses. For practitioners who have invested years and millions into proprietary DNNs, e.g. medical imaging, this seems like an inevitable disaster looming on the horizon. In this paper, we consider the problem of post-breach recovery for DNN models. We propose Neo, a new system that creates new versions of leaked models, alongside an inference time filter that detects and removes adversarial examples generated on previously leaked models. The classification surfaces of different model versions are slightly offset (by introducing hidden distributions), and Neo detects the overfitting of attacks to the leaked model used in its generation. We show that across a variety of tasks and attack methods, Neo is able to filter out attacks from leaked models with very high accuracy, and provides strong protection (7--10 recoveries) against attackers who repeatedly breach the server. Neo performs well against a variety of strong adaptive attacks, dropping slightly in # of breaches recoverable, and demonstrates potential as a complement to DNN defenses in the wild.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Shawn Shan",
        "Wen-Luan Ding",
        "Emily Wenger",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/e6d21d219b1f3321ed2354c229946373d779897a",
      "pdf_url": "https://arxiv.org/pdf/2205.10686",
      "publication_date": "2022-05-21",
      "keywords_matched": [
        "DNN weights leakage (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "14df6adb35c1e13d69d9f8c61f12c07bd7294ecf",
      "title": "MExMI: Pool-based Active Model Extraction Crossover Membership Inference",
      "abstract": null,
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yaxin Xiao",
        "Qingqing Ye",
        "Haibo Hu",
        "Huadi Zheng",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/14df6adb35c1e13d69d9f8c61f12c07bd7294ecf",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "8b2fb5e135323f8c69f11515ea3aceec86e6b66e",
      "title": "Stealthy Inference Attack on DNN via Cache-based Side-Channel Attacks",
      "abstract": "The advancement of deep neural networks (DNNs) motivates the deployment in various domains, including image classification, disease diagnoses, voice recognition, etc. Since some tasks that DNN undertakes are very sensitive, the label information is confidential and contains a commercial value or critical privacy. This paper demonstrates that DNNs also bring a new security threat, leading to the leakage of label information of input instances for the DNN models. In particular, we leverage the cache-based side-channel attack (SCA), i.e., Flush-Reload on the DNN (victim) models, to observe the execution of computation graphs, and create a database of them for building a classifier that the attacker can use to decide the label information of (unknown) input instances for victim models. Then we deploy the cache-based SCA on the same host machine with victim models and deduce the labels with the attacker's classification model to compromise the privacy and confidentiality of victim models. We explore different settings and classification techniques to achieve a high attack success rate of stealing label information from the victim models. Additionally, we consider two attacking scenarios: binary attacking identifies specific sensitive labels and others while multi-class attacking targets recognize all classes victim DNNs provide. Last, we implement the attack on both static DNN models with identical architectures for all inputs and dynamic DNN models with an adaptation of architectures for different inputs to demonstrate the vast existence of the proposed attack, including DenseNet 121, DenseNet 169, VGG 16, VGG 19, MobileNet v1, and MobileNet v2. Our experiment exhibits that MobileNet v1 is the most vulnerable one with 99% and 75.6% attacking success rates for binary and multi-class attacking scenarios, respectively.",
      "year": 2022,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Han Wang",
        "Syed Mahbub Hafiz",
        "Kartik Patwari",
        "Chen-Nee Chuah",
        "Zubair Shafiq",
        "H. Homayoun"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/8b2fb5e135323f8c69f11515ea3aceec86e6b66e",
      "pdf_url": "",
      "publication_date": "2022-03-14",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "33158daa5df8197872a06e415f2f277b026d9988",
      "title": "High-Fidelity Model Extraction Attacks via Remote Power Monitors",
      "abstract": "This paper shows the first side-channel attack on neural network (NN) IPs through a remote power monitor. We demonstrate that a remote monitor implemented with time-to-digital converters can be exploited to steal the weights from a hardware implementation of NN inference. Such an attack alleviates the need to have physical access to the target device and thus expands the attack vector to multi-tenant cloud FPGA platforms. Our results quantify the effectiveness of the attack on an FPGA implementation of NN inference and compare it to an attack with physical access. We demonstrate that it is indeed possible to extract the weights using DPA with 25000 traces if the SNR is sufficient. The paper, therefore, motivates secure virtualization-to protect the confidentiality of high-valued NN model IPs in multi-tenant execution environments, platform developers need to employ strong countermeasures against physical side-channel attacks.",
      "year": 2022,
      "venue": "International Conference on Artificial Intelligence Circuits and Systems",
      "authors": [
        "Anuj Dubey",
        "Emre Karabulut",
        "Amro Awad",
        "Aydin Aysu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/33158daa5df8197872a06e415f2f277b026d9988",
      "pdf_url": "",
      "publication_date": "2022-06-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "7065b725ebc95fdb95b58662aa5e7e69adf61339",
      "title": "Denoising of Transient Electromagnetic Data Based on the Minimum Noise Fraction-Deep Neural Network",
      "abstract": "There are many conventional methods that have been applied in transient electromagnetic (TEM) random noise suppression such as stacking-averaging, but when the TEM system works in urban areas with strong noise, these methods are not effective due to the extremely low signal-to-noise ratio (SNR). We propose a new method combining the minimum noise fraction (MNF) algorithm and deep learning. The MNF and the deep neural network (DNN) are used to extract the complex features of signals from the noisy signal data. After using MNF to improve the SNR of TEM to a certain extent, the convolutional neural network (CNN) and gated recurrent unit (GRU) were used to extract spatial and temporal features of the signal, and the training was guided by the double loss function. To verify the effectiveness of the method, we have done quantitative experiments on synthetic noise and field noise, respectively. The experimental results show that our method achieves the most advanced performance.",
      "year": 2022,
      "venue": "IEEE Geoscience and Remote Sensing Letters",
      "authors": [
        "Yishu Sun",
        "Sihe Huang",
        "Yang Zhang",
        "Jun Lin"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/7065b725ebc95fdb95b58662aa5e7e69adf61339",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "8119ab9eb8974693705bde0fa074439da40fda96",
      "title": "HDLock: exploiting privileged encoding to protect hyperdimensional computing models against IP stealing",
      "abstract": "Hyperdimensional Computing (HDC) is facing infringement issues due to straightforward computations. This work, for the first time, raises a critical vulnerability of HDC --- an attacker can reverse engineer the entire model, only requiring the unindexed hypervector memory. To mitigate this attack, we propose a defense strategy, namely HDLock, which significantly increases the reasoning cost of encoding. Specifically, HDLock adds extra feature hypervector combination and permutation in the encoding module. Compared to the standard HDC model, a two-layer-key HDLock can increase the adversarial reasoning complexity by 10 order of magnitudes without inference accuracy loss, with only 21% latency overhead.",
      "year": 2022,
      "venue": "Design Automation Conference",
      "authors": [
        "Shijin Duan",
        "Shaolei Ren",
        "Xiaolin Xu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/8119ab9eb8974693705bde0fa074439da40fda96",
      "pdf_url": "",
      "publication_date": "2022-03-18",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a0a63d6c89b5925591da1384413d660a4a3e49a6",
      "title": "DNNCloak: Secure DNN Models Against Memory Side-channel Based Reverse Engineering Attacks",
      "abstract": "As deep neural networks (DNN) expand their attention into various domains and the high cost of training a model, the structure of a DNN model has become a valuable intellectual property and needs to be protected. However, reversing DNN models by exploiting side-channel leakage has been demonstrated in various ways. Even if the model is encrypted and the processing hardware units are trusted, the attacker can still extract the model\u2019s structure and critical parameters through side channels, potentially posing significant commercial risks. In this paper, we begin by analyzing representative memory side-channel attacks on DNN models and identifying the primary causes of leakage. We also find that the full encryption used to protect model parameters could add extensive overhead. Based on our observations, we propose DNNCloak, a lightweight and secure framework aiming at mitigating reverse engineering attacks on common DNN architectures. DNNCloak includes a set of obfuscation schemes that increase the difficulty of reverse-engineering the DNN structure. Additionally, DNNCloak reduces the overhead of full weights encryption with an efficient matrix permutation scheme, resulting in reduced memory access time and enhanced security against retraining attacks on the model parameters. At last, we show how DNNCloak can defend DNN models from side-channel attacks effectively, with minimal performance overhead.",
      "year": 2022,
      "venue": "ICCD",
      "authors": [
        "Yuezhi Che",
        "Rujia Wang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/a0a63d6c89b5925591da1384413d660a4a3e49a6",
      "pdf_url": "",
      "publication_date": "2022-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "b82a66b5bcc78480131cb436d48b605ebcb891ee",
      "title": "Counteract Side-Channel Analysis of Neural Networks by Shuffling",
      "abstract": "Machine learning is becoming an essential part in almost every electronic device. Implementations of neural networks are mostly targeted towards computational performance or memory footprint. Nevertheless, security is also an important part in order to keep the network secret and protect the intellectual property associated to the network. Especially, since neural network implementations are demonstrated to be vulnerable to side-channel analysis, powerful and computational cheap countermeasures are in demand. In this work, we apply a shuffling countermeasure to a microcontroller implementation of a neural network to prevent side-channel analysis. The countermeasure is effective while the computational overhead is low. We investigate the extensions necessary for our countermeasure, and how shuffling increases the effort for an attack in theory. In addition, we demonstrate the increase in effort for an attacker through experiments on real side-channel measurements. Based on the mechanism of shuffling and our experimental results, we conclude that an attack on a commonly used neural network with shuffling is no longer feasible in a reasonable amount of time.",
      "year": 2022,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Manuel Brosch",
        "Matthias Probst",
        "G. Sigl"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/b82a66b5bcc78480131cb436d48b605ebcb891ee",
      "pdf_url": "",
      "publication_date": "2022-03-14",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a46d1f8e5ed6182df8c864e72160372cf2a14b13",
      "title": "On the Robustness of Dataset Inference",
      "abstract": "Machine learning (ML) models are costly to train as they can require a significant amount of data, computational resources and technical expertise. Thus, they constitute valuable intellectual property that needs protection from adversaries wanting to steal them. Ownership verification techniques allow the victims of model stealing attacks to demonstrate that a suspect model was in fact stolen from theirs. Although a number of ownership verification techniques based on watermarking or fingerprinting have been proposed, most of them fall short either in terms of security guarantees (well-equipped adversaries can evade verification) or computational cost. A fingerprinting technique, Dataset Inference (DI), has been shown to offer better robustness and efficiency than prior methods. The authors of DI provided a correctness proof for linear (suspect) models. However, in a subspace of the same setting, we prove that DI suffers from high false positives (FPs) -- it can incorrectly identify an independent model trained with non-overlapping data from the same distribution as stolen. We further prove that DI also triggers FPs in realistic, non-linear suspect models. We then confirm empirically that DI in the black-box setting leads to FPs, with high confidence. Second, we show that DI also suffers from false negatives (FNs) -- an adversary can fool DI (at the cost of incurring some accuracy loss) by regularising a stolen model's decision boundaries using adversarial training, thereby leading to an FN. To this end, we demonstrate that black-box DI fails to identify a model adversarially trained from a stolen dataset -- the setting where DI is the hardest to evade. Finally, we discuss the implications of our findings, the viability of fingerprinting-based ownership verification in general, and suggest directions for future work.",
      "year": 2022,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Sebastian Szyller",
        "Rui Zhang",
        "Jian Liu",
        "N. Asokan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/a46d1f8e5ed6182df8c864e72160372cf2a14b13",
      "pdf_url": "http://arxiv.org/pdf/2210.13631",
      "publication_date": "2022-10-24",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "db83dc37159c92e689bc246000e51b54c666eb04",
      "title": "A novel defense mechanism to protect users from profile cloning attack on Online Social Networks (OSNs)",
      "abstract": null,
      "year": 2022,
      "venue": "Peer-to-Peer Networking and Applications",
      "authors": [
        "Gordhan Jethava",
        "U. P. Rao"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/db83dc37159c92e689bc246000e51b54c666eb04",
      "pdf_url": "",
      "publication_date": "2022-07-05",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "dba696ce4856d7fad6a51847a8997145050eb9c3",
      "title": "GAME: Generative-Based Adaptive Model Extraction Attack",
      "abstract": null,
      "year": 2022,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Yi Xie",
        "Mengdie Huang",
        "Xiaoyu Zhang",
        "Changyu Dong",
        "W. Susilo",
        "Xiaofeng Chen"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/dba696ce4856d7fad6a51847a8997145050eb9c3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "53bb321ffc1864f9ed3fc689085f8bed943971a3",
      "title": "DynaMarks: Defending Against Deep Learning Model Extraction Using Dynamic Watermarking",
      "abstract": "The functionality of a deep learning (DL) model can be stolen via model extraction where an attacker obtains a surrogate model by utilizing the responses from a prediction API of the original model. In this work, we propose a novel watermarking technique called DynaMarks to protect the intellectual property (IP) of DL models against such model extraction attacks in a black-box setting. Unlike existing approaches, DynaMarks does not alter the training process of the original model but rather embeds watermark into a surrogate model by dynamically changing the output responses from the original model prediction API based on certain secret parameters at inference runtime. The experimental outcomes on Fashion MNIST, CIFAR-10, and ImageNet datasets demonstrate the efficacy of DynaMarks scheme to watermark surrogate models while preserving the accuracies of the original models deployed in edge devices. In addition, we also perform experiments to evaluate the robustness of DynaMarks against various watermark removal strategies, thus allowing a DL model owner to reliably prove model ownership.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Abhishek Chakraborty",
        "Daniel Xing",
        "Yuntao Liu",
        "Ankur Srivastava"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/53bb321ffc1864f9ed3fc689085f8bed943971a3",
      "pdf_url": "http://arxiv.org/pdf/2207.13321",
      "publication_date": "2022-07-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "95e2bfb5e01dc7cd666d59247c8849171aaf4e12",
      "title": "Exploration into the Explainability of Neural Network Models for Power Side-Channel Analysis",
      "abstract": "In this work, we present a comprehensive analysis of explainability of Neural Network (NN) models in the context of power Side-Channel Analysis (SCA), to gain insight into which features or Points of Interest (PoI) contribute the most to the classification decision. Although many existing works claim state-of-the-art accuracy in recovering secret key from cryptographic implementations, it remains to be seen whether the models actually learn representations from the leakage points. In this work, we evaluated the reasoning behind the success of a NN model, by validating the relevance scores of features derived from the network to the ones identified by traditional statistical PoI selection methods. Thus, utilizing the explainability techniques as a standard validation technique for NN models is justified.",
      "year": 2022,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Anupam Golder",
        "Ashwin Bhat",
        "A. Raychowdhury"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/95e2bfb5e01dc7cd666d59247c8849171aaf4e12",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3526241.3530346",
      "publication_date": "2022-06-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d1e08aa5c411179d72cfb971767f53ce8b9ce4ff",
      "title": "A ThreshoId-ImpIementation-Based Neural-Network Accelerator Securing Model Parameters and Inputs Against Power Side-Channel Attacks",
      "abstract": "Neural network (NN) hardware accelerators are being widely deployed on low-power loT nodes for energy-efficient decision making. Embedded NN implementations can use locally stored proprietary models, and may operate over private inputs (e.g., health monitors with patient-specific biomedical classifiers [6]), which must not be disclosed. Side-channel attacks (SCA) are a major concern in embedded systems where physical access to the operating hardware can allow attackers to recover secret data by exploiting information leakage through power consumption, timing and electromagnetic emissions [1, 7, 8]. As shown in Fig. 34.3.1, SCA on embedded NN implementations can reveal the model parameters [9] as well as the inputs [10]. To address these concerns, we present an energy - efficient ASlC solution for protecting both the model parameters and the input data against power-based SCA.",
      "year": 2022,
      "venue": "IEEE International Solid-State Circuits Conference",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "Samuel H. Fuller",
        "A. Chandrakasan"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/d1e08aa5c411179d72cfb971767f53ce8b9ce4ff",
      "pdf_url": "",
      "publication_date": "2022-02-20",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e0d3dbcc5819e70dd23deba8f0f00648fede12de",
      "title": "Fault Diagnosis of Tower Grounding Conductor Based on the Electromagnetic Measurement and Neural Network",
      "abstract": "The performance of the tower grounding conductor is very important for the safe and reliable operation of the power transmission system. Under the current excitation, the grounding conductor in different states will produce different magnetic fields on the earth\u2019s surface. By measuring the surface magnetic fields, the fault types and location of the grounding conductor can be evaluated. However, the process of traditional manual analysis is extremely complex. This article proposes a fault diagnosis method for tower grounding conductors based on deep learning to replace the manual diagnosis. The earth\u2019s surface magnetic field dataset generated by the grounding conductor consists of simulation data and experimental data. For practical application, the fine-tuning method is proposed to improve the diagnosis performance of the 1-D-convolutional neural network (1-D-CNN). Compared with the original 1-D-CNN model, the results demonstrate that the proposed fine-tuning 1-D-CNN has improved the diagnostic ability of the tower grounding conductor. For the six kinds of faults of rectangular tower grounding conductor, the average classification accuracy of the fine-tuning 1-D-CNN model reaches 80.50% when only the classifier and dense connection layers are trained. The proposed method can be used in the fault diagnosis of tower grounding conductors, which has significant potential applications in detecting and maintaining tower grounding conductors.",
      "year": 2022,
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": [
        "Caijiang Lu",
        "T. Zhang",
        "Shaoheng Sun",
        "Zhongqing Cao",
        "Mingyong Xin",
        "Guoqiang Fu",
        "Tao Wang",
        "Xi Wang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/e0d3dbcc5819e70dd23deba8f0f00648fede12de",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "89571d0a03b694c9f5470892a7fc5d980d793518",
      "title": "SeInspect: Defending Model Stealing via Heterogeneous Semantic Inspection",
      "abstract": null,
      "year": 2022,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Xinjing Liu",
        "Zhuo Ma",
        "Yang Liu",
        "Zhan Qin",
        "Junwei Zhang",
        "Zhuzhu Wang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/89571d0a03b694c9f5470892a7fc5d980d793518",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9efd7f888e7ca30c04eaf3fb53ab258782989fe3",
      "title": "Security and Privacy Challenges for Intelligent Internet of Things Devices 2022 TADW: Traceable and Antidetection Dynamic Watermarking of Deep Neural Networks",
      "abstract": "Deep neural networks (DNN) with incomparably advanced performance have been extensively applied in diverse fields (e.g., image recognition, natural language processing, and speech recognition). Training a high-performance DNN model requires a lot of training data and intellectual and computing resources, which bring a high cost to the model owners. Therefore, illegal model abuse (model theft, derivation, resale or redistribution, etc.) seriously infringes model owners\u2019 legitimate rights and interests. Watermarking is considered the main topic of DNN ownership protection. However, almost all existing watermarking works apply solely to image data. They do not trace the unique infringing model, and the adversary easily detects these ownership verification samples (trigger set) simultaneously. This paper introduces TADW, a dynamic watermarking scheme with tracking and antidetection abilities in the deep learning (DL) textual domain. Specifically, we propose a new approach to construct trigger set samples for antidetection and innovatively design a mapping algorithm that assigns a unique serial number (SN) to every watermarked model. Furthermore, we implement and detailedly evaluate TADW on 2 benchmark datasets and 3 popular DNNs. Experiment results show that TADW can successfully verify the ownership of the target model at a less than 0.5% accuracy cost and identify unique infringing models. In addition, TADW is excellently robust against different model modifications and can serve numerous users.",
      "year": 2022,
      "venue": "Security and Communication Networks",
      "authors": [
        "Jinwei Dong",
        "He Wang",
        "Zhipeng He",
        "Jun Niu",
        "Xiaoyan Zhu",
        "Gaofei Wu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9efd7f888e7ca30c04eaf3fb53ab258782989fe3",
      "pdf_url": "https://downloads.hindawi.com/journals/scn/2022/9505808.pdf",
      "publication_date": "2022-06-16",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2a7c3ecb7a424480687b3fd1c6f4f0b0a04b100a",
      "title": "PUFs Physical Learning: Accelerating the Enrollment via Delay-Based Model Extraction",
      "abstract": "The introduction of Physical Unclonable Functions (PUFs) has been originally motivated by their ability to resist physical attacks, particularly in anti-counterfeiting scenarios. In these one-way functions, machine learning, cryptanalysis, and side-channel attacks are common attack vectors threatening the promised PUF's property of unclonability. These attacks often emulate a PUF by employing a large number of Challenge-Response Pairs (CRPs). Some solutions to defeat such attacks are based on a protocol, where a model of the underlying PUF primitives should be extracted during the enrollment phase. In this article, we introduce a novel physical cloning approach applicable to FPGA-based implementations, which allows extracting the PUF's unique physical characteristics with a few number of Challenge-Response Pairs (CRPs), that increases only linearly for a higher number of PUF components. Indeed, our proposed approach significantly accelerates the enrollment phase and makes complex enrollment protocols feasible. Our core idea relies on an on-chip delay sensor, which can be realized by ordinary FPGA components, measuring the unique characteristic of the PUF elements. We demonstrate the feasibility of our introduced technique by practical experiments on different FPGA platforms, cloning a couple of (complex) PUF constructions, i.e., XOR APUF, iPUF, composed of delay-based Arbiter PUFs.",
      "year": 2022,
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": [
        "Anita Aghaie",
        "Maik Ender",
        "A. Moradi"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2a7c3ecb7a424480687b3fd1c6f4f0b0a04b100a",
      "pdf_url": "",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "cf95400543d5018168d8949e1f08805c72b6c162",
      "title": "Bias magnetic characteristic analysis and condition identification of transformers under DC bias magnetism conditions based on electromagnetic vibration and convolutional neural network",
      "abstract": null,
      "year": 2022,
      "venue": "Journal of Magnetism and Magnetic Materials",
      "authors": [
        "Wang Guo",
        "Xingmou Liu",
        "You Ma",
        "Yongming Yang",
        "ammad jadoo"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/cf95400543d5018168d8949e1f08805c72b6c162",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ea29c488e74411c15097b925f69ca19ea734af2f",
      "title": "Neural Network-Based Entropy: A New Metric for Evaluating Side-Channel Attacks",
      "abstract": null,
      "year": 2022,
      "venue": "J. Circuits Syst. Comput.",
      "authors": [
        "Jiafeng Cheng",
        "Nengyuan Sun",
        "Wenrui Liu",
        "Zhao Peng",
        "Chunyang Wang",
        "Caiban Sun",
        "Yufei Wang",
        "Yijian Bi",
        "Yiming Wen",
        "Hongliu Zhang",
        "Pengcheng Zhang",
        "S. Kose",
        "Weize Yu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/ea29c488e74411c15097b925f69ca19ea734af2f",
      "pdf_url": "",
      "publication_date": "2022-08-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ea411e6f457f45d59a74d79853a8d65ff7afdf22",
      "title": "Using Convolutional Neural Network to Redress Outliers in Clustering Based Side-Channel Analysis on Cryptosystem",
      "abstract": null,
      "year": 2022,
      "venue": "International Conference on Smart Computing and Communication",
      "authors": [
        "Anzhou Wang",
        "Shulin He",
        "Congming Wei",
        "Shaofei Sun",
        "Yaoling Ding",
        "Jiayao Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/ea411e6f457f45d59a74d79853a8d65ff7afdf22",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c082d0fdc703fd06f99a93f4683416f987d335ff",
      "title": "Privacy-Preserving DNN Model Authorization against Model Theft and Feature Leakage",
      "abstract": "Today\u2019s intelligent services are built on well-trained deep neural network (DNN) models, which usually require large private datasets along with a high cost for model training. It consequently makes the model providers cherish the pre-trained DNN models and only distribute them to authorized users. However, malicious users can steal these valuable models for abuse, illegal copy and redistribution. Attackers can also extract private features from even authorized models to leak partial training datasets. They both violate privacy. Existing techniques from secure community attempt to avoid parameter leakage during model authorization but yet cannot solve privacy issues sufficiently. In this paper, we propose a privacy-preserving model authorization approach, AgAuth, to resist the aforementioned privacy threats. We devise a novel scheme called Information-Agnostic Conversion (IAC) for forwarding procedure to eliminate residual features in model parameters. Based on it, we then propose Inference-on-Ciphertext (CiFer) mechanism for DNN reasoning, which includes three stages in each forwarding. The Encrypt phase first converts the proprietary model parameters to demonstrate uniform distribution. The Forward stage per-forms forwarding function without decryption at authorized side. Specifically, this stage just computes over ciphertext. The Decrypt phase finally recovers the information-agnostic outputs to informative output tensor for real-world services. In addition, we implement a prototype and conduct extensive experiments to evaluate its performance. The qualitative and quantitative results demonstrate that our solution AgAuth is privacy-preserving to defend against model theft and feature leakage, without accuracy loss or notable performance decrease.",
      "year": 2022,
      "venue": "ICC 2022 - IEEE International Conference on Communications",
      "authors": [
        "Qiushi Li",
        "Ju Ren",
        "Yuezhi Zhou",
        "Yaoxue Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c082d0fdc703fd06f99a93f4683416f987d335ff",
      "pdf_url": "",
      "publication_date": "2022-05-16",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1e49811d8f55244c2cd7e657ba3cebc5e5ed6dd5",
      "title": "A Survey on Side-Channel-based Reverse Engineering Attacks on Deep Neural Networks",
      "abstract": "Hardware side-channels have been exploited to leak sensitive information. With the emergence of deep learning, their hardware platforms have also been scrutinized for side-channel information leakage. It has been shown that the structure, weights, and input samples of deep neural networks (DNN) can all be the victim of reverse engineering attacks that rely on side-channel information leakage. In this paper, we survey existing work on hardware side-channel-based reverse engineering attacks on DNNs as well as the countermeasures.",
      "year": 2022,
      "venue": "International Conference on Artificial Intelligence Circuits and Systems",
      "authors": [
        "Yuntao Liu",
        "Michael Zuzak",
        "Daniel Xing",
        "Isaac McDaniel",
        "Priya Mittu",
        "Olsan Ozbay",
        "Abir Akib",
        "Ankur Srivastava"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1e49811d8f55244c2cd7e657ba3cebc5e5ed6dd5",
      "pdf_url": "",
      "publication_date": "2022-06-13",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a9635a34854fa4f6a158c7c600abb9763d645a44",
      "title": "Eucalypt clone modelling in agrosilvopastoral systems",
      "abstract": "\u2013 The objective of this work was to evaluate height-diameter, volumetric, and taper models to estimate the height, volume, and bole profile of trees of eucalypt clones ( Eucalyptus grandis \u00d7 Eucalyptus urophylla ) in an agrosilvopastoral system. Data were collected from permanent plots in an eight year-old agrosilvopastoral system, composed by three eucalypt clones (VE01, VE06, and VE07), located in the municipality of Coronel Xavier Chaves, in the state of Minas Gerais, Brazil. Two height-diameter, three volumetric, and four taper models were fit to the data of each clone and compared to each other, in order to select the best-fitting one. The equations fitted well to the observed data, and those of the models of Campos, Schumacher-Hall, and Garay stood out as the best ones. In addition, Graybill\u2019s F-test showed that the height-diameter and volumetric equations must be fitted separately for each genetic material. The model of Garay was the best taper model to estimate the bole profiles of all clones using a single equation.",
      "year": 2022,
      "venue": "Pesquisa Agropecu\u00e1ria Brasileira",
      "authors": [
        "Ad\u00eanio Louzeiro de Aguiar J\u00fanior",
        "S\u00edlvio N. Oliveira Neto",
        "Carlos Pedro Boechat Soares",
        "Marcelo Dias M\u00fcller",
        "Amana Magalh\u00e3es Matos Obolari",
        "Leonardo Henrique Ferreira Calsavara"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a9635a34854fa4f6a158c7c600abb9763d645a44",
      "pdf_url": "https://www.scielo.br/j/pab/a/cQVXG58Tz3p6qWQfx38xyrj/?lang=en&format=pdf",
      "publication_date": null,
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "c31794c04c50ec4386110cc9efa206dce344919b",
      "title": "DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks",
      "abstract": "Training deep neural networks (DNNs) requires large datasets and powerful computing resources, which has led some owners to restrict redistribution without permission. Watermarking techniques that embed confidential data into DNNs have been used to protect ownership, but these can degrade model performance and are vulnerable to watermark removal attacks. Recently, DeepJudge was introduced as an alternative approach to measuring the similarity between a suspect and a victim model. While DeepJudge shows promise in addressing the shortcomings of watermarking, it primarily addresses situations where the suspect model copies the victim\u2019s architecture. In this study, we introduce DeepTaster, a novel DNN fingerprinting technique, to address scenarios where a victim\u2019s data is unlawfully used to build a suspect model. DeepTaster can effectively identify such DNN model theft attacks, even when the suspect model\u2019s architecture deviates from the victim\u2019s. To accomplish this, DeepTaster generates adversarial images with perturbations, transforms them into the Fourier frequency domain, and uses these transformed images to identify the dataset used in a suspect model. The underlying premise is that adversarial images can capture the unique characteristics of DNNs built with a specific dataset. To demonstrate the effectiveness of DeepTaster, we evaluated the effectiveness of DeepTaster by assessing its detection accuracy on three datasets (CIFAR10, MNIST, and Tiny-ImageNet) across three model architectures (ResNet18, VGG16, and DenseNet161). We conducted experiments under various attack scenarios, including transfer learning, pruning, fine-tuning, and data augmentation. Specifically, in the Multi-Architecture Attack scenario, DeepTaster was able to identify all the stolen cases across all datasets, while DeepJudge failed to detect any of the cases.",
      "year": 2022,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Seonhye Park",
        "A. Abuadbba",
        "Shuo Wang",
        "Kristen Moore",
        "Yansong Gao",
        "Hyoungshick Kim",
        "Surya Nepal"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c31794c04c50ec4386110cc9efa206dce344919b",
      "pdf_url": "",
      "publication_date": "2022-11-24",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "16b149e4472f863cfee5999644e37c216900cd01",
      "title": "A Framework for Understanding Model Extraction Attack and Defense",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as-a-Service applications, where prediction services based on well-trained models are offered to users via pay-per-query. The lack of a defense mechanism can impose a high risk on the privacy of the server's model since an adversary could efficiently steal the model by querying only a few `good' data points. The interplay between a server's defense and an adversary's attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user's view and privacy from an adversary's view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the `equilibrium' between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results will be demonstrated by examples and experiments.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/16b149e4472f863cfee5999644e37c216900cd01",
      "pdf_url": "https://arxiv.org/pdf/2206.11480",
      "publication_date": "2022-06-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "b7c49323aaa05f3732d0c43767e659c39169f724",
      "title": "Understanding Model Extraction Games",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as- a-Service applications, where prediction services based on well- trained models are offered to users via the pay-per-query scheme. However, the lack of a defense mechanism can impose a high risk on the privacy of the server\u2019s model since an adversary could efficiently steal the model by querying only a few \u2018good\u2019 data points. The game between a server\u2019s defense and an adversary\u2019s attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user\u2019s view and privacy from an adversary\u2019s view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the \u2018equilibrium\u2019 between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results are demonstrated by examples and empirical experiments.",
      "year": 2022,
      "venue": "International Conference on Trust, Privacy and Security in Intelligent Systems and Applications",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b7c49323aaa05f3732d0c43767e659c39169f724",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "6482f6bf11a3b1c6f9c9f1ceaeddfcbebd68262c",
      "title": "Radar signal recognition based on deep convolutional neural network in complex electromagnetic environment",
      "abstract": "To solve the problem that tradition signal recognition algorithms cannot effectively recognize the contaminated and diverse radar signals in complex and variable Electronic Warfare (EW) environment, a new recognition method based on deep convolutional neural network (CNN) and time-frequency (TF) analysis is proposed. Firstly, the TF images of radar signals are extracted as the inputs to the CNN model. Then, a new network, called CNN-TF, is constructed to analyze these time-frequency images and use the robustness of CNN to suppress the noise interference. Thirdly, a complete and diverse signal librai7 is constructed based on the complex EW environment, and the librai7 is used to train and test CNN-TF. Finally, trained CNN-TF will be used for signal recognition. Simulation results show that the proposed algorithm not only improves the performance of signal recognition, but also has excellent anti-noise performance, which makes the proposed algorithm adapt to the complex and variable electronic warfare environment.",
      "year": 2022,
      "venue": "Annual Conference on Information Sciences and Systems",
      "authors": [
        "Zhang Qi",
        "Yewei Chen",
        "Yuan Liu",
        "Anqi Xu",
        "Li Li",
        "Jianpu Li"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/6482f6bf11a3b1c6f9c9f1ceaeddfcbebd68262c",
      "pdf_url": "",
      "publication_date": "2022-11-02",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "32d8e4dd6b800ba50d9224ae007707b345c7e711",
      "title": "Recovering the Weights of Convolutional Neural Network via Chosen Pixel Horizontal Power Analysis",
      "abstract": null,
      "year": 2022,
      "venue": "Wireless Algorithms, Systems, and Applications",
      "authors": [
        "Sihan He",
        "Weibin Wu",
        "Yanbin Li",
        "Lu Zhou",
        "Liming Fang",
        "Zhe Liu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/32d8e4dd6b800ba50d9224ae007707b345c7e711",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "772c3b717c10534e58ae3cd3b1b4538ef5454b55",
      "title": "Protecting SRAM PUF from BTI Aging-based Cloning Attack",
      "abstract": "SRAM Physical Unclonable Function (PUF) is currently one of the most popular PUFs, practically adopted in IC productions, to perform security primitives like encryption. However, previous works suggest responses of an SRAM PUF may be changeable due to one of the CMOS aging effects, Bias Temperature Instability (BTI). A physical counterfeit is thereby able to be produced by using BTI to change its responses, based on those of a target PUF. To prevent the BTI-based physical cloning attack, we propose a scheme without any modifications on the current SRAM PUF circuit, which is to pre-charge a challenged cell before it is powered up, so that its response can be affected by those transistors that cannot be precisely aged in the cloning process. We also show security and reliability metrics of SRAM PUFs are not affected by the extra pre-charge phase.",
      "year": 2022,
      "venue": "Symposium on Integrated Circuits and Systems Design",
      "authors": [
        "Shengyu Duan",
        "G. Sai"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/772c3b717c10534e58ae3cd3b1b4538ef5454b55",
      "pdf_url": "",
      "publication_date": "2022-08-22",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "bda00fec01defdb86c5d5ac165c9a1599e74d056",
      "title": "HWGN2: Side-channel Protected Neural Networks through Secure and Private Function Evaluation",
      "abstract": "Recent work has highlighted the risks of intellectual property (IP) piracy of deep learning (DL) models from the side-channel leakage of DL hardware accelerators. In response, to provide side-channel leakage resiliency to DL hardware accelerators, several approaches have been proposed, mainly borrowed from the methodologies devised for cryptographic implementations. Therefore, as expected, the same challenges posed by the complex design of such countermeasures should be dealt with. This is despite the fact that fundamental cryptographic approaches, specifically secure and private function evaluation, could potentially improve the robustness against side-channel leakage. To examine this and weigh the costs and benefits, we introduce hardware garbled NN (HWGN2), a DL hardware accelerator implemented on FPGA. HWGN2 also provides NN designers with the flexibility to protect their IP in real-time applications, where hardware resources are heavily constrained, through a hardware-communication cost trade-off. Concretely, we apply garbled circuits, implemented using a MIPS architecture that achieves up to 62.5x fewer logical and 66x less memory utilization than the state-of-the-art approaches at the price of communication overhead. Further, the side-channel resiliency of HWGN2 is demonstrated by employing the test vector leakage assessment (TVLA) test against both power and electromagnetic side-channels. This is in addition to the inherent feature of HWGN2: it ensures the privacy of users' input, including the architecture of NNs. We also demonstrate a natural extension to the malicious security modeljust as a by-product of our implementation.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Mohammad J. Hashemi",
        "Steffi Roy",
        "Domenic Forte",
        "F. Ganji"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/bda00fec01defdb86c5d5ac165c9a1599e74d056",
      "pdf_url": "http://arxiv.org/pdf/2208.03806",
      "publication_date": "2022-08-07",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3d62cb1d14a12b128e2d45b22b4239dd6417780a",
      "title": "Matryoshka: Stealing Functionality of Private ML Data by Hiding Models in Model",
      "abstract": "In this paper, we present a novel insider attack called Matryoshka, which employs an irrelevant scheduled-to-publish DNN model as a carrier model for covert transmission of multiple secret models which memorize the functionality of private ML data stored in local data centers. Instead of treating the parameters of the carrier model as bit strings and applying conventional steganography, we devise a novel parameter sharing approach which exploits the learning capacity of the carrier model for information hiding. Matryoshka simultaneously achieves: (i) High Capacity -- With almost no utility loss of the carrier model, Matryoshka can hide a 26x larger secret model or 8 secret models of diverse architectures spanning different application domains in the carrier model, neither of which can be done with existing steganography techniques; (ii) Decoding Efficiency -- once downloading the published carrier model, an outside colluder can exclusively decode the hidden models from the carrier model with only several integer secrets and the knowledge of the hidden model architecture; (iii) Effectiveness -- Moreover, almost all the recovered models have similar performance as if it were trained independently on the private data; (iv) Robustness -- Information redundancy is naturally implemented to achieve resilience against common post-processing techniques on the carrier before its publishing; (v) Covertness -- A model inspector with different levels of prior knowledge could hardly differentiate a carrier model from a normal model.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Xudong Pan",
        "Yifan Yan",
        "Sheng Zhang",
        "Mi Zhang",
        "Min Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3d62cb1d14a12b128e2d45b22b4239dd6417780a",
      "pdf_url": "http://arxiv.org/pdf/2206.14371",
      "publication_date": "2022-06-29",
      "keywords_matched": [
        "stealing functionality"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "06992347fb403d457fb09063c528fb68a50547c3",
      "title": "Continuous Authentication against Collusion Attacks",
      "abstract": "As mobile devices become more and more popular, users gain many conveniences. It has also made smartphone makers install new software and prebuilt hardware on their products, including many kinds of sensors. With improved storage and computing power, users also become accustomed to storing and interacting with personally sensitive information. Due to convenience and efficiency, mobile devices use gait authentication widely. In recent years, protecting the information security of mobile devices has become increasingly important. It has become a hot research area because smartphones are vulnerable to theft or unauthorized access. This paper proposes a novel attack model called a collusion attack. Firstly, we study the imitation attack in the general state and its results and propose and verify the feasibility of our attack. We propose a collusion attack model and train participants with quantified action specifications. The results demonstrate that our attack increases the attacker\u2019s false match rate only using an acceleration sensor in some systems sensor. Furthermore, we propose a multi-cycle defense model based on acceleration direction changes to improve the robustness of smartphone-based gait authentication methods against such attacks. Experimental results show that our defense model can significantly reduce the attacker\u2019s success rate.",
      "year": 2022,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Pin Lyu",
        "Wandong Cai",
        "Yao Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/06992347fb403d457fb09063c528fb68a50547c3",
      "pdf_url": "https://www.mdpi.com/1424-8220/22/13/4711/pdf?version=1655896310",
      "publication_date": "2022-06-22",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9286db7b087e6ba805d0526aeeeccfccf540069c",
      "title": "TP-NET: Training Privacy-Preserving Deep Neural Networks under Side-Channel Power Attacks",
      "abstract": "Privacy in deep learning is receiving tremendous attention with its wide applications in industry and academics. Recent studies have shown the internal structure of a deep neural network is easily inferred via side-channel power attacks in the training process. To address this pressing privacy issue, we propose TP-NET, a novel solution for training privacy-preserving deep neural networks under side-channel power attacks. The key contribution of TP-NET is the introduction of randomness into the internal structure of a deep neural network and the training process. Specifically, the workflow of TP-NET includes three steps: First, Independent Sub-network Construction, which generates multiple independent sub-networks via randomly se-lecting nodes in each hidden layer. Second, Sub-network Random Training, which randomly trains multiple sub-networks such that power traces keep random in the temporal domain. Third, Prediction, which outputs the predictions made by the most accu-rate sub-network to achieve high classification performance. The performance of TP-NET is evaluated under side-channel power attacks. The experimental results on two benchmark datasets demonstrate that TP-NET decreases the inference accuracy on the number of hidden nodes by at least 38.07% while maintaining competitive classification accuracy compared with traditional deep neural networks. Finally, a theoretical analysis shows that the power consumption of TP-NET depends on the number of sub-networks, the structure of each sub-network, and atomic operations in the training process.",
      "year": 2022,
      "venue": "International Symposium on Smart Electronic Systems",
      "authors": [
        "Hui Hu",
        "Jessa Gegax-Randazzo",
        "Clay Carper",
        "Mike Borowczak"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9286db7b087e6ba805d0526aeeeccfccf540069c",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "6a82176e62930e8e5bf8ba320e605404be7bf204",
      "title": "Seeds Don't Lie: An Adaptive Watermarking Framework for Computer Vision Models",
      "abstract": "In recent years, various watermarking methods were suggested to detect computer vision models obtained illegitimately from their owners, however they fail to demonstrate satisfactory robustness against model extraction attacks. In this paper, we present an adaptive framework to watermark a protected model, leveraging the unique behavior present in the model due to a unique random seed initialized during the model training. This watermark is used to detect extracted models, which have the same unique behavior, indicating an unauthorized usage of the protected model's intellectual property (IP). First, we show how an initial seed for random number generation as part of model training produces distinct characteristics in the model's decision boundaries, which are inherited by extracted models and present in their decision boundaries, but aren't present in non-extracted models trained on the same data-set with a different seed. Based on our findings, we suggest the Robust Adaptive Watermarking (RAW) Framework, which utilizes the unique behavior present in the protected and extracted models to generate a watermark key-set and verification model. We show that the framework is robust to (1) unseen model extraction attacks, and (2) extracted models which undergo a blurring method (e.g., weight pruning). We evaluate the framework's robustness against a naive attacker (unaware that the model is watermarked), and an informed attacker (who employs blurring strategies to remove watermarked behavior from an extracted model), and achieve outstanding (i.e.,>0.9) AUC values. Finally, we show that the framework is robust to model extraction attacks with different structure and/or architecture than the protected model.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Jacob Shams",
        "Ben Nassi",
        "I. Morikawa",
        "Toshiya Shimizu",
        "A. Shabtai",
        "Y. Elovici"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6a82176e62930e8e5bf8ba320e605404be7bf204",
      "pdf_url": "https://arxiv.org/pdf/2211.13644",
      "publication_date": "2022-11-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "2841458166d23bd76abd8f2a03b8d57c99522d44",
      "title": "Model Stealing Attack based on Sampling and Weighting",
      "abstract": null,
      "year": 2022,
      "venue": "",
      "authors": [
        "\u8bba\u6587 \u57fa\u4e8e\u91c7\u6837\u548c\u52a0\u6743\u635f\u5931\u51fd\u6570\u7684\u6a21\u578b\u7a83\u53d6\u653b\u51fb\u65b9\u6cd5 \u738b\u71a0\u65ed"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2841458166d23bd76abd8f2a03b8d57c99522d44",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f7b70da96899fdac65153e172e426c5b39d2bde5",
      "title": "Detecting Data-Free Model Stealing",
      "abstract": null,
      "year": 2022,
      "venue": "",
      "authors": [
        "Ashley Borum",
        "James Beetham",
        "Dr. Niels Da",
        "Vitoria Lobo",
        "Dr. Mubarak Shah"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f7b70da96899fdac65153e172e426c5b39d2bde5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e3bc283af5746de4ede17c123d39e02e819edcab",
      "title": "Characterizing Side-Channel Leakage of DNN Classifiers though Performance Counters",
      "abstract": "Rapid advancements in Deep Neural Networks (DNN) have led to their deployment in a wide range of com-mercial applications. DNN classifiers are powerful tools that drive a broad spectrum of important applications, from image recognition to autonomous vehicles. Like other applications, they have been shown to be vulnerable to side-channel information leakage. There have been several proof-of-concept attacks demon-strating the extraction of their model parameters and input data. However, no prior study has examined the possibility of using side-channels to extract the DNN classifier's decision or output. In this initial study, we aim to understand if there exists a correlation between the output class selected by a classifier and side-channel information collected while running the inference process on a CPU. Our initial evaluation shows that with the proposed approach it is possible to accurately recover the output class for model inputs via multiple side-channels: primarily power, but also branch mispredictions and cache misses.",
      "year": 2022,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Saikat Majumdar",
        "Mohammad Hossein Samavatian",
        "R. Teodorescu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e3bc283af5746de4ede17c123d39e02e819edcab",
      "pdf_url": "",
      "publication_date": "2022-06-27",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c3531dc827334763c8d41ccd6287a2820a840352",
      "title": "Model functionality stealing attacks based on real data awareness",
      "abstract": null,
      "year": 2022,
      "venue": "Journal of Image and Graphics",
      "authors": [
        "Yanming Li",
        "Changsheng Li",
        "Jiaqi Yu",
        "Ye Yuan",
        "Guoren Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c3531dc827334763c8d41ccd6287a2820a840352",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2d6f41bb62b116978dee9083c39ac5c17a586fa2",
      "title": "The Limits of Provable Security Against Model Extraction",
      "abstract": null,
      "year": 2022,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Ari Karchmer"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2d6f41bb62b116978dee9083c39ac5c17a586fa2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0a9b48b2100f95ce541899fd4b7aa60d985241f2",
      "title": "Enhancing Cybersecurity in Edge AI through Model Distillation and Quantization: A Robust and Efficient Approach",
      "abstract": "The rapid proliferation of Edge AI has introduced significant cybersecurity challenges, including adversarial attacks, model theft, and data privacy concerns. Traditional deep learning models deployed on edge devices often suffer from high computational complexity and memory requirements, making them vulnerable to exploitation. This paper explores the integration of model distillation and quantization techniques to enhance the security and efficiency of Edge AI systems. Model distillation reduces model complexity by transferring knowledge from a large, cumbersome model (teacher) to a compact, efficient one (student), thereby improving resilience against adversarial manipulations. \nQuantization further optimizes the student model by reducing bit precision, minimizing attack surfaces while maintaining performance. \nWe present a comprehensive analysis of how these techniques mitigate cybersecurity threats such as model inversion, membership inference, and evasion attacks. Additionally, we evaluate trade-offs between model accuracy, latency, and robustness in resource-constrained edge environments. Experimental results on benchmark datasets demonstrate that distilled and quantized models achieve comparable accuracy to their full-precision counterparts while significantly reducing vulnerability to cyber threats. Our findings highlight the potential of distillation and quantization as key enablers for secure, lightweight, and high-performance Edge AI deployments.",
      "year": 2022,
      "venue": "International Journal for Sciences and Technology",
      "authors": [
        "Mangesh Pujari",
        "Anshul Goel",
        "Ashwin Sharma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0a9b48b2100f95ce541899fd4b7aa60d985241f2",
      "pdf_url": "",
      "publication_date": "2022-11-25",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1921489a2801f053d0906079f0015ba6e68dd505",
      "title": "DIET-SNN: A Low-Latency Spiking Neural Network With Direct Input Encoding and Leakage and Threshold Optimization",
      "abstract": "Bioinspired spiking neural networks (SNNs), operating with asynchronous binary signals (or spikes) distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. The state-of-the-art SNNs suffer from high inference latency, resulting from inefficient input encoding and suboptimal settings of the neuron parameters (firing threshold and membrane leak). We propose DIET-SNN, a low-latency deep spiking network trained with gradient descent to optimize the membrane leak and the firing threshold along with other network parameters (weights). The membrane leak and threshold of each layer are optimized with end-to-end backpropagation to achieve competitive accuracy at reduced latency. The input layer directly processes the analog pixel values of an image without converting it to spike train. The first convolutional layer converts analog inputs into spikes where leaky-integrate-and-fire (LIF) neurons integrate the weighted inputs and generate an output spike when the membrane potential crosses the trained firing threshold. The trained membrane leak selectively attenuates the membrane potential, which increases activation sparsity in the network. The reduced latency combined with high activation sparsity provides massive improvements in computational efficiency. We evaluate DIET-SNN on image classification tasks from CIFAR and ImageNet datasets on VGG and ResNet architectures. We achieve top-1 accuracy of 69% with five timesteps (inference latency) on the ImageNet dataset with $12\\times $ less compute energy than an equivalent standard artificial neural network (ANN). In addition, DIET-SNN performs 20\u2013 $500\\times $ faster inference compared to other state-of-the-art SNN models.",
      "year": 2021,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Nitin Rathi",
        "K. Roy"
      ],
      "citation_count": 296,
      "url": "https://www.semanticscholar.org/paper/1921489a2801f053d0906079f0015ba6e68dd505",
      "pdf_url": "",
      "publication_date": "2021-10-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5654149297bbf8c55d220bb4bf924a71695ebad9",
      "title": "Neural network based adaptive event trigger control for a class of electromagnetic suspension systems",
      "abstract": null,
      "year": 2021,
      "venue": "Control Engineering Practice",
      "authors": [
        "Lei Liu",
        "Xiangshen Li",
        "Yanjun Liu",
        "Shaocheng Tong"
      ],
      "citation_count": 191,
      "url": "https://www.semanticscholar.org/paper/5654149297bbf8c55d220bb4bf924a71695ebad9",
      "pdf_url": "",
      "publication_date": "2021-01-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2569a7309142e40815cf556b6417059df9abbda8",
      "title": "Protecting Intellectual Property of Language Generation APIs with Lexical Watermark",
      "abstract": "Nowadays, due to the breakthrough in natural language generation (NLG), including machine translation, document summarization, image captioning, etc NLG models have been encapsulated in cloud APIs to serve over half a billion people worldwide and process over one hundred billion word generations per day. Thus, NLG APIs have already become essential profitable services in many commercial companies. Due to the substantial financial and intellectual investments, service providers adopt a pay-as-you-use policy to promote sustainable market growth. However, recent works have shown that cloud platforms suffer from financial losses imposed by model extraction attacks, which aim to imitate the functionality and utility of the victim services, thus violating the intellectual property (IP) of cloud APIs. This work targets at protecting IP of NLG APIs by identifying the attackers who have utilized watermarked responses from the victim NLG APIs. However, most existing watermarking techniques are not directly amenable for IP protection of NLG APIs. To bridge this gap, we first present a novel watermarking method for text generation APIs by conducting lexical modification to the original outputs. Compared with the competitive baselines, our watermark approach achieves better identifiable performance in terms of p-value, with fewer semantic losses. In addition, our watermarks are more understandable and intuitive to humans than the baselines. Finally, the empirical studies show our approach is also applicable to queries from different domains, and is effective on the attacker trained on a mixture of the corpus which includes less than 10% watermarked samples.",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Xuanli He",
        "Qiongkai Xu",
        "L. Lyu",
        "Fangzhao Wu",
        "Chenguang Wang"
      ],
      "citation_count": 115,
      "url": "https://www.semanticscholar.org/paper/2569a7309142e40815cf556b6417059df9abbda8",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/21321/21070",
      "publication_date": "2021-12-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "45ea6495958f04c1f02de1741c952dac1154d4f4",
      "title": "UnSplit: Data-Oblivious Model Inversion, Model Stealing, and Label Inference Attacks against Split Learning",
      "abstract": "Training deep neural networks often forces users to work in a distributed or outsourced setting, accompanied with privacy concerns. Split learning aims to address this concern by distributing the model among a client and a server. The scheme supposedly provides privacy, since the server cannot see the clients' models and inputs. We show that this is not true via two novel attacks. (1) We show that an honest-but-curious split learning server, equipped only with the knowledge of the client neural network architecture, can recover the input samples and obtain a functionally similar model to the client model, without being detected. (2) We show that if the client keeps hidden only the output layer of the model to ''protect'' the private labels, the honest-but-curious server can infer the labels with perfect accuracy. We test our attacks using various benchmark datasets and against proposed privacy-enhancing extensions to split learning. Our results show that plaintext split learning can pose serious risks, ranging from data (input) privacy to intellectual property (model parameters), and provide no more than a false sense of security.",
      "year": 2021,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Ege Erdogan",
        "Alptekin K\u00fcp\u00e7\u00fc",
        "A. E. Cicek"
      ],
      "citation_count": 101,
      "url": "https://www.semanticscholar.org/paper/45ea6495958f04c1f02de1741c952dac1154d4f4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3559613.3563201",
      "publication_date": "2021-08-20",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f2eb05b91e4fed91626c961372d7d189211aa552",
      "title": "Memristive electromagnetic induction effects on Hopfield neural network",
      "abstract": "Due to the existence of membrane potential differences, the electromagnetic induction flows can be induced in the interconnected neurons of Hopfield neural network (HNN). To express the induction flows, this paper presents a unified memristive HNN model using hyperbolic-type memristors to link neurons. By employing theoretical analysis along with multiple numerical methods, we explore the electromagnetic induction effects on the memristive HNN with three neurons. Three cases are classified and discussed. When using one memristor to link two neurons bidirectionally, the coexisting bifurcation behaviors and extreme events are disclosed with respect to the memristor coupling strength. When using two memristors to link three neurons, the antimonotonicity phenomena of periodic and chaotic bubbles are yielded, and the initial-related extreme events are emerged. When using three memristors to link three neurons end to end, the extreme events owning prominent riddled basins of attraction are demonstrated. In addition, we develop the printed circuit board (PCB)-based hardware experiments by synthesizing the memristive HNN, and the experimental results well confirm the memristive electromagnetic induction effects. Certainly, the PCB-based implementation will benefit the integrated circuit design for large-scale Hopfield neural network in the future.",
      "year": 2021,
      "venue": "Nonlinear dynamics",
      "authors": [
        "Chengjie Chen",
        "Fuhong Min",
        "Yunzhen Zhang",
        "B. Bao"
      ],
      "citation_count": 99,
      "url": "https://www.semanticscholar.org/paper/f2eb05b91e4fed91626c961372d7d189211aa552",
      "pdf_url": "https://www.researchsquare.com/article/rs-722277/latest.pdf",
      "publication_date": "2021-07-20",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "03df500a29fb8717662ea5626afd9e2b22b5d14c",
      "title": "Stealing Neural Network Structure Through Remote FPGA Side-Channel Analysis",
      "abstract": "Deep Neural Network (DNN) models have been extensively developed by companies for a wide range of applications. The development of a customized DNN model with great performance requires costly investments, and its structure (layers and hyper-parameters) is considered intellectual property and holds immense value. However, in this paper, we found the model secret is vulnerable when a cloud-based FPGA accelerator executes it. We demonstrate an end-to-end attack based on remote power side-channel analysis and machine-learning-based secret inference against different DNN models. The evaluation result shows that an attacker can reconstruct the layer and hyper-parameter sequence at over 90% accuracy using our method, which can significantly reduce her model development workload. We believe the threat presented by our attack is tangible, and new defense mechanisms should be developed against this threat.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Yicheng Zhang",
        "Rozhin Yasaei",
        "Hao Chen",
        "Zhou Li",
        "M. A. Faruque"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/03df500a29fb8717662ea5626afd9e2b22b5d14c",
      "pdf_url": "",
      "publication_date": "2021-02-17",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c3111e374ad14357172ef63e7063e0182f8030d4",
      "title": "Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models",
      "abstract": "Deep learning models, especially those large-scale and high-performance ones, can be very costly to train, demanding a considerable amount of data and computational resources. As a result, deep learning models have become one of the most valuable assets in modern artificial intelligence. Unauthorized duplication or reproduction of deep learning models can lead to copyright infringement and cause huge economic losses to model owners, calling for effective copyright protection techniques. Existing protection techniques are mostly based on watermarking, which embeds an owner-specified watermark into the model. While being able to provide exact ownership verification, these techniques are 1) invasive, i.e., they need to tamper with the training process, which may affect the model utility or introduce new security risks into the model; 2) prone to adaptive attacks that attempt to remove/replace the watermark or adversarially block the retrieval of the watermark; and 3) not robust to the emerging model extraction attacks. Latest fingerprinting work on deep learning models, though being non-invasive, also falls short when facing the diverse and ever-growing attack scenarios.In this paper, we propose a novel testing framework for deep learning copyright protection: DEEPJUDGE. DEEPJUDGE quantitatively tests the similarities between two deep learning models: a victim model and a suspect model. It leverages a diverse set of testing metrics and efficient test case generation algorithms to produce a chain of supporting evidence to help determine whether a suspect model is a copy of the victim model. Advantages of DEEPJUDGE include: 1) non-invasive, as it works directly on the model and does not tamper with the training process; 2) efficient, as it only needs a small set of seed test cases and a quick scan of the two models; 3) flexible, i.e., it can easily incorporate new testing metrics or test case generation methods to obtain more confident and robust judgement; and 4) fairly robust to model extraction attacks and adaptive attacks. We verify the effectiveness of DEEPJUDGE under three typical copyright infringement scenarios, including model finetuning, pruning and extraction, via extensive experiments on both image classification and speech recognition datasets with a variety of model architectures.",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Jialuo Chen",
        "Jingyi Wang",
        "Tinglan Peng",
        "Youcheng Sun",
        "Peng Cheng",
        "S. Ji",
        "Xingjun Ma",
        "Bo Li",
        "D. Song"
      ],
      "citation_count": 79,
      "url": "https://www.semanticscholar.org/paper/c3111e374ad14357172ef63e7063e0182f8030d4",
      "pdf_url": "https://arxiv.org/pdf/2112.05588",
      "publication_date": "2021-12-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "cd709a11dfb2308b0d2ffc779261165e92c49e2b",
      "title": "Dynamics analysis, hardware implementation and engineering applications of novel multi-style attractors in a neural network under electromagnetic radiation",
      "abstract": null,
      "year": 2021,
      "venue": "Chaos, Solitons & Fractals",
      "authors": [
        "Fei Yu",
        "Hui Shen",
        "ZiNan Zhang",
        "Yuanyuan Huang",
        "Shuo Cai",
        "Sichun Du"
      ],
      "citation_count": 67,
      "url": "https://www.semanticscholar.org/paper/cd709a11dfb2308b0d2ffc779261165e92c49e2b",
      "pdf_url": "",
      "publication_date": "2021-11-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c94981857a180002fd8d39f405f677800db35668",
      "title": "Design and FPGA Implementation of a Pseudo-random Number Generator Based on a Hopfield Neural Network Under Electromagnetic Radiation",
      "abstract": "When implementing a pseudo-random number generator (PRNG) for neural network chaos-based systems on FPGAs, chaotic degradation caused by numerical accuracy constraints can have a dramatic impact on the performance of the PRNG. To suppress this degradation, a PRNG with a feedback controller based on a Hopfield neural network chaotic oscillator is proposed, in which a neuron is exposed to electromagnetic radiation. We choose the magnetic flux across the cell membrane of the neuron as a feedback condition of the feedback controller to disturb other neurons, thus avoiding periodicity. The proposed PRNG is modeled and simulated on Vivado 2018.3 software and implemented and synthesized by the FPGA device ZYNQ-XC7Z020 on Xilinx using Verilog HDL code. As the basic entropy source, the Hopfield neural network with one neuron exposed to electromagnetic radiation has been implemented on the FPGA using the high precision 32-bit Runge Kutta fourth-order method (RK4) algorithm from the IEEE 754-1985 floating point standard. The post-processing module consists of 32 registers and 15 XOR comparators. The binary data generated by the scheme was tested and analyzed using the NIST 800.22 statistical test suite. The results show that it has high security and randomness. Finally, an image encryption and decryption system based on PRNG is designed and implemented on FPGA. The feasibility of the system is proved by simulation and security analysis.",
      "year": 2021,
      "venue": "Frontiers of Physics",
      "authors": [
        "Fei Yu",
        "ZiNan Zhang",
        "Hui Shen",
        "Yuanyuan Huang",
        "Shuo Cai",
        "Jie Jin",
        "Sichun Du"
      ],
      "citation_count": 66,
      "url": "https://www.semanticscholar.org/paper/c94981857a180002fd8d39f405f677800db35668",
      "pdf_url": "https://www.frontiersin.org/articles/10.3389/fphy.2021.690651/pdf",
      "publication_date": "2021-06-04",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c3d0749f519962331d323dd3c4ec1137544d6d03",
      "title": "Fingerprinting Deep Neural Networks - a DeepFool Approach",
      "abstract": "A well-trained deep learning classifier is an expensive intellectual property of the model owner. However, recently proposed model extraction attacks and reverse engineering techniques make model theft possible and similar quality deep learning solution reproducible at a low cost. To protect the interest and revenue of the model owner, watermarking on Deep Neural Network (DNN) has been proposed. However, the extra components and computations due to the embedded watermark tend to interfere with the model training process and result in inevitable degradation in classification accuracy. In this paper, we utilize the geometry characteristics inherited in the DeepFool algorithm to extract data points near the classification boundary of the target model for ownership verification. As the fingerprint is extracted after the training process has been completed, the original achievable classification accuracy will not be compromised. This countermeasure is founded on the hypothesis that different models possess different classification boundaries determined solely by the hyperparameters of the DNN and the training it has undergone. Therefore, given a set of fingerprint data points, a pirated model or its post-processed version will produce similar prediction but another originally designed and trained DNN for the same task will produce very different prediction even if they have similar or better classification accuracy. The effectiveness of the proposed Intellectual Property (IP) protection method is validated on the CIFAR-10, CIFAR-100 and ImageNet datasets. The results show a detection rate of 100% and a false positive rate of 0% for each dataset. More importantly, the fingerprint extraction and its run time are both dataset independent. It is on average ~130\u00d7 faster than two state-of-the-art fingerprinting methods.",
      "year": 2021,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Si Wang",
        "Chip-Hong Chang"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/c3d0749f519962331d323dd3c4ec1137544d6d03",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/147023/2/2021021379.pdf",
      "publication_date": "2021-05-01",
      "keywords_matched": [
        "model extraction",
        "model theft",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "43b010e8bb8cec29e1d96484d8cef40163eb10f3",
      "title": "A YOLOv3 Deep Neural Network Model to Detect Brain Tumor in Portable Electromagnetic Imaging System",
      "abstract": "This paper presents the detection of brain tumors through the YOLOv3 deep neural network model in a portable electromagnetic (EM) imaging system. YOLOv3 is a popular object detection model with high accuracy and improved computational speed. Initially, the scattering parameters are collected from the nine-antenna array setup with a tissue-mimicking head phantom, where one antenna acts as a transmitter and the other eight antennas act as receivers. The images are then reconstructed from the post-processed scattering parameters by applying the modified delay-multiply-and-sum algorithm that contains $416\\times 416$ pixels. Fifty sample images are collected from the different head regions through the EM imaging system. The images are later augmented to generate a final image data set for training, validation, and testing, where the data set contains 1000 images, including fifty samples with a single and double tumor. 80% of the images are utilized for training the network, whereas 10% are used for validation, and the rest 10% are utilized for testing purposes. The detection performance is investigated with the different image data sets. The achieved detection accuracy and F1 scores are 95.62% and 94.50%, respectively, which ensure better detection accuracy. The training accuracy and validation losses are 96.74% and 9.21%, respectively. The tumor detection with its location in different cases from the testing images is evaluated through YOLOv3, which demonstrates its potential in the portable electromagnetic head imaging system.",
      "year": 2021,
      "venue": "IEEE Access",
      "authors": [
        "Amran Hossain",
        "Mohammad Tariqul Islam",
        "Mohammad Shahidul Islam",
        "M. E. Chowdhury",
        "Ali F. Almutairi",
        "Qutaiba A. Razouqi",
        "N. Misran"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/43b010e8bb8cec29e1d96484d8cef40163eb10f3",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/9312710/09446998.pdf",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4e64a5baf6be16007e4d6e9c4098dd6ff62680e4",
      "title": "Chaotic resonance in Izhikevich neural network motifs under electromagnetic induction",
      "abstract": "Chaotic resonance (CR) is the response of a nonlinear system to weak signals enhanced by internal or external chaotic activity (such as the signal derived from Lorenz system). The triple-neuron feed-forward loop (FFL) Izhikevich neural network motifs with eight types are constructed as the nonlinear systems in this paper, and the effects of EMI on CR phenomenon in FFL neuronal network motifs are studied. It is found that both the single Izhikevich neural model under electromagnetic induction (EMI) and its network motifs exhibit CR phenomenon depending on the chaotic current intensity. There exists an optimal chaotic current intensity ensuring the best detection of weak signal in single Izhikevich neuron or its network motifs via CR. The EMI can enhance the ability of neuron to detect weak signals. For T1-FFL and T2-FFL motifs, the adjustment of EMI parameters makes T2-FFL show a more obvious CR phenomenon than that for T1-FFL motifs, which is different from the impact of system parameters (e.g., the weak signal frequency, the coupling strength, and the time delay) on CR. Another interesting phenomenon is that the variation of CR with time delay exhibits quasi-periodic characteristics. Our results showed that CR effect is a robust phenomenon which is observed in both single Izhikevich neuron and network motifs, which might help one understand how to improve the ability of weak signal detection and propagation in neuronal system.",
      "year": 2021,
      "venue": "Nonlinear dynamics",
      "authors": [
        "Guowei Wang",
        "Lijian Yang",
        "Xuan Zhan",
        "An-bang Li",
        "Ya Jia"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/4e64a5baf6be16007e4d6e9c4098dd6ff62680e4",
      "pdf_url": "https://www.researchsquare.com/article/rs-895256/latest.pdf",
      "publication_date": "2021-09-17",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2bf31e06af3408f0136147bf321fdeb024f50a1b",
      "title": "On-chip photonic diffractive optical neural network based on a spatial domain electromagnetic propagation model.",
      "abstract": "An integrated physical diffractive optical neural network (DONN) is proposed based on a standard silicon-on-insulator (SOI) substrate. This DONN has compact structure and can realize the function of machine learning with whole-passive fully-optical manners. The DONN structure is designed by the spatial domain electromagnetic propagation model, and the approximate process of the neuron value mapping is optimized well to guarantee the consistence between the pre-trained neuron value and the SOI integration implementation. This model can better ensure the manufacturability and the scale of the on-chip neural network, which can be used to guide the design and manufacturing of the real chip. The performance of our DONN is numerically demonstrated on the prototypical machine learning task of prediction of coronary heart disease from the UCI Heart Disease Dataset, and accuracy comparable to the state-of-the-art is achieved.",
      "year": 2021,
      "venue": "Optics Express",
      "authors": [
        "Tingzhao Fu",
        "Yubin Zang",
        "Honghao Huang",
        "Zhenmin Du",
        "Chengyang Hu",
        "Minghua Chen",
        "Sigang Yang",
        "Hong-wei Chen"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/2bf31e06af3408f0136147bf321fdeb024f50a1b",
      "pdf_url": "https://doi.org/10.1364/oe.435183",
      "publication_date": "2021-09-27",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c1cd3ef5bd9439de1f138ab5200a2c9cecf362af",
      "title": "Physical Side-Channel Attacks on Embedded Neural Networks: A Survey",
      "abstract": "During the last decade, Deep Neural Networks (DNN) have progressively been integrated on all types of platforms, from data centers to embedded systems including low-power processors and, recently, FPGAs. Neural Networks (NN) are expected to become ubiquitous in IoT systems by transforming all sorts of real-world applications, including applications in the safety-critical and security-sensitive domains. However, the underlying hardware security vulnerabilities of embedded NN implementations remain unaddressed. In particular, embedded DNN implementations are vulnerable to Side-Channel Analysis (SCA) attacks, which are especially important in the IoT and edge computing contexts where an attacker can usually gain physical access to the targeted device. A research field has therefore emerged and is rapidly growing in terms of the use of SCA including timing, electromagnetic attacks and power attacks to target NN embedded implementations. Since 2018, research papers have shown that SCA enables an attacker to recover inference models architectures and parameters, to expose industrial IP and endangers data confidentiality and privacy. Without a complete review of this emerging field in the literature so far, this paper surveys state-of-the-art physical SCA attacks relative to the implementation of embedded DNNs on micro-controllers and FPGAs in order to provide a thorough analysis on the current landscape. It provides a taxonomy and a detailed classification of current attacks. It first discusses mitigation techniques and then provides insights for future research leads.",
      "year": 2021,
      "venue": "Applied Sciences",
      "authors": [
        "M. M. Real",
        "R. Salvador"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/c1cd3ef5bd9439de1f138ab5200a2c9cecf362af",
      "pdf_url": "https://www.mdpi.com/2076-3417/11/15/6790/pdf?version=1627954373",
      "publication_date": "2021-07-23",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0d114776aa65bf3bbd0e5517de7b6727c529d6ae",
      "title": "Multistable dynamics in a Hopfield neural network under electromagnetic radiation and dual bias currents",
      "abstract": null,
      "year": 2021,
      "venue": "Nonlinear dynamics",
      "authors": [
        "Q. Wan",
        "Zidie Yan",
        "Fei Li",
        "Jiong Liu",
        "Simiao Chen"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/0d114776aa65bf3bbd0e5517de7b6727c529d6ae",
      "pdf_url": "",
      "publication_date": "2021-12-02",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c58266e1180a848310ff09143a71a4ba18e1ca86",
      "title": "S-MAPS: Scalable Mutual Authentication Protocol for Dynamic UAV Swarms",
      "abstract": "Unmanned Aerial Vehicles (UAVs) domain has seen rapid developments in recent years. UAVs have been deployed for many applications and missions like data transmission, cellular service provisioning, and computational offloading tasks etc. Yet, UAV deployment is still limited, partially owing to the security challenges it poses. UAVs are particularly vulnerable to physical capture, cloning attacks, eavesdropping, and man in the middle attacks. To address some of these security problems, this paper develops an authentication protocol for use in UAV swarms. To ensure physical security and rapid authentication, the proposed protocol uses Physical Unclonable Functions (PUFs). The protocol achieves high scalability compared to the state of the art by authenticating multiple devices at once. The proposed protocol supports dynamic topologies and multi-hop communication by using spanning tree-based traversal. It is also resistant to mobility, device tampering attack, etc., and its improvements are achieved at significantly lower communication and communication cost as compared to state-of-the-art protocols.",
      "year": 2021,
      "venue": "IEEE Transactions on Vehicular Technology",
      "authors": [
        "Gaurang Bansal",
        "B. Sikdar"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/c58266e1180a848310ff09143a71a4ba18e1ca86",
      "pdf_url": "",
      "publication_date": "2021-11-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "3b357acb6c807bc694372c773e507cdcef27d974",
      "title": "Leaky Nets: Recovering Embedded Neural Network Models and Inputs Through Simple Power and Timing Side-Channels\u2014Attacks and Defenses",
      "abstract": "With the recent advancements in machine learning theory, many commercial embedded microprocessors use neural network (NN) models for a variety of signal processing applications. However, their associated side-channel security vulnerabilities pose a major concern. There have been several proof-of-concept attacks demonstrating the extraction of their model parameters and input data. But, many of these attacks involve specific assumptions, have limited applicability, or pose huge overheads to the attacker. In this work, we study the side-channel vulnerabilities of embedded NN implementations by recovering their parameters using timing-based information leakage and simple power analysis side-channel attacks. We demonstrate our attacks on popular microcontroller platforms over networks of different precisions, such as floating point, fixed point, and binary networks. We are able to successfully recover not only the model parameters but also the inputs for the above networks. Countermeasures against timing-based attacks are implemented and their overheads are analyzed.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "A. Chandrakasan"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/3b357acb6c807bc694372c773e507cdcef27d974",
      "pdf_url": "",
      "publication_date": "2021-02-23",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "20a6a74e26f41d62fc85733e3fe07897d7e9eb90",
      "title": "Convolutional neural network inversion of airborne transient electromagnetic data",
      "abstract": "As an efficient geophysical exploration technique, airborne transient electromagnetics shows strong adaptability to complex terrains and can provide subsurface resistivity information rapidly with a dense spatial coverage. However, the huge volume of airborne transient electromagnetic data obtained from a large number of spatial locations presents a great challenge to real\u2010time airborne transient electromagnetic interpretation due to the high computational cost. Moreover, the inherent non\u2010uniqueness of the inverse problem also limits our ability to constrain the underground resistivity structure. In this study, we develop an entirely data\u2010driven convolutional neural network to solve the airborne transient electromagnetic inverse problem. Synthetic tests show that the convolutional neural network is computationally efficient and yields robust results. Compared with the Gauss\u2013Newton method, convolutional neural network inversion does not depend on the choices of an initial model and the regularization parameters and is less prone to getting trapped in a local minimum. We also demonstrate the general applicability of the convolutional neural network to three\u2010dimensional synthetic airborne transient electromagnetic responses and the field observations acquired from Leach Lake Basin, Fort Irwin, California. The efficient convolutional neural network inversion framework can support real\u2010time resistivity imaging of subsurface structures from airborne transient electromagnetic observations, providing a powerful tool for field explorations.",
      "year": 2021,
      "venue": "Geophysical Prospecting",
      "authors": [
        "Sihong Wu",
        "Qinghua Huang",
        "Li Zhao"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/20a6a74e26f41d62fc85733e3fe07897d7e9eb90",
      "pdf_url": "",
      "publication_date": "2021-08-19",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4b3eefba6fb0051ec4cecf30dce8b432c242f2b1",
      "title": "Watermarking Graph Neural Networks based on Backdoor Attacks",
      "abstract": "Graph Neural Networks (GNNs) have achieved promising performance in various real-world applications. Building a powerful GNN model is not a trivial task, as it requires a large amount of training data, powerful computing resources, and human expertise. Moreover, with the development of adversarial attacks, e.g., model stealing attacks, GNNs raise challenges to model authentication. To avoid copyright infringement on GNNs, verifying the ownership of the GNN models is necessary.This paper presents a watermarking framework for GNNs for both graph and node classification tasks. We 1) design two strategies to generate watermarked data for the graph classification task and one for the node classification task, 2) embed the watermark into the host model through training to obtain the watermarked GNN model, and 3) verify the ownership of the suspicious model in a black-box setting. The experiments show that our framework can verify the ownership of GNN models with a very high probability (up to 99%) for both tasks. We also explore our watermarking mechanism against an adaptive attacker with access to partial knowledge of the watermarked data. Finally, we experimentally show that our watermarking approach is robust against a state-of-the-art model extraction technique and four state-of-the-art defenses against backdoor attacks.",
      "year": 2021,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Jing Xu",
        "S. Picek"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/4b3eefba6fb0051ec4cecf30dce8b432c242f2b1",
      "pdf_url": "https://repository.ubn.ru.nl//bitstream/handle/2066/295585/295585.pdf",
      "publication_date": "2021-10-21",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "6583239ed1f9637d4eea8c5563aa13d719b712d5",
      "title": "Secure Automatic Speaker Verification (SASV) System Through sm-ALTP Features and Asymmetric Bagging",
      "abstract": "The growing number of voice-enabled devices and applications consider automatic speaker verification (ASV) a fundamental component. However, maximum outreach for ASV in critical domains e.g., financial services and health care, is not possible unless we overcome security breaches caused by voice cloning algorithms and replayed audios. Therefore, to overcome these vulnerabilities, a secure ASV (SASV) system based on the novel sign modified acoustic local ternary pattern (sm-ALTP) features and asymmetric bagging-based classifier-ensemble with enhanced attack vector is presented. The proposed audio representation approach clusters the high and low frequency components in audio frames by normally distributing frequency components against a convex function. Then, the neighborhood statistics are applied to capture the user specific vocal tract information. The proposed SASV system simultaneously verifies the bonafide speakers and detects the voice cloning attack, cloning algorithm used to synthesize cloned audio (in the defined settings), and voice-replay attacks over the ASVspoof 2019 dataset. In addition, the proposed method detects the voice replay and cloned voice replay attacks over the VSDC dataset. Both the voice cloning algorithm detection and cloned-replay attack detection are novel concepts introduced in this paper. The voice cloning algorithm detection module determines the voice cloning algorithm used to generate the fake audios. Whereas, the cloned voice replay attack detection is performed to determine the SASV behavior when audio samples are simultaneously contemplated with cloning and replay artifacts.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Muteb Aljasem",
        "Aun Irtaza",
        "Hafiz Malik",
        "Noushin Saba",
        "A. Javed",
        "K. Malik",
        "Mohammad Meharmohammadi"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/6583239ed1f9637d4eea8c5563aa13d719b712d5",
      "pdf_url": "https://doi.org/10.1109/tifs.2021.3082303",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "36654420b9a635bb860c2fc571a3fcc0a5398013",
      "title": "Nowhere to Hide: Efficiently Identifying Probabilistic Cloning Attacks in Large-Scale RFID Systems",
      "abstract": "Radio-Frequency Identification (RFID) is an emerging technology which has been widely applied in various scenarios, such as tracking, object monitoring, and social networks, etc. Cloning attacks can severely disturb the RFID systems, such as missed detection for the missing tags. Although there are some techniques with physical architecture design or complicated encryption and cryptography proposed to prevent the tags from being cloned, it is difficult to definitely avoid the cloning attack. Therefore, cloning attack detection and identification are critical for the RFID systems. Prior works rely on that each clone tag will reply to the reader when its corresponding genuine tag is queried. In this article, we consider a more general attack model, in which each clone tag replies to the reader\u2019s query with a predefined probability, i.e., attack probability. We concentrate on identifying the tags being attacked with the probability no less than a threshold $P_{t}$ with the required identification reliability $\\alpha $ . We first propose a basic protocol to Identify the Probabilistic Cloning Attacks with required identification reliability for the large-scale RFID systems called IPCA. Then we propose two enhanced protocols called MS-IPCA and S-IPCA respectively to improve the identification efficiency. We theoretically analyze the parameters of the proposed IPCA, MS-IPCA and S-IPCA protocols to maximize the identification efficiency. Finally we conduct extensive simulations to validate the effectiveness of the proposed protocols.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xin Ai",
        "Honglong Chen",
        "Kai Lin",
        "Zhibo Wang",
        "Jiguo Yu"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/36654420b9a635bb860c2fc571a3fcc0a5398013",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "a955a7b351821ab683a1070cf175f10d03d765c2",
      "title": "Complex-Valued Pix2pix\u2014Deep Neural Network for Nonlinear Electromagnetic Inverse Scattering",
      "abstract": "Nonlinear electromagnetic inverse scattering is an imaging technique with quantitative reconstruction and high resolution. Compared with conventional tomography, it takes into account the more realistic interaction between the internal structure of the scene and the electromagnetic waves. However, there are still open issues and challenges due to its inherent strong non-linearity, ill-posedness and computational cost. To overcome these shortcomings, we apply an image translation network, named as Complex-Valued Pix2pix, on the inverse scattering problem of electromagnetic field. Complex-Valued Pix2pix includes two parts of Generator and Discriminator. The Generator employs a multi-layer complex valued convolutional neural network, while the Discriminator computes the maximum likelihoods between the original value and the reconstructed value from the aspects of the two parts of the complex: real part and imaginary part, respectively. The results show that the Complex-Valued Pix2pix can learn the mapping from the initial contrast to the real contrast in microwave imaging models. Moreover, due to the introduction of discriminator, Complex-Valued Pix2pix can capture more features of nonlinearity than traditional Convolutional Neural Network (CNN) by confrontation training. Therefore, without considering the time cost of training, Complex-Valued Pix2pix may be a more effective way to solve inverse scattering problems than other deep learning methods. The main improvement of this work lies in the realization of a Generative Adversarial Network (GAN) in the electromagnetic inverse scattering problem, adding a discriminator to the traditional Convolutional Neural Network (CNN) method to optimize network training. It has the prospect of outperforming conventional methods in terms of both the image quality and computational efficiency.",
      "year": 2021,
      "venue": "Electronics",
      "authors": [
        "Liang Guo",
        "Guanfeng Song",
        "Hongsheng Wu"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/a955a7b351821ab683a1070cf175f10d03d765c2",
      "pdf_url": "https://www.mdpi.com/2079-9292/10/6/752/pdf?version=1616495420",
      "publication_date": "2021-03-22",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4f82688e6fff88ce92f1c927e5118728cd7d4867",
      "title": "A Theory-Guided Deep Neural Network for Time Domain Electromagnetic Simulation and Inversion Using a Differentiable Programming Platform",
      "abstract": "In this communication, a trainable theory-guided recurrent neural network (RNN) equivalent to the finite-difference-time-domain (FDTD) method is exploited to formulate electromagnetic propagation, solve Maxwell\u2019s equations, and the inverse problem on differentiable programming platform Pytorch. For forward modeling, the computation efficiency is substantially improved compared to conventional FDTD implemented on MATLAB. Gradient computation becomes more precise and faster than the traditional finite difference method benefiting from the accurate and efficient automatic differentiation on the differentiable programming platform. Moreover, by setting the trainable weights of RNN as the material-related parameters, an inverse problem can be solved by training the network. Numerical results demonstrate the effectiveness and efficiency of the method for forward and inverse electromagnetic modeling.",
      "year": 2021,
      "venue": "IEEE Transactions on Antennas and Propagation",
      "authors": [
        "Yanyan Hu",
        "Yuchen Jin",
        "Xuqing Wu",
        "Jiefu Chen"
      ],
      "citation_count": 31,
      "url": "https://www.semanticscholar.org/paper/4f82688e6fff88ce92f1c927e5118728cd7d4867",
      "pdf_url": "",
      "publication_date": "2021-07-26",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "12b0e9a2185229b5f2139cd5f9c9b26d34a01b70",
      "title": "NeurObfuscator: A Full-stack Obfuscation Tool to Mitigate Neural Architecture Stealing",
      "abstract": "Neural network stealing attacks have posed grave threats to neural network model deployment. Such attacks can be launched by extracting neural architecture information, such as layer sequence and dimension parameters, through leaky side-channels. To mitigate such attacks, we propose NeurObfuscator, a full-stack obfuscation tool to obfuscate the neural network architecture while preserving its functionality with very limited performance overhead. At the heart of this tool is a set of obfuscating knobs, including layer branching, layer widening, selective fusion and schedule pruning, that increase the number of operators, reduce/increase the latency, and number of cache and DRAM accesses. A genetic algorithm-based approach is adopted to orchestrate the combination of obfuscating knobs to achieve the best obfuscating effect on the layer sequence and dimension parameters so that the architecture information cannot be successfully extracted. Results on sequence obfuscation show that the proposed tool obfuscates a ResNet-18 ImageNet model to a totally different architecture (with 44 layer difference) without affecting its functionality with only 2% overall latency overhead. For dimension obfuscation, we demonstrate that an example convolution layer with 64 input and 128 output channels can be obfuscated to generate a layer with 207 input and 93 output channels with only a 2% latency overhead.",
      "year": 2021,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Jingtao Li",
        "Zhezhi He",
        "A. S. Rakin",
        "Deliang Fan",
        "C. Chakrabarti"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/12b0e9a2185229b5f2139cd5f9c9b26d34a01b70",
      "pdf_url": "https://arxiv.org/pdf/2107.09789",
      "publication_date": "2021-07-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c725eaed79d84ba73caaea3ae4d0e7298d3592d1",
      "title": "Teacher Model Fingerprinting Attacks Against Transfer Learning",
      "abstract": "Transfer learning has become a common solution to address training data scarcity in practice. It trains a specified student model by reusing or fine-tuning early layers of a well-trained teacher model that is usually publicly available. However, besides utility improvement, the transferred public knowledge also brings potential threats to model confidentiality, and even further raises other security and privacy issues. In this paper, we present the first comprehensive investigation of the teacher model exposure threat in the transfer learning context, aiming to gain a deeper insight into the tension between public knowledge and model confidentiality. To this end, we propose a teacher model fingerprinting attack to infer the origin of a student model, i.e., the teacher model it transfers from. Specifically, we propose a novel optimization-based method to carefully generate queries to probe the student model to realize our attack. Unlike existing model reverse engineering approaches, our proposed fingerprinting method neither relies on fine-grained model outputs, e.g., posteriors, nor auxiliary information of the model architecture or training dataset. We systematically evaluate the effectiveness of our proposed attack. The empirical results demonstrate that our attack can accurately identify the model origin with few probing queries. Moreover, we show that the proposed attack can serve as a stepping stone to facilitating other attacks against machine learning models, such as model stealing.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yufei Chen",
        "Chao Shen",
        "Cong Wang",
        "Yang Zhang"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/c725eaed79d84ba73caaea3ae4d0e7298d3592d1",
      "pdf_url": "",
      "publication_date": "2021-06-23",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1f99776ebfbac6da0689e8e43d36f61172c95a92",
      "title": "Back to the Basics: Seamless Integration of Side-Channel Pre-Processing in Deep Neural Networks",
      "abstract": "Deep learning approaches have become popular for Side-Channel Analysis (SCA) in the recent years. Especially Convolutional Neural Networks (CNN) due to their natural ability to overcome jitter-based as well as masking countermeasures. Most of the recent works have been focusing on optimising the performance on given dataset, for example finding optimal architecture and using ensemble, and bypass the need for trace pre-processing. However, trace pre-processing is a long studied topic and several proven techniques exist in the literature. There is no straightforward manner to integrate those techniques into deep learning based SCA. In this paper, we propose a generic framework which allows seamless integration of multiple, user defined pre-processing techniques into the neural network architecture. The framework is based on Multi-scale Convolutional Neural Networks ( $\\mathsf {MCNN}$ ) that were originally proposed for time series analysis. $\\mathsf {MCNN}$ are composed of multiple branches that can apply independent transformation to input data in each branch to extract the relevant features and allowing a better generalization of the model. In terms of SCA, these transformations can be used for integration of pre-processing techniques, such as phase-only correlation, principal component analysis, alignment methods, etc. We present successful results on generic network which generalizes to different publicly available datasets. Our findings show that it is possible to design a network that can be used in a more general way to analyze side-channel leakage traces and perform well across datasets.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Yoo-Seung Won",
        "Xiaolu Hou",
        "Dirmanto Jap",
        "J. Breier",
        "S. Bhasin"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/1f99776ebfbac6da0689e8e43d36f61172c95a92",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d4ac66823cdb39b6a79d5b4bb6986634ecd5a7e9",
      "title": "An Efficient Time-Domain Electromagnetic Algorithm Based on LSTM Neural Network",
      "abstract": "Although neural networks have been applied in many fields since they were first introduced, the feasibility of applying it to predict the solution of Maxwell's equations remains open. In this letter, we investigate the feasibility of utilizing the long- and short-term memory (LSTM) neural network to solve the time-domain electromagnetic (TDEM) forward problems. With ground truth datasets being generated from the finite-difference time-domain (FDTD) method, a novel LSTM-TDEM model structure is proposed, and trained by a new feasible algorithm specifically designed for time-domain simulation. The strong approximation ability of the LSTM-TDEM method can accurately predict the electromagnetic field distributions for topologies of different geometries, materials, and excitations at different locations. The effectiveness of the proposed LSTM-TDEM method has been validated by several numerical experiments. The average relative error can be as small as 0.63% for 2-D case and 0.35% for 3-D case. It is worth noting that the training data are only 5% (198/4087) and 30% (660/2189) in 2-D and 3-D cases, respectively. Meanwhile, compared with the traditional FDTD method, the proposed method greatly reduces the calculation time, with its speedup ratio more than 1800 and 44\u00a0000 times over the FDTD method in 2-D and 3-D cases, respectively.",
      "year": 2021,
      "venue": "IEEE Antennas and Wireless Propagation Letters",
      "authors": [
        "Fengbo Wu",
        "Mengning Fan",
        "Wenyuan Liu",
        "Bingyang Liang",
        "Yuanguo Zhou"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/d4ac66823cdb39b6a79d5b4bb6986634ecd5a7e9",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "73fad5cae8101ea2c32f918f180f8cead6b75c2d",
      "title": "Betalogger: Smartphone Sensor-based Side-channel Attack Detection and Text Inference Using Language Modeling and Dense MultiLayer Neural Network",
      "abstract": "With the recent advancement of smartphone technology in the past few years, smartphone usage has increased on a tremendous scale due to its portability and ability to perform many daily life tasks. As a result, smartphones have become one of the most valuable targets for hackers to perform cyberattacks, since the smartphone can contain individuals\u2019 sensitive data. Smartphones are embedded with highly accurate sensors. This article proposes BetaLogger, an Android-based application that highlights the issue of leaking smartphone users\u2019 privacy using smartphone hardware sensors (accelerometer, magnetometer, and gyroscope). BetaLogger efficiently infers the typed text (long or short) on a smartphone keyboard using Language Modeling and a Dense Multi-layer Neural Network (DMNN). BetaLogger is composed of two major phases: In the first phase, Text Inference Vector is given as input to the DMNN model to predict the target labels comprising the alphabet, and in the second phase, sequence generator module generate the output sequence in the shape of a continuous sentence. The outcomes demonstrate that BetaLogger generates highly accurate short and long sentences, and it effectively enhances the inference rate in comparison with conventional machine learning algorithms and state-of-the-art studies.",
      "year": 2021,
      "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
      "authors": [
        "A. R. Javed",
        "S. Rehman",
        "M. Khan",
        "M. Alazab",
        "H. Khan"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/73fad5cae8101ea2c32f918f180f8cead6b75c2d",
      "pdf_url": "http://qspace.qu.edu.qa/bitstream/10576/37656/2/3460392.pdf",
      "publication_date": "2021-06-23",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "cb47e5eafb9c57351aaab630b2307ff6d0c73188",
      "title": "Chaos control and analysis of fractional order neural network under electromagnetic radiation",
      "abstract": null,
      "year": 2021,
      "venue": "",
      "authors": [
        "F. Allehiany",
        "E. Mahmoud",
        "L. S. Jahanzaib",
        "P. Trikha",
        "Hammad Alotaibi"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/cb47e5eafb9c57351aaab630b2307ff6d0c73188",
      "pdf_url": "https://doi.org/10.1016/j.rinp.2020.103786",
      "publication_date": "2021-02-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "beec2b61ce9e50d81ff10a0d5e4e3c2a7e11f079",
      "title": "Imaging subsurface orebodies with airborne electromagnetic data using a recurrent neural network",
      "abstract": "Conventional interpretation of airborne electromagnetic data has been conducted by solving the inverse problem. However, with recent advances in machine learning (ML) techniques, a one-dimensional (1D) deep neural network inversion that predicts a 1D resistivity model using multi-frequency vertical magnetic fields and sensor height information at one location has been applied. Nevertheless, bacause the final interpretation of this 1D approach relies on connecting 1D resistivity models, 1D ML interpretation has low accuracy for the estimation of an isolated anomaly, as in conventional 1D inversion. Thus, we propose a two-dimensional (2D) interpretation technique that can overcome the limitations of 1D interpretation, and consider spatial continuity by using a recurrent neural network (RNN). We generated various 2D resistivity models, calculated the ratio of primary and induced secondary magnetic fields of vertical direction in ppm scale using vertical magnetic dipole source, and then trained the RNN using the resistivity models and the corresponding electromagnetic (EM) responses. To verify the validity of 2D RNN inversion, we applied the trained RNN to synthetic and field data. Through application of the field data, we demonstrated that the design of the training dataset is crucial to improve prediction performance in a 2D RNN inversion. In addition, we investigated changes in the RNN inversion results of field data dependent on the data preprocessing. We demonstrated that using two types of data, logarithmic transformed data and linear scale data, which having different patterns of input information can enhance the prediction performance of the EM inversion results.",
      "year": 2021,
      "venue": "Geophysics",
      "authors": [
        "M. Bang",
        "Seokmin Oh",
        "K. Noh",
        "S. Seol",
        "J. Byun"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/beec2b61ce9e50d81ff10a0d5e4e3c2a7e11f079",
      "pdf_url": "",
      "publication_date": "2021-08-24",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9c01d98827c36f143b0ce2cf073b1725135232e8",
      "title": "Study on propagation efficiency and fidelity of subthreshold signal in feed-forward hybrid neural network under electromagnetic radiation",
      "abstract": null,
      "year": 2021,
      "venue": "Nonlinear dynamics",
      "authors": [
        "Guowei Wang",
        "Mengyan Ge",
        "Lulu Lu",
        "Y. Jia",
        "Yunjie Zhao"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/9c01d98827c36f143b0ce2cf073b1725135232e8",
      "pdf_url": "",
      "publication_date": "2021-02-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "b4d93484790a68c575c4acbeac0cd99a58f46ebe",
      "title": "Copycat CNN: Are Random Non-Labeled Data Enough to Steal Knowledge from Black-box Models?",
      "abstract": null,
      "year": 2021,
      "venue": "Pattern Recognition",
      "authors": [
        "Jacson Rodrigues Correia-Silva",
        "Rodrigo Berriel",
        "C. Badue",
        "A. D. Souza",
        "Thiago Oliveira-Santos"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/b4d93484790a68c575c4acbeac0cd99a58f46ebe",
      "pdf_url": "http://arxiv.org/pdf/2101.08717",
      "publication_date": "2021-01-21",
      "keywords_matched": [
        "copycat CNN"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "b2212da1f8c968a60f70227623678a41fb63c3cf",
      "title": "Toward Invisible Adversarial Examples Against DNN-Based Privacy Leakage for Internet of Things",
      "abstract": "Deep neural networks (DNNs) can be utilized maliciously for compromising the privacy stored in electronic devices, e.g., identifying the images stored in a mobile phone connected to the Internet of Things (IoT). However, recent studies demonstrated that DNNs are vulnerable to adversarial examples, which are artificially designed perturbations in the original samples for misleading DNNs. Adversarial examples can be used to protect the DNN-based privacy leakage in mobile phones by replacing the photos with adversarial examples. To avoid affecting the normal use of photos, the adversarial examples need to be highly similar to original images. To handle a large number of photos stored in the devices at a proper time, the time efficiency of a method needs to be high enough. Previous methods cannot do well on both sides. In this article, we propose a broad class of selective gradient sign iterative algorithms to make adversarial examples useful in protecting the privacy of photos in IoT devices. By neglecting the unimportant image pixels in the iterative process of attacks according to the sort of first-order partial derivative, we control the optimization direction meticulously to reduce image distortions of adversarial examples without leveraging high time-consuming tricks. Extensive experimental results show that the proposed methods successfully fool the neural network classifiers for the image classification task with a small change in the visual effects and consume little calculating time simultaneously.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xuyang Ding",
        "Shuai Zhang",
        "Mengkai Song",
        "Xiaocong Ding",
        "Fagen Li"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/b2212da1f8c968a60f70227623678a41fb63c3cf",
      "pdf_url": "",
      "publication_date": "2021-01-15",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0189e3d93aa73ce3223899bfb47a1a71e7cde394",
      "title": "On the Security Risks of AutoML",
      "abstract": "Neural Architecture Search (NAS) represents an emerging machine learning (ML) paradigm that automatically searches for models tailored to given tasks, which greatly simplifies the development of ML systems and propels the trend of ML democratization. Yet, little is known about the potential security risks incurred by NAS, which is concerning given the increasing use of NAS-generated models in critical domains. This work represents a solid initial step towards bridging the gap. Through an extensive empirical study of 10 popular NAS methods, we show that compared with their manually designed counterparts, NAS-generated models tend to suffer greater vulnerability to various malicious attacks (e.g., adversarial evasion, model poisoning, and functionality stealing). Further, with both empirical and analytical evidence, we provide possible explanations for such phenomena: given the prohibitive search space and training cost, most NAS methods favor models that converge fast at early training stages; this preference results in architectural properties associated with attack vulnerability (e.g., high loss smoothness and low gradient variance). Our findings not only reveal the relationships between model characteristics and attack vulnerability but also suggest the inherent connections underlying different attacks. Finally, we discuss potential remedies to mitigate such drawbacks, including increasing cell depth and suppressing skip connects, which lead to several promising research directions.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Ren Pang",
        "Zhaohan Xi",
        "S. Ji",
        "Xiapu Luo",
        "Ting Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/0189e3d93aa73ce3223899bfb47a1a71e7cde394",
      "pdf_url": "",
      "publication_date": "2021-10-12",
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "6c2159b6efaab83678f98f68ec782f1ed8c8b36d",
      "title": "Stealing Machine Learning Parameters via Side Channel Power Attacks",
      "abstract": "Side-channel attacks target system implementation statistics, such as the electricity required to run a cryptographic function. Deriving cryptographic keys, such as AES keys, has become such a simplified process that extracting sensitive information from an otherwise secure algorithm requires only a $35USD microcontroller. While cryptographic algorithms indicate the presence of sensitive data, making them a preferable target, other systems hold valuable data with significantly less protection. Due to the ubiquity and rigidity of machine learning algorithms, the ability to infer model parameters has drastic security implications. This investigation extracted information from machine learning models through the use of traditional side-channel techniques. Specifically, a side-channel power analysis was performed using a ChipWhisperer Lite to extract information from Neural Networks and Linear Regression models running on a target microcontroller. Then, time series classification tasks were performed on the resultant power traces to determine the differences between the two models and their varied hyperparameters. Three such classification tasks were tested. In the first, a neural network was differentiated from a linear regression model with 100% accuracy. In the second, two neural networks with different sized hidden layers are classified with 97.92% accuracy. In the third, two virtually identical linear regression models are compared that differ only in the initial value of one hyperparameter. These models were only classified with 67.92% accuracy. Although the accuracy decreases as the models become more alike, these results indicate that machine learning model parameters can be inferred from power-based side-channel attacks.",
      "year": 2021,
      "venue": "IEEE Computer Society Annual Symposium on VLSI",
      "authors": [
        "Shaya Wolf",
        "Hui Hu",
        "Rafer Cooley",
        "Mike Borowczak"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/6c2159b6efaab83678f98f68ec782f1ed8c8b36d",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "stealing machine learning"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0242acadf4940cf94596b54f7fd6fb75af7bb20d",
      "title": "Beyond Model Extraction: Imitation Attack for Black-Box NLP APIs",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Qiongkai Xu",
        "Xuanli He",
        "L. Lyu",
        "Lizhen Qu",
        "Gholamreza Haffari"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0242acadf4940cf94596b54f7fd6fb75af7bb20d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "imitation attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0e7081f0f76faabda1f9c6497fc9a83b1c051c71",
      "title": "Ownership Verification of DNN Architectures via Hardware Cache Side Channels",
      "abstract": "Deep Neural Networks (DNN) are gaining higher commercial values in computer vision applications, e.g., image classification, video analytics, etc. This calls for urgent demands of the intellectual property (IP) protection of DNN models. In this paper, we present a novel watermarking scheme to achieve the ownership verification of DNN architectures. Existing works all embedded watermarks into the model parameters while treating the architecture as public property. These solutions were proven to be vulnerable by an adversary to detect or remove the watermarks. In contrast, we claim the model architectures as an important IP for model owners, and propose to implant watermarks into the architectures. We design new algorithms based on Neural Architecture Search (NAS) to generate watermarked architectures, which are unique enough to represent the ownership, while maintaining high model usability. Such watermarks can be extracted via side-channel-based model extraction techniques with high fidelity. We conduct comprehensive experiments on watermarked CNN models for image classification tasks and the experimental results show our scheme has negligible impact on the model performance, and exhibits strong robustness against various model transformations and adaptive attacks.",
      "year": 2021,
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "authors": [
        "Xiaoxuan Lou",
        "Shangwei Guo",
        "Jiwei Li",
        "Tianwei Zhang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0e7081f0f76faabda1f9c6497fc9a83b1c051c71",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/159773/2/main.pdf",
      "publication_date": "2021-02-06",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "c09eaf8e58fabd2371294b26a37376be960546a2",
      "title": "Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Sebastian Szyller",
        "Vasisht Duddu",
        "Tommi Grondahl",
        "N. Asokan"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/c09eaf8e58fabd2371294b26a37376be960546a2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2ce7526221d6f4c1ef3d9a745e147ba70678d857",
      "title": "Hyperparameters optimization of neural network using improved particle swarm optimization for modeling of electromagnetic inverse problems",
      "abstract": "Abstract Optimization of hyperparameters of artificial neural network (ANN) usually involves a trial and error approach which is not only computationally expensive but also fails to predict a near-optimal solution most of the time. To design a better optimized ANN model, evolutionary algorithms are widely utilized to determine hyperparameters. This work proposes hyperparameters optimization of the ANN model using an improved particle swarm optimization (IPSO) algorithm. The different ANN hyperparameters considered are a number of hidden layers, neurons in each hidden layer, activation function, and training function. The proposed technique is validated using inverse modeling of two meander line electromagnetic bandgap unit cells and a slotted ultra-wideband antenna loaded with EBG structures. Three other evolutionary algorithms viz. hybrid PSO, conventional PSO, and genetic algorithm are also adopted for the hyperparameter optimization of the ANN models for comparative analysis. Performances of all the models are evaluated using quantitative assessment parameters viz. mean square error, mean absolute percentage deviation, and coefficient of determination (R2). The comparative investigation establishes the accurate and efficient prediction capability of the ANN models tuned using IPSO compared to other evolutionary algorithms.",
      "year": 2021,
      "venue": "International journal of microwave and wireless technologies",
      "authors": [
        "Debanjali Sarkar",
        "T. Khan",
        "Fazal Ahmed Talukdar"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/2ce7526221d6f4c1ef3d9a745e147ba70678d857",
      "pdf_url": "",
      "publication_date": "2021-12-17",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "fe139f3d17dc9460332c198be352080b964ea939",
      "title": "Yes We can: Watermarking Machine Learning Models beyond Classification",
      "abstract": "Since machine learning models have become a valuable asset for companies, watermarking techniques have been developed to protect the intellectual property of these models and prevent model theft. We observe that current watermarking frameworks solely target image classification tasks, neglecting a considerable part of machine learning techniques. In this paper, we propose to address this lack and study the watermarking process of various machine learning techniques such as machine translation, regression, binary image classification and reinforcement learning models. We adapt current definitions to each specific technique and we evaluate the main characteristics of the watermarking process, in particular the robustness of the models against a rational adversary. We show that watermarking models beyond classification is possible while preserving their overall performance. We further investigate various attacks and discuss the importance of the performance metric in the verification process and its impact on the success of the adversary.",
      "year": 2021,
      "venue": "IEEE Computer Security Foundations Symposium",
      "authors": [
        "Sofiane Lounici",
        "M. Njeh",
        "Orhan Ermis",
        "Melek \u00d6nen",
        "S. Trabelsi"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/fe139f3d17dc9460332c198be352080b964ea939",
      "pdf_url": "https://hal.archives-ouvertes.fr/hal-03220793/file/publi-6532.pdf",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3813c369accb4b87b5882eec943511fe2e8b4767",
      "title": "Killing Two Birds with One Stone: Stealing Model and Inferring Attribute from BERT-based APIs",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Lingjuan Lyu",
        "Xuanli He",
        "Fangzhao Wu",
        "Lichao Sun"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/3813c369accb4b87b5882eec943511fe2e8b4767",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2c47435498b8e67873cce1cc86cfbf8397b18286",
      "title": "Timing Black-Box Attacks: Crafting Adversarial Examples through Timing Leaks against DNNs on Embedded Devices",
      "abstract": "Deep neural networks (DNNs) have been applied to various industries. In particular, DNNs on embedded devices have attracted considerable interest because they allow real-time and distributed processing on site. However, adversarial examples (AEs), which add small perturbations to the input data of DNNs to cause misclassification, are serious threats to DNNs. In this paper, a novel black-box attack is proposed to craft AEs based only on processing time, i.e., the side-channel leaks from DNNs on embedded devices. Unlike several existing black-box attacks that utilize output probability, the proposed attack exploits the relationship between the number of activated nodes and processing time without using training data, model architecture, parameters, substitute models, or output probability. The perturbations for AEs are determined by the differential processing time based on the input data of the DNNs in the proposed attack. The experimental results show that the AEs of the proposed attack effectively cause an increase in the number of activated nodes and the misclassification of one of the incorrect labels against the DNNs on a microcontroller unit. Moreover, these results indicate that the attack can evade gradient-masking and confidence reduction countermeasures, which conceal the output probability, to prevent the crafting of AEs against several black-box attacks. Finally, the countermeasures against the attack are implemented and evaluated to clarify that the implementation of an activation function with data-dependent timing leaks is the cause of the proposed attack.",
      "year": 2021,
      "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.",
      "authors": [
        "Tsunato Nakai",
        "Daisuke Suzuki",
        "T. Fujino"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/2c47435498b8e67873cce1cc86cfbf8397b18286",
      "pdf_url": "https://doi.org/10.46586/tches.v2021.i3.149-175",
      "publication_date": null,
      "keywords_matched": [
        "DNN weights leakage (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5df6b233b3e5eeb0f0da34158cbb53f34bb0960c",
      "title": "Side-Channel based Disassembler for AVR Micro-Controllers using Convolutional Neural Networks",
      "abstract": "Reverse engineering using Side Channel Attacks (SCA) have been known as a serious menace against embedded devices. The attacker could employ side channel data to retrieve some sensitive information from the device, security analysis, existence of a library in the device or execution of a special stream of codes. Side channel data could be gathered from the power consumption or electromagnetic radiations by the device. In this paper, we propose a disassembler to extract the instructions of the device under attack. A deep convolutional neural network is employed to make templates of the target to use it for real-time scenarios. Short Time Fourier Transform (STFT), and Mel-Frequency Cepstrum Coefficients (MFCC) are utilized as feature extractors. The proposed method consists of two different parts: 1) Hierarchical scenario and 2) Sole model. Atmel 8-bit AVR micro-controller is employed as the target device under attack. Our results indicate that, even with an experimental and low cost setup a vast number of instructions are detectable. The proposed method reaches 98.21% accuracy on the real code, outperforms state-of-the-art methods on the proposed dataset.",
      "year": 2021,
      "venue": "ISC Conference on Information Security and Cryptology",
      "authors": [
        "Pouya Narimani",
        "M. Akhaee",
        "Seyedamin Habibi"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/5df6b233b3e5eeb0f0da34158cbb53f34bb0960c",
      "pdf_url": "",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0d2e0d3c8fb930a9a577ec60152c219995cc6b10",
      "title": "Neural Network Stealing via Meltdown",
      "abstract": "Deep learning services are now deployed in various fields on top of cloud infrastructures. In such cloud environment, virtualization technology provides logically independent and isolated computing space for each tenant. However, recent studies demonstrate that by leveraging vulnerabilities of virtualization techniques and shared processor architectures in the cloud system, various side-channels can be established between cloud tenants. In this paper, we propose a novel attack scenario that can steal internal information of deep learning models by exploiting the Meltdown vulnerability in a multitenant system environment. On the basis of our experiment, the proposed attack method could extract internal information of a TensorFlow deep learning service with 92.875% accuracy and 1.325kB/s extraction speed.",
      "year": 2021,
      "venue": "International Conference on Information Networking",
      "authors": [
        "Hoyong Jeong",
        "Dohyun Ryu",
        "Junbeom Hur"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0d2e0d3c8fb930a9a577ec60152c219995cc6b10",
      "pdf_url": "",
      "publication_date": "2021-01-13",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "beaa48a90464b1d9df6d42566a42c7326cc75826",
      "title": "Shuffling Countermeasure against Power Side-Channel Attack for MLP with Software Implementation",
      "abstract": null,
      "year": 2021,
      "venue": "International Conference on Electrical and Control Engineering",
      "authors": [
        "Y. Nozaki",
        "M. Yoshikawa"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/beaa48a90464b1d9df6d42566a42c7326cc75826",
      "pdf_url": "",
      "publication_date": "2021-12-17",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "541d59eb64b3ce26c7846f5de0b8f39f4e16de2a",
      "title": "Study on intelligent anti\u2013electricity stealing early-warning technology based on convolutional neural networks",
      "abstract": "In recent years, electricity stealing has been repeatedly prohibited, and as the methods of stealing electricity have become more intelligent and concealed, it is growing increasingly difficult to extract high-dimensional data features of power consumption. In order to solve this problem, a correlation model of power-consumption data based on convolutional neural networks (CNN) is established. First, the original user signal is preprocessed to remove the noise. The user signal with a fixed signal length is then intercepted and the parallel class labelled. The segmented user signals and corresponding labels are input into the convolutional neural network for training, and the trained convolutional neural network is then used to detect and classify the test user signals. Finally, the actual steal leak dataset is used to verify the effectiveness of this algorithm, which proves that the algorithm can effectively carry out anti\u2013-electricity stealing by warning of abnormal power consumption behavior. There are lots of line traces on the surface of the broken ends which left in the cable cutting case crime scene along the high-speed railway in China. The line traces usually present nonlinear morphological features and has strong randomness. It is not very effective when using existing image-processing and three-dimensional scanning methods to do the trace comparison, therefore, a fast algorithm based on wavelet domain feature aiming at the nonlinear line traces is put forward to make fast trace analysis and infer the criminal tools. The proposed algorithm first applies wavelet decomposition to the 1-D signals which picked up by single point laser displacement sensor to partially reduce noises. After that, the dynamic time warping is employed to do trace feature similarity matching. Finally, using linear regression machine learning algorithm based on gradient descent method to do constant iteration. The experiment results of cutting line traces sample data comparison demonstrate the accuracy and reliability of the proposed algorithm.",
      "year": 2021,
      "venue": "Journal of Intelligent & Fuzzy Systems",
      "authors": [
        "Nan Pan",
        "Xin Shen",
        "Xiaojue Guo",
        "M. Cao",
        "Dilin Pan"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/541d59eb64b3ce26c7846f5de0b8f39f4e16de2a",
      "pdf_url": "",
      "publication_date": "2021-01-11",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "73ac60e0f1fa8bbe7de599df66d2cbaef9a9b268",
      "title": "Tenet: A Neural Network Model Extraction Attack in Multi-core Architecture",
      "abstract": "As neural networks (NNs) are being widely deployed in many cloud-oriented systems for safety-critical tasks, the privacy and security of NNs become significant concerns to users in the cloud platform that shares the computation infrastructure such as memory resource. In this work, we observed that the memory timing channel in the shared memory of cloud multi-core architecture poses the risk of network model information leakage. Based on the observation, we propose a learning-based method to steal the model architecture of the NNs by exploiting the memory timing channel without any high-level privilege or physical access. We first trained an end-to-end measurement network offline to learn the relation between memory timing information and NNs model architecture. Then, we performed an online attack and reconstructed the target model using the prediction from the measurement network. We evaluated the proposed attack method on a multi-core architecture simulator. The experimental results show that our learning-based attack method can reconstruct the target model with high accuracy and improve the adversarial attack success rate by 42.4%.",
      "year": 2021,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Chengsi Gao",
        "Bing Li",
        "Ying Wang",
        "Weiwei Chen",
        "Lei Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/73ac60e0f1fa8bbe7de599df66d2cbaef9a9b268",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3453688.3461512",
      "publication_date": "2021-06-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "44cc3c0d5a80fecd1b6285c73f79986852135162",
      "title": "ML-Stealer: Stealing Prediction Functionality of Machine Learning Models with Mere Black-Box Access",
      "abstract": "Machine Learning (ML) models are progressively deployed in many real-world applications to perform a wide range of tasks, but are exposed to the security and privacy threats which aim to infer the details and even steal the functionality of the ML models. Despite extensive attacking efforts which rely on white-box or gray-box access, how to perform attacks with black-box access continues to be elusive. Aspiring to fill this gap, we move one step further and present ML-Stealer that can steal the functionality of any type of ML models with mere black-box access. With two algorithm designs, namely, synthetic data generation and replica model construction, ML-Stealer can construct a deep neural network (DNN)-based replica model which has the similar prediction functionality to the victim ML model. ML-Stealer does not require any knowledge about the victim model, nor does it enforce the access to statistical information or samples of the victim's training data. Experiment results demonstrate that ML-Stealer can achieve the consistent prediction results with the victim model of an averaged testing accuracy of 85.6%, and up to 93.6% at best.",
      "year": 2021,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Gaoyang Liu",
        "Shijie Wang",
        "Borui Wan",
        "Zekun Wang",
        "Chen Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/44cc3c0d5a80fecd1b6285c73f79986852135162",
      "pdf_url": "",
      "publication_date": "2021-10-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "130ce87d6141db6954d50d43b402583f7682ead8",
      "title": "Data-Driven Electromagnetic Scalar Field Estimation of a Patch Antenna Using Deep Convolutional Neural Network",
      "abstract": "Artificial neural network (ANN) is emerging as an alternative approach for numerical electromagnetic (EM) antenna modeling. In this paper, we focus on predicting the near-field properties of antennas using a data-driven approach, and exploiting the universal function approximator feature of the ANNs. Specifically, we demonstrate the use of a convolutional neural network (CNN) to estimate the surface current on a patch antenna. This field analysis prediction allows a pathway for the prediction of near-field and far-field properties and has the potential to replace full EM modeling. Based on our results, our neural network predicted with only an 11% error on average the desired surface currents, showing promise for the proposed approach.",
      "year": 2021,
      "venue": "2021 IEEE International Symposium on Antennas and Propagation and USNC-URSI Radio Science Meeting (APS/URSI)",
      "authors": [
        "Md Rayhan Khan",
        "C. Zekios",
        "S. Bhardwaj",
        "S. Georgakopoulos"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/130ce87d6141db6954d50d43b402583f7682ead8",
      "pdf_url": "",
      "publication_date": "2021-12-04",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "03b7ef4ed1de617e408ed31ee2f9b7c0cb3e4546",
      "title": "Timing shift-based bi-residual network model for the detection of electricity stealing",
      "abstract": "With the increasing number of electricity stealing users, the interests of countries are jeopardized and it brings economic burden to the government. However, due to the small-scale stealing and its random time coherence, it is difficult to find electricity stealing users. To solve this issue, we first generate the hybrid dataset composed of real electricity data and specific electricity stealing data. Then, we put forward the timing shift-based bi-residual network (TS-BiResNet) model. It learns the features of electricity consumption data on two aspects, i.e., shallow features and deep features, and meanwhile takes time factor into consideration. The simulation results show that TS-BiResNet model can detect electricity stealing behaviors that are small scaled and randomly coherent with time. Besides, its detection accuracy is superior to the benchmark schemes, i.e., long short-term memory (LSTM), gated recurrent unit (GRU), combined convolutional neural network and LSTM (CNN-LSTM) and Bi-ResNet.",
      "year": 2021,
      "venue": "EURASIP Journal on Advances in Signal Processing",
      "authors": [
        "Jie Lu",
        "Jingfu Li",
        "Wenjiang Feng",
        "Yongqi Zou",
        "Juntao Zhang",
        "Yuan Li"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/03b7ef4ed1de617e408ed31ee2f9b7c0cb3e4546",
      "pdf_url": "https://asp-eurasipjournals.springeropen.com/track/pdf/10.1186/s13634-022-00865-4",
      "publication_date": "2021-12-10",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "33ae2ced125f5a1ebf14d6120b0049fe31e931c0",
      "title": "A Backpropagation Extreme Learning Machine Approach to Fast Training Neural Network-Based Side-Channel Attack",
      "abstract": "This work presented new Deep learning Side-channel Attack (DL-SCA) models that are based on Extreme Learning Machine (ELM). Unlike the conventional iterative backpropagation method, ELM is a fast learning algorithm that computes the trainable weights within a single iteration. Two models (Ensemble bpELM and CAE-ebpELM) are designed to perform SCA on AES with Boolean masking and desynchronization/jittering. The best models for both attack tasks can be trained 27x faster than MLP and 5x faster than CNN respectively. Verified and validated using ASCAD dataset, our models successfully recover all 16 subkeys using approximately 3K traces in the worst case scenario.",
      "year": 2021,
      "venue": "Asian Hardware-Oriented Security and Trust Symposium",
      "authors": [
        "Xuyang Huang",
        "M. Wong",
        "A. Do",
        "W. Goh"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/33ae2ced125f5a1ebf14d6120b0049fe31e931c0",
      "pdf_url": "",
      "publication_date": "2021-12-16",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d31062435210a9ccca679114ffc0e089744ae683",
      "title": "Bandwidth Utilization Side-Channel on ML Inference Accelerators",
      "abstract": "Accelerators used for machine learning (ML) inference provide great performance benefits over CPUs. Securing confidential model in inference against off-chip side-channel attacks is critical in harnessing the performance advantage in practice. Data and memory address encryption has been recently proposed to defend against off-chip attacks. In this paper, we demonstrate that bandwidth utilization on the interface between accelerators and the weight storage can serve a side-channel for leaking confidential ML model architecture. This side channel is independent of the type of interface, leaks even in the presence of data and memory address encryption and can be monitored through performance counters or through bus contention from an on-chip unprivileged process.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Sarbartha Banerjee",
        "Shijia Wei",
        "Prakash Ramrakhyani",
        "Mohit Tiwari"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/d31062435210a9ccca679114ffc0e089744ae683",
      "pdf_url": "",
      "publication_date": "2021-10-14",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "89e225f6d755599d14d25580315d60725977ca62",
      "title": "Fast Prediction for Electromagnetic Shielding Effectiveness of Ground-Via Distribution of SiP by Convolutional Neural Network",
      "abstract": "Ground vias are often used as an essential shielding structure in System-in-Package (SiP) to suppress electromagnetic leakage. In this paper, a new method based on Convolutional Neural Network (CNN) is adopted to predict the shielding effectiveness of ground-via distributions. In particular, the suitability of the pooling layer is studied separately. Furthermore, the appropriateness of using the convolutional layer is proved by a comparative experiment with a Deep Neural Network (DNN) model. The method proposed in this paper shows good accuracy in electromagnetic leakage prediction and has universal value in similar prediction tasks. Compared with traditional analysis methods, the proposed CNN model has a significant time advantage, making it possible to use other optimization algorithms to replace the artificial shielding structure design.",
      "year": 2021,
      "venue": "2021 13th Global Symposium on Millimeter-Waves & Terahertz (GSMM)",
      "authors": [
        "Zheming Gu",
        "Tuomin Tao",
        "Erping Li"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/89e225f6d755599d14d25580315d60725977ca62",
      "pdf_url": "",
      "publication_date": "2021-05-23",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "8c9027b106cee47d9012bff1a29e212207d4d1bc",
      "title": "Daps: A Dynamic Asynchronous Progress Stealing Model for MPI Communication",
      "abstract": "MPI provides nonblocking point-to-point and one-sided communication models to help applications achieve communication and computation overlap. These models provide the opportunity for MPI to offload data transfer to low level network hardware while the user process is computing. In practice, however, MPI implementations have to often handle complex data transfer in software due to limited capability of network hardware. Therefore, additional asynchronous progress is necessary to ensure prompt progress of these software-handled communication. Traditional mechanisms either spawn an additional background thread on each MPI process or launch a fixed number of helper processes on each node. Both mechanisms may degrade performance in user computation due to statically occupied CPU resources. The user has to fine-tune the progress resource deployment to gain overall performance. For complex multiphase applications, unfortunately, severe performance degradation may occur due to dynamically changing communication characteristics and thus changed progress requirement. This paper proposes a novel Dynamic Asynchronous Progress Stealing model, called Daps, to completely address the asynchronous progress complication. Daps is implemented inside the MPI runtime. It dynamically leverages idle MPI processes to steal communication progress tasks from other busy computing processes located on the same node. The basic concept of Daps is straightforward; however, various implementation challenges have to be resolved due to the unique requirements of interprocess data and code sharing. We present our design that ensures high performance while maintaining strict program correctness. We compare Daps with state-of-the-art asynchronous progress approaches by utilizing both microbenchmarks and HPC proxy applications.",
      "year": 2021,
      "venue": "IEEE International Conference on Cluster Computing",
      "authors": [
        "Kaiming Ouyang",
        "Min Si",
        "A. Hori",
        "Zizhong Chen",
        "P. Balaji"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8c9027b106cee47d9012bff1a29e212207d4d1bc",
      "pdf_url": "",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e651f76459263a85e4123a6838b71aaf5c645358",
      "title": "A New Foe in GPUs: Power Side-Channel Attacks on Neural Network",
      "abstract": null,
      "year": 2021,
      "venue": "IEEE International Symposium on Quality Electronic Design",
      "authors": [
        "Hyeran Jeon",
        "Nima Karimian",
        "T. Lehman"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/e651f76459263a85e4123a6838b71aaf5c645358",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5416ffbbed2cec2469ff08e5d4d7c8d2810e0d15",
      "title": "JAXED: Reverse Engineering DNN Architectures Leveraging JIT GEMM Libraries",
      "abstract": "General matrix multiplication (GEMM) libraries on x86 architectures have recently adopted Just-in-time (JIT) based optimizations to dramatically reduce the execution time of small and medium-sized matrix multiplication. The exploitation of the latest CPU architectural extensions, such as the AVX2 and AVX-512 extensions, are the target for these optimizations. Although JIT compilers can provide impressive speedups to GEMM libraries, they expose a new attack surface through the built-in JIT code caches. These software-based caches allow an adversary to extract sensitive information through carefully designed timing attacks. The attack surface of such libraries has become more prominent due to their widespread integration into popular Machine Learning (ML) frameworks such as PyTorch and Tensorflow.In our paper, we present a novel attack strategy for JIT-compiled GEMM libraries called JAXED. We demonstrate how an adversary can exploit the GEMM library\u2019s vulnerable state management to extract confidential CNN model hyperparameters. We show that using JAXED, one can successfully extract the hyperparameters of models with fully-connected layers with an average accuracy of 92%. Further, we demonstrate our attack against the final fully connected layer of 10 popular DNN models. Finally, we perform an end-to-end attack on MobileNetV2, on both the convolution and FC layers, successfully extracting model hyperparameters.",
      "year": 2021,
      "venue": "Seed",
      "authors": [
        "Malith Jayaweera",
        "Kaustubh Shivdikar",
        "Yanzhi Wang",
        "D. Kaeli"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/5416ffbbed2cec2469ff08e5d4d7c8d2810e0d15",
      "pdf_url": "",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "reverse engineering DNN"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "6ac1497696276d63b01b4589eadc4cdbfe50f827",
      "title": "Leakage Reuse for Energy Efficient Near-Memory Computing of Heterogeneous DNN Accelerators",
      "abstract": "The exploration of custom deep neural network (DNN) accelerators for highly energy constrained edge devices with on-device intelligence is gaining traction in the research community. Despite the superior throughput and performance of custom accelerators as compared to CPUs or GPUs, the energy efficiency and versatility of state-of-the-art DNN accelerators is constrained due to a) the storage and movement of a large volume of data and b) the limited scope of monolithic architectures, where the entire accelerator executes only a single model at any given time. In this paper, a multi-voltage domain heterogeneous DNN accelerator is proposed that executes multiple models simultaneously with different power-performance operating points. The proposed architecture concurrently implements near-memory computing and leakage reuse, where the leakage current of idle memory banks within each processing element is utilized to deliver current to the adjacently placed multiply-and-accumulate (MAC) units. The proposed architecture and circuit techniques are evaluated with SPICE simulation in a 65 nm CMOS technology. The simulation results indicate that the proposed heterogeneous architecture with leakage reuse results in an energy efficiency of 3.27 tera-operations per second per watt (TOPS/W) as compared to a conventional monolithic and single voltage domain architecture that exhibits an energy efficiency of 0.0458 TOPS/W. In addition, the proposed accelerator that implements the leakage reuse technique on only half of the memory elements storing the weights reduces the power consumption of the sub-arrays of processing elements by 26% (99.4 mW) as compared to an accelerator that does not apply leakage reuse.",
      "year": 2021,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Md. Shazzad Hossain",
        "I. Savidis"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/6ac1497696276d63b01b4589eadc4cdbfe50f827",
      "pdf_url": "https://doi.org/10.1109/jetcas.2021.3121687",
      "publication_date": "2021-12-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "57c33299e83db01a15807ed5e0f1f1abb2c92578",
      "title": "Towards Characterizing Model Extraction Queries and How to Detect Them",
      "abstract": null,
      "year": 2021,
      "venue": "",
      "authors": [
        "Zhanyuan Zhang",
        "Yizheng Chen",
        "David A. Wagner"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/57c33299e83db01a15807ed5e0f1f1abb2c92578",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "866dab5d9f5ec63ae4b323479266b2a20c7150b2",
      "title": "Parasite: Mitigating Physical Side-Channel Attacks Against Neural Networks",
      "abstract": null,
      "year": 2021,
      "venue": "SPACE",
      "authors": [
        "H. Chabanne",
        "J. Danger",
        "Linda Guiga",
        "U. K\u00fchne"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/866dab5d9f5ec63ae4b323479266b2a20c7150b2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "37454d318bd3300a96db6f61164d19c46e117e36",
      "title": "Electromagnetic Modeling of Microstrip Patch Antennas: A Neural Network Approach",
      "abstract": "This paper aims to elucidate a design procedure that can help solve both the synthesis and analysis parts of an electromagnetic microstrip patch antenna problem. Unlike numerical conventional modeling methods or maybe even analytical methods, artificial neural network are way faster and easier to perform due to their robust correlation and high accuracy. They are also computationally inexpensive when compared to tradition analytical and conventional methods alike. Due to their brevity and highly flexible nature, they are easy to train and work on and can be expanded or modelled in different ways as per the use case owing to their plasticity. Conventional methods are not only time consuming put also require high computational capacity unlike the neural network perspective which has been proposed here. Hence, this paper gives a brief overview of the same and also includes a brief background and basic working of artificial neural networks, activation functions, Radial Basis functions, etc. from both the analysis and synthesis perspective from a RF designers viewpoint. Unlike previous soft computing neural network approaches like back propagation and regression techniques to achieve the same purpose, this paper uses a custom designed Radial Basis Network due to its proven history of computational inexpensiveness and high accuracy and validates its accuracy through the results shown. Circuit design and yield optimizations using a method of moments(MoM) based electromagnetic simulation software was also done to verify accuracy and to feed the ground truth to the network. This neural network was implemented in python due to its vast resource library and ease of implementation.",
      "year": 2021,
      "venue": "2021 6th International Conference for Convergence in Technology (I2CT)",
      "authors": [
        "R. S",
        "N. Gupta"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/37454d318bd3300a96db6f61164d19c46e117e36",
      "pdf_url": "",
      "publication_date": "2021-04-02",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1759582a0b818cdb05dabdb8cb5349230011dcaa",
      "title": "A Novel Neural Network Cell Method for Solving Nonlinear Electromagnetic Problems",
      "abstract": "Effective analysis of nonlinear electromagnetic fields is essential for the accurate modeling of electromagnetic devices, such as transformers, generators, and motors. This paper proposes a novel approach of coupled neural network (NN) and cell method (CM) or NNCM for solving nonlinear electromagnetic problems with ferromagnetic domains. While the topologically linear relations of the cell complexes are mathematically assembled through a transformation in the Tonti diagram by the CM, and the constitutive nonlinear magnetic relations are dealt with by partially connected NN for the fast prediction of the permeability distribution inside the ferromagnetic domain. Since the construction of NN is directly related to the grid connections, a partially connected NN structure with a small number of neurons can reduce the computational cost of the training process. By using a compact NN, the proposed NNCM can effectively eliminate the time consuming iterations for determining the nonlinear permeability distribution, and improve the computational efficiency significantly. The NNCM is employed to analyze the transient electromagnetic field distribution inside a cylindrical ferromagnetic core. The results are compared with those obtained by the traditional iterative CM, which determines the nonlinear permeability distribution by lengthy numerical iterations, to verify the feasibility and effectiveness of the proposed NNCM.",
      "year": 2021,
      "venue": "Advanced Theory and Simulations",
      "authors": [
        "G. Zhu",
        "Longnv Li",
        "Weinong Fu",
        "Ming Xue",
        "Tao Liu",
        "Jianguo Zhu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/1759582a0b818cdb05dabdb8cb5349230011dcaa",
      "pdf_url": "",
      "publication_date": "2021-10-13",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1bedbcf7fe37eee6e34250cae96973841c3bb0a7",
      "title": "Digital Watermarking System for Hard Cover Objects Against Cloning Attacks",
      "abstract": "We consider an application of digital watermark system technique for hard carriers (paper or plastic cover objects in addition). Algorithms of watermark embedding and extraction are proposed and the corresponding error probabilities of the extracted bits both for informed and blind decoders are presented. The spread spectrum signals used for embedding of watermark are optimized on their parameters. Similar experimental results are presented for data matrices carriers depending on paper sizes of data matrix copies. A protection against so-called \u201ccloning\u201d attack is elaborated where certificates are copied by attack scanner or photo camera and next printed and fixed as forges. The formulas for a missing the cloning attack and false alarm probabilities are proved. A full-scale experiment with a real scanner and printer confirms that the reliability of cloning attack detection can be provided under appropriate selection of watermark system parameters.",
      "year": 2021,
      "venue": "2021 30th Conference of Open Innovations Association FRUCT",
      "authors": [
        "V. Korzhik",
        "V. Starostin",
        "V. Yakovlev",
        "D. Flaksman",
        "Ivan Bukshin",
        "B. Izotov"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/1bedbcf7fe37eee6e34250cae96973841c3bb0a7",
      "pdf_url": "",
      "publication_date": "2021-10-27",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "08b2808afb0b2b81325a0b77e2617b180b8a684e",
      "title": "Quantitative estimation of side-channel leaks with neural networks",
      "abstract": null,
      "year": 2021,
      "venue": "International Journal on Software Tools for Technology Transfer (STTT)",
      "authors": [
        "Saeid Tizpaz-Niari",
        "Pavol Cern\u00fd",
        "S. Sankaranarayanan",
        "Ashutosh Trivedi"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/08b2808afb0b2b81325a0b77e2617b180b8a684e",
      "pdf_url": "",
      "publication_date": "2021-05-26",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e21e1386e1ece2633b8f0b82c6f685ce9eddbbf5",
      "title": "Neural Networks as a Side-Channel Countermeasure: Challenges and Opportunities",
      "abstract": "Specialized acceleration hardware for artificial deep neural network inference is available from the cloud to the edge. FPGAs in particular are heavily advertised for the acceleration of neural network-based applications. Traditionally, those applications are classification or nonlinear regression tasks with the goal to approximate an unknown function. However, they can be trained to replicate a fully known deterministic function - a classical example being the boolean XOR - as well. On the other hand, side-channel attacks remain a concern from the cloud to the edge, where attackers are often able to extract secret information through direct or indirect measurements of observables like power, voltage, electromagnetic emanation or timing. In this work, we show how an FPGA-mapped neural network implementation of the AES S-Box can improve side-channel resistance against Correlation Power Analysis (CPA) attacks. Although the implementation of a hardware-optimized algorithm such as the AES as a neural network introduces significant overhead, the generality of the representation allows to mitigate leakage in a manner agnostic to the overlying cryptographic primitive. We demonstrate the benefits of a generic representation, by optimizing an initially vulnerable neural network implementation towards side-channel resilience, through careful choice of activation function and input representation. The implementation is evaluated both against an external attacker measuring power with an oscilloscope, as well as a remote, internal adversary, who is able to capture voltage traces through FPGA-internal sensors in multi-tenant FPGAs. Our results show, how external attacks on the optimized neural network are no longer possible with up to one million traces, whereas an internal attacker is still able to recover the secret key. The latter result also exposes that in some cases measurement through internal sensors can be even more beneficial for an attacker than physical access with measurement equipment.",
      "year": 2021,
      "venue": "IEEE Computer Society Annual Symposium on VLSI",
      "authors": [
        "Jonas Krautter",
        "M. Tahoori"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e21e1386e1ece2633b8f0b82c6f685ce9eddbbf5",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "56e39821bebdfa91f9056dc5e1249d927ca8bf23",
      "title": "Telepathic Headache: Mitigating Cache Side-Channel Attacks on Convolutional Neural Networks",
      "abstract": null,
      "year": 2021,
      "venue": "International Conference on Applied Cryptography and Network Security",
      "authors": [
        "H. Chabanne",
        "J. Danger",
        "Linda Guiga",
        "U. K\u00fchne"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/56e39821bebdfa91f9056dc5e1249d927ca8bf23",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "b9f14b52cf9c01e651a4c8e9b04344e9c3929ddc",
      "title": "Model Extraction and Adversarial Attacks on Neural Networks Model Extraction and Adversarial Attacks on Neural Networks Using Side-Channel Information Using Side-Channel Information",
      "abstract": null,
      "year": 2021,
      "venue": "",
      "authors": [
        "Tommy Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b9f14b52cf9c01e651a4c8e9b04344e9c3929ddc",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "bf617161968220f3c48751aa68ccc38df0961f0e",
      "title": "CNN-based Tree Model Extraction",
      "abstract": "We propose a method for the segmentation and structural reconstruction of tree stems and branches in cluttered environments. We use single images of monocular cameras and convolutional neural networks for the segmentation, centerline, and contour detection of trees (trunks and branches), and a deterministic approach to build tree model graphs, defining the positions of vertices and edges, on the points of segments' centerlines. Compared to previous stochastic methods, this approach, tested on a synthetic dataset, gives better segmentation accuracy with a significantly smaller computational complexity.",
      "year": 2021,
      "venue": "International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications",
      "authors": [
        "K. Alaya",
        "L. Cz\u00fani"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bf617161968220f3c48751aa68ccc38df0961f0e",
      "pdf_url": "",
      "publication_date": "2021-09-22",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "aa660c696749254dceee53270745d642779aa762",
      "title": "Machine learning privacy : analysis and implementation of model extraction attacks",
      "abstract": null,
      "year": 2021,
      "venue": "",
      "authors": [
        "Bc. V\u00edt Karafi\u00e1t"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/aa660c696749254dceee53270745d642779aa762",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "959f9fa181f57caaa61b7ec422c141cbfa09dedf",
      "title": "Towards Stealing Deep Neural Networks on Mobile Devices",
      "abstract": null,
      "year": 2021,
      "venue": "Security and Privacy in Communication Networks",
      "authors": [
        "S. Danda",
        "Xiaoyong Yuan",
        "Bo Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/959f9fa181f57caaa61b7ec422c141cbfa09dedf",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3f4a4c3d028bffe080b97fdf59a290b9d708955d",
      "title": "Neural Network Model Extraction Based on Adversarial Examples",
      "abstract": "The neural network model has been applied to all walks of life. By detecting the internal information of a black-box model, the attacker can obtain potential commercial value of the model. At the same time, understanding the model structure helps the attacker customize the strategy to attack the model. We have improved a model detection method based on input and output pairs to detect the internal information of the trained neural network black-box model. On the one hand, our work proved that adversarial examples are very likely to carry architecture information of the neural network model. On the other hand, we added adversarial examples to the model pre-detection module, and verified the positive effects of adversarial examples on model detection through experiments, which improved the accuracy of the meta-model and reduced the cost of model detection.",
      "year": 2021,
      "venue": "International Conference on Advanced Information Science and System",
      "authors": [
        "Huiwen Fang",
        "Chunhua Wu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3f4a4c3d028bffe080b97fdf59a290b9d708955d",
      "pdf_url": "",
      "publication_date": "2021-11-26",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "abbb0fd559ade70265f4f528df094fbbd8ae2040",
      "title": "Entangled Watermarks as a Defense against Model Extraction",
      "abstract": "Machine learning involves expensive data collection and training procedures. Model owners may be concerned that valuable intellectual property can be leaked if adversaries mount model extraction attacks. Because it is difficult to defend against model extraction without sacrificing significant prediction accuracy, watermarking leverages unused model capacity to have the model overfit to outlier input-output pairs, which are not sampled from the task distribution and are only known to the defender. The defender then demonstrates knowledge of the input-output pairs to claim ownership of the model at inference. The effectiveness of watermarks remains limited because they are distinct from the task distribution and can thus be easily removed through compression or other forms of knowledge transfer. \nWe introduce Entangled Watermarking Embeddings (EWE). Our approach encourages the model to learn common features for classifying data that is sampled from the task distribution, but also data that encodes watermarks. An adversary attempting to remove watermarks that are entangled with legitimate data is also forced to sacrifice performance on legitimate data. Experiments on MNIST, Fashion-MNIST, and Google Speech Commands validate that the defender can claim model ownership with 95% confidence after less than 10 queries to the stolen copy, at a modest cost of 1% accuracy in the defended model's performance.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Hengrui Jia",
        "Christopher A. Choquette-Choo",
        "Nicolas Papernot"
      ],
      "citation_count": 261,
      "url": "https://www.semanticscholar.org/paper/abbb0fd559ade70265f4f528df094fbbd8ae2040",
      "pdf_url": "",
      "publication_date": "2020-02-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "Primarily about watermarking, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "e4b1d7553020258d7e537e2cfa53865359389eac",
      "title": "Stealing Links from Graph Neural Networks",
      "abstract": "Graph data, such as social networks and chemical networks, contains a wealth of information that can help to build powerful applications. To fully unleash the power of graph data, a family of machine learning models, namely graph neural networks (GNNs), is introduced. Empirical results show that GNNs have achieved state-of-the-art performance in various tasks. \nGraph data is the key to the success of GNNs. High-quality graph is expensive to collect and often contains sensitive information, such as social relations. Various research has shown that machine learning models are vulnerable to attacks against their training data. Most of these models focus on data from the Euclidean space, such as images and texts. Meanwhile, little attention has been paid to the security and privacy risks of graph data used to train GNNs. \nIn this paper, we aim at filling the gap by proposing the first link stealing attacks against graph neural networks. Given a black-box access to a GNN model, the goal of an adversary is to infer whether there exists a link between any pair of nodes in the graph used to train the model. We propose a threat model to systematically characterize the adversary's background knowledge along three dimensions. By combination, we obtain a comprehensive taxonomy of 8 different link stealing attacks. We propose multiple novel methods to realize these attacks. Extensive experiments over 8 real-world datasets show that our attacks are effective at inferring links, e.g., AUC (area under the ROC curve) is above 0.95 in multiple cases.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinlei He",
        "Jinyuan Jia",
        "M. Backes",
        "N. Gong",
        "Yang Zhang"
      ],
      "citation_count": 206,
      "url": "https://www.semanticscholar.org/paper/e4b1d7553020258d7e537e2cfa53865359389eac",
      "pdf_url": "",
      "publication_date": "2020-05-05",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "3190a468d72b7c22371e91c33a718bc00d7733fb",
      "title": "Hidden extreme multistability with hyperchaos and transient chaos in a Hopfield neural network affected by electromagnetic radiation",
      "abstract": null,
      "year": 2020,
      "venue": "",
      "authors": [
        "Hairong Lin",
        "Chunhua Wang",
        "Yumei Tan"
      ],
      "citation_count": 165,
      "url": "https://www.semanticscholar.org/paper/3190a468d72b7c22371e91c33a718bc00d7733fb",
      "pdf_url": "",
      "publication_date": "2020-02-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3136dd4036331b4559b341560712942ca2e765e3",
      "title": "Machine Learning Security: Threats, Countermeasures, and Evaluations",
      "abstract": "Machine learning has been pervasively used in a wide range of applications due to its technical breakthroughs in recent years. It has demonstrated significant success in dealing with various complex problems, and shows capabilities close to humans or even beyond humans. However, recent studies show that machine learning models are vulnerable to various attacks, which will compromise the security of the models themselves and the application systems. Moreover, such attacks are stealthy due to the unexplained nature of the deep learning models. In this survey, we systematically analyze the security issues of machine learning, focusing on existing attacks on machine learning systems, corresponding defenses or secure learning techniques, and security evaluation methods. Instead of focusing on one stage or one type of attack, this paper covers all the aspects of machine learning security from the training phase to the test phase. First, the machine learning model in the presence of adversaries is presented, and the reasons why machine learning can be attacked are analyzed. Then, the machine learning security-related issues are classified into five categories: training set poisoning; backdoors in the training set; adversarial example attacks; model theft; recovery of sensitive training data. The threat models, attack approaches, and defense techniques are analyzed systematically. To demonstrate that these threats are real concerns in the physical world, we also reviewed the attacks in real-world conditions. Several suggestions on security evaluations of machine learning systems are also provided. Last, future directions for machine learning security are also presented.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Mingfu Xue",
        "Chengxiang Yuan",
        "Heyi Wu",
        "Yushu Zhang",
        "Weiqiang Liu"
      ],
      "citation_count": 141,
      "url": "https://www.semanticscholar.org/paper/3136dd4036331b4559b341560712942ca2e765e3",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09064510.pdf",
      "publication_date": "2020-04-13",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d1d9166a10921c3a6077b3232e797bd307d1714a",
      "title": "Influences of electromagnetic radiation distribution on chaotic dynamics of a neural network",
      "abstract": null,
      "year": 2020,
      "venue": "Applied Mathematics and Computation",
      "authors": [
        "Hairong Lin",
        "Chunhua Wang"
      ],
      "citation_count": 115,
      "url": "https://www.semanticscholar.org/paper/d1d9166a10921c3a6077b3232e797bd307d1714a",
      "pdf_url": "",
      "publication_date": "2020-03-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d0d8c912b20d2d081672f07b6926b620950a39c8",
      "title": "Leaky DNN: Stealing Deep-Learning Model Secret with GPU Context-Switching Side-Channel",
      "abstract": "Machine learning has been attracting strong interests in recent years. Numerous companies have invested great efforts and resources to develop customized deep-learning models, which are their key intellectual properties. In this work, we investigate to what extent the secret of deep-learning models can be inferred by attackers. In particular, we focus on the scenario that a model developer and an adversary share the same GPU when training a Deep Neural Network (DNN) model. We exploit the GPU side-channel based on context-switching penalties. This side-channel allows us to extract the fine-grained structural secret of a DNN model, including its layer composition and hyper-parameters. Leveraging this side-channel, we developed an attack prototype named MosConS, which applies LSTM-based inference models to identify the structural secret. Our evaluation of MosConS shows the structural information can be accurately recovered. Therefore, we believe new defense mechanisms should be developed to protect training against the GPU side-channel.",
      "year": 2020,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Junyin Wei",
        "Yicheng Zhang",
        "Zhe Zhou",
        "Zhou Li",
        "M. A. Faruque"
      ],
      "citation_count": 110,
      "url": "https://www.semanticscholar.org/paper/d0d8c912b20d2d081672f07b6926b620950a39c8",
      "pdf_url": "",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d76407a539addcb39bddef065c4061381591e78c",
      "title": "WAFFLE: Watermarking in Federated Learning",
      "abstract": "Federated learning is a distributed learning technique where machine learning models are trained on client devices in which the local training data resides. The training is coordinated via a central server which is, typically, controlled by the intended owner of the resulting model. By avoiding the need to transport the training data to the central server, federated learning improves privacy and efficiency. But it raises the risk of model theft by clients because the resulting model is available on every client device. Even if the application software used for local training may attempt to prevent direct access to the model, a malicious client may bypass any such restrictions by reverse engineering the application software. Watermarking is a well-known deterrence method against model theft by providing the means for model owners to demonstrate ownership of their models. Several recent deep neural network (DNN) watermarking techniques use backdooring: training the models with additional mislabeled data. Backdooring requires full access to the training data and control of the training process. This is feasible when a single party trains the model in a centralized manner, but not in a federated learning setting where the training process and training data are distributed among several client devices. In this paper, we present WAFFLE, the first approach to watermark DNN models trained using federated learning. It introduces a retraining step at the server after each aggregation of local models into the global model. We show that WAFFLE efficiently embeds a resilient watermark into models incurring only negligible degradation in test accuracy (-0.17%), and does not require access to training data. We also introduce a novel technique to generate the backdoor used as a watermark. It outperforms prior techniques, imposing no communication, and low computational (+3.2%) overhead11The research report version of this paper is also available in https://arxiv.org/abs/2008.07298, and the code for reproducing our work can be found at https://github.com/ssg-research/WAFFLE.",
      "year": 2020,
      "venue": "IEEE International Symposium on Reliable Distributed Systems",
      "authors": [
        "B. Atli",
        "Yuxi Xia",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 86,
      "url": "https://www.semanticscholar.org/paper/d76407a539addcb39bddef065c4061381591e78c",
      "pdf_url": "http://arxiv.org/pdf/2008.07298",
      "publication_date": "2020-08-17",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9419ceaf7a62bc1c1d9085f37632a2ecc884394c",
      "title": "One-dimensional deep learning inversion of electromagnetic induction data using convolutional neural network",
      "abstract": "\n Conventional geophysical inversion techniques suffer from several limitations including computational cost, nonlinearity, non-uniqueness and dimensionality of the inverse problem. Successful inversion of geophysical data has been a major challenge for decades. Here, a novel approach based on deep learning (DL) inversion via convolutional neural network (CNN) is proposed to instantaneously estimate subsurface electrical conductivity (\u03c3) layering from electromagnetic induction (EMI) data. In this respect, a fully convolutional network was trained on a large synthetic data set generated based on 1-D EMI forward model. The accuracy of the proposed approach was examined using several synthetic scenarios. Moreover, the trained network was used to find subsurface electromagnetic conductivity images (EMCIs) from EMI data measured along two transects from Chicken Creek catchment (Brandenburg, Germany). Dipole\u2013dipole electrical resistivity tomography data were measured as well to obtain reference subsurface \u03c3 distributions down to a 6\u00a0m depth. The inversely estimated models were juxtaposed and compared with their counterparts obtained from a spatially constrained deterministic algorithm as a standard code. Theoretical simulations demonstrated a well performance of the algorithm even in the presence of noise in data. Moreover, application of the DL inversion for subsurface imaging from Chicken Creek catchment manifested the accuracy and robustness of the proposed approach for EMI inversion. This approach returns subsurface \u03c3 distribution directly from EMI data in a single step without any iterations. The proposed strategy simplifies considerably EMI inversion and allows for rapid and accurate estimation of subsurface EMCI from multiconfiguration EMI data.",
      "year": 2020,
      "venue": "Geophysical Journal International",
      "authors": [
        "D. Moghadas"
      ],
      "citation_count": 78,
      "url": "https://www.semanticscholar.org/paper/9419ceaf7a62bc1c1d9085f37632a2ecc884394c",
      "pdf_url": "",
      "publication_date": "2020-04-09",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "935c1f02735f02a0d86443cec8b9fc33d98a627c",
      "title": "Physics-Informed Deep Neural Networks for Transient Electromagnetic Analysis",
      "abstract": "In this paper, we propose a deep neural network based model to predict the time evolution of field values in transient electrodynamics. The key component of our model is a recurrent neural network, which learns representations of long-term spatial-temporal dependencies in the sequence of its input data. We develop an encoder-recurrent-decoder architecture, which is trained with finite difference time domain simulations of plane wave scattering from distributed, perfect electric conducting objects. We demonstrate that, the trained network can emulate a transient electrodynamics problem with more than 17 times speed-up in simulation time compared to traditional finite difference time domain solvers.",
      "year": 2020,
      "venue": "IEEE Open Journal of Antennas and Propagation",
      "authors": [
        "Oameed Noakoasteen",
        "Shu Wang",
        "Z. Peng",
        "C. Christodoulou"
      ],
      "citation_count": 72,
      "url": "https://www.semanticscholar.org/paper/935c1f02735f02a0d86443cec8b9fc33d98a627c",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8566058/8821524/09158400.pdf",
      "publication_date": "2020-08-04",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4eb8818dbf9e5e384741136c5ba633455bcb72a8",
      "title": "SNIFF: Reverse Engineering of Neural Networks With Fault Attacks",
      "abstract": "Neural networks have been shown to be vulnerable against fault injection attacks. These attacks change the physical behavior of the device during the computation, resulting in a change of value that is currently being computed. They can be realized by various techniques, ranging from clock/voltage glitching to application of lasers to rowhammer. Previous works have mostly explored fault attacks for output misclassification, thus affecting the reliability of neural networks. In this article, we investigate the possibility to reverse engineer neural networks with fault attacks. Sign bit flip fault attack enables the reverse engineering by changing the sign of intermediate values. We develop the first exact extraction method on deep-layer feature extractor networks that provably allows the recovery of proprietary model parameters. Our experiments with Keras library show that the precision error for the parameter recovery for the tested networks is less than $10^{-13}$ with the usage of 64-bit floats, which improves the current state of the art by six orders of magnitude.",
      "year": 2020,
      "venue": "IEEE Transactions on Reliability",
      "authors": [
        "J. Breier",
        "Dirmanto Jap",
        "Xiaolu Hou",
        "S. Bhasin",
        "Yang Liu"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/4eb8818dbf9e5e384741136c5ba633455bcb72a8",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/155678/2/_IEEE_TREL__Reverse_Engineering_of_Neural_Networks_with_Fault_Attacks.pdf",
      "publication_date": "2020-02-23",
      "keywords_matched": [
        "reverse engineer neural network"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "512c95dd0b0a04892e257ddef60af96dd7a8b8f3",
      "title": "BoMaNet: Boolean Masking of an Entire Neural Network",
      "abstract": "Recent work on stealing machine learning (ML) models from inference engines with physical side-channel attacks warrant an urgent need for effective side-channel defenses. This work proposes the first fully-masked neural network inference engine design. Masking uses secure multi-party computation to split the secrets into random shares and to decorrelate the statistical relation of secret-dependent computations to side-channels (e.g., the power draw). In this work, we construct secure hardware primitives to mask all the linear and non-linear operations in a neural network. We address the challenge of masking integer addition by converting each addition into a sequence of XOR and AND gates and by augmenting Trichina's secure Boolean masking style. We improve the traditional Trichina's AND gates by adding pipelining elements for better glitch-resistance and we architect the whole design to sustain a throughput of 1 masked addition per cycle. We implement the proposed secure inference engine on a Xilinx Spartan-6 (XC6SLX75) FPGA. The results show that masking incurs an overhead of 3.5% in latency and 5.9\u00d7 in area. Finally, we demonstrate the security of the masked design with 2M traces.",
      "year": 2020,
      "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "authors": [
        "Anuj Dubey",
        "Rosario Cammarota",
        "Aydin Aysu"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/512c95dd0b0a04892e257ddef60af96dd7a8b8f3",
      "pdf_url": "",
      "publication_date": "2020-06-16",
      "keywords_matched": [
        "stealing machine learning",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0d43d80c40baec445bc39a8dfa4789d070786319",
      "title": "Reverse-Engineering Deep Neural Networks Using Floating-Point Timing Side-Channels",
      "abstract": "Trained Deep Neural Network (DNN) models have become valuable intellectual property. A new attack surface has emerged for DNNs: model reverse engineering. Several recent attempts have utilized various common side channels. However, recovering DNN parameters, weights and biases, remains a challenge. In this paper, we present a novel attack that utilizes a floating-point timing side channel to reverse-engineer parameters of multi-layer perceptron (MLP) models in software implementation, entirely and precisely. To the best of our knowledge, this is the first work that leverages a floating-point timing side-channel for effective DNN model recovery.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Gongye Cheng",
        "Yunsi Fei",
        "T. Wahl"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/0d43d80c40baec445bc39a8dfa4789d070786319",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "70c1f8f6c079b5a3782fa73eebc3255aada58dd8",
      "title": "(Quasi-)Real-Time Inversion of Airborne Time-Domain Electromagnetic Data via Artificial Neural Network",
      "abstract": "The possibility to have results very quickly after, or even during, the collection of electromagnetic data would be important, not only for quality check purposes, but also for adjusting the location of the proposed flight lines during an airborne time-domain acquisition. This kind of readiness could have a large impact in terms of optimization of the Value of Information of the measurements to be acquired. In addition, the importance of having fast tools for retrieving resistivity models from airborne time-domain data is demonstrated by the fact that Conductivity-Depth Imaging methodologies are still the standard in mineral exploration. In fact, they are extremely computationally efficient, and, at the same time, they preserve a very high lateral resolution. For these reasons, they are often preferred to inversion strategies even if the latter approaches are generally more accurate in terms of proper reconstruction of the depth of the targets and of reliable retrieval of true resistivity values of the subsurface. In this research, we discuss a novel approach, based on neural network techniques, capable of retrieving resistivity models with a quality comparable with the inversion strategy, but in a fraction of the time. We demonstrate the advantages of the proposed novel approach on synthetic and field datasets.",
      "year": 2020,
      "venue": "Remote Sensing",
      "authors": [
        "P. Bai",
        "G. Vignoli",
        "A. Viezzoli",
        "J. Nevalainen",
        "G. Vacca"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/70c1f8f6c079b5a3782fa73eebc3255aada58dd8",
      "pdf_url": "https://www.mdpi.com/2072-4292/12/20/3440/pdf?version=1603361484",
      "publication_date": "2020-10-20",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1f53fe2f4458bde77735482552dc6818ae5eb30f",
      "title": "GANRED: GAN-based Reverse Engineering of DNNs via Cache Side-Channel",
      "abstract": "In recent years, deep neural networks (DNN) have become an important type of intellectual property due to their high performance on various classification tasks. As a result, DNN stealing attacks have emerged. Many attack surfaces have been exploited, among which cache timing side-channel attacks are hugely problematic because they do not need physical probing or direct interaction with the victim to estimate the DNN model. However, existing cache-side-channel-based DNN reverse engineering attacks rely on analyzing the binary code of the DNN library that must be shared between the attacker and the victim in the main memory. In reality, the DNN library code is often inaccessible because 1) the code is proprietary, or 2) memory sharing has been disabled by the operating system. In our work, we propose GANRED, an attack approach based on the generative adversarial nets (GAN) framework which utilizes cache timing side-channel information to accurately recover the structure of DNNs without memory sharing or code access. The benefit of GANRED is four-fold. 1) There is no need for DNN library code analysis. 2) No shared main memory segment between the victim and the attacker is needed. 3) Our attack locates the exact structure of the victim model, unlike existing attacks which only narrow down the structure search space. 4) Our attack efficiently scales to deeper DNNs, exhibiting only linear growth in the number of layers in the victim DNN.",
      "year": 2020,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Yuntao Liu",
        "Ankur Srivastava"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/1f53fe2f4458bde77735482552dc6818ae5eb30f",
      "pdf_url": "",
      "publication_date": "2020-11-09",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a5991cc92fad9bbca68d8227a50dbd3eb165efe4",
      "title": "Model Reverse-Engineering Attack using Correlation Power Analysis against Systolic Array Based Neural Network Accelerator",
      "abstract": "Various deep neural network (DNN) accelerators have been proposed for artificial intelligence (AI) inference on edge devices. On the other hand, hardware security issues of the DNN accelerator have not been discussed well. Trained DNN models are important intellectual property and a valuable target for adversaries. In particular, when a DNN model is implemented on an edge device, adversaries can physically access the device and try to reveal the implemented DNN model. Therefore, the DNN execution environment on an edge device requires countermeasures such as data encryption on off-chip memory against various reverse-engineering attacks. In this paper, we reveal DNN model parameters by utilizing correlation power analysis (CPA) against a systolic array circuit that is widely used in DNN accelerator hardware. Our experimental results show that the adversary can extract trained model parameters from a DNN accelerator even if the DNN model parameters are protected with data encryption. The results suggest that countermeasures against side-channel leaks are important for implementing a DNN accelerator on FPGA or ASIC.",
      "year": 2020,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Kota Yoshida",
        "Takaya Kubota",
        "S. Okura",
        "M. Shiozaki",
        "T. Fujino"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/a5991cc92fad9bbca68d8227a50dbd3eb165efe4",
      "pdf_url": "",
      "publication_date": "2020-10-01",
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "fa6a822c037af9de96219c1646578d598d754dca",
      "title": "BP neural network and improved differential evolution for transient electromagnetic inversion",
      "abstract": null,
      "year": 2020,
      "venue": "Computational Geosciences",
      "authors": [
        "Ruiyou Li",
        "Huaiqing Zhang",
        "Qiong Zhuang",
        "Ruiheng Li",
        "Yue Chen"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/fa6a822c037af9de96219c1646578d598d754dca",
      "pdf_url": "",
      "publication_date": "2020-04-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "729fbe386e31e36ed5108ee276a6db223176a33d",
      "title": "Information Leakage by Model Weights on Federated Learning",
      "abstract": "Federated learning aggregates data from multiple sources while protecting privacy, which makes it possible to train efficient models in real scenes. However, although federated learning uses encrypted security aggregation, its decentralised nature makes it vulnerable to malicious attackers. A deliberate attacker can subtly control one or more participants and upload malicious model parameter updates, but the aggregation server cannot detect it due to encrypted privacy protection. Based on these problems, we find a practical and novel security risk in the design of federal learning. We propose an attack for conspired malicious participants to adjust the training data strategically so that the weight of a certain dimension in the aggregation model will rise or fall with a pattern. The trend of weights or parameters in the aggregation model forms meaningful signals, which is the risk of information leakage. The leakage is exposed to other participants in this federation but only available for participants who reach an agreement with the malicious participant, i.e., the receiver must be able to understand patterns of changes in weights. The attack effect is evaluated and verified on open-source code and data sets.",
      "year": 2020,
      "venue": "PPMLP@CCS",
      "authors": [
        "Xiaoyun Xu",
        "Jingzheng Wu",
        "Mutian Yang",
        "Tianyue Luo",
        "Xu Duan",
        "Weiheng Li",
        "Yanjun Wu",
        "Bin Wu"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/729fbe386e31e36ed5108ee276a6db223176a33d",
      "pdf_url": "",
      "publication_date": "2020-11-09",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "bb9c9e43aa5764d2d07b2b71c2cea68144f90972",
      "title": "Soil electrical conductivity imaging using a neural network-based forward solver: Applied to large-scale Bayesian electromagnetic inversion",
      "abstract": null,
      "year": 2020,
      "venue": "",
      "authors": [
        "D. Moghadas",
        "A. Behroozmand",
        "A. Christiansen"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/bb9c9e43aa5764d2d07b2b71c2cea68144f90972",
      "pdf_url": "",
      "publication_date": "2020-05-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "23a47ede9466d119448de4226aebe3193b7ce13d",
      "title": "An Enhanced Convolutional Neural Network in Side-Channel Attacks and Its Visualization",
      "abstract": "In recent years, the convolutional neural networks (CNNs) have received a lot of interest in the side-channel community. The previous work has shown that CNNs have the potential of breaking the cryptographic algorithm protected with masking or desynchronization. Before, several CNN models have been exploited, reaching the same or even better level of performance compared to the traditional side-channel attack (SCA). In this paper, we investigate the architecture of Residual Network and build a new CNN model called attention network. To enhance the power of the attention network, we introduce an attention mechanism - Convolutional Block Attention Module (CBAM) and incorporate CBAM into the CNN architecture. CBAM points out the informative points of the input traces and makes the attention network focus on the relevant leakages of the measurements. It is able to improve the performance of the CNNs. Because the irrelevant points will introduce the extra noises and cause a worse performance of attacks. We compare our attention network with the one designed for the masking AES implementation called ASCAD network in this paper. We show that the attention network has a better performance than the ASCAD network. Finally, a new visualization method, named Class Gradient Visualization (CGV) is proposed to recognize which points of the input traces have a positive influence on the predicted result of the neural networks. In another aspect, it can explain why the attention network is superior to the ASCAD network. We validate the attention network through extensive experiments on four public datasets and demonstrate that the attention network is efficient in different AES implementations.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Minhui Jin",
        "Mengce Zheng",
        "Honggang Hu",
        "Nenghai Yu"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/23a47ede9466d119448de4226aebe3193b7ce13d",
      "pdf_url": "",
      "publication_date": "2020-09-18",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "57cbdecb0b12f03a5c0fb95522fdc3494c87bd10",
      "title": "Modeling extra-deep electromagnetic logs using a deep neural network",
      "abstract": "Modern geosteering is heavily dependent on real-time interpretation of deep electromagnetic (EM) measurements. We have developed a methodology to construct a deep neural network (DNN) model trained to reproduce a full set of extra-deep EM logs consisting of 22 measurements per logging position. The model is trained in a 1D layered environment consisting of up to seven layers with different resistivity values. A commercial simulator provided by a tool vendor is used to generate a training data set. The data set size is limited because the simulator provided by the vendor is optimized for sequential execution. Therefore, we design a training data set that embraces the geologic rules and geosteering specifics supported by the forward model. We use this data set to produce an EM simulator based on a DNN without access to the proprietary information about the EM tool configuration or the original simulator source code. Despite using a relatively small training set size, the resulting DNN forward model is quite accurate for the considered examples: a multilayer synthetic case and a section of a published historical operation from the Goliat field. The observed average evaluation time of 0.15\u00a0ms per logging position makes it also suitable for future use as part of evaluation-hungry statistical and/or Monte Carlo inversion algorithms within geosteering workflows.",
      "year": 2020,
      "venue": "Geophysics",
      "authors": [
        "S. Alyaev",
        "M. Shahriari",
        "David Pardo",
        "\u00c1. J. Omella",
        "D. Larsen",
        "N. Jahani",
        "E. Suter"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/57cbdecb0b12f03a5c0fb95522fdc3494c87bd10",
      "pdf_url": "https://bird.bcamath.org/bitstream/20.500.11824/1409/1/geo2020-0389.1%20%282%29.pdf",
      "publication_date": "2020-05-18",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9b0758a89df635cded669bc5f9e3b9e2fa93d093",
      "title": "Effect of Microwave on Changes of Gallic Acid and Resveratrol in a Model Extraction Solution",
      "abstract": null,
      "year": 2020,
      "venue": "Food and Bioprocess Technology",
      "authors": [
        "Jiang-Feng Yuan",
        "Ting-ting Wang",
        "Da\u2010hong Wang",
        "Guo-Hua Zhou",
        "Guo-Xin Zou",
        "Yan Wang",
        "Mingjing Gong",
        "Bin Zhang"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/9b0758a89df635cded669bc5f9e3b9e2fa93d093",
      "pdf_url": "",
      "publication_date": "2020-05-04",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "85bab667c93b5983c641c03a81e8812669ba6ab8",
      "title": "Improved Hybrid Approach for Side-Channel Analysis Using Efficient Convolutional Neural Network and Dimensionality Reduction",
      "abstract": "Deep learning-based side channel attacks are burgeoning due to their better efficiency and performance, suppressing the traditional side-channel analysis. To launch the successful attack on a particular public key cryptographic (PKC) algorithm, a large number of samples per trace might need to be acquired to capture all the minor useful details from the leakage information, which increases the number of features per instance. The decreased instance-feature ratio increases the computational complexity of the deep learning-based attacks, limiting the attack efficiency. Moreover, data class imbalance can be a hindrance in accurate model training, leading to an accuracy paradox. We propose an efficient Convolutional Neural Network (CNN) based approach in which the dimensionality of the large leakage dataset is reduced, and then the data is processed using the proposed CNN based model. In the proposed model, the optimal number of convolutional blocks is used to build powerful features extractors within the cost limit. We have also analyzed and presented the impact of using the Synthetic Minority Over-sampling Technique (SMOTE) on the proposed model performance. We propose that a data-balancing step should be mandatory for analysis in the side channel attack scenario. We have also provided a performance-based comparative analysis between proposed and existing deep learning models for unprotected and protected Elliptic curve (ECC) Montgomery Power ladder implementations. The reduced network complexity, together with an improved attack efficiency, promote the proposed approach to be effectively used for side-channel attacks.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Naila Mukhtar",
        "A. Fournaris",
        "T. Khan",
        "Charis Dimopoulos",
        "Yinan Kong"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/85bab667c93b5983c641c03a81e8812669ba6ab8",
      "pdf_url": "https://doi.org/10.1109/access.2020.3029206",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e0739acbd33897cc67a991a29760eb3a31cc5ad3",
      "title": "Electromagnetic signal modulation recognition technology based on lightweight deep neural network",
      "abstract": null,
      "year": 2020,
      "venue": "",
      "authors": [
        "Sicheng Zhang",
        "Yun Lin",
        "Ya Tu",
        "S. Mao"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/e0739acbd33897cc67a991a29760eb3a31cc5ad3",
      "pdf_url": "",
      "publication_date": "2020-11-25",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e5f8dab798783e4909452920249aca6dce1fa1b0",
      "title": "\u201cIdentity Bracelets\u201d for Deep Neural Networks",
      "abstract": "The power of deep learning and the enormous effort and money required to build a deep learning model makes stealing them a hugely worthwhile and highly lucrative endeavor. Worse still, model theft requires little more than a high-school understanding of computer functions, which ensures a healthy and vibrant black market full of choice for any would-be pirate. As such, estimating how many neural network models are likely to be illegally reproduced and distributed in future is almost impossible. Therefore, we propose an embedded \u2018identity bracelet\u2019 for deep neural networks that acts as proof of a model\u2019s owner. Our solution is an extension to the existing trigger-set watermarking techniques that embeds a post-cryptographic-style serial number into the base deep neural network (DNN). Called a DNN-SN, this identifier works like an identity bracelet that proves a network\u2019s rightful owner. Further, a novel training method based on non-related multitask learning ensures that embedding the DNN-SN does not compromise model performance. Experimental evaluations of the framework confirm that a DNN-SN can be embedded into a model when training from scratch or in the student network component of Net2Net.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Xiangrui Xu",
        "Yaqin Li",
        "Cao Yuan"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/e5f8dab798783e4909452920249aca6dce1fa1b0",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09104681.pdf",
      "publication_date": null,
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "39bf1cd08237224cb0a5b372aa0201ba98f16e4f",
      "title": "Linear-Model-Inspired Neural Network for Electromagnetic Inverse Scattering",
      "abstract": "Electromagnetic inverse scattering problems (ISPs) aim to retrieve permittivities of dielectric scatterers from the scattering measurement. It is often highly nonlinear, causing the problem to be very difficult to solve. To alleviate the issue, this letter exploits a linear-model-based network (LMN) learning strategy, which benefits from both model complexity and data learning. By introducing a linear model for ISPs, a new model with network-driven regularizer is proposed. For attaining efficient end-to-end learning, the network architecture and hyper-parameter estimation are presented. Experimental results validate its superiority to some state of the art.",
      "year": 2020,
      "venue": "IEEE Antennas and Wireless Propagation Letters",
      "authors": [
        "Huilin Zhou",
        "Ouyang Tao",
        "Yadan Li",
        "Jian Liu",
        "Qiegen Liu"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/39bf1cd08237224cb0a5b372aa0201ba98f16e4f",
      "pdf_url": "https://arxiv.org/pdf/2003.01465",
      "publication_date": "2020-03-03",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "03a0a36bc355e71e758285da4fc7c89948c41997",
      "title": "A 1.02-\u03bcW STT-MRAM-Based DNN ECG Arrhythmia Monitoring SoC With Leakage-Based Delay MAC Unit",
      "abstract": "A low-power STT-MRAM-based mixed-mode electrocardiogram (ECG) arrhythmia monitoring SoC is proposed. The proposed SoC consists of 1-MB STT-MRAM, leakage-based delay multiply-and-accumulation (MAC) unit (LDMAC), and ECG analog front end (AFE). ResNet structure with 16 1-D convolution layers and max-pooling layers is adopted for the ECG arrhythmia detection with weight reusing and partial sum reusing scheme. A nonvolatile 1-MB STT-MRAM enables deep neural network (DNN) inference to achieve higher area efficiency, lower power consumption without external memory access. The proposed mixed-mode LDMAC consumes only 4.11-nW MAC power by reusing leakage current. The proposed SoC is fabricated in 28-nm FDSOI process with 7.29-mm2 area. It demonstrates ECG arrhythmia detection with 85.1% accuracy, which is the highest score reported, and the lowest power consumption of 1.02 ${\\mu }\\text{W}$ .",
      "year": 2020,
      "venue": "IEEE Solid-State Circuits Letters",
      "authors": [
        "Kyoung-Rog Lee",
        "Jihoon Kim",
        "Changhyeon Kim",
        "Donghyeon Han",
        "Juhyoung Lee",
        "Jinsu Lee",
        "Hongsik Jeong",
        "H. Yoo"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/03a0a36bc355e71e758285da4fc7c89948c41997",
      "pdf_url": "",
      "publication_date": "2020-09-18",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "be5e6f5309298af3c799779c27c69f90091a3699",
      "title": "Static Hand Gesture Recognition With Electromagnetic Scattered Field via Complex Attention Convolutional Neural Network",
      "abstract": "We present a novel learning-based static gesture recognition framework using electromagnetic (EM) scattered field data, which can efficiently address some significant issues in traditional vision-based recognition approaches. An end-to-end complex-valued attention convolutional neural network (CNN) is devised to train the gesture recognizer, wherein the attention module is designed to learn robust region-of-interest-aware features. Extensive numerical experiments are conducted on a public static hand gesture dataset. Both full and limited aperture measurements with transverse magnetic wave illumination are investigated. It is numerically shown that: first, both complex-valued convolutional and attention module contribute to the excellent performance. The recognition accuracy is above 99.0% for full aperture and even about 95.32% under the limited one-eighth aperture, respectively, and second, the proposed method not only has good scalability to the case with limited aperture, but also performs much better than previous state-of-the-art deep networks.",
      "year": 2020,
      "venue": "IEEE Antennas and Wireless Propagation Letters",
      "authors": [
        "Min Tan",
        "Jian Zhou",
        "Kuiwen Xu",
        "Zhiyou Peng",
        "Zhenchao Ma"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/be5e6f5309298af3c799779c27c69f90091a3699",
      "pdf_url": "",
      "publication_date": "2020-04-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c9c7498580d105ebac92ed80ec6698bb6dbeb76f",
      "title": "Simple Electromagnetic Analysis Against Activation Functions of Deep Neural Networks",
      "abstract": null,
      "year": 2020,
      "venue": "ACNS Workshops",
      "authors": [
        "Go Takatoi",
        "T. Sugawara",
        "K. Sakiyama",
        "Y. Li"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/c9c7498580d105ebac92ed80ec6698bb6dbeb76f",
      "pdf_url": "",
      "publication_date": "2020-10-19",
      "keywords_matched": [
        "electromagnetic analysis (title)",
        "(via citation)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "030f26db8d627646af1af63adbd4359bf16fc08f",
      "title": "Prediction of electromagnetic field patterns of optical waveguide using neural network",
      "abstract": null,
      "year": 2020,
      "venue": "Neural computing & applications (Print)",
      "authors": [
        "G. Alagappan",
        "C. Png"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/030f26db8d627646af1af63adbd4359bf16fc08f",
      "pdf_url": "",
      "publication_date": "2020-06-25",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "12393a8e6e7c750eae30c0c538663fa7b426f304",
      "title": "A New Privacy-Preserving Framework based on Edge-Fog-Cloud Continuum for Load Forecasting",
      "abstract": "As an essential part to intelligently fine-grained scheduling, planning and maintenance in smart grid and energy internet, short-term load forecasting makes great progress recently owing to the big data collected from smart meters and the leap forward in machine learning technologies. However, the centralized computing topology of classical electric information system, where individual electricity consumption data are frequently transmitted to the cloud center for load forecasting, tends to violate electric consumers\u2019 privacy as well as to increase the pressure on network bandwidth. To tackle the tricky issues, we propose a privacy-preserving framework based on the edge-fog-cloud continuum for smart grid. Specifically, 1) we gravitate the training of load forecasting models and forecasting workloads to distributed smart meters so that consumers\u2019 raw data are handled locally, and only the forecasting outputs that have been protected are reported to the cloud center via fog nodes; 2) we protect the local forecasting models that imply electricity features from model extraction attacks by model randomization; 3) we exploit a shuffle scheme among smart meters to protect the data ownership privacy, and utilize a re-encryption scheme to guarantee the forecasting data privacy. Finally, through comprehensive simulation and analysis, we validate our proposed privacy-preserving framework in terms of privacy protection, and computation and communication efficiency.",
      "year": 2020,
      "venue": "IEEE Wireless Communications and Networking Conference",
      "authors": [
        "S. Hou",
        "Hongjia Li",
        "Chang Yang",
        "Liming Wang"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/12393a8e6e7c750eae30c0c538663fa7b426f304",
      "pdf_url": "",
      "publication_date": "2020-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about privacy, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "93c0fa4c3b700b1d62affb1e3f1cde5707e29e55",
      "title": "Cell-Phone Classification: A Convolutional Neural Network Approach Exploiting Electromagnetic Emanations",
      "abstract": "In this paper, we propose a methodology to identify both the brand of a cell-phone, and the status of its camera by exploiting electromagnetic (EM) emanations. The method is composed of two parts: Feature extraction and Convolutional Neural Network (CNN). We first extract features by averaging magnitudes of short-time Fourier transform (STFT) of the measured EM signal, which helps to reduce input dimension of the neural network, and to filter spurious emissions. The extracted features are fed into the proposed CNN, which contains two convolutional layers (followed by max-pooling layers), and four fully-connected layers. Finally, we provide experimental results which exhibit more than 99% classification accuracy for the test signals.",
      "year": 2020,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "B. Yilmaz",
        "E. Ugurlu",
        "A. Zaji\u0107",
        "Milos Prvulovi\u0107"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/93c0fa4c3b700b1d62affb1e3f1cde5707e29e55",
      "pdf_url": "",
      "publication_date": "2020-05-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7282e208d9063a460ad0306103a76557387322f6",
      "title": "Practical Side-Channel Based Model Extraction Attack on Tree-Based Machine Learning Algorithm",
      "abstract": null,
      "year": 2020,
      "venue": "ACNS Workshops",
      "authors": [
        "Dirmanto Jap",
        "Ville Yli-M\u00e4yry",
        "Akira Ito",
        "Rei Ueno",
        "S. Bhasin",
        "N. Homma"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/7282e208d9063a460ad0306103a76557387322f6",
      "pdf_url": "",
      "publication_date": "2020-10-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "36a2854b9a81d7944fb9e0f6b3efd3fe49f3d2f8",
      "title": "Noise Reduction Power Stealing Detection Model Based on Self-Balanced Data Set",
      "abstract": "In recent years, various types of power theft incidents have occurred frequently, and the training of the power-stealing detection model is susceptible to the influence of the imbalanced data set and the data noise, which leads to errors in power-stealing detection. Therefore, a power-stealing detection model is proposed, which is based on Improved Conditional Generation Adversarial Network (CWGAN), Stacked Convolution Noise Reduction Autoencoder (SCDAE) and Lightweight Gradient Boosting Decision Machine (LightGBM). The model performs Generation- Adversarial operations on the original unbalanced power consumption data to achieve the balance of electricity data, and avoids the interference of the imbalanced data set on classifier training. In addition, the convolution method is used to stack the noise reduction auto-encoder to achieve dimension reduction of power consumption data, extract data features and reduce the impact of random noise. Finally, LightGBM is used for power theft detection. The experiments show that CWGAN can effectively balance the distribution of power consumption data. Comparing the detection indicators of the power-stealing model with various advanced power-stealing models on the same data set, it is finally proved that the proposed model is superior to other models in the detection of power stealing.",
      "year": 2020,
      "venue": "Energies",
      "authors": [
        "Haiqing Liu",
        "Zhi-qiang Li",
        "Yuancheng Li"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/36a2854b9a81d7944fb9e0f6b3efd3fe49f3d2f8",
      "pdf_url": "https://www.mdpi.com/1996-1073/13/7/1763/pdf",
      "publication_date": "2020-04-07",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "fcd525adae1f16e3f80fc34cca48106a02b449e1",
      "title": "Neural Network Model for Parameter Inversion in Electromagnetic Wave and Plasma Interaction Systems",
      "abstract": "A multiple regression and machine learning approach is proposed to solve multiple plasma parameter inversion in complex systems. For an electromagnetic (EM) wave and plasma interaction system, it is common to retrieve plasma parameters based on the reflected or scattered EM field, whereas multiple parameter inversion becomes particularly challenging for anisotropic inhomogeneous plasma medium. Artificial neural network (ANN) is applied to retrieve the plasma density fluctuation in inhomogeneous medium. The method is computationally efficient and robust based solely on time series of data collected at a fixed number of spatial locations. The inversion algorithm opens venues and application for inhomogeneous plasma parameter inversion for EM wave and plasma interaction system at different frequencies.",
      "year": 2020,
      "venue": "IEEE Transactions on Plasma Science",
      "authors": [
        "Yangyang Zhang",
        "H. Fu",
        "Yun Sui"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/fcd525adae1f16e3f80fc34cca48106a02b449e1",
      "pdf_url": "",
      "publication_date": "2020-05-11",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "83e2b8ab293b74f2b9347f22ac12b9175e96351e",
      "title": "Stored Grain Inventory Management Using Neural-Network-Based Parametric Electromagnetic Inversion",
      "abstract": "We present a neural network architecture to determine the volume and complex permittivity of grain stored in metal bins. The neural networks output the grain height, cone angle and complex permittivity of the grain, using the input of experimental field data ( $S$ -parameters) from an electromagnetic imaging system consisting of 24 transceivers installed in the bin. Key for practical applications, the neural networks are trained on synthetic data sets but generate the parametric information using experimental data as input, without the use of calibration objects or open-short-load measurements. To accomplish this, we formulate a data normalization scheme that enables the use of a loss function that directly compares measured $S$ -parameters and simulation model fields. The normalization strategy and the ability to train on synthetic data means we do not need to collect experimental training data. We demonstrate the applicability of this synthetically trained neural network to experimental data from two different bin geometries, and discuss the ability of these neural networks to successfully infer parameters that can be used for grain inventory management. Our neural-network-based approach enables rapid inference, providing a more cost-effective long-term solution than existing optimization-based parametric inversion methods.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Keeley Edwards",
        "N. Geddert",
        "Kennedy Krakalovich",
        "R. Kruk",
        "M. Asefi",
        "J. Lovetri",
        "C. Gilmore",
        "I. Jeffrey"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/83e2b8ab293b74f2b9347f22ac12b9175e96351e",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09260139.pdf",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "06166afe614094369c5633957feb7bcdf6c98d1a",
      "title": "Preventing Neural Network Weight Stealing via Network Obfuscation",
      "abstract": null,
      "year": 2020,
      "venue": "Sai",
      "authors": [
        "K\u00e1lm\u00e1n Szentannai",
        "Jalal Al-Afandi",
        "A. Horv\u00e1th"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/06166afe614094369c5633957feb7bcdf6c98d1a",
      "pdf_url": "",
      "publication_date": "2020-07-16",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7e45bfee1d303c2823ef48f6b6bb145f85eb0671",
      "title": "Learning From A Big Brother - Mimicking Neural Networks in Profiled Side-channel Analysis",
      "abstract": "Recently, deep learning has emerged as a powerful technique for side-channel attacks, capable of even breaking common countermeasures. Still, trained models are generally large, and thus, performing evaluation becomes resource-intensive. The resource requirements increase in realistic settings where traces can be noisy, and countermeasures are active. In this work, we exploit mimicking to compress the learned models. We demonstrate up to 300 times compression of a state-of-the-art CNN. The mimic shallow network can also achieve much better accuracy as compared to when trained on original data and even reach the performance of a deeper network.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Daan van der Valk",
        "Marina Kr\u010dek",
        "S. Picek",
        "S. Bhasin"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/7e45bfee1d303c2823ef48f6b6bb145f85eb0671",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "82e2e111d2a81fe8efaabec44b5d7d3c74cfa95d",
      "title": "Stealing Your Data from Compressed Machine Learning Models",
      "abstract": "Machine learning models have been widely deployed in many real-world tasks. When a non-expert data holder wants to use a third-party machine learning service for model training, it is critical to preserve the confidentiality of the training data. In this paper, we for the first time explore the potential privacy leakage in a scenario that a malicious ML provider offers data holder customized training code including model compression which is essential in practical deployment The provider is unable to access the training process hosted by the secured third party, but could inquire models when they are released in public. As a result, adversary can extract sensitive training data with high quality even from these deeply compressed models that are tailored for resource-limited devices. Our investigation shows that existing compressions like quantization, can serve as a defense against such an attack, by degrading the model accuracy and memorized data quality simultaneously. To overcome this defense, we take an initial attempt to design a simple but stealthy quantized correlation encoding attack flow from an adversary perspective. Three integrated components-data pre-processing, layer-wise data-weight correlation regularization, data-aware quantization, are developed accordingly. Extensive experimental results show that our framework can preserve the evasiveness and effectiveness of stealing data from compressed models.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Nuo Xu",
        "Qi Liu",
        "Tao Liu",
        "Zihao Liu",
        "Xiaochen Guo",
        "Wujie Wen"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/82e2e111d2a81fe8efaabec44b5d7d3c74cfa95d",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2551c23c2ffd1e75b22c8d9f511902854dcb4dfa",
      "title": "ACD: An Adaptable Approach for RFID Cloning Attack Detection",
      "abstract": "With the rapid development of the internet of things, radio frequency identification (RFID) technology plays an important role in various fields. However, RFID systems are vulnerable to cloning attacks. This is the fabrication of one or more replicas of a genuine tag, which behave exactly as a genuine tag and fool the reader to gain legal authorization, leading to potential financial loss or reputation damage. Many advanced solutions have been proposed to combat cloning attacks, but they require extra hardware resources, or they cannot detect a clone tag in time. In this article, we make a fresh attempt to counterattack tag cloning based on spatiotemporal collisions. We propose adaptable clone detection (ACD), which can intuitively and accurately display the positions of abnormal tags in real time. It uses commercial off-the-shelf (COTS) RFID devices without extra hardware resources. We evaluate its performance in practice, and the results confirm its success at detecting cloning attacks. The average accuracy can reach 98.7%, and the recall rate can reach 96%. Extensive experiments show that it can adapt to a variety of RFID application scenarios.",
      "year": 2020,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Weiqing Huang",
        "Yanfang Zhang",
        "Yue Feng"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/2551c23c2ffd1e75b22c8d9f511902854dcb4dfa",
      "pdf_url": "https://www.mdpi.com/1424-8220/20/8/2378/pdf",
      "publication_date": "2020-04-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "001db238123a7a2c08c4ba8186c9b1a4460d3fb2",
      "title": "SCNet: A Neural Network for Automated Side-Channel Attack",
      "abstract": "The side-channel attack is an attack method based on the information gained about implementations of computer systems, rather than weaknesses in algorithms. Information about system characteristics such as power consumption, electromagnetic leaks and sound can be exploited by the side-channel attack to compromise the system. Much research effort has been directed towards this field. However, such an attack still requires strong skills, thus can only be performed effectively by experts. Here, we propose SCNet, which automatically performs side-channel attacks. And we also design this network combining with side-channel domain knowledge and different deep learning model to improve the performance and better to explain the result. The results show that our model achieves good performance with fewer parameters. The proposed model is a useful tool for automatically testing the robustness of computer systems.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Guanlin Li",
        "Chang Liu",
        "Han Yu",
        "Yanhong Fan",
        "Libang Zhang",
        "Zongyue Wang",
        "Meiqin Wang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/001db238123a7a2c08c4ba8186c9b1a4460d3fb2",
      "pdf_url": "",
      "publication_date": "2020-08-02",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "6f9786f75f0a199b0899ddbe42fff476a33d9ca7",
      "title": "Resistivity-depth imaging of airborne transient electromagnetic method based on an artificial neural network",
      "abstract": "\n <p>An artificial neural network, which is an important part of artificial intelligence, has been widely used to many fields such as information processing, automation and economy, and geophysical data processing as one of the efficient tools. However, the application in geophysical electromagnetic method is still relatively few. In this paper, BP neural network was combined with airborne transient electromagnetic method for imaging subsurface geological structures.</p><p>We developed an artificial neural network code to map the distribution of geologic conductivity in the subsurface for the airborne transient electromagnetic method. It avoids complex derivation of electromagnetic field formula and only requires input and transfer functions to obtain the quasi-resistivity image section. First, training sample set, which is airborne transient electromagnetic response of homogeneous half-space models with the different resistivity, is formed and network model parameters include the flight altitude and the time constant, which were taken as input variables of the network, and pseudo-resistivity are taken as output variables. Then, a double hidden layer BP neural network is established in accordance with the mapping relationship between quasi-resistivity and airborne transient electromagnetic response. By analyzing mean square error curve, the training&#160;termination criterion of BP neural network is presented. Next, the trained BP neural network is used to interpret the airborne transient electromagnetic responses of various typical layered geo-electric models, and it is compared with those of the all-time apparent resistivity algorithm. After a lot of tests, reasonable BP neural network parameters were selected, and the mapping from airborne TEM quasi-resistivity was realized. The results show that the resistivity imaging from BP neural network approach is much closer to the true resistivity of model, and the response to anomalous bodies is better than that of all-time apparent resistivity numerical method. Finally, this imaging technique was use to process the field data acquired by the airborne transient method from Huayangchuan area. Quasi-resistivity depth section calculated by BP neural network and all-time apparent resistivity is in good agreement with the actual geological situation, which further verifies the effectiveness and practicability of this algorithm.</p>\n",
      "year": 2020,
      "venue": "",
      "authors": [
        "Jifeng Zhang",
        "B. Feng",
        "Dong Li"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/6f9786f75f0a199b0899ddbe42fff476a33d9ca7",
      "pdf_url": "",
      "publication_date": "2020-03-09",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5a1dc7833ff4095cae1f18bb8a4d286b9b25a4fe",
      "title": "Resistivity-depth Imaging with the Airborne Transient Electromagnetic Method Based on an Artificial Neural Network",
      "abstract": "We developed an artificial neural network to map the distribution of geologic conductivity in the earth subsurface using the airborne transient electromagnetic method. The artificial neural network avoids the need for complex derivations of electromagnetic field formulas and requires only input and transfer functions to obtain a quasi-resistivity image. First, training sample set from the airborne transient electromagnetic response of homogeneous half-space models with different resistivities was formed, and network model parameters, including the flight altitude, time constant, and response amplitude, were determined. Then, a double-hidden-layer back-propagation (BP) neural network was established based on the mapping relationship between quasi-resistivity and airborne transient electromagnetic response. By analyzing the mean square error curve, the training termination criterion of the BP neural network was determined. Next, the trained BP neural network was used to interpret the airborne transient electromagnetic responses of various typical layered geo-electric models, and the results were compared with that from the all-time apparent resistivity algorithm. The comparison indicated that the resistivity imaging from the BP neural network approach was much closer to the true resistivity of the model, and the response to anomalous bodies was better than that from an all-time apparent resistivity. Finally, this imaging technique was used to process field data acquired by employing the airborne transient method from the HuaYin survey area. Quasi-resistivity depth sections calculated with the BP neural network and the actual geological situation were in good.",
      "year": 2020,
      "venue": "",
      "authors": [
        "B. Feng",
        "Jifeng Zhang",
        "Dong Li",
        "Yang Bai"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/5a1dc7833ff4095cae1f18bb8a4d286b9b25a4fe",
      "pdf_url": "",
      "publication_date": "2020-09-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a2923ace209e6193effa49a5c254b29d77b2ee0d",
      "title": "Stealing Black-Box Functionality Using The Deep Neural Tree Architecture",
      "abstract": "This paper makes a substantial step towards cloning the functionality of black-box models by introducing a Machine learning (ML) architecture named Deep Neural Trees (DNTs). This new architecture can learn to separate different tasks of the black-box model, and clone its task-specific behavior. We propose to train the DNT using an active learning algorithm to obtain faster and more sample-efficient training. In contrast to prior work, we study a complex \"victim\" black-box model based solely on input-output interactions, while at the same time the attacker and the victim model may have completely different internal architectures. The attacker is a ML based algorithm whereas the victim is a generally unknown module, such as a multi-purpose digital chip, complex analog circuit, mechanical system, software logic or a hybrid of these. The trained DNT module not only can function as the attacked module, but also provides some level of explainability to the cloned model due to the tree-like nature of the proposed architecture.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Daniel Teitelman",
        "I. Naeh",
        "Shie Mannor"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a2923ace209e6193effa49a5c254b29d77b2ee0d",
      "pdf_url": "",
      "publication_date": "2020-02-23",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9ff6c3660fbb535d72fa100758e59df0f71945f8",
      "title": "Green Lighting ML: Confidentiality, Integrity, and Availability of Machine Learning Systems in Deployment",
      "abstract": "Security and ethics are both core to ensuring that a machine learning system can be trusted. In production machine learning, there is generally a hand-off from those who build a model to those who deploy a model. In this hand-off, the engineers responsible for model deployment are often not privy to the details of the model and thus, the potential vulnerabilities associated with its usage, exposure, or compromise. Techniques such as model theft, model inversion, or model misuse may not be considered in model deployment, and so it is incumbent upon data scientists and machine learning engineers to understand these potential risks so they can communicate them to the engineers deploying and hosting their models. This is an open problem in the machine learning community and in order to help alleviate this issue, automated systems for validating privacy and security of models need to be developed, which will help to lower the burden of implementing these hand-offs and increasing the ubiquity of their adoption.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Abhishek Gupta",
        "Erick Galinkin"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/9ff6c3660fbb535d72fa100758e59df0f71945f8",
      "pdf_url": "",
      "publication_date": "2020-07-09",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "05b8495df20c2539d25448b278660e776175fccb",
      "title": "Research on Side-Channel Attack Based on the Synergy between SNR and Convolutional Neural Networks",
      "abstract": "The data encryption process in the encryption chip will be leaked by means of timing operation time, probing data collecting operation power and electromagnetic signal interception, which makes side channel attack possible. Nowadays, the research on template creation of template attacks has shifted from Gaussian distribution to the use of machine learning algorithms to create templates. With many parameters, it is difficult to find a suitable network structure. Based on many experimental studies, the experience of a convolutional neural network structure suitable for side channel analysis is summarized and proposed.",
      "year": 2020,
      "venue": "Journal of Physics: Conference Series",
      "authors": [
        "Zixin Liu",
        "Ting Zhu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/05b8495df20c2539d25448b278660e776175fccb",
      "pdf_url": "https://doi.org/10.1088/1742-6596/1575/1/012026",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "550ae0ee2a43d419ea92b899505b96a64eaa213e",
      "title": "Analysis of Electromagnetic Band Gap Structure using Artificial Neural Network for UWB Applications",
      "abstract": "In this paper, a planar meander-line electromagnetic band-gap (EBG) structure for UWB applications is proposed. The EBG resonating at 7.5 GHz is designed and analyzed using artificial neural network (ANN) technique. The resonant frequency and the corresponding |S11| are predicted using geometrical parameters of the EBG unit cell. The proposed ANN model with single hidden layer is trained using Levenberg-Marquardt (LM) algorithm. The network is trained using different sets of hidden neurons in the hidden layer and activation function, in order to select the optimum ones. The least mean square error (MSE) is obtained when the network is trained using log-sigmoid transfer function with 40 hidden neurons in the hidden layer. Also, performance of the ANN model is validated using other statistical parameters viz. mean absolute percentage error (MAPE), and correlation coefficient (CC).",
      "year": 2020,
      "venue": "2020 Advanced Communication Technologies and Signal Processing (ACTS)",
      "authors": [
        "Debanjali Sarkar",
        "T. Khan",
        "F. Talukdar"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/550ae0ee2a43d419ea92b899505b96a64eaa213e",
      "pdf_url": "",
      "publication_date": "2020-12-04",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f0d3386725f66c8436f2b37a419fddcea3e0f302",
      "title": "Model Extraction Oriented Data Publishing with k-anonymity",
      "abstract": null,
      "year": 2020,
      "venue": "International Workshop on Security",
      "authors": [
        "Takeru Fukuoka",
        "Yuji Yamaoka",
        "Takeaki Terada"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/f0d3386725f66c8436f2b37a419fddcea3e0f302",
      "pdf_url": "",
      "publication_date": "2020-09-02",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "30d6ef1b7d590094b62f38e97603122f65aa0d09",
      "title": "Model Extraction",
      "abstract": null,
      "year": 2020,
      "venue": "S-Parameters for Signal Integrity",
      "authors": [],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/30d6ef1b7d590094b62f38e97603122f65aa0d09",
      "pdf_url": "",
      "publication_date": "2020-02-06",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "91ffcaa35fd6690ff799298aa75bf9a42199a834",
      "title": "Conformational changes of \u03b2-thalassemia major hemoglobin and oxidative status of plasma after in vitro exposure to extremely low-frequency electromagnetic fields: An artificial neural network analysis",
      "abstract": "ABSTRACT Electromagnetic fields (EMF) can generate reactive oxygen species and induce oxidative modifications. We investigated the effects of extremely low-frequency electromagnetic fields (ELF-EMF) on oxidative status of plasma and erythrocytes in \u03b2-thalassemia major patients and design artificial neural networks (ANN) for evaluating the oxyHb concentration. Blood samples were obtained from age and sex-matched healthy donors (n = 12) and major \u03b2-thalassemia patients (n = 12) and subjected to 0.5 and 1 mT and 50 Hz of EMF. Plasma oxidative status was estimated after 1 and 2 h exposure to ELE-EMF. Structural changes of plasma proteins were investigated by Native PAGE and SDS-PAGE. Moreover; multilayer perceptron (MLP) method was applied for designing a feed forward ANN model to predict the impact of these oxidative and antioxidative parameters on oxyHb concentration. Two hour exposure to ELF-EMF induced significant oxidative changes on major \u03b2-thalassemia samplesElectrophoretic profiles showed two high molecular weight (HMW) protein aggregates in plasma samples from healthy donors and major \u03b2-thalassemia patients. According to our ANN design, the main predictors of oxyHb concentration were optical density of Hb at 542, 340, 569, 630, 577, and 420 nm and metHb and hemichrome (HC) concentration. Accuracy of the proposed ANN model was shown by predicted by observed chart (y = 1.3 + 0.96x, R2 = 0.942), sum of squares errors (SSR), and relative errors (RE). Our results showed the detailed effects of ELF-EMF on Hb structure and oxidative balance of plasma in major \u03b2-thalassemia patients and significance of ANN analysis during normal and pathologic conditions.",
      "year": 2020,
      "venue": "Electromagnetic Biology and Medicine",
      "authors": [
        "Saeideh Rahmani",
        "Hadi Ansarihadipour",
        "M. Bayatiani",
        "A. Khosrowbeygi",
        "S. Babaei",
        "Yousef Rasmi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/91ffcaa35fd6690ff799298aa75bf9a42199a834",
      "pdf_url": "",
      "publication_date": "2020-10-23",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "629c5459c03d7327829eb866cd3bcd2f158ad3ba",
      "title": "Adversarial Imitation Attack",
      "abstract": "Deep learning models are known to be vulnerable to adversarial examples. A practical adversarial attack should require as little as possible knowledge of attacked models. Current substitute attacks need pre-trained models to generate adversarial examples and their attack success rates heavily rely on the transferability of adversarial examples. Current score-based and decision-based attacks require lots of queries for the attacked models. In this study, we propose a novel adversarial imitation attack. First, it produces a replica of the attacked model by a two-player game like the generative adversarial networks (GANs). The objective of the generative model is to generate examples that lead the imitation model returning different outputs with the attacked model. The objective of the imitation model is to output the same labels with the attacked model under the same inputs. Then, the adversarial examples generated by the imitation model are utilized to fool the attacked model. Compared with the current substitute attacks, imitation attacks can use less training data to produce a replica of the attacked model and improve the transferability of adversarial examples. Experiments demonstrate that our imitation attack requires less training data than the black-box substitute attacks, but achieves an attack success rate close to the white-box attack on unseen data with no query.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Mingyi Zhou",
        "Jing Wu",
        "Yipeng Liu",
        "Xiaolin Huang",
        "Shuaicheng Liu",
        "Xiang Zhang",
        "Ce Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/629c5459c03d7327829eb866cd3bcd2f158ad3ba",
      "pdf_url": "",
      "publication_date": "2020-03-28",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "b3f8a8d2accdc5c80a3e901f3f3abd0391c644de",
      "title": "Model Extraction Defense using Modified Variational Autoencoder",
      "abstract": null,
      "year": 2020,
      "venue": "",
      "authors": [
        "Yash Gupta"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b3f8a8d2accdc5c80a3e901f3f3abd0391c644de",
      "pdf_url": "",
      "publication_date": "2020-06-04",
      "keywords_matched": [
        "model extraction",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d51515601024e2b0cd12abf878b5d7db8adda6ff",
      "title": "An Electromagnetic Neural Network for Inverse Source Modeling of Wire-Like Objects",
      "abstract": "We propose a new electromagnetic neural network to extract the spatial features of wire objects via a novel spatial singularity expansion method (S-SEM). The proposed approach utilizes a recently-found radiation function that holds the spatial parameters of targets defined as the surface current and the geometrical details. Through EM machine learning, an estimation of these parameters is performed in a form of inverse problems for a single wire system. The estimated parameters, which are the S-SEM's poles and strengths, are validated and compared with numerical results obtained from the EM solver where an excellent agreement is observed.",
      "year": 2020,
      "venue": "2020 IEEE International Symposium on Antennas and Propagation and North American Radio Science Meeting",
      "authors": [
        "Abdelelah M. Alzahed",
        "Y. Antar",
        "S. Mikki"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d51515601024e2b0cd12abf878b5d7db8adda6ff",
      "pdf_url": "",
      "publication_date": "2020-07-05",
      "keywords_matched": [
        "electromagnetic neural network"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "b27fec3e197ef802999cb3fbedf832945cfef6a1",
      "title": "Detection of Cloned Recognizers: A Defending Method against Recognizer Cloning Attack",
      "abstract": null,
      "year": 2020,
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "authors": [
        "Y. Mori",
        "Kazuaki Nakamura",
        "Naoko Nitta",
        "N. Babaguchi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b27fec3e197ef802999cb3fbedf832945cfef6a1",
      "pdf_url": "",
      "publication_date": "2020-12-07",
      "keywords_matched": [
        "cloning attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2cb1f4688f8ba457b4fa2ed36deae5a98f4249ca",
      "title": "Model inversion attacks against collaborative inference",
      "abstract": "The prevalence of deep learning has drawn attention to the privacy protection of sensitive data. Various privacy threats have been presented, where an adversary can steal model owners' private data. Meanwhile, countermeasures have also been introduced to achieve privacy-preserving deep learning. However, most studies only focused on data privacy during training, and ignored privacy during inference. In this paper, we devise a new set of attacks to compromise the inference data privacy in collaborative deep learning systems. Specifically, when a deep neural network and the corresponding inference task are split and distributed to different participants, one malicious participant can accurately recover an arbitrary input fed into this system, even if he has no access to other participants' data or computations, or to prediction APIs to query this system. We evaluate our attacks under different settings, models and datasets, to show their effectiveness and generalization. We also study the characteristics of deep learning models that make them susceptible to such inference privacy threats. This provides insights and guidelines to develop more privacy-preserving collaborative systems and algorithms.",
      "year": 2019,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Zecheng He",
        "Tianwei Zhang",
        "R. Lee"
      ],
      "citation_count": 354,
      "url": "https://www.semanticscholar.org/paper/2cb1f4688f8ba457b4fa2ed36deae5a98f4249ca",
      "pdf_url": "",
      "publication_date": "2019-12-09",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "Primarily about privacy, not model stealing",
      "filter_confidence": "medium",
      "filter_stage": "topic"
    },
    {
      "paper_id": "b12c15503435cffee16bdd156766dcf390eeb8f8",
      "title": "CSI NN: Reverse Engineering of Neural Network Architectures Through Electromagnetic Side Channel",
      "abstract": null,
      "year": 2019,
      "venue": "USENIX Security Symposium",
      "authors": [
        "L. Batina",
        "S. Bhasin",
        "Dirmanto Jap",
        "S. Picek"
      ],
      "citation_count": 309,
      "url": "https://www.semanticscholar.org/paper/b12c15503435cffee16bdd156766dcf390eeb8f8",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "555b847ee9c39c12d5764f4e11b888b331a89cfb",
      "title": "Defending Against Neural Network Model Stealing Attacks Using Deceptive Perturbations",
      "abstract": "Machine learning architectures are readily available, but obtaining the high quality labeled data for training is costly. Pre-trained models available as cloud services can be used to generate this costly labeled data, and would allow an attacker to replicate trained models, effectively stealing them. Limiting the information provided by cloud based models by omitting class probabilities has been proposed as a means of protection but significantly impacts the utility of the models. In this work, we illustrate how cloud based models can still provide useful class probability information for users, while significantly limiting the ability of an adversary to steal the model. Our defense perturbs the model's final activation layer, slightly altering the output probabilities. This forces the adversary to discard the class probabilities, requiring significantly more queries before they can train a model with comparable performance. We evaluate our defense under diverse scenarios and defense aware attacks. Our evaluation shows our defense can degrade the accuracy of the stolen model at least 20%, or increase the number of queries required by an adversary 64 fold, all with a negligible decrease in the protected model accuracy.",
      "year": 2019,
      "venue": "2019 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Taesung Lee",
        "Ben Edwards",
        "Ian Molloy",
        "D. Su"
      ],
      "citation_count": 109,
      "url": "https://www.semanticscholar.org/paper/555b847ee9c39c12d5764f4e11b888b331a89cfb",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8834415/8844588/08844598.pdf",
      "publication_date": "2019-05-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "1f25ad13444dcd9898a8500428f2c97bdb05369c",
      "title": "Open DNN Box by Power Side-Channel Attack",
      "abstract": "Deep neural networks are becoming popular and important assets of many AI companies. However, recent studies indicate that they are also vulnerable to adversarial attacks. Adversarial attacks can be either white-box or black-box. The white-box attacks assume full knowledge of the models while the black-box ones assume none. In general, revealing more internal information can enable much more powerful and efficient attacks. However, in most real-world applications, the internal information of embedded AI devices is unavailable. Therefore, in this brief, we propose a side-channel information based technique to reveal the internal information of black-box models. Specifically, we have made the following contributions: (1) different from previous works, we use side-channel information to reveal internal network architecture in embedded devices; (2) we construct models for internal parameter estimation that no research has been reached yet; and (3) we validate our methods on real-world devices and applications. The experimental results show that our method can achieve 96.50% accuracy on average. Such results suggest that we should pay strong attention to the security problem of many AI devices, and further propose corresponding defensive strategies in the future.",
      "year": 2019,
      "venue": "IEEE Transactions on Circuits and Systems - II - Express Briefs",
      "authors": [
        "Yun Xiang",
        "Zhuangzhi Chen",
        "Zuohui Chen",
        "Zebin Fang",
        "Haiyang Hao",
        "Jinyin Chen",
        "Yi Liu",
        "Zhefu Wu",
        "Qi Xuan",
        "Xiaoniu Yang"
      ],
      "citation_count": 101,
      "url": "https://www.semanticscholar.org/paper/1f25ad13444dcd9898a8500428f2c97bdb05369c",
      "pdf_url": "https://arxiv.org/pdf/1907.10406",
      "publication_date": "2019-07-21",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9329e150170534e99d169a345aa68aeb004279ca",
      "title": "The Attack of the Clones against Proof-of-Authority",
      "abstract": "In this paper, we explore vulnerabilities and countermeasures of the recently proposed blockchain consensus based on proof-of-authority. The proof-of-work blockchains, like Bitcoin and Ethereum, have been shown both theoretically and empirically vulnerable to double spending attacks. This is why Byzantine fault tolerant consensus algorithms have gained popularity in the blockchain context for their ability to tolerate a limited number t of attackers among n participants. We formalize the recently proposed proof-of-authority consensus algorithms that are Byzantine fault tolerant by describing the Aura and Clique protocols present in the two mainstream implementations of Ethereum. We then introduce the Cloning Attack and show how to apply it to double spend in each of these protocols with a single malicious node. Our results show that the Cloning Attack against Aura is always successful while the same attack against Clique is about twice as fast and succeeds in most cases.",
      "year": 2019,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Parinya Ekparinya",
        "Vincent Gramoli",
        "Guillaume Jourjon"
      ],
      "citation_count": 89,
      "url": "https://www.semanticscholar.org/paper/9329e150170534e99d169a345aa68aeb004279ca",
      "pdf_url": "https://doi.org/10.14722/ndss.2020.24082",
      "publication_date": "2019-02-26",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "bd120cdcf499babb814ea1e645bf077f2dca01ad",
      "title": "The removal of the high-frequency motion-induced noise in helicopter-borne transient electromagnetic data based on wavelet neural network",
      "abstract": "In helicopter-borne transient electromagnetic (HTEM) signal processing, removal of motion-induced noise is one of the most important steps. A special type of short-term noise, which could be classified as high-frequency motion-induced noise (HFM noise) based on its cause and time-frequency features, was observed in the field data of the Chinese Academy of Sciences-HTEM system. Because the HFM noise is an in-band noise for the HTEM response, it usually remains after the normal denoising procedure developed for the conventional motion-induced noise. To solve this problem, we have developed a three-stage workflow to remove the HFM noise using the wavelet neural network (WNN). In the first stage, the WNN training is performed, and the data segment in which the HFM noise is dominant is selected as the sample set. In the second stage, the HFM noise corresponding to the data segment in which the earth\u2019s response coexisted with the HFM noise is predicted using the well-trained WNN. In the last stage, the predicted HFM noise is removed from the corresponding original data. As an example, we applied our workflow in the field data observed in Inner Mongolia, the HFM noise is removed effectively, and the results provide a strong data foundation for the subsequent processing procedures.",
      "year": 2019,
      "venue": "Geophysics",
      "authors": [
        "Xin Wu",
        "G. Xue",
        "Pan Xiao",
        "Jutao Li",
        "Lihua Liu",
        "G. Fang"
      ],
      "citation_count": 78,
      "url": "https://www.semanticscholar.org/paper/bd120cdcf499babb814ea1e645bf077f2dca01ad",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "8bedaca9ebb1d055046643802683d013fd1b2da9",
      "title": "BDPL: A Boundary Differentially Private Layer Against Machine Learning Model Extraction Attacks",
      "abstract": null,
      "year": 2019,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Huadi Zheng",
        "Qingqing Ye",
        "Haibo Hu",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/8bedaca9ebb1d055046643802683d013fd1b2da9",
      "pdf_url": "",
      "publication_date": "2019-09-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7542385f3672cfee43adc964a90efb0b19f6b9fa",
      "title": "Deep Neural Network Attribution Methods for Leakage Analysis and Symmetric Key Recovery",
      "abstract": null,
      "year": 2019,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Benjamin Hettwer",
        "Stefan Gehrer",
        "Tim G\u00fcneysu"
      ],
      "citation_count": 55,
      "url": "https://www.semanticscholar.org/paper/7542385f3672cfee43adc964a90efb0b19f6b9fa",
      "pdf_url": "",
      "publication_date": "2019-08-12",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a7dd719ce38048df879463f2b2722521754e53a7",
      "title": "Malware Detection in Embedded Systems Using Neural Network Model for Electromagnetic Side-Channel Signals",
      "abstract": null,
      "year": 2019,
      "venue": "Journal of Hardware and Systems Security",
      "authors": [
        "Haider Adnan Khan",
        "Nader Sehatbakhsh",
        "Luong N. Nguyen",
        "Milos Prvulovi\u0107",
        "A. Zaji\u0107"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/a7dd719ce38048df879463f2b2722521754e53a7",
      "pdf_url": "",
      "publication_date": "2019-08-22",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "fa30abd5d9233ccde88502d5bae300e6666d3417",
      "title": "Make Some Noise. Unleashing the Power of Convolutional Neural Networks for Profiled Side-channel Analysis",
      "abstract": "Profiled side-channel analysis based on deep learning, and more precisely Convolutional Neural Networks, is a paradigm showing significant potential. The results, although scarce for now, suggest that such techniques are even able to break cryptographic implementations protected with countermeasures. In this paper, we start by proposing a new Convolutional Neural Network instance able to reach high performance for a number of considered datasets. We compare our neural network with the one designed for a particular dataset with masking countermeasure and we show that both are good designs but also that neither can be considered as a superior to the other one.Next, we address how the addition of artificial noise to the input signal can be actually beneficial to the performance of the neural network. Such noise addition is equivalent to the regularization term in the objective function. By using this technique, we are able to reduce the number of measurements needed to reveal the secret key by orders of magnitude for both neural networks. Our new convolutional neural network instance with added noise is able to break the implementation protected with the random delay countermeasure by using only 3 traces in the attack phase. To further strengthen our experimental results, we investigate the performance with a varying number of training samples, noise levels, and epochs. Our findings show that adding noise is beneficial throughout all training set sizes and epochs.",
      "year": 2019,
      "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.",
      "authors": [
        "Jaehun Kim",
        "S. Picek",
        "Annelie Heuser",
        "S. Bhasin",
        "A. Hanjalic"
      ],
      "citation_count": 49,
      "url": "https://www.semanticscholar.org/paper/fa30abd5d9233ccde88502d5bae300e6666d3417",
      "pdf_url": "https://tches.iacr.org/index.php/TCHES/article/download/8292/7642",
      "publication_date": "2019-05-09",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "697cbfa1a3ea202a5710138b089ecefca3f5d935",
      "title": "Imaging subsurface resistivity structure from airborne electromagnetic induction data using deep neural network",
      "abstract": "ABSTRACT Due to the rapid development and spread of deep learning technologies, potential applications of artificial intelligence technology in the field of geophysical inversion are being explored. In this study, we applied a deep neural network (DNN) to reconstruct one-dimensional electrical resistivity structures from airborne electromagnetic (AEM) data for varying sensor heights. We used numerical models and their simulated AEM responses to train the DNN to be an inversion operator, and determined that it was possible to train the DNN without the use of stabilisers on the subsurface structures. We compared the quantitative performance of DNN and Gauss\u2013Newton inversion of synthetic datasets, and demonstrated that DNN inversion reconstructed the subsurface structure more accurately, and within a significantly shorter period. We subsequently applied DNN inversion to a field dataset to quantify the effectiveness and applicability of the proposed method for real data. The results of the current study will open new avenues for real-time imaging of subsurface structures from AEM data.",
      "year": 2019,
      "venue": "Exploration Geophysics",
      "authors": [
        "K. Noh",
        "D. Yoon",
        "J. Byun"
      ],
      "citation_count": 43,
      "url": "https://www.semanticscholar.org/paper/697cbfa1a3ea202a5710138b089ecefca3f5d935",
      "pdf_url": "",
      "publication_date": "2019-10-22",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ce90e2faf697bff599ea8cac9dd47bda9911abf4",
      "title": "Performance Analysis and Dynamic Evolution of Deep Convolutional Neural Network for Electromagnetic Inverse Scattering",
      "abstract": "The solution of electromagnetic (EM) inverse scattering problems is hindered by challenges, such as ill-posedness, nonlinearity, and high computational costs. Recently, deep learning was shown to be a promising tool in addressing these challenges. In particular, it is possible to establish a connection between a deep convolutional neural network (CNN) and iterative solution methods of EM inverse scattering. This led to the development of an efficient CNN-based solution to EM inverse problems, termed DeepNIS. It has been shown that DeepNIS can outperform conventional inverse scattering solution methods in terms of both the image quality and computational time. In this letter, we evaluate the DeepNIS performance as a function of the number of layers using structure similarity index measure and mean square error metrics. In addition, we probe the dynamic evolution behavior of DeepNIS by examining its near-isometry property and show that, after a proper training stage, the proposed CNN has near-optimal stability properties.",
      "year": 2019,
      "venue": "IEEE Antennas and Wireless Propagation Letters",
      "authors": [
        "Lianlin Li",
        "Longgang Wang",
        "F. Teixeira"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/ce90e2faf697bff599ea8cac9dd47bda9911abf4",
      "pdf_url": "https://arxiv.org/pdf/1901.02610",
      "publication_date": "2019-01-09",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0a82250312de7809d139d0244f4b8bf3f183472d",
      "title": "Unleashing the Power of Convolutional Neural Networks for Profiled Side-channel Analysis",
      "abstract": null,
      "year": 2019,
      "venue": "",
      "authors": [
        "Jaehun Kim",
        "S. Picek",
        "Annelie Heuser",
        "S. Bhasin",
        "A. Hanjalic"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/0a82250312de7809d139d0244f4b8bf3f183472d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "eb9b3de60c1039550e9457510eb463dae29037c3",
      "title": "Fouling resistance prediction based on GA\u2013Elman neural network for circulating cooling water with electromagnetic anti-fouling treatment",
      "abstract": null,
      "year": 2019,
      "venue": "Journal of the Energy Institute",
      "authors": [
        "Jianguo Wang",
        "Zhenzhong Lv",
        "Yandong Liang",
        "Lijuan Deng",
        "Zhiwei Li"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/eb9b3de60c1039550e9457510eb463dae29037c3",
      "pdf_url": "",
      "publication_date": "2019-10-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7c2427863bf72ce1b679735ff46d6832a2e12d9b",
      "title": "High-resolution palaeovalley classification from airborne\nelectromagnetic imaging and deep neural network training using\ndigital elevation model data",
      "abstract": "Abstract. Palaeovalleys are buried ancient river valleys that often form productive aquifers, especially in the semi-arid and arid areas of Australia. Delineating their extent and hydrostratigraphy is however a challenging task in groundwater system characterization. This study developed a methodology based on the deep learning super-resolution convolutional neural network (SRCNN) approach, to convert electrical conductivity (EC) estimates from an airborne electromagnetic (AEM) survey in South Australia to a high-resolution binary palaeovalley map. The SRCNN was trained and tested with a synthetic training dataset, where valleys were generated from readily available digital elevation model (DEM) data from the AEM survey area. Electrical conductivities typical of valley sediments were generated by Archie\u2019s Law, and subsequently blurred by down-sampling and bicubic interpolation to represent noise from the AEM survey, inversion and interpolation. After a model training step, the SRCNN successfully removed such noise, and reclassified the low-resolution, unimodal but skewed EC values into a high-resolution palaeovalley index following a bimodal distribution. The latter allows distinguishing valley from non-valley pixels. Furthermore, a realistic spatial connectivity structure of the palaeovalley was predicted when compared with borehole lithology logs and valley bottom flatness indicator. Overall the methodology permitted to better constrain the three-dimensional palaeovalley geometry from AEM images that are becoming more widely available for groundwater prospecting.\n",
      "year": 2019,
      "venue": "",
      "authors": [
        "Zhenjiao Jiang",
        "D. Mallants",
        "L. Peeters",
        "Lei Gao",
        "G. Mari\u00e9thoz"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/7c2427863bf72ce1b679735ff46d6832a2e12d9b",
      "pdf_url": "https://hess.copernicus.org/articles/23/2561/2019/hess-23-2561-2019.pdf",
      "publication_date": "2019-01-21",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4ea06d39ff6c8f75c3a1e12c81b1595f72effdeb",
      "title": "Proposed Guidelines for the Responsible Use of Explainable Machine Learning.",
      "abstract": "Explainable machine learning (ML) enables human learning from ML, human appeal of automated model decisions, regulatory compliance, and security audits of ML models. Explainable ML (i.e. explainable artificial intelligence or XAI) has been implemented in numerous open source and commercial packages and explainable ML is also an important, mandatory, or embedded aspect of commercial predictive modeling in industries like financial services. However, like many technologies, explainable ML can be misused, particularly as a faulty safeguard for harmful black-boxes, e.g. fairwashing or scaffolding, and for other malevolent purposes like stealing models and sensitive training data. To promote best-practice discussions for this already in-flight technology, this short text presents internal definitions and a few examples before covering the proposed guidelines. This text concludes with a seemingly natural argument for the use of interpretable models and explanatory, debugging, and disparate impact testing methods in life- or mission-critical ML systems.",
      "year": 2019,
      "venue": "",
      "authors": [
        "Patrick Hall",
        "Navdeep Gill",
        "N. Schmidt"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/4ea06d39ff6c8f75c3a1e12c81b1595f72effdeb",
      "pdf_url": "",
      "publication_date": "2019-06-08",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "a63fcc399d69068fc2ce6cd38a7dddfe1e95c2e2",
      "title": "Evasion Attacks Against Watermarking Techniques found in MLaaS Systems",
      "abstract": "Deep neural networks have had enormous impact on various domains of computer science applications, considerably outperforming previous state-of-the-art machine learning techniques. To achieve this performance, neural networks need large quantities of data and huge computational resources, which heavily increase their costs. The increased cost of building a good deep neural network model gives rise to a need for protecting this investment from potential copyright infringements. Legitimate owners of a machine learning model want to be able to reliably track and detect a malicious adversary that tries to steal the intellectual property related to the model. This threat is very relevant to Machine Learning as a Service (MLaaS) systems, where a provider supplies APIs to clients, allowing them to interact with their trained proprietary deep learning models. Recently, this problem was tackled by introducing in deep neural networks the concept of watermarking, which allows a legitimate owner to embed some secret information (watermark) in a given model. Through the use of this watermark, the legitimate owners, remotely interacting with a model through input queries, are able to detect a copyright infringement, and prove the ownership of their models that were stolen/copied illegally. In this paper, we focus on assessing the robustness and reliability of state-of-the-art deep neural network watermarking schemes. In particular we show that, a malicious adversary, even in scenarios where the watermark is difficult to remove, can still evade the verification of copyright infringements from the legitimate owners, thus avoiding the detection of the model theft.",
      "year": 2019,
      "venue": "Swiss Conference on Data Science",
      "authors": [
        "Dorjan Hitaj",
        "B. Hitaj",
        "L. Mancini"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/a63fcc399d69068fc2ce6cd38a7dddfe1e95c2e2",
      "pdf_url": "",
      "publication_date": "2019-06-01",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d2affd606828c410d9a68c627a099101220ac8cb",
      "title": "A Spintronics Memory PUF for Resilience Against Cloning Counterfeit",
      "abstract": "With the widespread use of electronic devices in embedding or processing sensitive information, new hardware security primitives have emerged to improve the shortcomings of traditional secure data storage. One of these solutions, the physically unclonable function (PUF), which is widely used for authentication and cryptography applications, extracts the unique and unclonable value from the circuit physical properties. However, a cloning attack on memory-based PUF has been demonstrated from the circuit back-side tampering, questioning its unclonable property and opening counterfeiting vulnerability in an untrusted supply chain. Spin transfer torque-magnetic random-access memory (STT-MRAM) is a promising technology due to its nonvolatility, scalability, and CMOS compatibility, therefore envisioned to be used in many embedded secure devices. In this paper, we reveal the vulnerability of existing STT-MRAM PUF solutions to back-side attacks by modeling different levels of tampering and their impact at electrical and logical levels. We propose a tamper resilient methodology for the STT-MRAM PUF design based on its switching properties. The resilience of the proposed solution to different levels of tampering and a 100% detection are confirmed with simulation results.",
      "year": 2019,
      "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems",
      "authors": [
        "Samir Ben Dodo",
        "R. Bishnoi",
        "Sarath Mohanachandran Nair",
        "M. Tahoori"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/d2affd606828c410d9a68c627a099101220ac8cb",
      "pdf_url": "",
      "publication_date": "2019-11-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "3624e4ef54b4646ebe0ab040000d685664dbc937",
      "title": "Neural Network Model Assessment for Side-Channel Analysis",
      "abstract": null,
      "year": 2019,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Guilherme Perin",
        "Baris Ege",
        "L. Chmielewski"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/3624e4ef54b4646ebe0ab040000d685664dbc937",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9380c5e24c564331053e53d63889cfb588416b1e",
      "title": "An Intelligent Lightning Warning System Based on Electromagnetic Field and Neural Network",
      "abstract": "Prediction of lightning occurrence has significant relevance for reducing potential damage to electric installations, buildings, and humans. However, the existing lightning warning system (LWS) operates using the threshold method and has low prediction accuracy. In this paper, an intelligent LWS based on an electromagnetic field and the artificial neural network was developed for improving lightning prediction accuracy. An electric field mill sensor and a pair of loop antennas were designed to detect the real-time electric field and the magnetic field induced by lightning, respectively. The change rate of electric field, temperature, and humidity acquired 2 min before lightning strikes, were used for developing the neural network using the back propagation algorithm. After observing and predicting lightning strikes over six months, it was verified that the proposed LWS had a prediction accuracy of 93.9%.",
      "year": 2019,
      "venue": "Energies",
      "authors": [
        "Guoming Wang",
        "Woo-Hyun Kim",
        "G. Kil",
        "Dae-Won Park",
        "Sung-Wook Kim"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/9380c5e24c564331053e53d63889cfb588416b1e",
      "pdf_url": "https://www.mdpi.com/1996-1073/12/7/1275/pdf?version=1554803167",
      "publication_date": "2019-04-02",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e8a58e29056a1db21dd8cf5bcb17eaf4db648aaf",
      "title": "Prediction Poisoning: Utility-Constrained Defenses Against Model Stealing Attacks",
      "abstract": null,
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/e8a58e29056a1db21dd8cf5bcb17eaf4db648aaf",
      "pdf_url": "",
      "publication_date": "2019-06-26",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "06f2af0d0d2701b7af029269eaa1c94855dd5d50",
      "title": "Stealing Knowledge from Protected Deep Neural Networks Using Composite Unlabeled Data",
      "abstract": "As state-of-the-art deep neural networks are deployed at the core of more advanced Al-based products and services, the incentive for copying them (i.e., their intellectual properties) by rival adversaries is expected to increase considerably over time. The best way to extract or steal knowledge from such networks is by querying them using a large dataset of random samples and recording their output, followed by training a student network to mimic these outputs, without making any assumption about the original networks. The most effective way to protect against such a mimicking attack is to provide only the classification result, without confidence values associated with the softmax layer.In this paper, we present a novel method for generating composite images for attacking a mentor neural network using a student model. Our method assumes no information regarding the mentor's training dataset, architecture, or weights. Further assuming no information regarding the mentor's softmax output values, our method successfully mimics the given neural network and steals all of its knowledge. We also demonstrate that our student network (which copies the mentor) is impervious to watermarking protection methods, and thus would not be detected as a stolen model.Our results imply, essentially, that all current neural networks are vulnerable to mimicking attacks, even if they do not divulge anything but the most basic required output, and that the student model which mimics them cannot be easily detected and singled out as a stolen copy using currently available techniques.",
      "year": 2019,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Itay Mosafi",
        "Eli David",
        "N. Netanyahu"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/06f2af0d0d2701b7af029269eaa1c94855dd5d50",
      "pdf_url": "https://arxiv.org/pdf/1912.03959",
      "publication_date": "2019-07-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "6d3c97ec0e86b1291b33de4e9777df545770ff70",
      "title": "Poster: Recovering the Input of Neural Networks via Single Shot Side-channel Attacks",
      "abstract": "The interplay between machine learning and security is becoming more prominent. New applications using machine learning also bring new security risks. Here, we show it is possible to reverse-engineer the inputs to a neural network with only a single-shot side-channel measurement assuming the attacker knows the neural network architecture being used.",
      "year": 2019,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "L. Batina",
        "S. Bhasin",
        "Dirmanto Jap",
        "S. Picek"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/6d3c97ec0e86b1291b33de4e9777df545770ff70",
      "pdf_url": "",
      "publication_date": "2019-11-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d7325101238091ce12cf2ad80e8e85533534c08a",
      "title": "A fast approximation for 1-D inversion of transient electromagnetic data by using a back propagation neural network and improved particle swarm optimization",
      "abstract": "Abstract. As one of the most active nonlinear inversion methods in transient\nelectromagnetic (TEM) inversion, the back propagation (BP) neural network\nhas high efficiency because the complicated forward model calculation is\nunnecessary in iteration. The global optimization ability of the particle\nswarm optimization (PSO) is adopted for amending the BP's sensitivity to its initial\nparameters, which avoids it falling into a local optimum. A chaotic-oscillation inertia weight PSO (COPSO) is proposed for accelerating\nconvergence. The COPSO-BP algorithm performance is validated by two typical\ntesting functions, two geoelectric models inversions and a field\nexample. The results show that the COPSO-BP method is more accurate, stable and needs relatively less training time. The proposed algorithm has a\nhigher fitting degree for the data inversion, and it is feasible to use it in\ngeophysical inverse applications.",
      "year": 2019,
      "venue": "",
      "authors": [
        "Ruiyou Li",
        "Huaiqing Zhang",
        "Nian Yu",
        "Ruiheng Li",
        "Qiong Zhuang"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/d7325101238091ce12cf2ad80e8e85533534c08a",
      "pdf_url": "https://npg.copernicus.org/articles/26/445/2019/npg-26-445-2019.pdf",
      "publication_date": "2019-11-26",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-21",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0135f83ab34d66668306b2ad8207d028da2539e4",
      "title": "MimosaNet: An Unrobust Neural Network Preventing Model Stealing",
      "abstract": "Deep Neural Networks are robust to minor perturbations of the learned network parameters and their minor modifications do not change the overall network response significantly. This allows space for model stealing, where a malevolent attacker can steal an already trained network, modify the weights and claim the new network his own intellectual property. In certain cases this can prevent the free distribution and application of networks in the embedded domain. In this paper, we propose a method for creating an equivalent version of an already trained fully connected deep neural network that can prevent network stealing: namely, it produces the same responses and classification accuracy, but it is extremely sensitive to weight changes.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "K\u00e1lm\u00e1n Szentannai",
        "Jalal Al-Afandi",
        "A. Horv\u00e1th"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/0135f83ab34d66668306b2ad8207d028da2539e4",
      "pdf_url": "",
      "publication_date": "2019-07-02",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-29",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ec2de6ca1857b6ec36f8aba1b940281d6f0a6246",
      "title": "The Feasibility of Deep Learning Use for Adversarial Model Extraction in the Cybersecurity Domain",
      "abstract": null,
      "year": 2019,
      "venue": "Ideal",
      "authors": [
        "M. Chora\u015b",
        "M. Pawlicki",
        "R. Kozik"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/ec2de6ca1857b6ec36f8aba1b940281d6f0a6246",
      "pdf_url": "",
      "publication_date": "2019-11-14",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1a593b834e01da0e1bf3c15cf8ee4420285227fd",
      "title": "Convolutional Neural Network Based Side-Channel Attacks with Customized Filters",
      "abstract": null,
      "year": 2019,
      "venue": "International Conference on Information, Communications and Signal Processing",
      "authors": [
        "Man Wei",
        "Danping Shi",
        "Siwei Sun",
        "Peng Wang",
        "Lei Hu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/1a593b834e01da0e1bf3c15cf8ee4420285227fd",
      "pdf_url": "",
      "publication_date": "2019-12-15",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7488bd1a98fd5d366a9076287e014321bbb17674",
      "title": "Deep Neural Network Based Prediction of Leak-Off Pressure in Offshore Norway",
      "abstract": "\n Leak-off pressure (LOP) is an important parameter to determine a weight of drilling mud and in-situ horizontal stresses. When the well pressure become higher than the LOP, it can cause a wellbore instability during drilling, such as a mud loss. Thus, accurate prediction of LOP is important for safe and economical drilling for the oil and gas industry. In this study, we present a novel prediction model for the leak-off pressure (LOP) offshore Norway. The model uses a deep neural network (DNN) applied on a public wellbore database provided by the Norwegian Petroleum Directorate (NPD).\n We used a Python-based web scrapping tool to collect data from more than 6400 wells (1800 exploration wells and 4600 development wells) from the NPD factpages. Then, we analyzed the collected data to investigate impacts of spatial and regional factors on the collected LOPs. The DNN model was structured to predict the leak off pressure offshore Norway using open source libraries Keras and Tensor Flow. The model tests have various hidden layers (i.e. 3, 5, and 10 layers). In order to avoid overfitting, we specified an early-stop algorithm. In our study, we took 80% of the data as the training set keeping the remaining 20% to test the model. In total, the database consists of around 3000 leak-off pressure data from about 1800 exploration wells, and grouped in geographical area (North Sea, Norwegian Sea, Barents Sea groups).\n The LOPs of the North Sea and the Norwegian Sea show a bi-linear trend with depth. The LOPs that are measured from deeper than 2-3 km below sea level show clear a deviation in trend, with a steeper increase compared to the shallower section. The steeper part of the bi-linear trend at greated sub-surface depths can be related to a coupling with tectonic stresses from base rocks. The data from the Barents Sea shows more scattered LOP compared to the other regions offshore Norway. The scattered data seem to relate to the complex geological history on the Barents sea. In general, the accuracy of the prediction increases with the number of hidden layers. However, when the number of the hidden layer exceed 5, there was no significant improvement in the accuracy of prediction. The validation test shows relatively good prediction of LOP with an MAE (Mean Absolute Error) of less than 0.07 even for areas experiencing complex geological history such as the deep subsurface of the Norwegian Sea and the shallow subsurface of the Barents sea.\n This study clearly demonstrates how a data-driven approach combined with machine learning algorithms can provide hidden patterns of not only LOP itself but also the additional information about the lithology, the stress history and the geographical frequency of exploration.",
      "year": 2019,
      "venue": "Day 2 Tue, May 07, 2019",
      "authors": [
        "J. Choi",
        "E. Skurtveit",
        "L. Grande"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7488bd1a98fd5d366a9076287e014321bbb17674",
      "pdf_url": "",
      "publication_date": "2019-04-26",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4f95e417a5aeeaf01635a17d58e512d1c9527d86",
      "title": "Improved Study of Side-Channel Attacks Using Recurrent Neural Networks",
      "abstract": null,
      "year": 2019,
      "venue": "",
      "authors": [
        "Rony Chowdhury",
        "M. Naser"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4f95e417a5aeeaf01635a17d58e512d1c9527d86",
      "pdf_url": "https://doi.org/10.18122/td/1611/boisestate",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a391a74b82dbb86b120c78976f21546c08d7116e",
      "title": "Analysis of Information Leakage from MCU using Neural Network",
      "abstract": "Electromagnetic leakage from an operating microcontroller unit (MCU) can be intercepted and analyzed to deduce the instructions being processed. This is helpful to understanding eavesdropping and to later protecting against it. In this work, harnessing a deep neural network, we analyze the massive electromagnetic (EM) leakage information to extract the instructions run in a microcontroller unit (MCU). An EM leakage acquisition environment is built, and the leakage signal of an MCU is collected. A multi-layer convolutional neural network is constructed for the side channel analysis and identification. The recognition accuracy is over 95% in the training phase, and more than 75% in the prediction phase. This experiment proves that the electromagnetic leakage emitted can be collected by appropriate methods and analyzed by deep learning technology. It can effectively deduce the instructions running in an MCU, which lays a foundation for the protection against eavesdropping.",
      "year": 2019,
      "venue": "International Workshop on Electromagnetic Compatibility of Integrated Circuits",
      "authors": [
        "Siping Gao",
        "Yingkai Guo",
        "Z. Aung",
        "Yongxin Guo"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a391a74b82dbb86b120c78976f21546c08d7116e",
      "pdf_url": "",
      "publication_date": "2019-10-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3ba1d07d015d843dbfa6b6b4a8e433b3ee2caa18",
      "title": "Research on Key Protocol Technology of Safe Access and Imitation Attack for Power Industrial Control Terminal Equipment",
      "abstract": "Industrial control system is widely used in social production activities and its security is directly related to national security and social stability. In this paper, the security threats and protection status of power industry control system are analyzed. A framework of information security evaluation system for power industry control system is proposed, which consists of three parts: experimental verification environment, product detection capability and security service capability, and the construction scheme of the system is discussed. A hardware-in-the-loop simulation and verification platform is constructed to realize the simulation and verification of power industry control system tools. The vulnerability of power industry control protocol can be found in depth. The problem of ambiguous attack mechanism and lack of verification means of power industry control system is discussed, which can provide support for the research of attack and protection of power industry control system.",
      "year": 2019,
      "venue": "IOP Conference Series: Materials Science and Engineering",
      "authors": [
        "Xiaoqiang Wang",
        "Rui",
        "Zhao Xuehai Yu",
        "Jinye Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3ba1d07d015d843dbfa6b6b4a8e433b3ee2caa18",
      "pdf_url": "https://doi.org/10.1088/1757-899x/677/4/042068",
      "publication_date": "2019-12-10",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4718c6fb38ede2595f543bfbfdb1067eccac88de",
      "title": "DeepNIS: Deep Neural Network for Nonlinear Electromagnetic Inverse Scattering",
      "abstract": "Nonlinear electromagnetic (EM) inverse scattering is a quantitative and super-resolution imaging technique, in which more realistic interactions between the internal structure of scene and EM wavefield are taken into account in the imaging procedure, in contrast to conventional tomography. However, it poses important challenges arising from its intrinsic strong nonlinearity, ill-posedness, and expensive computational costs. To tackle these difficulties, we, for the first time to our best knowledge, exploit a connection between the deep neural network (DNN) architecture and the iterative method of nonlinear EM inverse scattering. This enables the development of a novel DNN-based methodology for nonlinear EM inverse problems (termed here DeepNIS). The proposed DeepNIS consists of a cascade of multilayer complex-valued residual convolutional neural network modules. We numerically and experimentally demonstrate that the DeepNIS outperforms remarkably conventional nonlinear inverse scattering methods in terms of both the image quality and computational time. We show that DeepNIS can learn a general model approximating the underlying EM inverse scattering system. It is expected that the DeepNIS will serve as powerful tool in treating highly nonlinear EM inverse scattering problems over different frequency bands, which are extremely hard and impractical to solve using conventional inverse scattering methods.",
      "year": 2018,
      "venue": "IEEE Transactions on Antennas and Propagation",
      "authors": [
        "Lianlin Li",
        "Longgang Wang",
        "F. Teixeira",
        "Che Liu",
        "A. Nehorai",
        "T. Cui"
      ],
      "citation_count": 372,
      "url": "https://www.semanticscholar.org/paper/4718c6fb38ede2595f543bfbfdb1067eccac88de",
      "pdf_url": "https://arxiv.org/pdf/1810.03990",
      "publication_date": "2018-10-04",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "6c0aef543a15edc1a52fe3490281e3fe36dac486",
      "title": "Reverse Engineering Convolutional Neural Networks Through Side-channel Information Leaks",
      "abstract": "A convolutional neural network (CNN) model represents a crucial piece of intellectual property in many applications. Revealing its structure or weights would leak confidential information. In this paper we present novel reverse-engineering attacks on CNNs running on a hardware accelerator, where an adversary can feed inputs to the accelerator and observe the resulting off-chip memory accesses. Our study shows that even with data encryption, the adversary can infer the underlying network structure by exploiting the memory and timing side-channels. We further identify the information leakage on the values of weights when a CNN accelerator performs dynamic zero pruning for off-chip memory accesses. Overall, this work reveals the importance of hiding off-chip memory access pattern to truly protect confidential CNN models.",
      "year": 2018,
      "venue": "Design Automation Conference",
      "authors": [
        "Weizhe Hua",
        "Zhiru Zhang",
        "G. Suh"
      ],
      "citation_count": 274,
      "url": "https://www.semanticscholar.org/paper/6c0aef543a15edc1a52fe3490281e3fe36dac486",
      "pdf_url": "",
      "publication_date": "2018-06-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "669a04d8bbf8d530a8d6e1900f8b63dd906b4050",
      "title": "I Know What You See: Power Side-Channel Attack on Convolutional Neural Network Accelerators",
      "abstract": "Deep learning has become the de-facto computational paradigm for various kinds of perception problems, including many privacy-sensitive applications such as online medical image analysis. No doubt to say, the data privacy of these deep learning systems is a serious concern. Different from previous research focusing on exploiting privacy leakage from deep learning models, in this paper, we present the first attack on the implementation of deep learning models. To be specific, we perform the attack on an FPGA-based convolutional neural network accelerator and we manage to recover the input image from the collected power traces without knowing the detailed parameters in the neural network. For the MNIST dataset, our power side-channel attack is able to achieve up to 89% recognition accuracy.",
      "year": 2018,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Lingxiao Wei",
        "Yannan Liu",
        "Bo Luo",
        "Yu LI",
        "Qiang Xu"
      ],
      "citation_count": 220,
      "url": "https://www.semanticscholar.org/paper/669a04d8bbf8d530a8d6e1900f8b63dd906b4050",
      "pdf_url": "https://arxiv.org/pdf/1803.05847",
      "publication_date": "2018-03-05",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f5014e34ed13191082cd20cc279ca4cc9adee84f",
      "title": "Stealing Neural Networks via Timing Side Channels",
      "abstract": "Deep learning is gaining importance in many applications. However, Neural Networks face several security and privacy threats. This is particularly significant in the scenario where Cloud infrastructures deploy a service with Neural Network model at the back end. Here, an adversary can extract the Neural Network parameters, infer the regularization hyperparameter, identify if a data point was part of the training data, and generate effective transferable adversarial examples to evade classifiers. This paper shows how a Neural Network model is susceptible to timing side channel attack. In this paper, a black box Neural Network extraction attack is proposed by exploiting the timing side channels to infer the depth of the network. Although, constructing an equivalent architecture is a complex search problem, it is shown how Reinforcement Learning with knowledge distillation can effectively reduce the search space to infer a target model. The proposed approach has been tested with VGG architectures on CIFAR10 data set. It is observed that it is possible to reconstruct substitute models with test accuracy close to the target models and the proposed approach is scalable and independent of type of Neural Network architectures.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Vasisht Duddu",
        "D. Samanta",
        "D. V. Rao",
        "V. Balas"
      ],
      "citation_count": 146,
      "url": "https://www.semanticscholar.org/paper/f5014e34ed13191082cd20cc279ca4cc9adee84f",
      "pdf_url": "",
      "publication_date": "2018-12-31",
      "keywords_matched": [
        "neural network extraction attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "dcbd26094751f5064a6d9b8c7e0a347d49e0d3a5",
      "title": "Security Analysis of Deep Neural Networks Operating in the Presence of Cache Side-Channel Attacks",
      "abstract": "Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary's capability to conduct black-box attacks against the model. This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels. First, we define the threat model for these attacks: our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim's deep learning (DL) system is running and passively monitors the accesses of the target functions in the shared framework. Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique. Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victim's entire network architecture. In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having observed only one forward propagation. Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting. From this meta-model, we evaluate the importance of the observed attributes in the fingerprinting process. Third, we propose and evaluate new framework-level defense techniques that obfuscate our attacker's observations. Our empirical security analysis represents a step toward understanding the DNNs' vulnerability to cache side-channel attacks.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Sanghyun Hong",
        "Michael Davinroy",
        "Yigitcan Kaya",
        "S. Locke",
        "Ian Rackow",
        "Kevin Kulda",
        "Dana Dachman-Soled",
        "Tudor Dumitras"
      ],
      "citation_count": 94,
      "url": "https://www.semanticscholar.org/paper/dcbd26094751f5064a6d9b8c7e0a347d49e0d3a5",
      "pdf_url": "",
      "publication_date": "2018-09-27",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "046563f8a0ea014064268eb95536bb521aa5f9cd",
      "title": "Chaotic dynamics in a neural network under electromagnetic radiation",
      "abstract": null,
      "year": 2018,
      "venue": "",
      "authors": [
        "Xiaoyu Hu",
        "Chongxin Liu",
        "Ling Liu",
        "Junkang Ni",
        "Yapeng Yao"
      ],
      "citation_count": 79,
      "url": "https://www.semanticscholar.org/paper/046563f8a0ea014064268eb95536bb521aa5f9cd",
      "pdf_url": "",
      "publication_date": "2018-02-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1123310a9b95b66dd089e7fc05bcf68fc1d0567a",
      "title": "Deep neural network for pixel-level electromagnetic particle identification in the MicroBooNE liquid argon time projection chamber",
      "abstract": "We have developed a convolutional neural network that can make a pixel-level prediction of objects in image data recorded by a liquid argon time projection chamber (LArTPC) for the first time. We describe the network design, training techniques, and software tools developed to train this network. The goal of this work is to develop a complete deep neural network based data reconstruction chain for the MicroBooNE detector. We show the first demonstration of a network's validity on real LArTPC data using MicroBooNE collection plane images. The demonstration is performed for stopping muon and a nu(mu) charged-current neutral pion data samples.",
      "year": 2018,
      "venue": "Physical Review D",
      "authors": [
        "MicroBooNE collaboration C. Adams",
        "M. Alrashed",
        "Rui An",
        "J. Anthony",
        "J. Asaadi",
        "A. Ashkenazi",
        "M. Auger",
        "S. Balasubramanian",
        "B. Baller",
        "C. Barnes",
        "G. Barr",
        "M. Bass",
        "F. Bay",
        "A. Bhat",
        "K. Bhattacharya",
        "M. Bishai",
        "A. Blake",
        "T. Bolton",
        "L. Camilleri",
        "D. Caratelli",
        "I. Terrazas",
        "R. Carr",
        "R. C. Fern\u00e1ndez",
        "F. Cavanna",
        "G. Cerati",
        "Y. Chen",
        "E. Church",
        "D. Cianci",
        "E. Cohen",
        "G. Collin",
        "J. Conrad",
        "M. Convery",
        "L. Cooper-Troendle",
        "J. I. Crespo-Anad\u00f3n",
        "M. Tutto",
        "D. Devitt",
        "A. Diaz",
        "K. Duffy",
        "S. Dytman",
        "B. Eberly",
        "A. Ereditato",
        "L. E. Sanchez",
        "J. Esquivel",
        "J. Evans",
        "A. Fadeeva",
        "R. Fitzpatrick",
        "B. Fleming",
        "D. Franco",
        "A. Furmanski",
        "D. Garcia-Gamez",
        "G. Garvey",
        "V. Genty",
        "D. Goeldi",
        "S. Gollapinni",
        "O. Goodwin",
        "E. Gramellini",
        "H. Greenlee",
        "R. Grosso",
        "R. Guenette",
        "P. Guzowski",
        "A. Hackenburg",
        "P. Hamilton",
        "O. Hen",
        "J. Hewes",
        "C. Hill",
        "G. Horton-Smith",
        "A. Hourlier",
        "E. Huang",
        "C. James",
        "J. J. D. Vries",
        "L. Jiang",
        "R. Johnson",
        "Janhavi Joshi",
        "H. Jostlein",
        "Y. Jwa",
        "G. Karagiorgi",
        "W. Ketchum",
        "B. Kirby",
        "M. Kirby",
        "T. Kobilarcik",
        "I. Kreslo",
        "Y. Li",
        "A. Lister",
        "B. Littlejohn",
        "S. Lockwitz",
        "D. Lorca",
        "W. Louis",
        "M. Luethi",
        "B. Lundberg",
        "X. Luo",
        "A. Marchionni",
        "S. Marcocci",
        "C. Mariani",
        "J. Marshall",
        "J. Mart\u00edn-Albo",
        "D. A. Caicedo",
        "A. Mastbaum",
        "V. Meddage",
        "T. Mettler",
        "G. Mills",
        "K. Mistry",
        "A. Mogan",
        "J. Moon",
        "M. Mooney",
        "C. Moore",
        "J. Mousseau",
        "M. Murphy",
        "R. Murrells",
        "D. Naples",
        "P. Nienaber",
        "J. Nowak",
        "O. Palamara",
        "V. Pandey",
        "V. Paolone",
        "A. Papadopoulou",
        "V. Papavassiliou",
        "S. Pate",
        "Z. Pavlovic",
        "E. Piasetzky",
        "D. Porzio",
        "G. Pulliam",
        "X. Qian",
        "J. Raaf",
        "A. Rafique",
        "L. Rochester",
        "M. Ross-Lonergan",
        "C. R. V. Rohr",
        "B. Russell",
        "D. Schmitz",
        "A. Schukraft",
        "W. Seligman",
        "M. Shaevitz",
        "R. Sharankova",
        "J. Sinclair",
        "A. Smith",
        "E. Snider",
        "M. Soderberg",
        "S. Soldner-Rembold",
        "S. Soleti",
        "P. Spentzouris",
        "J. Spitz",
        "J. John",
        "T. Strauss",
        "K. Sutton",
        "S. Sword-Fehlberg",
        "A. Szelc",
        "N. Tagg",
        "W. Tang",
        "K. Terao",
        "M. Thomson",
        "R. Thornton",
        "M. Toups",
        "Y. Tsai",
        "S. Tufanli",
        "T. Usher",
        "W. V. D. Pontseele",
        "R. G. Water",
        "B. Viren",
        "M. Weber",
        "H. Wei",
        "D. A. Wickremasinghe",
        "K. Wierman",
        "Z. Williams",
        "S. Wolbers",
        "T. Wongjirad",
        "K. Woodruff",
        "T. Yang",
        "G. Yarbrough",
        "L. Yates",
        "G. Zeller",
        "J. Zennamo",
        "C. Zhang"
      ],
      "citation_count": 69,
      "url": "https://www.semanticscholar.org/paper/1123310a9b95b66dd089e7fc05bcf68fc1d0567a",
      "pdf_url": "http://link.aps.org/pdf/10.1103/PhysRevD.99.092001",
      "publication_date": "2018-08-22",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "905ad646e5745afe6a3b02617cd8452655232c0d",
      "title": "CSI Neural Network: Using Side-channels to Recover Your Artificial Neural Network Information",
      "abstract": "Machine learning has become mainstream across industries. Numerous examples proved the validity of it for security applications. In this work, we investigate how to reverse engineer a neural network by using only power side-channel information. To this end, we consider a multilayer perceptron as the machine learning architecture of choice and assume a non-invasive and eavesdropping attacker capable of measuring only passive side-channel leakages like power consumption, electromagnetic radiation, and reaction time. \nWe conduct all experiments on real data and common neural net architectures in order to properly assess the applicability and extendability of those attacks. Practical results are shown on an ARM CORTEX-M3 microcontroller. Our experiments show that the side-channel attacker is capable of obtaining the following information: the activation functions used in the architecture, the number of layers and neurons in the layers, the number of output classes, and weights in the neural network. Thus, the attacker can effectively reverse engineer the network using side-channel information. \nNext, we show that once the attacker has the knowledge about the neural network architecture, he/she could also recover the inputs to the network with only a single-shot measurement. Finally, we discuss several mitigations one could use to thwart such attacks.",
      "year": 2018,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "L. Batina",
        "S. Bhasin",
        "Dirmanto Jap",
        "S. Picek"
      ],
      "citation_count": 67,
      "url": "https://www.semanticscholar.org/paper/905ad646e5745afe6a3b02617cd8452655232c0d",
      "pdf_url": "",
      "publication_date": "2018-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c5edd98d2a94dd02cdf92d52343140321e51b068",
      "title": "Have You Stolen My Model? Evasion Attacks Against Deep Neural Network Watermarking Techniques",
      "abstract": "Deep neural networks have had enormous impact on various domains of computer science, considerably outperforming previous state of the art machine learning techniques. To achieve this performance, neural networks need large quantities of data and huge computational resources, which heavily increases their construction costs. The increased cost of building a good deep neural network model gives rise to a need for protecting this investment from potential copyright infringements. Legitimate owners of a machine learning model want to be able to reliably track and detect a malicious adversary that tries to steal the intellectual property related to the model. Recently, this problem was tackled by introducing in deep neural networks the concept of watermarking, which allows a legitimate owner to embed some secret information(watermark) in a given model. The watermark allows the legitimate owner to detect copyright infringements of his model. This paper focuses on verifying the robustness and reliability of state-of- the-art deep neural network watermarking schemes. We show that, a malicious adversary, even in scenarios where the watermark is difficult to remove, can still evade the verification by the legitimate owners, thus avoiding the detection of model theft.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Dorjan Hitaj",
        "L. Mancini"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/c5edd98d2a94dd02cdf92d52343140321e51b068",
      "pdf_url": "",
      "publication_date": "2018-09-03",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "42155d5c8c6e46ef26a0d7212123128a364b07a0",
      "title": "Defending Against Model Stealing Attacks Using Deceptive Perturbations",
      "abstract": null,
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Taesung Lee",
        "Ben Edwards",
        "Ian Molloy",
        "D. Su"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/42155d5c8c6e46ef26a0d7212123128a364b07a0",
      "pdf_url": "",
      "publication_date": "2018-05-31",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "dc32688d395c57cff2dbecf65b9c5b6a52a5c10c",
      "title": "Convolutional Neural Network Based Side-Channel Attacks in Time-Frequency Representations",
      "abstract": null,
      "year": 2018,
      "venue": "Smart Card Research and Advanced Application Conference",
      "authors": [
        "Guang Yang",
        "Huizhong Li",
        "Jingdian Ming",
        "Yongbin Zhou"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/dc32688d395c57cff2dbecf65b9c5b6a52a5c10c",
      "pdf_url": "",
      "publication_date": "2018-11-12",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2cb2f7c824a089422ac210b3f77d2e078b017f66",
      "title": "Model Extraction and Active Learning",
      "abstract": null,
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Varun Chandrasekaran",
        "Kamalika Chaudhuri",
        "Irene Giacomelli",
        "S. Jha",
        "Songbai Yan"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/2cb2f7c824a089422ac210b3f77d2e078b017f66",
      "pdf_url": "",
      "publication_date": "2018-11-05",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7e7157625088e62582f45409cb1704844ddb5535",
      "title": "Poisoning Machine Learning Based Wireless IDSs via Stealing Learning Model",
      "abstract": null,
      "year": 2018,
      "venue": "Wireless Algorithms, Systems, and Applications",
      "authors": [
        "Pan Li",
        "Wentao Zhao",
        "Qiang Liu",
        "Xiao Liu",
        "Linyuan Yu"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/7e7157625088e62582f45409cb1704844ddb5535",
      "pdf_url": "",
      "publication_date": "2018-06-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "2822dce260dc4930ccd276a9ae6392960d6b460d",
      "title": "Combating Tag Cloning with COTS RFID Devices",
      "abstract": "In RFID systems, a cloning attack is to fabricate one or more replicas of a genuine tag, so that these replicas behave exactly the same as the genuine tag and fool the reader for getting legal authorization, leading to potential financial loss or reputation damage for the corporations. These replicas are called clone tags. Although many advanced solutions have been proposed to combat cloning attack, they need to either modify the MAC- layer protocols or increase extra hardware resources, which cannot be deployed on commercial off-the-shelf (COTS) RFID devices for practical use. In this paper, we take a fresh attempt to counterattack tag cloning based on COTS RFID devices and the universal C1G2 standard, without any software redesign or hardware augment needed. The basic idea is to use the RF signal profile to characterize each tag. Since these physical-layer data are measured by the reader and susceptible to various environmental factors, they are hard to be estimated by the attackers; let alone be cloned. Even so, we assert that it is challenging to identify clone tags as the signal data from a genuine tag and its clones are all mixed together. Besides, the tag moving has a great impact on the measured RF signals. To overcome these challenges, we propose a clustering-based scheme that detects the cloning attack in the still scene and a chain- based scheme for clone detection in the dynamic scene, respectively. Extensive experiments on COTS RFID devices demonstrate that the detection accuracy of our approaches reaches 99.8% in a still case and 99.3% in a dynamic scene.",
      "year": 2018,
      "venue": "Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks",
      "authors": [
        "Xingyu Chen",
        "Jia Liu",
        "Xia Wang",
        "Xiaocong Zhang",
        "Yanyan Wang",
        "Lijun Chen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/2822dce260dc4930ccd276a9ae6392960d6b460d",
      "pdf_url": "",
      "publication_date": "2018-06-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "65d21b754a788782f05e43d6786e342ba8284cff",
      "title": "Neutralizing BLE Beacon-Based Electronic Attendance System Using Signal Imitation Attack",
      "abstract": "Many emerging location- or proximity-based applications use Bluetooth low energy (BLE) beacons thanks to the increasing popularity of the technology in mobile systems. An outstanding example is the BLE beacon-based electronic attendance system (BEAS) used in many universities today to increase the efficiency of lectures. Despite its popularity and usefulness, however, BEAS has not been thoroughly analyzed for its potential vulnerabilities. In this paper, we neutralize a university\u2019s BEAS by maliciously cheating attendance (i.e., faking attendance while the subject is not physically present at the location) in various scenarios using signal imitation attack, and investigate its possible vulnerabilities. The BEAS exploited in this paper is a commercial system actually used in a well-known university. After the exploitation experiment, we analyze the system\u2019s weaknesses and present possible counter-measures. Furthermore, additional attack methods are shown to re-counteract those possible counter-measures and to discuss the fundamental challenges, deficiencies, and suggestions in electronic attendance systems using BLE beacons.",
      "year": 2018,
      "venue": "IEEE Access",
      "authors": [
        "Moonbeom Kim",
        "Jongho Lee",
        "Jeongyeup Paek"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/65d21b754a788782f05e43d6786e342ba8284cff",
      "pdf_url": "https://doi.org/10.1109/access.2018.2884488",
      "publication_date": "2018-12-03",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "08c1a578af25fbca2f5bf86d8a9752503b5805e7",
      "title": "DNN model extraction attacks using prediction interfaces",
      "abstract": null,
      "year": 2018,
      "venue": "",
      "authors": [
        "A. Dmitrenko"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/08c1a578af25fbca2f5bf86d8a9752503b5805e7",
      "pdf_url": "",
      "publication_date": "2018-08-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "93ee45cc18b168f4e51481f9b95516d9556e8cef",
      "title": "Prediction of the Force on a Projectile in an Electromagnetic Launcher Coil with Multilayer Neural Network",
      "abstract": "The force on the projectile in the electromagnetic launchers varies according to the the excitation value and the position of the projectile in the winding. In this study, 3D model of coil and projectile used in electromagnetic launchers have been created and analyzed by finite element method. The force characteristic on the projectile has been obtained by changing the excitation value of the winding and the position of the projectile using parametric solution method. In finite element analysis, more accurate analysis can be performed by defining smaller solution steps. However, the analysis time is prolonged due to the increase in the number of variables. Taking into consideration the duration of analysis, the force prediction has been carried out using multilayer neural network models consisting of one hidden layer and two hidden layers. Successful results have been obtained in the force prediction studies with multilayer neural networks.",
      "year": 2018,
      "venue": "Sakarya University Journal of Computer and Information Sciences",
      "authors": [
        "A. Dalcal\u0131",
        "Onursal \u00c7etin",
        "C. Ocak",
        "Feyzullah Temurta\u015f"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/93ee45cc18b168f4e51481f9b95516d9556e8cef",
      "pdf_url": "http://saucis.sakarya.edu.tr/en/download/article-file/600139",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "52551687c11c0e1668ad50bced2bb7a18204aefb",
      "title": "Analysis of IoT devices via API Exploitation and Model Extraction",
      "abstract": null,
      "year": 2018,
      "venue": "",
      "authors": [
        "Alexandre Eraclides",
        "Alexandre gpehotmail.com"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/52551687c11c0e1668ad50bced2bb7a18204aefb",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "68564f11e79c19195d0e82854a0aa156d7764922",
      "title": "Interpreting Blackbox Models via Model Extraction",
      "abstract": "Interpretability has become incredibly important as machine learning is increasingly used to inform consequential decisions. We propose to construct global explanations of complex, blackbox models in the form of a decision tree approximating the original model---as long as the decision tree is a good approximation, then it mirrors the computation performed by the blackbox model. We devise a novel algorithm for extracting decision tree explanations that actively samples new training points to avoid overfitting. We evaluate our algorithm on a random forest to predict diabetes risk and a learned controller for cart-pole. Compared to several baselines, our decision trees are both substantially more accurate and equally or more interpretable based on a user study. Finally, we describe several insights provided by our interpretations, including a causal issue validated by a physician.",
      "year": 2017,
      "venue": "arXiv.org",
      "authors": [
        "Osbert Bastani",
        "Carolyn Kim",
        "Hamsa Bastani"
      ],
      "citation_count": 182,
      "url": "https://www.semanticscholar.org/paper/68564f11e79c19195d0e82854a0aa156d7764922",
      "pdf_url": "",
      "publication_date": "2017-05-23",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "model stealing mentioned only in passing",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a44698dc615e9020513fa468a5a1ffd5dfdeed8d",
      "title": "Effects of ion channel blocks on electrical activity of stochastic Hodgkin-Huxley neural network under electromagnetic induction",
      "abstract": null,
      "year": 2017,
      "venue": "Neurocomputing",
      "authors": [
        "Ying Xu",
        "Y. Jia",
        "Mengyan Ge",
        "Lulu Lu",
        "Lijian Yang",
        "Xuan Zhan"
      ],
      "citation_count": 125,
      "url": "https://www.semanticscholar.org/paper/a44698dc615e9020513fa468a5a1ffd5dfdeed8d",
      "pdf_url": "",
      "publication_date": "2017-12-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0ce3a23e7895b42bd0d11c19aa882ec85ab77c21",
      "title": "Electromagnetic Compatibility Estimator Using Scaled Conjugate Gradient Backpropagation Based Artificial Neural Network",
      "abstract": null,
      "year": 2017,
      "venue": "IEEE Transactions on Industrial Informatics",
      "authors": [
        "Chetan B. Khadse",
        "M. Chaudhari",
        "V. Borghate"
      ],
      "citation_count": 43,
      "url": "https://www.semanticscholar.org/paper/0ce3a23e7895b42bd0d11c19aa882ec85ab77c21",
      "pdf_url": "",
      "publication_date": "2017-05-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "090b9f33bf878f7630e7715c1cd2897df6d89fbd",
      "title": "Symbolic Model Extraction for Web Application Verification",
      "abstract": null,
      "year": 2017,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Ivan Bocic",
        "T. Bultan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/090b9f33bf878f7630e7715c1cd2897df6d89fbd",
      "pdf_url": "",
      "publication_date": "2017-05-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "8d1192d8f79827f00fa3efe392e5b4a165f5ef8d",
      "title": "Assessing the Performance of Automated Model Extraction Rules",
      "abstract": null,
      "year": 2017,
      "venue": "Integrated Spatial Databases",
      "authors": [
        "Jorge Echeverr\u00eda",
        "Francisca P\u00e9rez",
        "\u00d3. Pastor",
        "Carlos Cetina"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/8d1192d8f79827f00fa3efe392e5b4a165f5ef8d",
      "pdf_url": "https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1150&context=isd2014",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "963098553570745043ba198ffb12996f1a17ab95",
      "title": "A neural network identifier for electromagnetic thermotherapy systems",
      "abstract": null,
      "year": 2017,
      "venue": "",
      "authors": [
        "C. Tai",
        "Wei-Cheng Wang",
        "Yuan Jui Hsu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/963098553570745043ba198ffb12996f1a17ab95",
      "pdf_url": "",
      "publication_date": "2017-03-14",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4bcd67d7dd2f5fd18986c8c28f254e9a36af6fe7",
      "title": "A Reference Architecture for Online Performance Model Extraction in Virtualized Environments",
      "abstract": null,
      "year": 2016,
      "venue": "ICPE Companion",
      "authors": [
        "Simon Spinner",
        "J. Walter",
        "Samuel Kounev"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/4bcd67d7dd2f5fd18986c8c28f254e9a36af6fe7",
      "pdf_url": "",
      "publication_date": "2016-03-12",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "c6b9bd9b8783279c78e8d94fcb97f3ca4dd4b436",
      "title": "Side-Channel Information Characterisation Based on Cascade-Forward Back-Propagation Neural Network",
      "abstract": null,
      "year": 2016,
      "venue": "Journal of electronic testing",
      "authors": [
        "Ehsan Saeedi",
        "Md. Selim Hossain",
        "Yinan Kong"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/c6b9bd9b8783279c78e8d94fcb97f3ca4dd4b436",
      "pdf_url": "",
      "publication_date": "2016-05-14",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "20a199be55d274fc986f32f043ae606d5ce09767",
      "title": "A model of food stealing with asymmetric information",
      "abstract": null,
      "year": 2016,
      "venue": "",
      "authors": [
        "M. Broom",
        "J. Rycht\u00e1\u0159"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/20a199be55d274fc986f32f043ae606d5ce09767",
      "pdf_url": "https://openaccess.city.ac.uk/id/eprint/12831/7/CC%20BY-NC-ND%204.0.pdf",
      "publication_date": "2016-06-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3547c3fefa6bfdf07b71893a7d2894189e397529",
      "title": "High Dimensional Electromagnetic Interference Signal Clustering Based On SOM Neural Network",
      "abstract": "In this paper, we study the spectral characteristics and global representations of strongly nonlinear, non-stationary electromagnetic interferences (EMI), which is of great significance in analysing the mathematical modelling of electromagnetic capability (EMC) for a large scale integrated system. We firstly propose to use Self-Organizing Feature Map Neural Network (SOM) to cluster EMI signals. To tackle with the high dimensionality of EMI signals, we combine the dimension reduction and clustering approaches, and find out the global features of different interference factors, in order to finally provide precise mathematical simulation models for EMC design, analysis, forecasting and evaluation. Experimental results have demonstrated the validity and effectiveness of the proposed method.",
      "year": 2016,
      "venue": "",
      "authors": [
        "Hongyi Li",
        "Di Zhao",
        "S. Xu",
        "Pidong Wang",
        "Jiaxin Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3547c3fefa6bfdf07b71893a7d2894189e397529",
      "pdf_url": "https://doi.org/10.7251/els1620027l",
      "publication_date": "2016-07-15",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "41b409788d637d25b28fced1411ecc80a5b46b29",
      "title": "Nitrate and Sulfate Estimations in Water Sources Using a Planar Electromagnetic Sensor Array and Artificial Neural Network Method",
      "abstract": null,
      "year": 2015,
      "venue": "IEEE Sensors Journal",
      "authors": [
        "A. N. M. Nor",
        "M. Faramarzi",
        "M. Yunus",
        "S. Ibrahim"
      ],
      "citation_count": 49,
      "url": "https://www.semanticscholar.org/paper/41b409788d637d25b28fced1411ecc80a5b46b29",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "073e060587a32e9eac3c37105dd1454ecf5a0d80",
      "title": "Deterministic Detection of Cloning Attacks for Anonymous RFID Systems",
      "abstract": null,
      "year": 2015,
      "venue": "IEEE Transactions on Industrial Informatics",
      "authors": [
        "Kai Bu",
        "Mingjie Xu",
        "Xuan Liu",
        "Jiaqing Luo",
        "Shigeng Zhang",
        "Minyu Weng"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/073e060587a32e9eac3c37105dd1454ecf5a0d80",
      "pdf_url": "",
      "publication_date": "2015-09-29",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "62835b529bce481e0e51128463c4176a087c9cdc",
      "title": "A new nonlinear model extraction methodology for GaN HEMTs subject to trapping effects",
      "abstract": null,
      "year": 2015,
      "venue": "2015 IEEE MTT-S International Microwave Symposium",
      "authors": [
        "L. Nunes",
        "J. M. Gomes",
        "P. Cabral",
        "J. Pedro"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/62835b529bce481e0e51128463c4176a087c9cdc",
      "pdf_url": "",
      "publication_date": "2015-05-17",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "58341bdbc16d5b0f4624e1109c670b74a1268421",
      "title": "FinFET Centric Variability-Aware Compact Model Extraction and Generation Technology Supporting DTCO",
      "abstract": null,
      "year": 2015,
      "venue": "IEEE Transactions on Electron Devices",
      "authors": [
        "Xingsheng Wang",
        "B. Cheng",
        "D. Reid",
        "Andrew Pender",
        "P. Asenov",
        "C. Millar",
        "A. Asenov"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/58341bdbc16d5b0f4624e1109c670b74a1268421",
      "pdf_url": "https://eprints.gla.ac.uk/108434/1/108434.pdf",
      "publication_date": "2015-08-14",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "532d52b130c67736e1ea275196644a1fc02c3e6e",
      "title": "The Mathematical Model and the Problem of Optimal Partitioning of Shared Memory for Work-Stealing Deques",
      "abstract": null,
      "year": 2015,
      "venue": "International Conference on Parallel Architectures and Compilation Techniques",
      "authors": [
        "A. Sokolov",
        "E. Barkovsky"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/532d52b130c67736e1ea275196644a1fc02c3e6e",
      "pdf_url": "",
      "publication_date": "2015-08-31",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "342d508244501218f9be07ee1ab67b7eb3d34086",
      "title": "Detecting Cloning Attack in Low-Cost Passive RFID Tags An Analytic Comparison between KILL Passwords and Synchronized Secrets Obinna",
      "abstract": null,
      "year": 2015,
      "venue": "",
      "authors": [
        "Stanley Okpara"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/342d508244501218f9be07ee1ab67b7eb3d34086",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "be73148fe884f5e824cffb5046781524eb6a602e",
      "title": "Room model extraction device, room model extraction system, room model extraction program, and room model extraction method",
      "abstract": null,
      "year": 2015,
      "venue": "",
      "authors": [
        "\u82f1\u4e4b \u7802\u7530"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/be73148fe884f5e824cffb5046781524eb6a602e",
      "pdf_url": "",
      "publication_date": "2015-03-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "605e8d2f41607c531a632ed928bc3997ea016567",
      "title": "Generalized framework for context-specific metabolic model extraction methods",
      "abstract": "Genome-scale metabolic models (GEMs) are increasingly applied to investigate the physiology not only of simple prokaryotes, but also eukaryotes, such as plants, characterized with compartmentalized cells of multiple types. While genome-scale models aim at including the entirety of known metabolic reactions, mounting evidence has indicated that only a subset of these reactions is active in a given context, including: developmental stage, cell type, or environment. As a result, several methods have been proposed to reconstruct context-specific models from existing genome-scale models by integrating various types of high-throughput data. Here we present a mathematical framework that puts all existing methods under one umbrella and provides the means to better understand their functioning, highlight similarities and differences, and to help users in selecting a most suitable method for an application.",
      "year": 2014,
      "venue": "Frontiers in Plant Science",
      "authors": [
        "Semid\u00e1n Robaina Est\u00e9vez",
        "Z. Nikoloski"
      ],
      "citation_count": 82,
      "url": "https://www.semanticscholar.org/paper/605e8d2f41607c531a632ed928bc3997ea016567",
      "pdf_url": "https://www.frontiersin.org/articles/10.3389/fpls.2014.00491/pdf",
      "publication_date": "2014-09-19",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "medium",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "666f3571b6313d2aa0f26d1f21a37a750e2ad8ad",
      "title": "Detecting cloning attack in Social Networks using classification and clustering techniques",
      "abstract": null,
      "year": 2014,
      "venue": "International Conference on Recent Trends in Information Technology",
      "authors": [
        "S. Kiruthiga",
        "P. Kola Sujatha",
        "A. Kannan"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/666f3571b6313d2aa0f26d1f21a37a750e2ad8ad",
      "pdf_url": "",
      "publication_date": "2014-04-10",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "25d378877b59eb00f838f277fb4cb4529c9b0153",
      "title": "Coupled electromagnetic/thermal machine design optimization based on finite element analysis with application of artificial neural network",
      "abstract": null,
      "year": 2014,
      "venue": "European Conference on Cognitive Ergonomics",
      "authors": [
        "Wenying Jiang",
        "T. Jahns"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/25d378877b59eb00f838f277fb4cb4529c9b0153",
      "pdf_url": "",
      "publication_date": "2014-11-13",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ba2b1eaddff998d95a5a3f6657b182ab58cf5526",
      "title": "New Model Introductions, Cannibalization and Market Stealing: Evidence from Shopbot Data",
      "abstract": null,
      "year": 2014,
      "venue": "",
      "authors": [
        "M. Haynes",
        "S. Thompson",
        "P. Wright"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/ba2b1eaddff998d95a5a3f6657b182ab58cf5526",
      "pdf_url": "http://eprints.whiterose.ac.uk/95237/1/PW1.pdf",
      "publication_date": "2014-07-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e565f219ec8642aef6bf5ed2ae5d22806de67e57",
      "title": "A Novel Method of Inconsistent Collision Detection to Prevent Cloning Attacks in High-Security Wireless Body Area Networks",
      "abstract": null,
      "year": 2014,
      "venue": "",
      "authors": [
        "Govindharajan Uma",
        "Gowri",
        "Rajagopal Sivakumar"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e565f219ec8642aef6bf5ed2ae5d22806de67e57",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5db90d711803767e27a28f868508b4789b66affd",
      "title": "Evaluation of the Copycat Model for Predicting Complex Network Growth",
      "abstract": null,
      "year": 2014,
      "venue": "",
      "authors": [
        "Tiago A. Schieber",
        "L. Carpi",
        "M. Ravetti"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/5db90d711803767e27a28f868508b4789b66affd",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "copycat model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d84c0d4dbab81757490213742b1e75dc258bd3ba",
      "title": "Feature model extraction from large collections of informal product descriptions",
      "abstract": null,
      "year": 2013,
      "venue": "ESEC/FSE 2013",
      "authors": [
        "J. Davril",
        "Edouard Delfosse",
        "N. Hariri",
        "M. Acher",
        "J. Cleland-Huang",
        "P. Heymans"
      ],
      "citation_count": 160,
      "url": "https://www.semanticscholar.org/paper/d84c0d4dbab81757490213742b1e75dc258bd3ba",
      "pdf_url": "https://hal.inria.fr/hal-00859475/file/fse13main-id221-p-18686-final.pdf",
      "publication_date": "2013-08-18",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e9c3fad3993b9942cc0523e6d064fe3d420ff07e",
      "title": "Ontology-Based Partial Building Information Model Extraction",
      "abstract": null,
      "year": 2013,
      "venue": "Journal of computing in civil engineering",
      "authors": [
        "Le Zhang",
        "R. Issa"
      ],
      "citation_count": 97,
      "url": "https://www.semanticscholar.org/paper/e9c3fad3993b9942cc0523e6d064fe3d420ff07e",
      "pdf_url": "",
      "publication_date": "2013-11-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9817db5e4ebdb347bdf7efae8046b207da796aef",
      "title": "Power analysis of large-scale, real-time neural networks on SpiNNaker",
      "abstract": null,
      "year": 2013,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Evangelos Stromatias",
        "F. Galluppi",
        "Cameron Patterson",
        "S. Furber"
      ],
      "citation_count": 94,
      "url": "https://www.semanticscholar.org/paper/9817db5e4ebdb347bdf7efae8046b207da796aef",
      "pdf_url": "",
      "publication_date": "2013-08-04",
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "79f5585b7a39ab24d354a2eed03b0e1453a49bc1",
      "title": "Optimization of Power Analysis Using Neural Network",
      "abstract": null,
      "year": 2013,
      "venue": "Smart Card Research and Advanced Application Conference",
      "authors": [
        "Zdenek Martinasek",
        "J. Hajny",
        "L. Malina"
      ],
      "citation_count": 76,
      "url": "https://www.semanticscholar.org/paper/79f5585b7a39ab24d354a2eed03b0e1453a49bc1",
      "pdf_url": "",
      "publication_date": "2013-11-27",
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4be848dfc77b2535fdf5716ebb7635c5f99e87e9",
      "title": "Unreconciled Collisions Uncover Cloning Attacks in Anonymous RFID Systems",
      "abstract": null,
      "year": 2013,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Kai Bu",
        "Xuan Liu",
        "Jiaqing Luo",
        "Bin Xiao",
        "Guiyi Wei"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/4be848dfc77b2535fdf5716ebb7635c5f99e87e9",
      "pdf_url": "http://hdl.handle.net/10397/14328",
      "publication_date": "2013-03-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "1034f4a32a33501c67fad62440bde0e811887dbc",
      "title": "Enhancing and identifying cloning attacks in online social networks",
      "abstract": null,
      "year": 2013,
      "venue": "International Conference on Ubiquitous Information Management and Communication",
      "authors": [
        "Zifei Shan",
        "Haowen Cao",
        "Jason Lv",
        "Cong Yan",
        "Annie Liu"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/1034f4a32a33501c67fad62440bde0e811887dbc",
      "pdf_url": "",
      "publication_date": "2013-01-17",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5e9d4b1c4bc2a92564a77908d4d35b302fac5c4d",
      "title": "GSM OTA SIM Cloning Attack and Cloning Resistance in EAP-SIM and USIM",
      "abstract": null,
      "year": 2013,
      "venue": "IEEE International Conference on Social Computing",
      "authors": [
        "Jaspreet Singh",
        "Ron Ruhl",
        "Dale Lindskog"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/5e9d4b1c4bc2a92564a77908d4d35b302fac5c4d",
      "pdf_url": "",
      "publication_date": "2013-09-08",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "78fb962e1f3e7d0945ebe72193d3c15c2c1c0207",
      "title": "The limits of clone model standardization",
      "abstract": null,
      "year": 2013,
      "venue": "International Workshop on Software Clones",
      "authors": [
        "Jan Harder"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/78fb962e1f3e7d0945ebe72193d3c15c2c1c0207",
      "pdf_url": "",
      "publication_date": "2013-05-19",
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e22539cf9a6ed945aba33e4b8f9ee8553892c446",
      "title": "Game-theoretic analysis of node capture and cloning attack with multiple attackers in wireless sensor networks",
      "abstract": null,
      "year": 2012,
      "venue": "IEEE Conference on Decision and Control",
      "authors": [
        "Quanyan Zhu",
        "L. Bushnell",
        "T. Ba\u015far"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/e22539cf9a6ed945aba33e4b8f9ee8553892c446",
      "pdf_url": "",
      "publication_date": "2012-12-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "57cf37f2056fbd8364c80a83d64be927feb63a70",
      "title": "Geo-accurate model extraction from three-dimensional image-derived point clouds",
      "abstract": null,
      "year": 2012,
      "venue": "Defense + Commercial Sensing",
      "authors": [
        "David Nilosek",
        "Shaohui Sun",
        "C. Salvaggio"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/57cf37f2056fbd8364c80a83d64be927feb63a70",
      "pdf_url": "",
      "publication_date": "2012-05-08",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "6544e0772d9d2d9e23303179ceb4d3f30aba6ae0",
      "title": "TVMCM: A trusted VM clone model in cloud computing",
      "abstract": null,
      "year": 2012,
      "venue": "International Conference on New Trends in Information Science, Service Science and Data Mining",
      "authors": [
        "Wei-Ming Ma",
        "Xiaoyong Li",
        "Yong Shi",
        "Yu Guo"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/6544e0772d9d2d9e23303179ceb4d3f30aba6ae0",
      "pdf_url": "",
      "publication_date": "2012-10-01",
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "ac9769a7792ab7fda358af7fdd7464583132b840",
      "title": "Cloning Attack Authenticator in Wireless Sensor Networks",
      "abstract": null,
      "year": 2012,
      "venue": "",
      "authors": [
        "A. Vanathi"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/ac9769a7792ab7fda358af7fdd7464583132b840",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "a55a338cb359c1455bc979aee8ad637ca247e020",
      "title": "Analysis of metabolites by magnetic resonance spectroscopy on a subclavian steal model in atherosclerotic rabbits",
      "abstract": null,
      "year": 2012,
      "venue": "",
      "authors": [
        "Jiang Li-xian"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a55a338cb359c1455bc979aee8ad637ca247e020",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "e76752c9762fbd90cd4aefdbbbefa163d09ae118",
      "title": "Symmetries in Quantum Key Distribution and the Connection between Optimal Attacks and Optimal Cloning",
      "abstract": "We investigate the connection between the optimal collective eavesdropping attack and the optimal cloning attack where the eavesdropper employs an optimal cloner to attack the quantum key distribution (QKD) protocol. The analysis is done in the context of the security proof in Refs. [1, 2] for discrete variable protocols in d-dimensional Hilbert spaces. We consider a scenario in which the protocols and cloners are equipped with symmetries. These symmetries are used to dene a quantum cloning scenario. We nd that, in general, it does not hold that the optimal attack is an optimal cloner. However, there are classes of protocols, where we can identify an optimal attack by an optimal cloner. We analyze protocols with 2, d and d + 1 mutually unbiased bases where d is a prime, and show that for the protocols with 2 and d + 1 MUBs the optimal attack is an optimal cloner, but for the protocols with d MUBs, it is not 1 . Finally, we give criteria to identify protocols which have dierent signal states, but the same optimal attack. Using these criteria, we present qubit protocols which have the same optimal attack as the BB84 protocol or the 6-state protocol.",
      "year": 2011,
      "venue": "",
      "authors": [
        "A. Ferenczi",
        "N. Lutkenhaus"
      ],
      "citation_count": 74,
      "url": "https://www.semanticscholar.org/paper/e76752c9762fbd90cd4aefdbbbefa163d09ae118",
      "pdf_url": "http://arxiv.org/pdf/1112.3396",
      "publication_date": "2011-12-15",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "7ba2f13bbc21baa4353593b9e168292317c48efd",
      "title": "Presence of mononuclear cells in normal and affected laminae from the black walnut extract model of laminitis.",
      "abstract": null,
      "year": 2011,
      "venue": "Equine Veterinary Journal",
      "authors": [
        "R. R. Faleiros",
        "G. Nuovo",
        "A. Flechtner",
        "J. Belknap"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/7ba2f13bbc21baa4353593b9e168292317c48efd",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "extract model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "3102c84eebc9e4b3e9b7f34747c872c53c036866",
      "title": "A contract-extended push-pull-clone model",
      "abstract": null,
      "year": 2011,
      "venue": "International Conference on Collaborative Computing",
      "authors": [
        "H. Truong",
        "C. Ignat",
        "Pascal Molli"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3102c84eebc9e4b3e9b7f34747c872c53c036866",
      "pdf_url": "https://hal.inria.fr/hal-00761038/file/c-ppc.pdf",
      "publication_date": "2011-10-15",
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "32478a09717677f06958cdbbdc3ff1ef15b39ef5",
      "title": "A mean field model of work stealing in large-scale systems",
      "abstract": null,
      "year": 2010,
      "venue": "Measurement and Modeling of Computer Systems",
      "authors": [
        "Nicolas Gast",
        "B. Gaujal"
      ],
      "citation_count": 108,
      "url": "https://www.semanticscholar.org/paper/32478a09717677f06958cdbbdc3ff1ef15b39ef5",
      "pdf_url": "https://infoscience.epfl.ch/record/169412/files/workstealing_gast.pdf",
      "publication_date": "2010-06-12",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5cfe6101e204bd3669201b5ddbec6e8a1d333b03",
      "title": "Effect of intravenous lidocaine administration on laminar inflammation in the black walnut extract model of laminitis.",
      "abstract": null,
      "year": 2010,
      "venue": "Equine Veterinary Journal",
      "authors": [
        "Jarred M. Williams",
        "Y. J. Lin",
        "John P. Loftus",
        "R. R. Faleiros",
        "J. Peroni",
        "J. Hubbell",
        "W. Ravis",
        "J. Belknap"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/5cfe6101e204bd3669201b5ddbec6e8a1d333b03",
      "pdf_url": "",
      "publication_date": "2010-04-01",
      "keywords_matched": [
        "extract model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "0c2a550b30a97abe3766d514098bc1590183a0b3",
      "title": "The Plateau: Imitation Attack Resistance of Gait Biometrics",
      "abstract": null,
      "year": 2010,
      "venue": "IFIP Conference on Policies and Research in Identity Management",
      "authors": [
        "B. Mjaaland"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/0c2a550b30a97abe3766d514098bc1590183a0b3",
      "pdf_url": "",
      "publication_date": "2010-11-18",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "6f83064144b659cb9576fa04d1dc48c89f1b1e99",
      "title": "Quantum Cryptography with Several Cloning Attacks",
      "abstract": "Problem statement: In a previous research, we investigated the quantum key distribution of the well known BB84 protocol with several intercept and resend attacks. In the present research, we studied the effect of many eavesdroppers cloning at tacks of the Bennett-Brassard cryptographic protocol on the quantum error and mutual informatio n between honest parties and information with sender for each eavesdropper. Approach: The quantum error and the mutual information were calculated analytically and computed for arbitrary number of cloning attacks. Our objective in this study was to know if the number of the eavesdropper s and their angle of cloning act on the safety of information. Results: It was found that the quantum error and the secure d/no secured transition depend strongly on the number of eavesdropper and their an gle of attacks. The particular cases where all eavesdroppers collaborate were also investigated. Conclusion: Furthermore, the cloning attack's quantum error is lower than the intercept and resen ds attacks one, which means that the cloning attack s is the optimal one for arbitrary number of eavesdro pper.",
      "year": 2010,
      "venue": "",
      "authors": [
        "M. Dehmani",
        "H. Ez-Zahraouy",
        "A. Benyoussef"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/6f83064144b659cb9576fa04d1dc48c89f1b1e99",
      "pdf_url": "https://doi.org/10.3844/jcssp.2010.684.688",
      "publication_date": "2010-07-31",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "a3474a051f454516d9fd2cb7fc1c12cb6edebb85",
      "title": "Efficient Detecting of RFID Tag Cloning Attacks using Chaos Theory",
      "abstract": null,
      "year": 2010,
      "venue": "",
      "authors": [
        "M. Babaie",
        "H. Rahimov"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a3474a051f454516d9fd2cb7fc1c12cb6edebb85",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-22",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "18506b25ef0c811162b1b51b9301a8eaa572c57d",
      "title": "Indices of inflammation in the lung and liver in the early stages of the black walnut extract model of equine laminitis.",
      "abstract": null,
      "year": 2009,
      "venue": "Veterinary Immunology and Immunopathology",
      "authors": [
        "A. Stewart",
        "A. Pettigrew",
        "A. Cochran",
        "J. Belknap"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/18506b25ef0c811162b1b51b9301a8eaa572c57d",
      "pdf_url": "",
      "publication_date": "2009-06-15",
      "keywords_matched": [
        "extract model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "d0e6254805e30303659dbd37b61ab4ab06189359",
      "title": "Enhancing RFID Tag Resistance against Cloning Attack",
      "abstract": null,
      "year": 2009,
      "venue": "2009 Third International Conference on Network and System Security",
      "authors": [
        "J. Abawajy"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/d0e6254805e30303659dbd37b61ab4ab06189359",
      "pdf_url": "",
      "publication_date": "2009-10-19",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "4cf22a03e71fa83089e7e653aa91e83b1ba8e019",
      "title": "Model Extraction for Sockets-based Distributed Programs",
      "abstract": null,
      "year": 2009,
      "venue": "",
      "authors": [
        "Alexander Heu\u00dfner"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4cf22a03e71fa83089e7e653aa91e83b1ba8e019",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "f633337732ed97548a26392a235ed1fd02dd3c14",
      "title": "Model Extraction for ARINC 653 Based Avionics Software",
      "abstract": null,
      "year": 2007,
      "venue": "SPIN",
      "authors": [
        "Pedro de la C\u00e1mara",
        "M. Gallardo",
        "P. Merino"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/f633337732ed97548a26392a235ed1fd02dd3c14",
      "pdf_url": "",
      "publication_date": "2007-07-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "9ec7656ff39d07fe6fd283466aed4bc56d35b2c2",
      "title": "Photon-number-splitting versus cloning attacks in practical implementations of the Bennett-Brassard 1984 protocol for quantum cryptography",
      "abstract": "In practical quantum cryptography, the source sometimes produces multiphoton pulses, thus enabling the eavesdropper Eve to perform the powerful photon-number-splitting (PNS) attack. Recently, it was shown by Curty and Luetkenhaus [Phys. Rev. A 69, 042321 (2004)] that the PNS attack is not always the optimal attack when two photons are present: if errors are present in the correlations Alice-Bob and if Eve cannot modify Bob's detection efficiency, Eve gains a larger amount of information using another attack based on a 2{yields}3 cloning machine. In this work, we extend this analysis to all distances Alice-Bob. We identify a new incoherent 2{yields}3 cloning attack which performs better than those described before. Using it, we confirm that, in the presence of errors, Eve's better strategy uses 2{yields}3 cloning attacks instead of the PNS. However, this improvement is very small for the implementations of the Bennett-Brassard 1984 (BB84) protocol. Thus, the existence of these new attacks is conceptually interesting but basically does not change the value of the security parameters of BB84. The main results are valid both for Poissonian and sub-Poissonian sources.",
      "year": 2004,
      "venue": "",
      "authors": [
        "A. Niederberger",
        "V. Scarani",
        "N. Gisin"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/9ec7656ff39d07fe6fd283466aed4bc56d35b2c2",
      "pdf_url": "https://access.archive-ouverte.unige.ch/access/metadata/c176b0db-379c-4db9-aac3-8a97b2f2de82/download",
      "publication_date": "2004-08-19",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No model stealing terminology in abstract",
      "filter_confidence": "high",
      "filter_stage": "topic"
    },
    {
      "paper_id": "59eff9aba3d091222b83d48eae60c81d1606346a",
      "title": "Vascular steal model of human temporal lobe epileptogenicity: the relationship between electrocorticographic interhemispheric propagation time and cerebral blood flow.",
      "abstract": null,
      "year": 2000,
      "venue": "Medical Hypotheses",
      "authors": [
        "M. Weinand"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/59eff9aba3d091222b83d48eae60c81d1606346a",
      "pdf_url": "",
      "publication_date": "2000-05-01",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "7e636ba98486dfa19ab8ebc986a8f0c4f03be36a",
      "title": "Muscle contraction-induced steal model in the anesthetized cat.",
      "abstract": null,
      "year": 1998,
      "venue": "Journal of pharmacological and toxicological methods",
      "authors": [
        "S. Poucher"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7e636ba98486dfa19ab8ebc986a8f0c4f03be36a",
      "pdf_url": "",
      "publication_date": "1998-11-01",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "626eb188fa4e5530ea06cd0f140e6752877b7da4",
      "title": "An accurate neural network model of FET for intermodulation and power analysis",
      "abstract": null,
      "year": 1996,
      "venue": "European Microwave Conference",
      "authors": [
        "J. Rousset",
        "Youssef Harkouss",
        "Juan-Mari Collantes",
        "M. Campovecchio"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/626eb188fa4e5530ea06cd0f140e6752877b7da4",
      "pdf_url": "",
      "publication_date": "1996-10-01",
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "479779a8de7147f09ffece0c0894e7b2858f0c42",
      "title": "The Impact of Message Length and Medium on Imitation Attack Creation and Detection by Humans",
      "abstract": null,
      "year": null,
      "venue": "",
      "authors": [
        "Sarah Lazbin"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/479779a8de7147f09ffece0c0894e7b2858f0c42",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    },
    {
      "paper_id": "5d74cc67a112ed43d0c3b13c5e40c73d9572bc55",
      "title": "Stealing a Defended Model without Data",
      "abstract": null,
      "year": null,
      "venue": "",
      "authors": [
        "Brett Reynolds",
        "James Beetham",
        "Dr. Mubarak Shah"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5d74cc67a112ed43d0c3b13c5e40c73d9572bc55",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05",
      "filter_reason": "No abstract available for verification",
      "filter_confidence": "low",
      "filter_stage": "relevance"
    }
  ]
}